<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Java中的魔法类-Unsafe]]></title>
    <url>%2Fposts%2F83c72f86.html</url>
    <content type="text"><![CDATA[Unsafe是位于sun.misc包下的一个类，主要提供一些用于执行低级别、不安全操作的方法，如直接访问系统内存资源、自主管理内存资源等，这些方法在提升Java运行效率、增强Java语言底层资源操作能力方面起到了很大的作用。 但是，这个类的作者不希望我们使用它，因为我们虽然我们获取到了对底层的控制权，但是也增大了风险，安全性正是Java相对于C++/C的优势。因为该类在sun.misc包下，默认是被BootstrapClassLoader加载的。如果我们在程序中去调用这个类的话，我们使用的类加载器肯定是 AppClassLoader,问题是在Unsafe中是这样写的： 1234567891011121314private static final Unsafe theUnsafe;private Unsafe() &#123;&#125;@CallerSensitivepublic static Unsafe getUnsafe() &#123; Class var0 = Reflection.getCallerClass(); if (!VM.isSystemDomainLoader(var0.getClassLoader())) &#123; throw new SecurityException("Unsafe"); &#125; else &#123; return theUnsafe; &#125;&#125; 将构造函数私有，然后提供了一个静态方法去获取当前类实例。在getUnsafe()方法中首先判断当前类加载器是否为空，因为使用 BootstrapClassLoader 本身就是空，它是用c++实现的，这样就限制了我们在自己的代码中使用这个类。 但是同时作者也算是给我们提供了一个后门，因为Java有反射机制。调用的思路就是将theUnsafe对象设置为可见。 1234Field theUnsafeField = Unsafe.class.getDeclaredField("theUnsafe");theUnsafeField.setAccessible(true);Unsafe unsafe = (Unsafe) theUnsafeField.get(null);System.out.println(unsafe); unsafe类功能介绍： 内存操作 这部分主要包含堆外内存的分配、拷贝、释放、给定地址值操作等方法。 123456789101112131415161718//分配内存, 相当于C++的malloc函数public native long allocateMemory(long bytes);//扩充内存public native long reallocateMemory(long address, long bytes);//释放内存public native void freeMemory(long address);//在给定的内存块中设置值public native void setMemory(Object o, long offset, long bytes, byte value);//内存拷贝public native void copyMemory(Object srcBase, long srcOffset, Object destBase, long destOffset, long bytes);//获取给定地址值，忽略修饰限定符的访问限制。与此类似操作还有: getInt，getDouble，getLong，getChar等public native Object getObject(Object o, long offset);//为给定地址设置值，忽略修饰限定符的访问限制，与此类似操作还有: putInt,putDouble，putLong，putChar等public native void putObject(Object o, long offset, Object x);//获取给定地址的byte类型的值（当且仅当该内存地址为allocateMemory分配时，此方法结果为确定的）public native byte getByte(long address);//为给定地址设置byte类型的值（当且仅当该内存地址为allocateMemory分配时，此方法结果才是确定的）public native void putByte(long address, byte x); 通常，我们在Java中创建的对象都处于堆内内存（heap）中，堆内内存是由JVM所管控的Java进程内存，并且它们遵循JVM的内存管理机制，JVM会采用垃圾回收机制统一管理堆内存。与之相对的是堆外内存，存在于JVM管控之外的内存区域，Java中对堆外内存的操作，依赖于Unsafe提供的操作堆外内存的native方法。 使用堆外内存的原因 对垃圾回收停顿的改善。由于堆外内存是直接受操作系统管理而不是JVM，所以当我们使用堆外内存时，即可保持较小的堆内内存规模。从而在GC时减少回收停顿对于应用的影响。 提升程序I/O操作的性能。通常在I/O通信过程中，会存在堆内内存到堆外内存的数据拷贝操作，对于需要频繁进行内存间数据拷贝且生命周期较短的暂存数据，都建议存储到堆外内存。 典型应用 DirectByteBuffer是Java用于实现堆外内存的一个重要类，通常用在通信过程中做缓冲池，如在Netty、MINA等NIO框架中应用广泛。DirectByteBuffer对于堆外内存的创建、使用、销毁等逻辑均由Unsafe提供的堆外内存API来实现。 下面的代码为DirectByteBuffer构造函数，创建DirectByteBuffer的时候，通过Unsafe.allocateMemory分配内存、Unsafe.setMemory进行内存初始化，而后构建Cleaner对象用于跟踪DirectByteBuffer对象的垃圾回收，以实现当DirectByteBuffer被垃圾回收时，分配的堆外内存一起被释放。 12345678910111213141516171819202122232425262728293031DirectByteBuffer(int cap) &#123; // package-private super(-1, 0, cap, cap); boolean pa = VM.isDirectMemoryPageAligned(); int ps = Bits.pageSize(); long size = Math.max(1L, (long)cap + (pa ? ps : 0)); Bits.reserveMemory(size, cap); long base = 0; try &#123; //分配内存，返回基地址 base = unsafe.allocateMemory(size); &#125; catch (OutOfMemoryError x) &#123; Bits.unreserveMemory(size, cap); throw x; &#125; //内存初始化 unsafe.setMemory(base, size, (byte) 0); if (pa &amp;&amp; (base % ps != 0)) &#123; // Round up to page boundary address = base + ps - (base &amp; (ps - 1)); &#125; else &#123; address = base; &#125; //跟踪directbytebuffer 对象的垃圾回收，实现堆外内存的释放 cleaner = Cleaner.create(this, new Deallocator(base, size, cap)); att = null;&#125; 上面最后一句代码通过Cleaner.create()来进行对象监控，释放堆外内存。这里是如何做到的呢？跟踪一下Cleaner类： 123456public class Cleaner extends PhantomReference&lt;Object&gt; &#123; public static Cleaner create(Object var0, Runnable var1) &#123; return var1 == null ? null : add(new Cleaner(var0, var1)); &#125;&#125; 可以看到继承了PhantomReference，Java中的4大引用类型我们都知道。PhantomReference的作用于其他的Refenrence作用大有不同。像 SoftReference、WeakReference都是为了保证引用的类对象能在不用的时候及时的被回收，但是 PhantomReference 并不会决定对象的生命周期。如果一个对象仅持有虚引用，那么它就和没有任何引用一样，对象不可达时就会被垃圾回收器回收，但是任何时候都无法通过虚引用获得对象。虚引用主要用来跟踪对象被垃圾回收器回收的活动。 那他的作用到底是啥呢？准确来说 PhantomReference 给使用者提供了一种机制-来监控对象的垃圾回收的活动。 可能这样说不是太明白，我来举个例子： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546package com.rickiyang.learn.javaagent;import java.lang.ref.PhantomReference;import java.lang.ref.Reference;import java.lang.ref.ReferenceQueue;import java.lang.reflect.Field;/** * @author rickiyang * @date 2019-08-08 * @Desc */public class TestPhantomReference &#123; public static boolean isRun = true; public static void main(String[] args) throws Exception &#123; String str = new String("123"); System.out.println(str.getClass() + "@" + str.hashCode()); final ReferenceQueue&lt;String&gt; referenceQueue = new ReferenceQueue&lt;&gt;(); new Thread(() -&gt; &#123; while (isRun) &#123; Object obj = referenceQueue.poll(); if (obj != null) &#123; try &#123; Field rereferent = Reference.class.getDeclaredField("referent"); rereferent.setAccessible(true); Object result = rereferent.get(obj); System.out.println("gc will collect：" + result.getClass() + "@" + result.hashCode() + "\t" + result); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;).start(); PhantomReference&lt;String&gt; weakRef = new PhantomReference&lt;&gt;(str, referenceQueue); str = null; Thread.currentThread().sleep(2000); System.gc(); Thread.currentThread().sleep(2000); isRun = false; &#125;&#125; 上面这段代码的含义是new PhantomReference()，因为PhantomReference必须的维护一个ReferenceQueue用来保存当前被虚引用的对象。上例中手动去调用referenceQueue.poll()方法，这里你需要注意的是并不是我们主动去释放queue中的对象，你跟踪进去 poll() 方法可以看到有一个全局锁对象，只有当当前对象失去了引用之后才会释放锁，poll()方法才能执行。在执行poll()方法释放对象的时候我们可以针对这个对象做一些监控。这就是 PhantomReference 的意义所在。 说回到 Cleaner， 通过看源码，create()方法调用了add()方法，在Cleaner类里面维护了一个双向链表，将每一个add进来的Cleaner对象都添加到这个链表中维护。那么在Cleaner 链表中的对象实在何时被释放掉呢？ 注意到 Cleaner中有一个clean()方法： 12345678910111213141516171819public void clean() &#123; if (remove(this)) &#123; try &#123; this.thunk.run(); &#125; catch (final Throwable var2) &#123; AccessController.doPrivileged(new PrivilegedAction&lt;Void&gt;() &#123; public Void run() &#123; if (System.err != null) &#123; (new Error("Cleaner terminated abnormally", var2)).printStackTrace(); &#125; System.exit(1); return null; &#125; &#125;); &#125; &#125;&#125; remove()方法是将该对象从内部维护的双向链表中清除。下面紧跟着是thunk.run() ，thunk = 我们通过create()方法传进来的参数，在``DirectByteBuffer中那就是：Cleaner.create(this, new Deallocator(base, size, cap))`,Deallocator类也是一个线程： 123456789101112131415161718private static class Deallocator implements Runnable &#123; private static Unsafe unsafe = Unsafe.getUnsafe(); //省略无关 代码 public void run() &#123; if (address == 0) &#123; // Paranoia return; &#125; unsafe.freeMemory(address); address = 0; Bits.unreserveMemory(size, capacity); &#125; &#125; 看到在run方法中调用了freeMemory()去释放掉对象。 在 Reference类中调用了该方法，Reference 类中的静态代码块 有个一内部类：ReferenceHandler,它继承了 Thread，在run方法中调用了 tryHandlePending()，并且被设置为守护线程，意味着会循环不断的处理pending链表中的对象引用。 这里要注意的点是： Cleaner本身不带有清理逻辑，所有的逻辑都封装在thunk中，因此thunk是怎么实现的才是最关键的。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364static &#123; ThreadGroup tg = Thread.currentThread().getThreadGroup(); for (ThreadGroup tgn = tg; tgn != null; tg = tgn, tgn = tg.getParent()); Thread handler = new ReferenceHandler(tg, "Reference Handler"); /* If there were a special system-only priority greater than * MAX_PRIORITY, it would be used here */ handler.setPriority(Thread.MAX_PRIORITY); handler.setDaemon(true); handler.start(); // provide access in SharedSecrets SharedSecrets.setJavaLangRefAccess(new JavaLangRefAccess() &#123; @Override public boolean tryHandlePendingReference() &#123; return tryHandlePending(false); &#125; &#125;);&#125;static boolean tryHandlePending(boolean waitForNotify) &#123; Reference&lt;Object&gt; r; Cleaner c; try &#123; synchronized (lock) &#123; if (pending != null) &#123; r = pending; //如果当前Reference对象是Cleaner类型的就进行特殊处理 c = r instanceof Cleaner ? (Cleaner) r : null; // unlink 'r' from 'pending' chain pending = r.discovered; r.discovered = null; &#125; else &#123; // The waiting on the lock may cause an OutOfMemoryError // because it may try to allocate exception objects. if (waitForNotify) &#123; lock.wait(); &#125; // retry if waited return waitForNotify; &#125; &#125; &#125; catch (OutOfMemoryError x) &#123; Thread.yield(); // retry return true; &#125; catch (InterruptedException x) &#123; // retry return true; &#125; // clean 不为空的时候，走清理的逻辑 if (c != null) &#123; c.clean(); return true; &#125; ReferenceQueue&lt;? super Object&gt; q = r.queue; if (q != ReferenceQueue.NULL) q.enqueue(r); return true;&#125; tryHandlePending这段代码的意思是： 如果一个对象经过JVM检测他已经没有强引用了，但是还有 弱引用 或者 软引用 或者 虚引用的情况下，那么就会把此对象放到一个名为pending的链表里，这个链表是通过Reference.discovered域连接在一起的。 ReferenceHandler这个线程会一直从链表中取出被pending的对象，它可能是WeakReference，也可能是SoftReference，当然也可能是PhantomReference和Cleaner。如果是Cleaner，那就直接调用Cleaner的clean方法，然后就结束了。其他的情况下，要交给这个对象所关联的queue，以便于后续的处理。 关于堆外内存分配和回收的代码我们就先分析到这里。需要注意的是对外内存回收的时机也是不确定的，所以不要持续分配一些大对象到堆外，如果没有被回收掉，这是一件很可怕的事情。毕竟它无法被JVM检测到。 内存屏障 硬件层的内存屏障分为两种：Load Barrier 和 Store Barrier即读屏障和写屏障。内存屏障有两个作用：阻止屏障两侧的指令重排序；强制把写缓冲区/高速缓存中的脏数据等写回主内存，让缓存中相应的数据失效。在Unsafe中提供了三个方法来操作内存屏障： 123456//读屏障，禁止load操作重排序。屏障前的load操作不能被重排序到屏障后，屏障后的load操作不能被重排序到屏障前public native void loadFence();//写屏障，禁止store操作重排序。屏障前的store操作不能被重排序到屏障后，屏障后的store操作不能被重排序到屏障前public native void storeFence();//全能屏障，禁止load、store操作重排序public native void fullFence(); 先简单了解两个指令： Store：将处理器缓存的数据刷新到内存中。 Load：将内存存储的数据拷贝到处理器的缓存中。 JVM平台提供了一下几种内存屏障： 屏障类型 指令示例 说明 LoadLoad Barriers Load1;LoadLoad;Load2 该屏障确保Load1数据的装载先于Load2及其后所有装载指令的的操作 StoreStore Barriers Store1;StoreStore;Store2 该屏障确保Store1立刻刷新数据到内存(使其对其他处理器可见)该操作先于Store2及其后所有存储指令的操作 LoadStore Barriers Load1;LoadStore;Store2 确保Load1的数据装载先于Store2及其后所有的存储指令刷新数据到内存的操作 StoreLoad Barriers Store1;StoreLoad;Load2 该屏障确保Store1立刻刷新数据到内存的操作先于Load2及其后所有装载装载指令的操作。它会使该屏障之前的所有内存访问指令(存储指令和访问指令)完成之后,才执行该屏障之后的内存访问指令 StoreLoad Barriers同时具备其他三个屏障的效果，因此也称之为全能屏障（mfence），是目前大多数处理器所支持的；但是相对其他屏障，该屏障的开销相对昂贵。 loadFence 实现了LoadLoad Barriers，该操作禁止了指令的重排序。 storeFence 实现了 StoreStore Barriers，确保屏障前的写操作能够立刻刷入到主内存，并且确保屏障前的写操作一定先于屏障后的写操作。即保证了内存可见性和禁止指令重排序。 fullFence 实现了 StoreLoad Barriers，强制所有在mfence指令之前的store/load指令，都在该mfence指令执行之前被执行；所有在mfence指令之后的store/load指令，都在该mfence指令执行之后被执行。 在 JDK 中调用了 内存屏障这几个方法的实现类有 StampedLock。关于StampedLock的实现我们后面会专门抽出一篇去讲解。它并没有去实现AQS队列。而是采用了 其他方式实现。 系统相关 这部分包含两个获取系统相关信息的方法。 1234//返回系统指针的大小。返回值为4（32位系统）或 8（64位系统）。public native int addressSize(); //内存页的大小，此值为2的幂次方。public native int pageSize(); 在 java.nio下的Bits类中调用了pagesize()方法计算系统中页大小： 1234567private static int pageSize = -1;static int pageSize() &#123; if (pageSize == -1) pageSize = unsafe().pageSize(); return pageSize;&#125; 线程调度 线程调度中提供的方法包括：线程的挂起，恢复 和 对象锁机制等，其中获取对象的监视器锁方法已经被标记为弃用。 12345678910111213// 终止挂起的线程，恢复正常.java.util.concurrent包中挂起操作都是在LockSupport类实现的，其底层正是使用这两个方法public native void unpark(Object thread);// 线程调用该方法，线程将一直阻塞直到超时，或者是中断条件出现。public native void park(boolean isAbsolute, long time);//获得对象锁（可重入锁）@Deprecatedpublic native void monitorEnter(Object o);//释放对象锁@Deprecatedpublic native void monitorExit(Object o);//尝试获取对象锁@Deprecatedpublic native boolean tryMonitorEnter(Object o); 将一个线程进行挂起是通过 park 方法实现的，调用park()后，线程将一直 阻塞 直到 超时 或者 中断 等条件出现。unpark可以释放一个被挂起的线程，使其恢复正常。整个并发框架中对线程的挂起操作被封装在LockSupport类中，LockSupport 类中有各种版本 pack 方法，但最终都调用了Unsafe.park()方法。 我们来看一个例子： 123456789101112131415161718192021222324252627282930313233343536373839404142434445package leetcode;import sun.misc.Unsafe;import java.lang.reflect.Field;import java.util.concurrent.TimeUnit;/** * @author: rickiyang * @date: 2019/8/10 * @description: */public class TestUsafe &#123; private static Thread mainThread; public Unsafe getUnsafe() throws Exception &#123; Field theUnsafeField = Unsafe.class.getDeclaredField("theUnsafe"); theUnsafeField.setAccessible(true); return (Unsafe) theUnsafeField.get(null); &#125; public void testPark() throws Exception &#123; Unsafe unsafe = getUnsafe(); mainThread = Thread.currentThread(); System.out.println(String.format("park %s", mainThread.getName())); unsafe.park(false, TimeUnit.SECONDS.toNanos(3)); new Thread(() -&gt; &#123; System.out.println(String.format("%s unpark %s", Thread.currentThread().getName(), mainThread.getName())); unsafe.unpark(mainThread); &#125;).start(); System.out.println("main thread is done"); &#125; public static void main(String[] args) throws Exception &#123; TestUsafe testUsafe = new TestUsafe(); testUsafe.testPark(); &#125;&#125; 运行上面的例子，那你会发现在第29行 park方法设置了超时时间为3秒后，会阻塞当前主线程，直到超时时间到达，下面的代码才会继续执行。 对象操作 Unsafe类中提供了多个方法来进行 对象实例化 和 获取对象的偏移地址 的操作： 1234567891011121314151617181920212223242526272829// 传入一个Class对象并创建该实例对象，但不会调用构造方法public native Object allocateInstance(Class&lt;?&gt; cls) throws InstantiationException;// 获取字段f在实例对象中的偏移量public native long objectFieldOffset(Field f);// 返回值就是f.getDeclaringClass()public native Object staticFieldBase(Field f);// 静态属性的偏移量，用于在对应的Class对象中读写静态属性public native long staticFieldOffset(Field f);// 获得给定对象偏移量上的int值，所谓的偏移量可以简单理解为指针指向该变量；的内存地址，// 通过偏移量便可得到该对象的变量，进行各种操作public native int getInt(Object o, long offset);// 设置给定对象上偏移量的int值public native void putInt(Object o, long offset, int x);// 获得给定对象偏移量上的引用类型的值public native Object getObject(Object o, long offset);// 设置给定对象偏移量上的引用类型的值public native void putObject(Object o, long offset, Object x););// 设置给定对象的int值，使用volatile语义，即设置后立马更新到内存对其他线程可见public native void putIntVolatile(Object o, long offset, int x);// 获得给定对象的指定偏移量offset的int值，使用volatile语义，总能获取到最新的int值。public native int getIntVolatile(Object o, long offset);// 与putIntVolatile一样，但要求被操作字段必须有volatile修饰public native void putOrderedInt(Object o, long offset, int x); allocateInstance方法在这几个场景下很有用：跳过对象的实例化阶段（通过构造函数）、忽略构造函数的安全检查（反射newInstance()时）、你需要某类的实例但该类没有public的构造函数。 举个例子： 12345678910111213141516171819202122232425262728293031public class User &#123; private String name; private int age; private static String address = "beijing"; public User()&#123; name = "xiaoming"; &#125; public String getname()&#123; return name; &#125;&#125; /** * 实例化对象 * @throws Exception */public void newInstance() throws Exception&#123; TestUsafe testUsafe = new TestUsafe(); Unsafe unsafe = testUsafe.getUnsafe(); User user = new User(); System.out.println(user.getname()); User user1 = User.class.newInstance(); System.out.println(user1.getname()); User o = (User)unsafe.allocateInstance(User.class); System.out.println(o.getname());&#125; 打印的结果可以看到最后输出的是null，说明构造函数未被加载。可以进一步实验，将User类中的构造函数设置为 private，你会发现在前面两种实例化方式检查期就报错。但是第三种是可以用的。这是因为allocateInstance只是给对象分配了内存，它并不会初始化对象中的属性。 下面是对象操作的使用示例： 123456789101112131415161718192021222324252627282930313233public void testObject() throws Exception&#123; TestUsafe testUsafe = new TestUsafe(); Unsafe unsafe = testUsafe.getUnsafe(); //通过allocateInstance创建对象,为其分配内存地址，不会加载构造函数 User user = (User) unsafe.allocateInstance(User.class); System.out.println(user); // Class &amp;&amp; Field Class&lt;? extends User&gt; userClass = user.getClass(); Field name = userClass.getDeclaredField("name"); Field age = userClass.getDeclaredField("age"); Field location = userClass.getDeclaredField("address"); // 获取实例域name和age在对象内存中的偏移量并设置值 System.out.println(unsafe.objectFieldOffset(name)); unsafe.putObject(user, unsafe.objectFieldOffset(name), "xiaoming"); System.out.println(unsafe.objectFieldOffset(age)); unsafe.putInt(user, unsafe.objectFieldOffset(age), 18); System.out.println(user); // 获取定义location字段的类 Object staticFieldBase = unsafe.staticFieldBase(location); System.out.println(staticFieldBase); // 获取static变量address的偏移量 long staticFieldOffset = unsafe.staticFieldOffset(location); // 获取static变量address的值 System.out.println(unsafe.getObject(staticFieldBase, staticFieldOffset)); // 设置static变量address的值 unsafe.putObject(staticFieldBase, staticFieldOffset, "tianjin"); System.out.println(user + " " + user.getAddress());&#125; 对象实例布局与内存大小 一个Java对象占用多大的内存空间呢？这个问题很值得读者朋友去查一下。 因为这个输出本篇的重点所以简单说一下。一个 Java 对象在内存中由对象头、示例数据和对齐填充构成。对象头存储了对象运行时的基本数据，如 hashCode、锁状态、GC 分代年龄、类型指针等等。实例数据是对象中的非静态字段值，可能是一个原始类型的值，也可能是一个指向其他对象的指针。对齐填充就是 padding，保证对象都采用 8 字节对齐。除此以外，在 64 位虚拟机中还可能会开启指针压缩，将 8 字节的指针压缩为 4 字节，这里就不再过多介绍了。 也就是说一个 Java 对象在内存中，首先是对象头，然后是各个类中字段的排列，这之间可能会有 padding 填充。这样我们大概就能理解字段偏移量的含义了，它实际就是每个字段在内存中所处的位置。 12345678910111213141516public class User &#123; private String name; private int age;&#125;TestUsafe testUsafe = new TestUsafe();Unsafe unsafe = testUsafe.getUnsafe();for (Field field : User.class.getDeclaredFields()) &#123; System.out.println(field.getName() + "-" + field.getType() + ": " + unsafe.objectFieldOffset(field));&#125;结果：name-class java.lang.String: 16age-int: 12 从上面的运行结果中可以： age：偏移值为12，即前面 12 个字节的对象头； name：name从16字节开始，因为int 类型的age占了4个字节。 继续算下去整个对象占用的空间，对象头12，age 4，name 是指针类型，开启指针压缩占用4个字节，那么User对象整个占用20字节，因为上面说的padding填充，必须8字节对齐，那么实际上会补上4个字节的填充，即一共占用了24个字节。 按照这种计算方式，我们可以字节写一个计算size的工具类： 123456789101112131415161718192021222324252627public static long sizeOf(Object o) throws Exception&#123; TestUsafe testUsafe = new TestUsafe(); Unsafe unsafe = testUsafe.getUnsafe(); HashSet&lt;Field&gt; fields = new HashSet&lt;Field&gt;(); Class c = o.getClass(); while (c != Object.class) &#123; for (Field f : c.getDeclaredFields()) &#123; if ((f.getModifiers() &amp; Modifier.STATIC) == 0) &#123; fields.add(f); &#125; &#125; //如果有继承父类的话，父类中的属性也是要计算的 c = c.getSuperclass(); &#125; //计算每个字段的偏移量，因为第一个字段的偏移量即在对象头的基础上偏移的 //所以只需要比较当前偏移量最大的字段即表示这是该对象最后一个字段的位置 long maxSize = 0; for (Field f : fields) &#123; long offset = unsafe.objectFieldOffset(f); if (offset &gt; maxSize) &#123; maxSize = offset; &#125; &#125; //上面计算的是对象最后一个字段的偏移量起始位置，java中对象最大长度是8个字节(long) //这里的计算方式是 将 当前偏移量 / 8 + 8字节 的padding return ((maxSize/8) + 1) * 8;&#125; 上面的工具类计算的结果也是24。 class相关操作 1234567891011121314//静态属性的偏移量，用于在对应的Class对象中读写静态属性public native long staticFieldOffset(Field f);//获取一个静态字段的对象指针public native Object staticFieldBase(Field f);//判断是否需要初始化一个类，通常在获取一个类的静态属性的时候（因为一个类如果没初始化，它的静态属性也不会初始化）使用。 当且仅当ensureClassInitialized方法不生效时返回falsepublic native boolean shouldBeInitialized(Class&lt;?&gt; c);//确保类被初始化public native void ensureClassInitialized(Class&lt;?&gt; c);//定义一个类，可用于动态创建类，此方法会跳过JVM的所有安全检查，默认情况下，ClassLoader（类加载器）和ProtectionDomain（保护域）实例来源于调用者public native Class&lt;?&gt; defineClass(String name, byte[] b, int off, int len, ClassLoader loader, ProtectionDomain protectionDomain);//定义一个匿名类，可用于动态创建类public native Class&lt;?&gt; defineAnonymousClass(Class&lt;?&gt; hostClass, byte[] data, Object[] cpPatches); 数组 操作 数组操作主要有两个方法： 1234//返回数组中第一个元素的偏移地址public native int arrayBaseOffset(Class&lt;?&gt; arrayClass);//返回数组中一个元素占用的大小public native int arrayIndexScale(Class&lt;?&gt; arrayClass); CAS操作 相信所有的开发者对这个词都不陌生，在AQS类中使用了无锁的方式来进行并发控制，主要就是CAS的功劳。 CAS的全称是Compare And Swap 即比较交换，其算法核心思想如下 执行函数：CAS(V,E,N) 包含3个参数 V表示要更新的变量 E表示预期值 N表示新值 如果V值等于E值，则将V的值设为N。若V值和E值不同，则说明已经有其他线程做了更新，则当前线程什么都不做。通俗的理解就是CAS操作需要我们提供一个期望值，当期望值与当前线程的变量值相同时，说明没有别的线程修改该值，当前线程可以进行修改，也就是执行CAS操作，但如果期望值与当前线程不符，则说明该值已被其他线程修改，此时不执行更新操作，但可以选择重新读取该变量再尝试再次修改该变量，也可以放弃操作。 Unsafe类中提供了三个方法来进行CAS操作： 12345public final native boolean compareAndSwapObject(Object o, long offset, Object expected, Object update);public final native boolean compareAndSwapInt(Object o, long offset, int expected,int update); public final native boolean compareAndSwapLong(Object o, long offset, long expected, long update); 另外，在 JDK1.8中新增了几个 CAS 的方法，他们的实现是基于上面三个方法做的一层封装： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748 //1.8新增，给定对象o，根据获取内存偏移量指向的字段，将其增加delta， //这是一个CAS操作过程，直到设置成功方能退出循环，返回旧值 public final int getAndAddInt(Object o, long offset, int delta) &#123; int v; do &#123; //获取内存中最新值 v = getIntVolatile(o, offset); //通过CAS操作 &#125; while (!compareAndSwapInt(o, offset, v, v + delta)); return v; &#125;//1.8新增，方法作用同上，只不过这里操作的long类型数据 public final long getAndAddLong(Object o, long offset, long delta) &#123; long v; do &#123; v = getLongVolatile(o, offset); &#125; while (!compareAndSwapLong(o, offset, v, v + delta)); return v; &#125; //1.8新增，给定对象o，根据获取内存偏移量对于字段，将其 设置为新值newValue， //这是一个CAS操作过程，直到设置成功方能退出循环，返回旧值 public final int getAndSetInt(Object o, long offset, int newValue) &#123; int v; do &#123; v = getIntVolatile(o, offset); &#125; while (!compareAndSwapInt(o, offset, v, newValue)); return v; &#125;// 1.8新增，同上，操作的是long类型 public final long getAndSetLong(Object o, long offset, long newValue) &#123; long v; do &#123; v = getLongVolatile(o, offset); &#125; while (!compareAndSwapLong(o, offset, v, newValue)); return v; &#125; //1.8新增，同上，操作的是引用类型数据 public final Object getAndSetObject(Object o, long offset, Object newValue) &#123; Object v; do &#123; v = getObjectVolatile(o, offset); &#125; while (!compareAndSwapObject(o, offset, v, newValue)); return v; &#125; CAS在java.util.concurrent.atomic相关类、Java AQS、CurrentHashMap等实现上有非常广泛的应用。]]></content>
      <categories>
        <category>不为人知的Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Gin框架使用--常用api使用介绍(19)]]></title>
    <url>%2Fposts%2Fbe93bdd3.html</url>
    <content type="text"><![CDATA[Go web框架目前有很多，Beego，Gin，Iris，Revel等等。目前国内使用量比较多的是前两个，鉴于Beego的源码一直被人吐槽并且Beego使用起来太过于臃肿，框架本身构造的大而全，很多功能不一定会是你想要的这些原因，我们着重关注Gin框架的使用。Gin没有像Beego那样什么都做，它只专注于web请求的封装，如果你想做缓存，想连接数据库等等还需要使用别的框架或者使用原生的API。 Gin的github地址如下： Gin点我 安装Gin： 1go get github.com/gin-gonic/gin 1. 构建一个http请求 服务端构建一个http请求： 12345678910111213141516171819202122package mainimport ( "github.com/gin-gonic/gin" "net/http")func HelloWordGet (c *gin.Context) &#123; c.String(http.StatusOK, "hello world get")&#125;func HelloWordPost (c *gin.Context) &#123; c.String(http.StatusOK, "hello world post")&#125;func main()&#123; // 注册一个默认的路由器 router := gin.Default() // 最基本的用法 router.GET("/HelloWordGet", HelloWordGet) router.POST("/HelloWordPost", HelloWordPost) // 绑定端口是8080 router.Run(":8080")&#125; 启动main函数，你可以在浏览器中调用链接，当然也可以使用http包去发送请求： 12345678910111213141516171819package mainimport ( "fmt" "io/ioutil" "net/http")func main() &#123; resp, _ := http.Get("http://localhost:8080/HelloWordGet") bytes, _ := ioutil.ReadAll(resp.Body) fmt.Printf("get request: %s \n",string(bytes)) response, _ := http.Post("http://localhost:8080/HelloWordPost", "application/x-www-form-urlencode", nil) bytes1, _ := ioutil.ReadAll(response.Body) fmt.Printf("get request: %s \n",string(bytes1))&#125; 2. 带参数的路由 如果想模仿restful api 的方式在 路由上面带参数应该如何实现呢。Gin提供了两种匹配方式： 一种是在路由上使用:paramName; 一种是在路由上使用:paramName/*action，paramName后面如果跟有任何路由，只要前缀一样都会到该route上。 123456789101112131415161718192021222324package mainimport ( "github.com/gin-gonic/gin" "net/http")func HelloWordGet (c *gin.Context) &#123; name := c.Param("name") c.String(http.StatusOK, "hello world get %s",name)&#125;func HelloWordGet1 (c *gin.Context) &#123; name := c.Param("name") c.String(http.StatusOK, "hello world get %s",name)&#125;func main()&#123; router := gin.Default() router.GET("/hello/get/:name", HelloWordGet) router.GET("/hello/get/:name/*action", HelloWordGet1) router.Run(":8080")&#125; Param()方法用于接收来自于路由上的参数。 测试： 1234567891011121314package mainimport ( "fmt" "io/ioutil" "net/http" "strings")func main() &#123; resp, _ := http.Get("http://localhost:8080/hello/get/xiaoming") bytes, _ := ioutil.ReadAll(resp.Body) fmt.Printf("get request: %s \n",string(bytes))&#125; 3. Get，Post 请求方式传参 &amp; 使用自定义表单方式接收参数 12345678910111213141516171819202122232425262728293031323334353637package mainimport ( "github.com/gin-gonic/gin" "net/http")type User struct &#123; Name string Age int sex byte&#125;func HelloWordGet1 (c *gin.Context) &#123; var user User c.Bind(&amp;user) c.String(http.StatusOK, "hello world get %s,%s",user.Name,user.Age)&#125;func HelloWordGet (c *gin.Context) &#123; name := c.DefaultQuery("name","NULL") age := c.Query("age") c.String(http.StatusOK, "hello world get %s,%s",name,age)&#125;func HelloWordPost (c *gin.Context) &#123; name := c.PostForm("name") sex := c.DefaultPostForm("sex", "1") c.JSON(http.StatusOK,gin.H&#123;"status":0,"data":name+":"+sex,"message":"success"&#125;)&#125;func main()&#123; router := gin.Default() router.GET("/hello/get/", HelloWordGet) router.GET("/hello/get1/", HelloWordGe1t) router.POST("/hello/post", HelloWordPost) router.Run(":8080")&#125; Get，Post请求方式接收参数使用的方法为Query或者DefaultQuery()，区别就是一个可以设置默认值一个不可以。 注意到在Post方法中使用了JSON()方法来返回json格式的对象。 测试： 123456789101112131415161718192021package mainimport ( "fmt" "io/ioutil" "net/http" "strings")func main() &#123; resp, _ := http.Get("http://localhost:8080/hello/get?name=xiohong&amp;age=1") bytes, _ := ioutil.ReadAll(resp.Body) fmt.Printf("get request: %s \n",string(bytes)) body := "&#123;\"name\":\"xiaoming\",\"age\":12,\"sex\":1&#125;" response, _ := http.Post("http://localhost:8080/hello/post", "application/x-www-form-urlencode", strings.NewReader(body)) bytes1, _ := ioutil.ReadAll(response.Body) fmt.Printf("get request: %s \n",string(bytes1))&#125; 4.单文件上传 使用c.Request.FormFile(param)方法来获取文件。 1234567891011121314151617181920212223242526272829303132333435363738package mainimport ( "fmt" "github.com/gin-gonic/gin" "io" "log" "net/http" "os")//接收客户端上传的文件然后写入本地func UploadFile(c *gin.Context) &#123; file, header , err := c.Request.FormFile("file") filename := header.Filename fmt.Println(header.Filename) // 创建临时接收文件 out, err := os.Create("copy_"+filename) if err != nil &#123; log.Fatal(err) &#125; defer out.Close() // Copy数据 _, err = io.Copy(out, file) if err != nil &#123; log.Fatal(err) &#125; c.JSON(http.StatusOK,gin.H&#123;"status":0,"data":nil,"message":"success"&#125;)&#125;func main()&#123; router := gin.Default() router.POST("/hello/upload", UploadFile) router.Run(":8080")&#125; 测试： 构造Post请求的时候，文件要使用multipart去承载，可以用它去设置header格式，设置文件名等等。 1234567891011121314151617181920212223242526package mainimport ( bytes3 "bytes" "fmt" "io" "io/ioutil" "mime/multipart" "net/http" "os" "strings")func main() &#123; buf := new(bytes3.Buffer) w := multipart.NewWriter(buf) fw,_ := w.CreateFormFile("file", "1.png") //这里的uploadFile必须和服务器端的FormFile-name一致 fd,_ := os.Open("c:/1.png") defer fd.Close() io.Copy(fw, fd) w.Close() resp1,_ := http.Post("http://localhost:8080/hello/upload", w.FormDataContentType(), buf) bytes2, _ := ioutil.ReadAll(resp1.Body) fmt.Printf("get request: %s \n",string(bytes2))&#125; 5. 多文件上传 1234567891011121314151617181920212223242526272829package mainimport ( "fmt" "github.com/gin-gonic/gin" "io" "log" "net/http" "os")//多文件上传，接收多文件，然后写入本地func UploadMultFile(c *gin.Context) &#123; form, _ := c.MultipartForm() files := form.File["file"] for _,file := range files &#123; c.SaveUploadedFile(file,"c:/copy_"+file.Filename) &#125; c.JSON(http.StatusOK,gin.H&#123;"status":0,"data":nil,"message":"success"&#125;)&#125;func main()&#123; router := gin.Default() router.POST("/hello/multiUpload", UploadMultFile) router.Run(":8080")&#125; 测试： 1234567891011121314151617181920212223242526272829303132package mainimport ( bytes3 "bytes" "fmt" "io" "io/ioutil" "mime/multipart" "net/http" "os")func main() &#123; buf := new(bytes3.Buffer) w := multipart.NewWriter(buf) fw,_ := w.CreateFormFile("file", "1.png") fd,_ := os.Open("c:/1.png") defer fd.Close() io.Copy(fw, fd) fw1,_ := w.CreateFormFile("file", "2.png") fd1,_ := os.Open("c:/2.png") defer fd1.Close() io.Copy(fw1, fd1) w.Close() resp1,_ := http.Post("http://localhost:8080/hello/multiUpload", w.FormDataContentType(), buf) bytes2, _ := ioutil.ReadAll(resp1.Body) fmt.Printf("get request: %s \n",string(bytes2))&#125; 6. 路由分组–Java中Controller上的mapping 上面的示例中我们每构建一个Get或者Post请求，都要将完整的路由路径写入方法参数中。Java的同学可能就会抱怨了，为啥不能像SpringMVC中的Controller一样，定义一个统一的Controller mapping 前缀，然后整个Controller中的路由都会带上该前缀呢。别急，Gin也会考虑到这种使用方式，所以提供了路由分组的功能。 12345678910111213141516func main()&#123; router := gin.Default() helloGroup := router.Group("/hello") helloGroup.GET("/get", HelloWordGet) helloGroup.POST("/post", HelloWordPost) helloGroup.POST("/upload", UploadFile) helloGroup.POST("/multiUpload", UploadMultFile) userGroup := router.Group("/user") userGroup.GET("/get", HelloWordGet) userGroup.POST("/post", HelloWordPost) userGroup.POST("/upload", UploadFile) router.Run(":8080")&#125; Group（param）方法可以上我们设置一个组名，使用该组对象发出的请求自动会带上该组名作为前缀。 7. 提供静态文件服务 可以将你本地的某些目录作为静态文件存储的目录对外提供访问。 1234567891011func main()&#123; router := gin.Default() //当前项目的根目录 router.StaticFS("/all",http.Dir(".")) //指定目录 router.Static("/files","c:/bin") //指定文件 router.StaticFile("specialFile","./a/1.png") router.Run(":8080")&#125; 第一个参数为url，第二个参数是要展示的路径。 8. 加载html模板 Java中我们可以使用thymeleaf或者freemarker来加载html模板，Gin中提供了这个功能，同时也定义了一套自己的模板解析方法。 使用LoadHTMLGlob() 或者 LoadHTMLFiles() ： 12345678910111213func main()&#123; router := gin.Default() router.LoadHTMLGlob("templates/*") //router.LoadHTMLFiles("templates/template1.html", "templates/template2.html") router.GET("/index", func(c *gin.Context) &#123; c.HTML(http.StatusOK, "index.tmpl", gin.H&#123; "title": "我就是个标题", "content": "我想说一句话", &#125;) &#125;) router.Run(":8080")&#125; index.tmpl文件内容： 12345678&lt;html&gt;&lt;body&gt;&lt;h1&gt;&#123;&#123; .title &#125;&#125;&lt;/h1&gt;&lt;p&gt;&#123;&#123; .content &#125;&#125;&lt;/p&gt;&lt;/body&gt;&lt;/html&gt; 自定义渲染分隔符： 默认是用两个大括号隔开，你也可是设置成别的： 123r := gin.Default()r.Delims("&#123;", "&#125;")r.LoadHTMLGlob("/templates") 自定义函数在模板中使用： 1234567891011121314151617181920func formatAsDate(t time.Time) string &#123; year, month, day := t.Date() return fmt.Sprintf("%d-%02d-%02d", year, month, day)&#125;func main() &#123; router := gin.Default() router.Delims("&#123;", "&#125;") router.SetFuncMap(template.FuncMap&#123; "formatAsDate": formatAsDate, &#125;) router.LoadHTMLGlob("templates/*") router.GET("/index", func(c *gin.Context) &#123; c.HTML(http.StatusOK, "index.tmpl", map[string]interface&#123;&#125;&#123; "time": time.Date(2019, 06, 16, 10, 0, 0, 0, time.UTC), &#125;) &#125;) router.Run(":8080")&#125; index.tmpl文件内容： 1234567&lt;html&gt;&lt;body&gt;&lt;p&gt;&#123;.time | formatAsDate&#125;&lt;/p&gt;&lt;/body&gt;&lt;/html&gt; 9.重定向 发布HTTP重定向很容易，支持内部和外部链接: 123router.GET("/redirect", func(c *gin.Context) &#123; c.Redirect(http.StatusMovedPermanently, "/index")&#125;) 这一篇先介绍一些常用的使用，更多的 api 使用我们后面用到的时候再详细的介绍。]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>Go基础学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Caffeine Cache-高性能Java本地缓存组件]]></title>
    <url>%2Fposts%2F356521fa.html</url>
    <content type="text"><![CDATA[前面刚说到Guava Cache，他的优点是封装了get，put操作；提供线程安全的缓存操作；提供过期策略；提供回收策略；缓存监控。当缓存的数据超过最大值时，使用LRU算法替换。这一篇我们将要谈到一个新的本地缓存框架：Caffeine Cache。它也是站在巨人的肩膀上-Guava Cache，借着他的思想优化了算法发展而来。 本篇博文主要介绍Caffine Cache 的使用方式，以及Caffine Cache在SpringBoot中的使用。 1. Caffine Cache 在算法上的优点-W-TinyLFU 说到优化，Caffine Cache到底优化了什么呢？我们刚提到过LRU，常见的缓存淘汰算法还有FIFO，LFU： FIFO：先进先出，在这种淘汰算法中，先进入缓存的会先被淘汰，会导致命中率很低。 LRU：最近最少使用算法，每次访问数据都会将其放在我们的队尾，如果需要淘汰数据，就只需要淘汰队首即可。仍然有个问题，如果有个数据在 1 分钟访问了 1000次，再后 1 分钟没有访问这个数据，但是有其他的数据访问，就导致了我们这个热点数据被淘汰。 LFU：最近最少频率使用，利用额外的空间记录每个数据的使用频率，然后选出频率最低进行淘汰。这样就避免了 LRU 不能处理时间段的问题。 上面三种策略各有利弊，实现的成本也是一个比一个高，同时命中率也是一个比一个好。Guava Cache虽然有这么多的功能，但是本质上还是对LRU的封装，如果有更优良的算法，并且也能提供这么多功能，相比之下就相形见绌了。 LFU的局限性：在 LFU 中只要数据访问模式的概率分布随时间保持不变时，其命中率就能变得非常高。比如有部新剧出来了，我们使用 LFU 给他缓存下来，这部新剧在这几天大概访问了几亿次，这个访问频率也在我们的 LFU 中记录了几亿次。但是新剧总会过气的，比如一个月之后这个新剧的前几集其实已经过气了，但是他的访问量的确是太高了，其他的电视剧根本无法淘汰这个新剧，所以在这种模式下是有局限性。 LRU的优点和局限性：LRU可以很好的应对突发流量的情况，因为他不需要累计数据频率。但LRU通过历史数据来预测未来是局限的，它会认为最后到来的数据是最可能被再次访问的，从而给与它最高的优先级。 在现有算法的局限性下，会导致缓存数据的命中率或多或少的受损，而命中略又是缓存的重要指标。HighScalability网站刊登了一篇文章，由前Google工程师发明的W-TinyLFU——一种现代的缓存 。Caffine Cache就是基于此算法而研发。Caffeine 因使用 Window TinyLfu 回收策略，提供了一个近乎最佳的命中率。 当数据的访问模式不随时间变化的时候，LFU的策略能够带来最佳的缓存命中率。然而LFU有两个缺点： 首先，它需要给每个记录项维护频率信息，每次访问都需要更新，这是个巨大的开销； 其次，如果数据访问模式随时间有变，LFU的频率信息无法随之变化，因此早先频繁访问的记录可能会占据缓存，而后期访问较多的记录则无法被命中。 因此，大多数的缓存设计都是基于LRU或者其变种来进行的。相比之下，LRU并不需要维护昂贵的缓存记录元信息，同时也能够反应随时间变化的数据访问模式。然而，在许多负载之下，LRU依然需要更多的空间才能做到跟LFU一致的缓存命中率。因此，一个“现代”的缓存，应当能够综合两者的长处。 TinyLFU维护了近期访问记录的频率信息，作为一个过滤器，当新记录来时，只有满足TinyLFU要求的记录才可以被插入缓存。如前所述，作为现代的缓存，它需要解决两个挑战： 一个是如何避免维护频率信息的高开销； 另一个是如何反应随时间变化的访问模式。 首先来看前者，TinyLFU借助了数据流Sketching技术，Count-Min Sketch显然是解决这个问题的有效手段，它可以用小得多的空间存放频率信息，而保证很低的False Positive Rate。但考虑到第二个问题，就要复杂许多了，因为我们知道，任何Sketching数据结构如果要反应时间变化都是一件困难的事情，在Bloom Filter方面，我们可以有Timing Bloom Filter，但对于CMSketch来说，如何做到Timing CMSketch就不那么容易了。TinyLFU采用了一种基于滑动窗口的时间衰减设计机制，借助于一种简易的reset操作：每次添加一条记录到Sketch的时候，都会给一个计数器上加1，当计数器达到一个尺寸W的时候，把所有记录的Sketch数值都除以2，该reset操作可以起到衰减的作用 。 W-TinyLFU主要用来解决一些稀疏的突发访问元素。在一些数目很少但突发访问量很大的场景下，TinyLFU将无法保存这类元素，因为它们无法在给定时间内积累到足够高的频率。因此W-TinyLFU就是结合LFU和LRU，前者用来应对大多数场景，而LRU用来处理突发流量。 在处理频率记录的方案中，你可能会想到用hashMap去存储，每一个key对应一个频率值。那如果数据量特别大的时候，是不是这个hashMap也会特别大呢。由此可以联想到 Bloom Filter，对于每个key，用n个byte每个存储一个标志用来判断key是否在集合中。原理就是使用k个hash函数来将key散列成一个整数。 在W-TinyLFU中使用Count-Min Sketch记录我们的访问频率，而这个也是布隆过滤器的一种变种。如下图所示: ![](F:\markdown\Java学习\Caffeine Cache-高性能Java本地缓存组件\1.png) 如果需要记录一个值，那我们需要通过多种Hash算法对其进行处理hash，然后在对应的hash算法的记录中+1，为什么需要多种hash算法呢？由于这是一个压缩算法必定会出现冲突，比如我们建立一个byte的数组，通过计算出每个数据的hash的位置。比如张三和李四，他们两有可能hash值都是相同，比如都是1那byte[1]这个位置就会增加相应的频率，张三访问1万次，李四访问1次那byte[1]这个位置就是1万零1，如果取李四的访问评率的时候就会取出是1万零1，但是李四命名只访问了1次啊，为了解决这个问题，所以用了多个hash算法可以理解为long[][]二维数组的一个概念，比如在第一个算法张三和李四冲突了，但是在第二个，第三个中很大的概率不冲突，比如一个算法大概有1%的概率冲突，那四个算法一起冲突的概率是1%的四次方。通过这个模式我们取李四的访问率的时候取所有算法中，李四访问最低频率的次数。所以他的名字叫Count-Min Sketch。 2. 使用 Caffeine Cache 的github地址：点我。 目前的最新版本是： 12345&lt;dependency&gt; &lt;groupId&gt;com.github.ben-manes.caffeine&lt;/groupId&gt; &lt;artifactId&gt;caffeine&lt;/artifactId&gt; &lt;version&gt;2.6.2&lt;/version&gt;&lt;/dependency&gt; 2.1 缓存填充策略 Caffeine Cache提供了三种缓存填充策略：手动、同步加载和异步加载。 1.手动加载 在每次get key的时候指定一个同步的函数，如果key不存在就调用这个函数生成一个值。 12345678910111213141516171819202122232425/** * 手动加载 * @param key * @return */public Object manulOperator(String key) &#123; Cache&lt;String, Object&gt; cache = Caffeine.newBuilder() .expireAfterWrite(1, TimeUnit.SECONDS) .expireAfterAccess(1, TimeUnit.SECONDS) .maximumSize(10) .build(); //如果一个key不存在，那么会进入指定的函数生成value Object value = cache.get(key, t -&gt; setValue(key).apply(key)); cache.put("hello",value); //判断是否存在如果不存返回null Object ifPresent = cache.getIfPresent(key); //移除一个key cache.invalidate(key); return value;&#125;public Function&lt;String, Object&gt; setValue(String key)&#123; return t -&gt; key + "value";&#125; 2. 同步加载 构造Cache时候，build方法传入一个CacheLoader实现类。实现load方法，通过key加载value。 12345678910111213141516/** * 同步加载 * @param key * @return */public Object syncOperator(String key)&#123; LoadingCache&lt;String, Object&gt; cache = Caffeine.newBuilder() .maximumSize(100) .expireAfterWrite(1, TimeUnit.MINUTES) .build(k -&gt; setValue(key).apply(key)); return cache.get(key);&#125;public Function&lt;String, Object&gt; setValue(String key)&#123; return t -&gt; key + "value";&#125; 3. 异步加载 AsyncLoadingCache是继承自LoadingCache类的，异步加载使用Executor去调用方法并返回一个CompletableFuture。异步加载缓存使用了响应式编程模型。 如果要以同步方式调用时，应提供CacheLoader。要以异步表示时，应该提供一个AsyncCacheLoader，并返回一个CompletableFuture。 1234567891011121314151617181920 /** * 异步加载 * * @param key * @return */public Object asyncOperator(String key)&#123; AsyncLoadingCache&lt;String, Object&gt; cache = Caffeine.newBuilder() .maximumSize(100) .expireAfterWrite(1, TimeUnit.MINUTES) .buildAsync(k -&gt; setAsyncValue(key).get()); return cache.get(key);&#125;public CompletableFuture&lt;Object&gt; setAsyncValue(String key)&#123; return CompletableFuture.supplyAsync(() -&gt; &#123; return key + "value"; &#125;);&#125; 2.2 回收策略 Caffeine提供了3种回收策略：基于大小回收，基于时间回收，基于引用回收。 1. 基于大小的过期方式 基于大小的回收策略有两种方式：一种是基于缓存大小，一种是基于权重。 1234567891011// 根据缓存的计数进行驱逐LoadingCache&lt;String, Object&gt; cache = Caffeine.newBuilder() .maximumSize(10000) .build(key -&gt; function(key));// 根据缓存的权重来进行驱逐（权重只是用于确定缓存大小，不会用于决定该缓存是否被驱逐）LoadingCache&lt;String, Object&gt; cache1 = Caffeine.newBuilder() .maximumWeight(10000) .weigher(key -&gt; function1(key)) .build(key -&gt; function(key)); maximumWeight与maximumSize不可以同时使用。 2.基于时间的过期方式 1234567891011121314151617181920212223242526// 基于固定的到期策略进行退出LoadingCache&lt;String, Object&gt; cache = Caffeine.newBuilder() .expireAfterAccess(5, TimeUnit.MINUTES) .build(key -&gt; function(key));LoadingCache&lt;String, Object&gt; cache1 = Caffeine.newBuilder() .expireAfterWrite(10, TimeUnit.MINUTES) .build(key -&gt; function(key));// 基于不同的到期策略进行退出LoadingCache&lt;String, Object&gt; cache2 = Caffeine.newBuilder() .expireAfter(new Expiry&lt;String, Object&gt;() &#123; @Override public long expireAfterCreate(String key, Object value, long currentTime) &#123; return TimeUnit.SECONDS.toNanos(seconds); &#125; @Override public long expireAfterUpdate(@Nonnull String s, @Nonnull Object o, long l, long l1) &#123; return 0; &#125; @Override public long expireAfterRead(@Nonnull String s, @Nonnull Object o, long l, long l1) &#123; return 0; &#125; &#125;).build(key -&gt; function(key)); Caffeine提供了三种定时驱逐策略： expireAfterAccess(long, TimeUnit):在最后一次访问或者写入后开始计时，在指定的时间后过期。假如一直有请求访问该key，那么这个缓存将一直不会过期。 expireAfterWrite(long, TimeUnit): 在最后一次写入缓存后开始计时，在指定的时间后过期。 expireAfter(Expiry): 自定义策略，过期时间由Expiry实现独自计算。 缓存的删除策略使用的是惰性删除和定时删除。这两个删除策略的时间复杂度都是O(1)。 3. 基于引用的过期方式 Java中四种引用类型 引用类型 被垃圾回收时间 用途 生存时间 强引用 Strong Reference 从来不会 对象的一般状态 JVM停止运行时终止 软引用 Soft Reference 在内存不足时 对象缓存 内存不足时终止 弱引用 Weak Reference 在垃圾回收时 对象缓存 gc运行后终止 虚引用 Phantom Reference 从来不会 可以用虚引用来跟踪对象被垃圾回收器回收的活动，当一个虚引用关联的对象被垃圾收集器回收之前会收到一条系统通知 JVM停止运行时终止 12345678910// 当key和value都没有引用时驱逐缓存LoadingCache&lt;String, Object&gt; cache = Caffeine.newBuilder() .weakKeys() .weakValues() .build(key -&gt; function(key));// 当垃圾收集器需要释放内存时驱逐LoadingCache&lt;String, Object&gt; cache1 = Caffeine.newBuilder() .softValues() .build(key -&gt; function(key)); 注意：AsyncLoadingCache不支持弱引用和软引用。 Caffeine.weakKeys()： 使用弱引用存储key。如果没有其他地方对该key有强引用，那么该缓存就会被垃圾回收器回收。由于垃圾回收器只依赖于身份(identity)相等，因此这会导致整个缓存使用身份 (==) 相等来比较 key，而不是使用 equals()。 Caffeine.weakValues() ：使用弱引用存储value。如果没有其他地方对该value有强引用，那么该缓存就会被垃圾回收器回收。由于垃圾回收器只依赖于身份(identity)相等，因此这会导致整个缓存使用身份 (==) 相等来比较 key，而不是使用 equals()。 Caffeine.softValues() ：使用软引用存储value。当内存满了过后，软引用的对象以将使用最近最少使用(least-recently-used ) 的方式进行垃圾回收。由于使用软引用是需要等到内存满了才进行回收，所以我们通常建议给缓存配置一个使用内存的最大值。 softValues() 将使用身份相等(identity) (==) 而不是equals() 来比较值。 Caffeine.weakValues()和Caffeine.softValues()不可以一起使用。 3. 移除事件监听 1234Cache&lt;String, Object&gt; cache = Caffeine.newBuilder() .removalListener((String key, Object value, RemovalCause cause) -&gt; System.out.printf("Key %s was removed (%s)%n", key, cause)) .build(); 4. 写入外部存储 CacheWriter 方法可以将缓存中所有的数据写入到第三方。 12345678910LoadingCache&lt;String, Object&gt; cache2 = Caffeine.newBuilder() .writer(new CacheWriter&lt;String, Object&gt;() &#123; @Override public void write(String key, Object value) &#123; // 写入到外部存储 &#125; @Override public void delete(String key, Object value, RemovalCause cause) &#123; // 删除外部存储 &#125; &#125;) .build(key -&gt; function(key)); 如果你有多级缓存的情况下，这个方法还是很实用。 注意：CacheWriter不能与弱键或AsyncLoadingCache一起使用。 5. 统计 与Guava Cache的统计一样。 1234Cache&lt;String, Object&gt; cache = Caffeine.newBuilder() .maximumSize(10_000) .recordStats() .build(); 通过使用Caffeine.recordStats(), 可以转化成一个统计的集合. 通过 Cache.stats() 返回一个CacheStats。CacheStats提供以下统计方法： 12345hitRate(): 返回缓存命中率evictionCount(): 缓存回收数量averageLoadPenalty(): 加载新值的平均时间 3. SpringBoot 中默认Cache-Caffine Cache SpringBoot 1.x版本中的默认本地cache是Guava Cache。在2.x（Spring Boot 2.0(spring 5) ）版本中已经用Caffine Cache取代了Guava Cache。毕竟有了更优的缓存淘汰策略。 下面我们来说在SpringBoot2.x版本中如何使用cache。 1. 引入依赖： 123456789&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-cache&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.github.ben-manes.caffeine&lt;/groupId&gt; &lt;artifactId&gt;caffeine&lt;/artifactId&gt; &lt;version&gt;2.6.2&lt;/version&gt;&lt;/dependency&gt; 2. 添加注解开启缓存支持 添加@EnableCaching注解： 12345678@SpringBootApplication@EnableCachingpublic class SingleDatabaseApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(SingleDatabaseApplication.class, args); &#125;&#125; 3. 配置文件的方式注入相关参数 properties文件 12spring.cache.cache-names=cache1spring.cache.caffeine.spec=initialCapacity=50,maximumSize=500,expireAfterWrite=10s 或Yaml文件 1234567spring: cache: type: caffeine cache-names: - userCache caffeine: spec: maximumSize=1024,refreshAfterWrite=60s 如果使用refreshAfterWrite配置,必须指定一个CacheLoader.不用该配置则无需这个bean,如上所述,该CacheLoader将关联被该缓存管理器管理的所有缓存，所以必须定义为CacheLoader&lt;Object, Object&gt;，自动配置将忽略所有泛型类型。 123456789101112131415161718192021222324252627282930313233import com.github.benmanes.caffeine.cache.CacheLoader;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;/** * @author: rickiyang * @date: 2019/6/15 * @description: */@Configurationpublic class CacheConfig &#123; /** * 相当于在构建LoadingCache对象的时候 build()方法中指定过期之后的加载策略方法 * 必须要指定这个Bean，refreshAfterWrite=60s属性才生效 * @return */ @Bean public CacheLoader&lt;String, Object&gt; cacheLoader() &#123; CacheLoader&lt;String, Object&gt; cacheLoader = new CacheLoader&lt;String, Object&gt;() &#123; @Override public Object load(String key) throws Exception &#123; return null; &#125; // 重写这个方法将oldValue值返回回去，进而刷新缓存 @Override public Object reload(String key, Object oldValue) throws Exception &#123; return oldValue; &#125; &#125;; return cacheLoader; &#125;&#125; Caffeine常用配置说明： 123456789101112131415161718192021222324252627initialCapacity=[integer]: 初始的缓存空间大小maximumSize=[long]: 缓存的最大条数maximumWeight=[long]: 缓存的最大权重expireAfterAccess=[duration]: 最后一次写入或访问后经过固定时间过期expireAfterWrite=[duration]: 最后一次写入后经过固定时间过期refreshAfterWrite=[duration]: 创建缓存或者最近一次更新缓存后经过固定的时间间隔，刷新缓存weakKeys: 打开key的弱引用weakValues：打开value的弱引用softValues：打开value的软引用recordStats：开发统计功能注意：expireAfterWrite和expireAfterAccess同时存在时，以expireAfterWrite为准。maximumSize和maximumWeight不可以同时使用weakValues和softValues不可以同时使用 需要说明的是，使用配置文件的方式来进行缓存项配置，一般情况能满足使用需求，但是灵活性不是很高，如果我们有很多缓存项的情况下写起来会导致配置文件很长。所以一般情况下你也可以选择使用bean的方式来初始化Cache实例。 下面的演示使用bean的方式来注入： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101package com.rickiyang.learn.cache;import com.github.benmanes.caffeine.cache.CacheLoader;import com.github.benmanes.caffeine.cache.Caffeine;import org.apache.commons.compress.utils.Lists;import org.springframework.cache.CacheManager;import org.springframework.cache.caffeine.CaffeineCache;import org.springframework.cache.support.SimpleCacheManager;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.context.annotation.Primary;import java.util.ArrayList;import java.util.List;import java.util.concurrent.TimeUnit;/** * @author: rickiyang * @date: 2019/6/15 * @description: */@Configurationpublic class CacheConfig &#123; /** * 创建基于Caffeine的Cache Manager * 初始化一些key存入 * @return */ @Bean @Primary public CacheManager caffeineCacheManager() &#123; SimpleCacheManager cacheManager = new SimpleCacheManager(); ArrayList&lt;CaffeineCache&gt; caches = Lists.newArrayList(); List&lt;CacheBean&gt; list = setCacheBean(); for(CacheBean cacheBean : list)&#123; caches.add(new CaffeineCache(cacheBean.getKey(), Caffeine.newBuilder().recordStats() .expireAfterWrite(cacheBean.getTtl(), TimeUnit.SECONDS) .maximumSize(cacheBean.getMaximumSize()) .build())); &#125; cacheManager.setCaches(caches); return cacheManager; &#125; /** * 初始化一些缓存的 key * @return */ private List&lt;CacheBean&gt; setCacheBean()&#123; List&lt;CacheBean&gt; list = Lists.newArrayList(); CacheBean userCache = new CacheBean(); userCache.setKey("userCache"); userCache.setTtl(60); userCache.setMaximumSize(10000); CacheBean deptCache = new CacheBean(); deptCache.setKey("userCache"); deptCache.setTtl(60); deptCache.setMaximumSize(10000); list.add(userCache); list.add(deptCache); return list; &#125; class CacheBean &#123; private String key; private long ttl; private long maximumSize; public String getKey() &#123; return key; &#125; public void setKey(String key) &#123; this.key = key; &#125; public long getTtl() &#123; return ttl; &#125; public void setTtl(long ttl) &#123; this.ttl = ttl; &#125; public long getMaximumSize() &#123; return maximumSize; &#125; public void setMaximumSize(long maximumSize) &#123; this.maximumSize = maximumSize; &#125; &#125;&#125; 创建了一个SimpleCacheManager作为Cache的管理对象，然后初始化了两个Cache对象，分别存储user，dept类型的缓存。当然构建Cache的参数设置我写的比较简单，你在使用的时候酌情根据需要配置参数。 4. 使用注解来对 cache 增删改查 我们可以使用spring提供的 @Cacheable、@CachePut、@CacheEvict等注解来方便的使用caffeine缓存。 如果使用了多个cahce，比如redis、caffeine等，必须指定某一个CacheManage为@primary，在@Cacheable注解中没指定 cacheManager 则使用标记为primary的那个。 cache方面的注解主要有以下5个： @Cacheable 触发缓存入口（这里一般放在创建和获取的方法上，@Cacheable注解会先查询是否已经有缓存，有会使用缓存，没有则会执行方法并缓存） @CacheEvict 触发缓存的eviction（用于删除的方法上） @CachePut 更新缓存且不影响方法执行（用于修改的方法上，该注解下的方法始终会被执行） @Caching 将多个缓存组合在一个方法上（该注解可以允许一个方法同时设置多个注解） @CacheConfig 在类级别设置一些缓存相关的共同配置（与其它缓存配合使用） 说一下@Cacheable 和 @CachePut的区别： @Cacheable：它的注解的方法是否被执行取决于Cacheable中的条件，方法很多时候都可能不被执行。 @CachePut：这个注解不会影响方法的执行，也就是说无论它配置的条件是什么，方法都会被执行，更多的时候是被用到修改上。 简要说一下Cacheable类中各个方法的使用： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051public @interface Cacheable &#123; /** * 要使用的cache的名字 */ @AliasFor("cacheNames") String[] value() default &#123;&#125;; /** * 同value()，决定要使用那个/些缓存 */ @AliasFor("value") String[] cacheNames() default &#123;&#125;; /** * 使用SpEL表达式来设定缓存的key，如果不设置默认方法上所有参数都会作为key的一部分 */ String key() default ""; /** * 用来生成key，与key()不可以共用 */ String keyGenerator() default ""; /** * 设定要使用的cacheManager，必须先设置好cacheManager的bean，这是使用该bean的名字 */ String cacheManager() default ""; /** * 使用cacheResolver来设定使用的缓存，用法同cacheManager，但是与cacheManager不可以同时使用 */ String cacheResolver() default ""; /** * 使用SpEL表达式设定出发缓存的条件，在方法执行前生效 */ String condition() default ""; /** * 使用SpEL设置出发缓存的条件，这里是方法执行完生效，所以条件中可以有方法执行后的value */ String unless() default ""; /** * 用于同步的，在缓存失效（过期不存在等各种原因）的时候，如果多个线程同时访问被标注的方法 * 则只允许一个线程通过去执行方法 */ boolean sync() default false;&#125; 基于注解的使用方法： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546package com.rickiyang.learn.cache;import com.rickiyang.learn.entity.User;import org.springframework.cache.annotation.CacheEvict;import org.springframework.cache.annotation.CachePut;import org.springframework.cache.annotation.Cacheable;import org.springframework.stereotype.Service;/** * @author: rickiyang * @date: 2019/6/15 * @description: 本地cache */@Servicepublic class UserCacheService &#123; /** * 查找 * 先查缓存，如果查不到，会查数据库并存入缓存 * @param id */ @Cacheable(value = "userCache", key = "#id", sync = true) public void getUser(long id)&#123; //查找数据库 &#125; /** * 更新/保存 * @param user */ @CachePut(value = "userCache", key = "#user.id") public void saveUser(User user)&#123; //todo 保存数据库 &#125; /** * 删除 * @param user */ @CacheEvict(value = "userCache",key = "#user.id") public void delUser(User user)&#123; //todo 保存数据库 &#125;&#125; 如果你不想使用注解的方式去操作缓存，也可以直接使用SimpleCacheManager获取缓存的key进而进行操作。 注意到上面的key使用了spEL 表达式。Spring Cache提供了一些供我们使用的SpEL上下文数据，下表直接摘自Spring官方文档： 名称 位置 描述 示例 methodName root对象 当前被调用的方法名 #root.methodname method root对象 当前被调用的方法 #root.method.name target root对象 当前被调用的目标对象实例 #root.target targetClass root对象 当前被调用的目标对象的类 #root.targetClass args root对象 当前被调用的方法的参数列表 #root.args[0] caches root对象 当前方法调用使用的缓存列表 #root.caches[0].name Argument Name 执行上下文 当前被调用的方法的参数，如findArtisan(Artisan artisan),可以通过#artsian.id获得参数 #artsian.id result 执行上下文 方法执行后的返回值（仅当方法执行后的判断有效，如 unless cacheEvict的beforeInvocation=false） #result 注意： 1.当我们要使用root对象的属性作为key时我们也可以将“#root”省略，因为Spring默认使用的就是root对象的属性。 如 1@Cacheable(key = "targetClass + methodName +#p0") 2.使用方法参数时我们可以直接使用“#参数名”或者“#p参数index”。 如： 12@Cacheable(value="userCache", key="#id")@Cacheable(value="userCache", key="#p0") SpEL提供了多种运算符 类型 运算符 关系 &lt;，&gt;，&lt;=，&gt;=，==，!=，lt，gt，le，ge，eq，ne 算术 +，- ，* ，/，%，^ 逻辑 &amp;&amp;，||，!，and，or，not，between，instanceof 条件 ?: (ternary)，?: (elvis) 正则表达式 matches 其他类型 ?.，?[…]，![…]，^[…]，$[…]]]></content>
      <categories>
        <category>java开发工具类</category>
      </categories>
      <tags>
        <tag>工具类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Guava cache使用总结]]></title>
    <url>%2Fposts%2Fe7b103bc.html</url>
    <content type="text"><![CDATA[缓存分为本地缓存和远端缓存。常见的远端缓存有Redis，MongoDB；本地缓存一般使用map的方式保存在本地内存中。一般我们在业务中操作缓存，都会操作缓存和数据源两部分。如：put数据时，先插入DB，再删除原来的缓存；ge数据时，先查缓存，命中则返回，没有命中时，需要查询DB，再把查询结果放入缓存中 。如果访问量大，我们还得兼顾本地缓存的线程安全问题。必要的时候也要考虑缓存的回收策略。 今天说的 Guava Cache 是google guava中的一个内存缓存模块，用于将数据缓存到JVM内存中。他很好的解决了上面提到的几个问题： 很好的封装了get、put操作，能够集成数据源 ； 线程安全的缓存，与ConcurrentMap相似，但前者增加了更多的元素失效策略，后者只能显示的移除元素； Guava Cache提供了三种基本的缓存回收方式：基于容量回收、定时回收和基于引用回收。定时回收有两种：按照写入时间，最早写入的最先回收；按照访问时间，最早访问的最早回收； 监控缓存加载/命中情况 Guava Cache的架构设计灵感ConcurrentHashMap，在简单场景中可以通过HashMap实现简单数据缓存，但如果要实现缓存随时间改变、存储的数据空间可控则缓存工具还是很有必要的。Cache存储的是键值对的集合，不同时是还需要处理缓存过期、动态加载等算法逻辑，需要额外信息实现这些操作，对此根据面向对象的思想，还需要做方法与数据的关联性封装，主要实现的缓存功能有：自动将节点加载至缓存结构中，当缓存的数据超过最大值时，使用LRU算法替换；它具备根据节点上一次被访问或写入时间计算缓存过期机制，缓存的key被封装在WeakReference引用中，缓存的value被封装在WeakReference或SoftReference引用中；还可以统计缓存使用过程中的命中率、异常率和命中率等统计数据。 构建缓存对象 我们先看一个示例，再来讲解使用方式： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485package com.rickiyang.learn.cache;import com.google.common.cache.CacheBuilder;import com.google.common.cache.CacheLoader;import com.google.common.cache.LoadingCache;import java.text.SimpleDateFormat;import java.util.Date;import java.util.Random;import java.util.concurrent.TimeUnit;/** * @author: rickiyang * @date: 2019/6/12 * @description: */public class GuavaCacheService &#123; public void setCache() &#123; LoadingCache&lt;Integer, String&gt; cache = CacheBuilder.newBuilder() //设置并发级别为8，并发级别是指可以同时写缓存的线程数 .concurrencyLevel(8) //设置缓存容器的初始容量为10 .initialCapacity(10) //设置缓存最大容量为100，超过100之后就会按照LRU最近虽少使用算法来移除缓存项 .maximumSize(100) //是否需要统计缓存情况,该操作消耗一定的性能,生产环境应该去除 .recordStats() //设置写缓存后n秒钟过期 .expireAfterWrite(60, TimeUnit.SECONDS) //设置读写缓存后n秒钟过期,实际很少用到,类似于expireAfterWrite //.expireAfterAccess(17, TimeUnit.SECONDS) //只阻塞当前数据加载线程，其他线程返回旧值 //.refreshAfterWrite(13, TimeUnit.SECONDS) //设置缓存的移除通知 .removalListener(notification -&gt; &#123; System.out.println(notification.getKey() + " " + notification.getValue() + " 被移除,原因:" + notification.getCause()); &#125;) //build方法中可以指定CacheLoader，在缓存不存在时通过CacheLoader的实现自动加载缓存 .build(new DemoCacheLoader()); //模拟线程并发 new Thread(() -&gt; &#123; //非线程安全的时间格式化工具 SimpleDateFormat simpleDateFormat = new SimpleDateFormat("HH:mm:ss"); try &#123; for (int i = 0; i &lt; 10; i++) &#123; String value = cache.get(1); System.out.println(Thread.currentThread().getName() + " " + simpleDateFormat.format(new Date()) + " " + value); TimeUnit.SECONDS.sleep(3); &#125; &#125; catch (Exception ignored) &#123; &#125; &#125;).start(); new Thread(() -&gt; &#123; SimpleDateFormat simpleDateFormat = new SimpleDateFormat("HH:mm:ss"); try &#123; for (int i = 0; i &lt; 10; i++) &#123; String value = cache.get(1); System.out.println(Thread.currentThread().getName() + " " + simpleDateFormat.format(new Date()) + " " + value); TimeUnit.SECONDS.sleep(5); &#125; &#125; catch (Exception ignored) &#123; &#125; &#125;).start(); //缓存状态查看 System.out.println(cache.stats().toString()); &#125; /** * 随机缓存加载,实际使用时应实现业务的缓存加载逻辑,例如从数据库获取数据 */ public static class DemoCacheLoader extends CacheLoader&lt;Integer, String&gt; &#123; @Override public String load(Integer key) throws Exception &#123; System.out.println(Thread.currentThread().getName() + " 加载数据开始"); TimeUnit.SECONDS.sleep(8); Random random = new Random(); System.out.println(Thread.currentThread().getName() + " 加载数据结束"); return "value:" + random.nextInt(10000); &#125; &#125;&#125; 上面一段代码展示了如何使用Cache创建一个缓存对象并使用它。 LoadingCache是Cache的子接口，相比较于Cache，当从LoadingCache中读取一个指定key的记录时，如果该记录不存在，则LoadingCache可以自动执行加载数据到缓存的操作。 在调用CacheBuilder的build方法时，必须传递一个CacheLoader类型的参数，CacheLoader的load方法需要我们提供实现。当调用LoadingCache的get方法时，如果缓存不存在对应key的记录，则CacheLoader中的load方法会被自动调用从外存加载数据，load方法的返回值会作为key对应的value存储到LoadingCache中，并从get方法返回。 当然如果你不想指定重建策略，那么你可以使用无参的build()方法，它将返回Cache类型的构建对象。 CacheBuilder 是Guava 提供的一个快速构建缓存对象的工具类。CacheBuilder类采用builder设计模式，它的每个方法都返回CacheBuilder本身，直到build方法被调用。 该类中提供了很多的参数设置选项，你可以设置cache的默认大小，并发数，存活时间，过期策略等等。 可选配置分析 缓存的并发级别 Guava提供了设置并发级别的api，使得缓存支持并发的写入和读取。同 ConcurrentHashMap 类似Guava cache的并发也是通过分离锁实现。在一般情况下，将并发级别设置为服务器cpu核心数是一个比较不错的选择。 1234CacheBuilder.newBuilder() // 设置并发级别为cpu核心数 .concurrencyLevel(Runtime.getRuntime().availableProcessors()) .build(); 缓存的初始容量设置 我们在构建缓存时可以为缓存设置一个合理大小初始容量，由于Guava的缓存使用了分离锁的机制，扩容的代价非常昂贵。所以合理的初始容量能够减少缓存容器的扩容次数。 1234CacheBuilder.newBuilder() // 设置初始容量为100 .initialCapacity(100) .build(); 设置最大存储 Guava Cache可以在构建缓存对象时指定缓存所能够存储的最大记录数量。当Cache中的记录数量达到最大值后再调用put方法向其中添加对象，Guava会先从当前缓存的对象记录中选择一条删除掉，腾出空间后再将新的对象存储到Cache中。 基于容量的清除(size-based eviction): 通过CacheBuilder.maximumSize(long)方法可以设置Cache的最大容量数，当缓存数量达到或接近该最大值时，Cache将清除掉那些最近最少使用的缓存; 基于权重的清除: 使用CacheBuilder.weigher(Weigher)指定一个权重函数，并且用CacheBuilder.maximumWeight(long)指定最大总重。比如每一项缓存所占据的内存空间大小都不一样，可以看作它们有不同的“权重”（weights）。 缓存清除策略 1. 基于存活时间的清除 expireAfterWrite 写缓存后多久过期 expireAfterAccess 读写缓存后多久过期 refreshAfterWrite 写入数据后多久过期,只阻塞当前数据加载线程,其他线程返回旧值 这几个策略时间可以单独设置,也可以组合配置。 2. 上面提到的基于容量的清除 3. 显式清除 任何时候，你都可以显式地清除缓存项，而不是等到它被回收，Cache接口提供了如下API： 个别清除：Cache.invalidate(key) 批量清除：Cache.invalidateAll(keys) 清除所有缓存项：Cache.invalidateAll() 4. 基于引用的清除（Reference-based Eviction） 在构建Cache实例过程中，通过设置使用弱引用的键、或弱引用的值、或软引用的值，从而使JVM在GC时顺带实现缓存的清除，不过一般不轻易使用这个特性。 CacheBuilder.weakKeys()：使用弱引用存储键。当键没有其它（强或软）引用时，缓存项可以被垃圾回收。因为垃圾回收仅依赖恒等式，使用弱引用键的缓存用而不是equals比较键。 CacheBuilder.weakValues()：使用弱引用存储值。当值没有其它（强或软）引用时，缓存项可以被垃圾回收。因为垃圾回收仅依赖恒等式，使用弱引用值的缓存用而不是equals比较值。 CacheBuilder.softValues()：使用软引用存储值。软引用只有在响应内存需要时，才按照全局最近最少使用的顺序回收。考虑到使用软引用的性能影响，我们通常建议使用更有性能预测性的缓存大小限定（见上文，基于容量回收）。使用软引用值的缓存同样用==而不是equals比较值。 清理什么时候发生 也许这个问题有点奇怪，如果设置的存活时间为一分钟，难道不是一分钟后这个key就会立即清除掉吗？我们来分析一下如果要实现这个功能，那Cache中就必须存在线程来进行周期性地检查、清除等工作，很多cache如redis、ehcache都是这样实现的。 使用CacheBuilder构建的缓存不会”自动”执行清理和回收工作，也不会在某个缓存项过期后马上清理，也没有诸如此类的清理机制。相反，它会在写操作时顺带做少量的维护工作，或者偶尔在读操作时做——如果写操作实在太少的话。 这样做的原因在于：如果要自动地持续清理缓存，就必须有一个线程，这个线程会和用户操作竞争共享锁。此外，某些环境下线程创建可能受限制，这样CacheBuilder就不可用了。参考如下示例： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768package com.rickiyang.learn.cache;import com.google.common.cache.Cache;import com.google.common.cache.CacheBuilder;import java.text.SimpleDateFormat;import java.util.Date;import java.util.concurrent.TimeUnit;/** * @author: rickiyang * @date: 2019/6/12 * @description: */public class GuavaCacheService &#123; static Cache&lt;Integer, String&gt; cache = CacheBuilder.newBuilder() .expireAfterWrite(5, TimeUnit.SECONDS) .build(); public static void main(String[] args) throws Exception &#123; new Thread(() -&gt; &#123; while (true) &#123; SimpleDateFormat sdf = new SimpleDateFormat("HH:mm:ss"); System.out.println(sdf.format(new Date()) + " size: " + cache.size()); try &#123; Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; &#125; &#125; &#125;).start(); SimpleDateFormat sdf = new SimpleDateFormat("HH:mm:ss"); cache.put(1, "a"); System.out.println("写入 key:1 ,value:" + cache.getIfPresent(1)); Thread.sleep(10000); cache.put(2, "b"); System.out.println("写入 key:2 ,value:" + cache.getIfPresent(2)); Thread.sleep(10000); System.out.println(sdf.format(new Date()) + " sleep 10s , key:1 ,value:" + cache.getIfPresent(1)); System.out.println(sdf.format(new Date()) + " sleep 10s, key:2 ,value:" + cache.getIfPresent(2)); &#125;&#125;部分输出结果：23:57:36 size: 0写入 key:1 ,value:a23:57:38 size: 123:57:40 size: 123:57:42 size: 123:57:44 size: 123:57:46 size: 1写入 key:2 ,value:b23:57:48 size: 123:57:50 size: 123:57:52 size: 123:57:54 size: 123:57:56 size: 123:57:56 sleep 10s , key:1 ,value:null23:57:56 sleep 10s, key:2 ,value:null23:57:58 size: 023:58:00 size: 023:58:02 size: 0 ... ... 上面程序设置了缓存过期时间为5S，每打印一次当前的size需要2S，打印了5次size之后写入key 2，此时的size为1，说明在这个时候才把第一次应该过期的key 1给删除。 给移除操作添加一个监听器： 可以为Cache对象添加一个移除监听器，这样当有记录被删除时可以感知到这个事件。 12345RemovalListener&lt;String, String&gt; listener = notification -&gt; System.out.println("[" + notification.getKey() + ":" + notification.getValue() + "] is removed!"); Cache&lt;String,String&gt; cache = CacheBuilder.newBuilder() .maximumSize(5) .removalListener(listener) .build(); 但是要注意的是： 默认情况下，监听器方法是在移除缓存时同步调用的。因为缓存的维护和请求响应通常是同时进行的，代价高昂的监听器方法在同步模式下会拖慢正常的缓存请求。在这种情况下，你可以使用RemovalListeners.asynchronous(RemovalListener, Executor)把监听器装饰为异步操作。 自动加载 上面我们说过使用get方法的时候如果key不存在你可以使用指定方法去加载这个key。在Cache构建的时候通过指定CacheLoder的方式。如果你没有指定，你也可以在get的时候显式的调用call方法来设置key不存在的补救策略。 Cache的get方法有两个参数，第一个参数是要从Cache中获取记录的key，第二个记录是一个Callable对象。 当缓存中已经存在key对应的记录时，get方法直接返回key对应的记录。如果缓存中不包含key对应的记录，Guava会启动一个线程执行Callable对象中的call方法，call方法的返回值会作为key对应的值被存储到缓存中，并且被get方法返回。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162package com.rickiyang.learn.cache;import com.google.common.cache.Cache;import com.google.common.cache.CacheBuilder;import java.util.concurrent.Callable;import java.util.concurrent.ExecutionException;/** * @author: rickiyang * @date: 2019/6/12 * @description: */public class GuavaCacheService &#123; private static Cache&lt;String, String&gt; cache = CacheBuilder.newBuilder() .maximumSize(3) .build(); public static void main(String[] args) &#123; new Thread(() -&gt; &#123; System.out.println("thread1"); try &#123; String value = cache.get("key", new Callable&lt;String&gt;() &#123; public String call() throws Exception &#123; System.out.println("thread1"); //加载数据线程执行标志 Thread.sleep(1000); //模拟加载时间 return "thread1"; &#125; &#125;); System.out.println("thread1 " + value); &#125; catch (ExecutionException e) &#123; e.printStackTrace(); &#125; &#125;).start(); new Thread(() -&gt; &#123; System.out.println("thread2"); try &#123; String value = cache.get("key", new Callable&lt;String&gt;() &#123; public String call() throws Exception &#123; System.out.println("thread2"); //加载数据线程执行标志 Thread.sleep(1000); //模拟加载时间 return "thread2"; &#125; &#125;); System.out.println("thread2 " + value); &#125; catch (ExecutionException e) &#123; e.printStackTrace(); &#125; &#125;).start(); &#125;&#125;输出结果为：thread1thread2thread2thread1 thread2thread2 thread2 可以看到输出结果：两个线程都启动，输出thread1，thread2，接着又输出了thread2，说明进入了thread2的call方法了，此时thread1正在阻塞，等待key被设置。然后thread1 得到了value是thread2，thread2的结果自然也是thread2。 这段代码中有两个线程共享同一个Cache对象，两个线程同时调用get方法获取同一个key对应的记录。由于key对应的记录不存在，所以两个线程都在get方法处阻塞。此处在call方法中调用Thread.sleep(1000)模拟程序从外存加载数据的时间消耗。 从结果中可以看出，虽然是两个线程同时调用get方法，但只有一个get方法中的Callable会被执行(没有打印出load2)。Guava可以保证当有多个线程同时访问Cache中的一个key时，如果key对应的记录不存在，Guava只会启动一个线程执行get方法中Callable参数对应的任务加载数据存到缓存。当加载完数据后，任何线程中的get方法都会获取到key对应的值。 统计信息 可以对Cache的命中率、加载数据时间等信息进行统计。在构建Cache对象时，可以通过CacheBuilder的recordStats方法开启统计信息的开关。开关开启后Cache会自动对缓存的各种操作进行统计，调用Cache的stats方法可以查看统计后的信息。 12345678910111213141516171819202122232425262728293031323334353637package com.rickiyang.learn.cache;import com.google.common.cache.Cache;import com.google.common.cache.CacheBuilder;/** * @author: rickiyang * @date: 2019/6/12 * @description: */public class GuavaCacheService &#123; public static void main(String[] args) &#123; Cache&lt;String, String&gt; cache = CacheBuilder.newBuilder() .maximumSize(3) .recordStats() //开启统计信息开关 .build(); cache.put("1", "v1"); cache.put("2", "v2"); cache.put("3", "v3"); cache.put("4", "v4"); cache.getIfPresent("1"); cache.getIfPresent("2"); cache.getIfPresent("3"); cache.getIfPresent("4"); cache.getIfPresent("5"); cache.getIfPresent("6"); System.out.println(cache.stats()); //获取统计信息 &#125;&#125;输出：CacheStats&#123;hitCount=3, missCount=3, loadSuccessCount=0, loadExceptionCount=0, totalLoadTime=0, evictionCount=1&#125;]]></content>
      <categories>
        <category>java开发工具类</category>
      </categories>
      <tags>
        <tag>工具类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL一键生成实体文件的神器-ginbro(18)]]></title>
    <url>%2Fposts%2Fcb2d4bc7.html</url>
    <content type="text"><![CDATA[Java转过来的同学对Mybatis的使用肯定不陌生，特别是对一堆表去生成相应的dao和entity的时候使用Mybatis generator所带来的感触，无比深刻。前面我们也讲过原生的数据库使用，讲过gorm的使用，无论是原生的还是第三方的数据库工具，都是需要我们手动的写struct和相应的CRUD的。今天说的这一款插件跟Java中的Mybatis generator一样，都是能帮我们简化表与实体对应关系。 安装 1go get github.com/dejavuzhou/ginbro 默认安装到了$GOPATH/bin中。 使用 接下来使用如下方式去连接你的数据库，以下拿MYSQL数据库示例： 1ginbro gen -u userName -p pwd -a "IP:port" -d databaseName -o "projectName" 上述命令表示连接相应的数据库，用户名密码，对应的库名。最后是你想生成的工程名称。 相应的参数： 12345678Flags: --config string 指定config文件名 (default is $HOME/ginbro.yaml) -h, --help help for ginbro -a, --mysqlAddr string MySQL host:port (default "127.0.0.1:3306") -c, --mysqlCharset string MySQL charset (default "utf8") -d, --mysqlDatabase string MySQL database name -p, --mysqlPassword string MySQL password (default "password") -u, --mysqlUser string MySQL user name (default "root") 工程默认生成到$GOPATH/src目录下。 工程结构如下： 123456789demo config/ 配置文件所在目录 handlers/ 使用gin 生成对http接口 models/ 数据库表对应的实体 static/ 静态文件 swagger/ swagger tasks/ 定时任务相关 main.go 启动类 config.toml 主配置文件 运行main函数，控制台可以看到swagger的访问地址： http://127.0.0.1:5555/swagger 访问swagger你能看到给你生成了一些表对应的CRUD的接口。 ginbro 生成app代码包含功能简介 生成完善RESTful APIs 应用 自动生成完善的Swagger文档 自动生成数据库表的模型和标注 支持 JWT Authorization Bearer 身份验证 and JWT 中间件 支持登陆防火墙 支持静态资源替代nginx 可配置的跨域cors中间件 用户友好的自定义配置 支持定时任务 支持图形工具GUI 内置高效率的内存数据库 依赖框架 1234567891011go get github.com/gin-contrib/corsgo get github.com/gin-contrib/staticgo get github.com/gin-gonic/autotlsgo get github.com/gin-gonic/gingo get github.com/sirupsen/logrusgo get github.com/spf13/vipergo get github.com/spf13/cobrago get github.com/go-redis/redisgo get github.com/go-sql-driver/mysqlgo get github.com/jinzhu/gormgo get github.com/dgrijalva/jwt-go gin框架相关，viper配置文件解析框架，gorm数据库框架。 开发计划 已完成： Auth 和 JWT middleware 支持一键生产jwt密码验证 分页总数做mem缓存 json不现实password等隐私字段 生成友好的.gitignore go test 单元测试 完善go doc 未完成： 支持MongoDB数据库 更具体数据映射关联模型 支持PostgreSQL数据库 支持生成gRPC服务 更详细的gorm tag信息 swaggerDoc参数说明继续优化 支持其他语言框架(php-laravel/lumne ,python flask …) sqlite 注意 mysql表中没有id/ID/Id/iD字段将不会生成路由和模型 json字段 在update/create的时候 必须使可以序列号的json字符串(eg0:&quot;{}&quot; eg1:&quot;[]&quot;),否则mysql会报错]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>Go基础学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go orm框架gorm学习(17)]]></title>
    <url>%2Fposts%2F9784e2f1.html</url>
    <content type="text"><![CDATA[之前咱们学习过原生的Go连接MYSQL的方法，使用Go自带的&quot;database/sql&quot;数据库连接api，&quot;github.com/go-sql-driver/mysql&quot;MYSQL驱动，通过比较原生的写法去写sql和处理事务。目前开源界也有很多封装好的orm操作框架，帮我们简省一些重复的操作，提高代码可读性。gorm就是这样的一款作品，我们来学习一下gorm的操作流程。 安装 1go get -u github.com/jinzhu/gorm 数据库连接 要连接到数据库首先要导入驱动程序。例如 1import _ &quot;github.com/go-sql-driver/mysql&quot; 为了方便记住导入路径，GORM包装了一些驱动。 1234import _ &quot;github.com/jinzhu/gorm/dialects/mysql&quot;// import _ &quot;github.com/jinzhu/gorm/dialects/postgres&quot;// import _ &quot;github.com/jinzhu/gorm/dialects/sqlite&quot;// import _ &quot;github.com/jinzhu/gorm/dialects/mssql&quot; 所以包名可以改为如上： 1234567891011import ( "github.com/jinzhu/gorm" _ "github.com/jinzhu/gorm/dialects/mysql")func main() &#123; db, err := gorm.Open("mysql", "user:password@tcp(IP:port)/dbname?charset=utf8&amp;parseTime=True&amp;loc=Local") db.DB().SetMaxIdleConns(10) db.DB().SetMaxOpenConns(100) defer db.Close()&#125; 注：为了处理time.Time，你需要包括parseTime作为参数。 数据模型定义 表名，列名如何对应结构体 在Gorm中，表名是结构体名的复数形式，列名是字段名的蛇形小写。 即，如果有一个user表，那么如果你定义的结构体名为：User，gorm会默认表名为users而不是user。 例如有如下表结构定义： 12345678910111213CREATE TABLE `areas` ( `id` int(11) NOT NULL AUTO_INCREMENT COMMENT '主键id', `area_id` int(11) NOT NULL COMMENT '区县id', `area_name` varchar(45) NOT NULL COMMENT '区县名', `city_id` int(11) NOT NULL COMMENT '城市id', `city_name` varchar(45) NOT NULL COMMENT '城市名称', `province_id` int(11) NOT NULL COMMENT '省份id', `province_name` varchar(45) NOT NULL COMMENT '省份名称', `area_status` tinyint(3) NOT NULL DEFAULT '1' COMMENT '该条区域信息是否可用 ： 1:可用 2：不可用', `created_at` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间', `updated_at` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间', PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8mb4 COMMENT='区域表' 那么对应的结构体定义如下： 123456789101112type Area struct &#123; Id int AreaId int AreaName string CityId int CityName string ProvinceId int ProvinceName string AreaStatus int CreatedAt time.Time UpdatedAt time.Time&#125; 如何全局禁用表名复数呢？ 可以在创建数据库连接的时候设置如下参数： 12// 全局禁用表名复数db.SingularTable(true) // 如果设置为true,`User`的默认表名为`user`,使用`TableName`设置的表名不受影响 这样的话，表名默认即为结构体的首字母小写形式。 CRUD 使用 下面我们使用一张User表来就CRUD做一些操作示例： 表结构如下： 12345678910CREATE TABLE `user` ( `id` bigint(20) NOT NULL AUTO_INCREMENT, `name` varchar(30) NOT NULL DEFAULT '', `age` int(3) NOT NULL DEFAULT '0', `sex` tinyint(3) NOT NULL DEFAULT '0', `phone` varchar(40) NOT NULL DEFAULT '', `create_at` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, `updated_at` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=8 DEFAULT CHARSET=utf8mb4 首先初始化数据库连接： 1234567891011121314151617181920212223242526package mainimport ( "github.com/jinzhu/gorm" _ "github.com/jinzhu/gorm/dialects/mysql")var db *gorm.DBtype User struct &#123; Id int Name string Age int Sex byte Phone string&#125;func init() &#123; var err error db, err = gorm.Open("mysql", "root:123456@tcp(127.0.0.1:3306)/test?charset=utf8&amp;parseTime=True&amp;loc=Local") if err != nil &#123; panic(err) &#125; //设置全局表名禁用复数 db.SingularTable(true)&#125; 下面所有的操作都是在上面的初始化连接上执行的操作。 插入 123456//插入数据func (user *User) Insert() &#123; //这里使用了Table()函数，如果你没有指定全局表名禁用复数，或者是表名跟结构体名不一样的时候 //你可以自己在sql中指定表名。这里是示例，本例中这个函数可以去除。 db.Table("user").Create(user)&#125; 更新 1234567891011121314//注意，Model方法必须要和Update方法一起使用//使用效果相当于Model中设置更新的主键key（如果没有where指定，那么默认更新的key为id），Update中设置更新的值//如果Model中没有指定id值，且也没有指定where条件，那么将更新全表//相当于：update user set name='xiaoming' where id=1;user := User&#123;Id: 1,Name:"xiaoming"&#125;db.Model(&amp;user).Update(user)//注意到上面Update中使用了一个Struct，你也可以使用map对象。//需要注意的是：使用Struct的时候，只会更新Struct中这些非空的字段。//对于string类型字段的""，int类型字段0，bool类型字段的false都被认为是空白值，不会去更新表//下面这个更新操作只使用了where条件没有在Model中指定id//update user set name='xiaohong' wehre sex=1db.Model(&amp;User&#123;&#125;).Where("sex = ?",1).Update("name","xiaohong") 如果你想手动将某个字段set为空值, 可以使用单独选定某些字段的方式来更新： 12user := User&#123;Id: 1&#125;db.Model(&amp;user).Select("name").Update(map[string]interface&#123;&#125;&#123;"name":"","age":0&#125;) 忽略掉某些字段： 当你的更新的参数为结构体，而结构体中某些字段你又不想去更新，那么可以使用Omit方法过滤掉这些不想update到库的字段： 12user := User&#123;Id: 1,Name:"xioaming",Age:12&#125;db.Model(&amp;user).Omit("name").Update(&amp;user) 删除 123456//delete from user where id=1;user := User&#123;Id: 1&#125;db.Delete(&amp;user)//delete from user where id &gt; 11;db.Delete(&amp;User&#123;&#125;,"id &gt; ?",11) 事务 1234567891011121314151617func CreateAnimals(db *gorm.DB) err &#123; tx := db.Begin() // 注意，一旦你在一个事务中，使用tx作为数据库句柄 if err := tx.Create(&amp;Animal&#123;Name: "Giraffe"&#125;).Error; err != nil &#123; tx.Rollback() return err &#125; if err := tx.Create(&amp;Animal&#123;Name: "Lion"&#125;).Error; err != nil &#123; tx.Rollback() return err &#125; tx.Commit() return nil&#125; 查询： 12345678910111213141516171819202122232425func (user *User) query() (u []User) &#123; //查询所有记录 db.Find(&amp;u) //Find方法可以带 where 参数 db.Find(&amp;u,"id &gt; ? and age &gt; ?",2,12) //带where 子句的查询，注意where要在find前面 db.Where("id &gt; ?", 2).Find(&amp;u) // where name in ("xiaoming","xiaohong") db.Where("name in (?)",[]string&#123;"xiaoming","xiaohong"&#125;).Find(&amp;u) //获取第一条记录，按照主键顺序排序 db.First(&amp;u) //First方法可以带where 条件 db.First(&amp;u,"where sex = ?",1) //获取最后一条记录，按照主键顺序排序 //同样 last方法也可以带where条件 db.Last(&amp;u) return u&#125; 注意：方法中带的&amp;u表示是返回值用u这个对象来接收。 上面的查询都将返回表中所有的字段，如果你想指定查询某些字段该怎么做呢？ 指定查询字段-Select 12//指定查询字段db.Select("name,age").Where(map[string]interface&#123;&#125;&#123;"age":12,"sex":1&#125;).Find(&amp;u) 使用Struct和map作为查询条件 12345//使用Struct，相当于：select * from user where age =12 and sex=1db.Where(&amp;User&#123;Age:12,Sex:1&#125;).Find(&amp;u)//等同上一句db.Where(map[string]interface&#123;&#125;&#123;"age":12,"sex":1&#125;).Find(&amp;u) not 条件的使用 12345//where name not in ("xiaoming","xiaohong")db.Not("name","xiaoming","xiaohong").Find(&amp;u)//同上db.Not("name",[]string&#123;"xiaoming","xiaohong"&#125;).Find(&amp;u) or 的使用 12//where age &gt; 12 or sex = 1db.Where("age &gt; ?",12).Or("sex = ?",1).Find(&amp;u) order by 的使用 12//order by age descdb.Where("age &gt; ?",12).Or("sex = ?",1).Order("age desc").Find(&amp;u) limit 的使用 12//limit 10db.Not("name",[]string&#123;"xiaoming","xiaohong"&#125;).Limit(10).Find(&amp;u) offset 的使用 12//limit 300,10db.Not("name",[]string&#123;"xiaoming","xiaohong"&#125;).Limit(10).Offset(300).Find(&amp;u) count(*) 123//count(*)var count intdb.Table("user").Where("age &gt; ?",0).Count(&amp;count) 注意：这里你在指定表名的情况下sql为：select count(*) from user where age &gt; 0; 如上代码如果改为： 123var count intvar user []Userdb.Where("age &gt; ?",0).Find(&amp;user).Count(&amp;count) 相当于你先查出来[]User，然后统计这个list的长度。跟你预期的sql不相符。 group &amp; having 12345rows, _ := db.Table("user").Select("count(*),sex").Group("sex"). Having("age &gt; ?", 10).Rows()for rows.Next() &#123; fmt.Print(rows.Columns())&#125; join 1db.Table("user u").Select("u.name,u.age").Joins("left join user_ext ue on u.user_id = ue.user_id").Row() 如果有多个连接，用多个Join方法即可。 原生函数 123db.Exec("DROP TABLE user;")db.Exec("UPDATE user SET name=? WHERE id IN (?)", "xiaoming", []int&#123;11,22,33&#125;)db.Exec("select * from user where id &gt; ?",10).Scan(&amp;user) 一些函数 FirstOrInit 和 FirstOrCreate 获取第一个匹配的记录，若没有，则根据条件初始化一个新的记录： 1234//注意：where条件只能使用Struct或者map。如果这条记录不存在，那么会新增一条name=xiaoming的记录db.FirstOrInit(&amp;u,User&#123;Name:&quot;xiaoming&quot;&#125;)//同上db.FirstOrCreate(&amp;u,User&#123;Name:&quot;xiaoming&quot;&#125;) Attrs 如果没有找到记录，则使用Attrs中的数据来初始化一条记录： 123//使用attrs来初始化参数，如果未找到数据则使用attrs中的数据来初始化一条//注意：attrs 必须 要和FirstOrInit 或者 FirstOrCreate 连用db.Where(User&#123;Name:"xiaoming"&#125;).Attrs(User&#123;Name:"xiaoming",Age:12&#125;).FirstOrInit(&amp;u) Assign 12//不管是否找的到，最终返回结构中都将带上Assign指定的参数db.Where("age &gt; 12").Assign(User&#123;Name:"xiaoming"&#125;).FirstOrInit(&amp;u) Pluck 如果user表中你只想查询age这一列，该怎么返回呢，gorm提供了Pluck函数用于查询单列，返回数组： 12var ages []intdb.Find(&amp;u).Pluck("age",&amp;ages) Scan Scan函数可以将结果转存储到另一个结构体中。 123456type SubUser struct&#123; Name string Age int&#125;db.Table("user").Select("name,age").Scan(&amp;SubUser) sql.Row &amp; sql.Rows row和rows用户获取查询结果。 12345678910111213//查询一行row := db.Table("user").Where("name = ?", "xiaoming").Select("name, age").Row() // (*sql.Row)//获取一行的结果后，调用Scan方法来将返回结果赋值给对象或者结构体row.Scan(&amp;name, &amp;age)//查询多行rows, err := db.Model(&amp;User&#123;&#125;).Where("sex = ?",1).Select("name, age, phone").Rows() // (*sql.Rows, error)defer rows.Close()for rows.Next() &#123; ... rows.Scan(&amp;name, &amp;age, &amp;email) ...&#125; 日志 Gorm有内置的日志记录器支持，默认情况下，它会打印发生的错误。 12345678// 启用Logger，显示详细日志db.LogMode(true)// 禁用日志记录器，不显示任何日志db.LogMode(false)// 调试单个操作，显示此操作的详细日志db.Debug().Where("name = ?", "xiaoming").First(&amp;User&#123;&#125;)]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>Go基础学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go中sync包学习(16)]]></title>
    <url>%2Fposts%2Fa7eb32ee.html</url>
    <content type="text"><![CDATA[前面刚讲到goroutine和channel，通过goroutine启动一个协程，通过channel的方式在多个goroutine中传递消息来保证并发安全。今天我们来学习sync包，这个包是Go提供的基础包，提供了锁的支持。但是Go官方给的建议是：不要以共享内存的方式来通信，而是要以通信的手段来共享内存。所以他们是提倡使用channel的方式来实现并发控制。 学过Java的同学对锁的概念肯定不陌生，在Java中提供Sychronized关键字提供独占锁，Lock类提供读写锁。在sync包中实现的功能也是与锁相关，包中主要包含的对象有： Locker：提供了加锁和解锁的接口 Cond：条件等待通过 Wait 让例程等待，通过 Signal 让一个等待的例程继续，通过 Broadcast 让所有等待的例程继续。 Map：线程安全的map ，同时被多个goroutines调用是安全的。 Mutex：互斥锁，用来保证在任一时刻，只能有一个例程访问某对象。实现了Locker接口。Mutex 的初始值为解锁状态，Mutex 通常作为其它结构体的匿名字段使用，使该结构体具有 Lock 和 Unlock 方法 Once：Once 是一个可以被多次调用但是只执行一次，若每次调用Do时传入参数f不同，但是只有第一个才会被执行。 Pool：用于存储临时对象，它将使用完毕的对象存入对象池中，在需要的时候取出来重复使用，其中存放的临时对象随时可能被 GC 回收掉如果该对象不再被其它变量引用 RWMutex：读写互斥锁，RWMutex 比 Mutex 多了一个“读锁定”和“读解锁”，可以让多个例程同时读取某对象。RWMutex 的初始值为解锁状态。RWMutex 通常作为其它结构体的匿名字段使用。 WaitGroup ：用于等待一组例程的结束。主例程在创建每个子例程的时候先调用 Add 增加等待计数，每个子例程在结束时调用 Done 减少例程计数。之后主例程通过 Wait 方法开始等待，直到计数器归零才继续执行。 1. Mutex 互斥锁使用 我们先用Go写一段经典的并发场景： 1234567891011121314151617package mainimport ( "fmt" "time")func main() &#123; var a = 0 for i := 0;i&lt;1000;i++&#123; go func(i int) &#123; a += 1 fmt.Println(a) &#125;(i) &#125; time.Sleep(time.Second)&#125; 运行这段程序，你会发现最后输出的不是1000。 这个时候你可以使用Mutex： 123456789101112131415161718192021package mainimport ( "fmt" "sync" "time")func main() &#123; var a = 0 var lock sync.Mutex for i := 0;i&lt;1000;i++&#123; go func(i int) &#123; lock.Lock() a += 1 fmt.Println(a) lock.Unlock() &#125;(i) &#125; time.Sleep(time.Second)&#125; Mutex实现了Locker接口，所以他有Lock()方法和Unlock()方法。只需要在需要同步的代码块上下使用这两个方法就好。 Mutex等同于Java中的Synchronized关键字或者Lock。 2. 读写锁-RWMutex 类似于Java中的ReadWriteLock。读写锁有如下四个方法： 123456写操作的锁定和解锁* func (*RWMutex) Lock* func (*RWMutex) Unlock读操作的锁定和解锁* func (*RWMutex) Rlock* func (*RWMutex) RUnlock 当有一个 goroutine 获得写锁定，其它无论是读锁定还是写锁定都将阻塞直到写解锁； 当有一个 goroutine 获得读锁定，其它读锁定仍然可以继续 ； 当有一个或任意多个读锁定，写锁定将等待所有读锁定解锁之后才能够进行写锁定 。 总结上面的三句话可以得出结论： 同时只能有一个 goroutine 能够获得写锁定; 同时可以有任意多个 goroutine 获得读锁定; 同时只能存在写锁定或读锁定（读和写互斥）。 看一个读写锁的例子： 12345678910111213141516171819202122232425262728293031323334353637383940414243package mainimport ( "fmt" "strconv" "sync" "time")var ( rwLock sync.RWMutex data = "")func read(ran int) &#123; time.Sleep(time.Duration(ran) * time.Microsecond) rwLock.RLock() fmt.Printf("读操作开始:%s\n",data) data = "" rwLock.RUnlock()&#125;func write(subData string) &#123; rwLock.Lock() data = subData fmt.Printf("写操作开始：%s\n",data) rwLock.Unlock()&#125;func deduce() &#123; for i:=0;i&lt;10;i++ &#123; go write(strconv.Itoa(i)) &#125; for i:=0;i&lt;10;i++ &#123; go read(i * 100) &#125;&#125;func main() &#123; deduce() time.Sleep(2*time.Second)&#125; 运行上面的程序，会发现写操作都执行了，但是读操作不是将所有写的数字都读出来了。这是因为读操作是可以同时有多个goroutine获取锁的，但是写操作只能同时有一个goroutine执行。 3. WaitGroup WaitGroup 用于等待一组 goroutine 结束，它有三个方法： 123func (wg *WaitGroup) Add(delta int)func (wg *WaitGroup) Done()func (wg *WaitGroup) Wait() 与Java中类比的话，相似与CountDownLatch。 123456789101112131415161718192021222324252627282930313233343536package mainimport ( "fmt" "sync" "time")func goWithMountain(p int,wg *sync.WaitGroup) &#123; defer wg.Done() fmt.Printf("%d,我已经上来了\n",p)&#125;func main() &#123; var wg sync.WaitGroup wg.Add(10) for i:=0;i&lt;10;i++ &#123; go goWithMountain(i,&amp;wg) &#125; wg.Wait() time.Sleep(2*time.Second) fmt.Printf("=登山结束\n")&#125;输出：0,我已经上来了9,我已经上来了3,我已经上来了7,我已经上来了8,我已经上来了6,我已经上来了2,我已经上来了4,我已经上来了5,我已经上来了1,我已经上来了=登山结束 是不是有一样一样的呢。 4. Cond条件变量 与互斥量不同，条件变量的作用并不是保证在同一时刻仅有一个线程访问某一个共享数据，而是在对应的共享数据的状态发生变化时，通知其他因此而被阻塞的线程。条件变量总是与互斥量组合使用。互斥量为共享数据的访问提供互斥支持，而条件变量可以就共享数据的状态的变化向相关线程发出通知。 下面给出主要的几个函数： 1234func NewCond(l Locker) *Cond：用于创建条件，根据实际情况传入sync.Mutex或者sync.RWMutex的指针，一定要是指针，否则会发生复制导致锁的失效func (c *Cond) Broadcast()：唤醒条件上的所有goroutinefunc (c *Cond) Signal()：随机唤醒等待队列上的goroutine，随机的方式效率更高func (c *Cond) Wait()：挂起goroutine的操作 看一个读写操作的例子： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869package mainimport ( "bytes" "fmt" "io" "sync" "time")type MyDataBucket struct &#123; br *bytes.Buffer gmutex *sync.RWMutex rcond *sync.Cond //读操作需要用到的条件变量&#125;func NewDataBucket() *MyDataBucket &#123; buf := make([]byte, 0) db := &amp;MyDataBucket&#123; br: bytes.NewBuffer(buf), gmutex: new(sync.RWMutex), &#125; db.rcond = sync.NewCond(db.gmutex.RLocker()) return db&#125;func (db *MyDataBucket) Read(i int) &#123; db.gmutex.RLock() defer db.gmutex.RUnlock() var data []byte var d byte var err error for &#123; //读取一个字节 if d, err = db.br.ReadByte(); err != nil &#123; if err == io.EOF &#123; if string(data) != "" &#123; fmt.Printf("reader-%d: %s\n", i, data) &#125; db.rcond.Wait() data = data[:0] continue &#125; &#125; data = append(data, d) &#125;&#125;func (db *MyDataBucket) Put(d []byte) (int, error) &#123; db.gmutex.Lock() defer db.gmutex.Unlock() //写入一个数据块 n, err := db.br.Write(d) db.rcond.Broadcast() return n, err&#125;func main() &#123; db := NewDataBucket() go db.Read(1) go db.Read(2) for i := 0; i &lt; 10; i++ &#123; go func(i int) &#123; d := fmt.Sprintf("data-%d", i) db.Put([]byte(d)) &#125;(i) time.Sleep(100 * time.Millisecond) &#125;&#125; 上例中，读操作必依赖于写操作先写入数据才能开始读。当读取的数据为空的时候，会先调用wait()方法阻塞当前方法，在Put方法中写完数据之后会调用Broadcast()去广播，告诉阻塞者可以开始了。 5.Pool 临时对象池 Pool 用于存储临时对象，它将使用完毕的对象存入对象池中，在需要的时候取出来重复使用，目的是为了避免重复创建相同的对象造成 GC 负担过重。从 Pool 中取出对象时，如果 Pool 中没有对象，将返回 nil，但是如果给 Pool.New 字段指定了一个函数的话，Pool 将使用该函数创建一个新对象返回。 sync.Pool可以安全被多个线程同时使用，保证线程安全。这个Pool和我们一般意义上的Pool不太一样 ，Pool无法设置大小，所以理论上只受限于系统内存大小。Pool中的对象不支持自定义过期时间及策略，究其原因，Pool并不是一个Cache。 看一个小例子： 12345678910111213141516171819202122232425262728293031323334353637383940414243package mainimport ( "fmt" "sync")func main() &#123; //我们创建一个Pool，并实现New()函数 sp := sync.Pool&#123; New: func() interface&#123;&#125; &#123; return make([]int, 16) &#125;, &#125; item := sp.Get() fmt.Println("item : ", item) //我们对item进行操作 //New()返回的是interface&#123;&#125;，我们需要通过类型断言来转换 for i := 0; i &lt; len(item.([]int)); i++ &#123; item.([]int)[i] = i &#125; fmt.Println("item : ", item) //使用完后，我们把item放回池中，让对象可以重用 sp.Put(item) //再次从池中获取对象 item2 := sp.Get() //注意这里获取的对象就是上面我们放回池中的对象 fmt.Println("item2 : ", item2) //我们再次获取对象 item3 := sp.Get() //因为池中的对象已经没有了，所以又重新通过New()创建一个新对象，放入池中，然后返回 //所以item3是大小为16的空[]int fmt.Println("item3 : ", item3)&#125;输出：item : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]item : [0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15]item2 : [0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15]item3 : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] 6. Once 执行一次 Once 的作用是多次调用但只执行一次，Once 只有一个方法，Once.Do()，向 Do 传入一个函数，这个函数在第一次执行 Once.Do() 的时候会被调用，以后再执行 Once.Do() 将没有任何动作，即使传入了其它的函数，也不会被执行，如果要执行其它函数，需要重新创建一个 Once 对象。 看一个很简单的例子： 123456789101112131415161718192021222324package mainimport ( "fmt" "sync")func main() &#123; var once sync.Once onceBody := func() &#123; fmt.Println("我只会出现一次") &#125; done := make(chan bool) for i := 0; i &lt; 3; i++ &#123; go func() &#123; once.Do(onceBody) done &lt;- true &#125;() &#125; for i := 0; i &lt; 3; i++ &#123; &lt;-done &#125;&#125;]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>Go基础学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go中的并发编程和goroutine(15)]]></title>
    <url>%2Fposts%2F5eb88d0d.html</url>
    <content type="text"><![CDATA[并发编程对于任何语言来说都不是一件简单的事情。Go在设计之初主打高并发，为使用者提供了goroutine，使用的方式虽然简单，但是用好却不是那么容易，我们一起来学习Go中的并发编程。 1. 并行和并发 并行(parallel)： 指在同一时刻，有多条指令在多个处理器上同时执行。 并发(concurrency)： 指在同一时刻只能有一条指令执行，但多个进程指令被快速的轮换执行，使得在宏观上具有多个进程同时执行的效果，但在微观上并不是同时执行的，只是把时间分成若干段，通过cpu时间片轮转使多个进程快速交替的执行。 2. 进程和线程 进程： 我们在操作系统中的每一次操作相当于触发了一个进程，打开一个浏览器，点开任务管理器，等等。 线程： 轻量级的进程，本质仍是进程 。独立地址空间，拥有PCB，最小分配资源单位，可看成是只有一个线程的进程；而线程是程序的最小的执行单位，有独立的PCB，线程拥有自己的栈空间，但没有独立的地址空间 ，共享当前进程的地址空间。 对操作系统来说，线程是最小的执行单元，进程是最小的资源管理单元。 无论进程还是线程，都是由操作系统所管理的。 3.协程 **协程：**coroutine，协同式程序。协程不是轻量级的线程, 协程与线程的关系并不像是线程与进程的关系。 先了解一些概念： CPU切换 在每个任务运行前， CPU 都需要知道任务从哪里加载，又从哪里开始运行。也就是说，需要系统事先给他设置好 CPU 寄存器和程序计数器（Program Counter,PC） CPU 寄存器：是 CPU 内置的容量小、但速度极快的内存 程序计数器：是用来存储 CPU 正在执行的指令位置、或者即将执行的下一条指令位置 它们都是 CPU 在运行任何任务前，必须依赖的环境，因此也被叫做 CPU 上下文。 上下文切换：就是先把前一个任务的 CPU 上下文（也就是 CPU 寄存器和程序计数器）保存起来，然后加载新任务的上下文到这些寄存器和程序计数器，最后再跳转到程序计数器所指的新位置，运行新任务。 根据任务的不同，又分为进程上下文切换、线程上下文切换、中断上下文切换。 进程的上下文切换 进程的运行态： Linux 按照特权等级，把进程的运行空间分为内核空间和用户空间 。在这两种空间中运行的进程状态分别称为内核态和用户态。 内核空间(Ring 0)：具有最高权限，可以直接访问所有资源 用户空间(Ring 3)：只能访问受限资源，不能直接访问内存等硬件设备，必须通过系统调用进入到内核中，才能访问这些特权资源 系统调用： 从用户态到内核态的转变，需要通过系统调用来完成。比如查看文件时，需要执行多次系统调用：open、read、write、close等。系统调用的过程如下： 把 CPU 寄存器里原来用户态的指令位置保存起来； 为了执行内核代码，CPU 寄存器需要更新为内核态指令的新位置，最后跳转到内核态运行内核任务； 系统调用结束后，CPU 寄存器需要恢复原来保存的用户态，然后再切换到用户空间，继续运行进程； 所以，一次系统调用的过程，其实是发生了两次 CPU 上下文切换。 什么是进程上下文切换： 进程执行终止，它之前顺颂的CPU就会被释放出来，这时就从就绪队列中取出下一个等待时间片的进程； 当某个进程的时间片耗尽，它就会被系统挂起，切换到其他等待CPU的进程运行； 某个进程因为需要的系统资源比较大(比如内存不足),这时候该进程会被挂起，系统会调度其他进程执行； 当有优先级更高的进程(系统操作进程)需要时间片，为了保证优先级更高的进程能够执行，当前进程会被挂起； 如果当前进程中有sleep函数，他也会被挂起； 进程上下文切换和系统调用的区别： 进程上下文切换：进程之间的切换，从一个进程切换到另一个；进程是由内核来管理和调度，进程的切换只发生在内核态；进程上下文切换过程中，需要将该进程的虚拟内存、栈、全局变量保存起来，以供下次使用； 系统调用：是在一个进程中的进程状态切换；切换过程中无需改变进程的虚拟内存，栈，全局变量等相关信息，从内核态到用户态相当于新开辟了一块虚拟内存； 线程的上下文切换 线程是调度的基本单位，而进程则是资源拥有的基本单位。 当进程只有一个线程时，可以认为进程就等于线程。 当进程拥有多个线程时，这些线程会共享相同的虚拟内存和全局变量等资源。这些资源在上下文切换时是不需要修改的。线程也有自己的私有数据，比如栈和寄存器等，这些在上下文切换时也是需要保存的。 线程上下文切换有两种情况： 前后两个线程属于不同进程，因为资源不共享，所以切换过程就跟进程上下文切换是一样的； 前后两个线程属于同一个进程，因为虚拟内存是共享的，所以在切换时，虚拟内存这些资源就保持不动，只需要切换线程的私有数据、寄存器等不共享的数据。 中断上下文切换 中断处理会打断进程的正常调度和执行。在打断其他进程时，需要将进程当前的状态保存下来，中断结束后，进程仍然可以从原来的状态恢复运行。 中断上下文切换并不涉及到进程的用户态。所以，即便中断过程打断了一个正处在用户态的进程，也不需要保存和恢复这个进程的虚拟内存、全局变量等用户态资源。中断上下文，其实只包括内核态中断服务程序执行所必须的状态，包括 CPU 寄存器、内核堆栈、硬件中断参数等。 在有线程的前提下，提出来协程，它到底解决了什么问题呢？ 我们知道线程的出现是为了减小进程的切换开销，提高多核的利用率。当程序运行到某个IO发送阻塞的时候，可以切换到其他线程去执行，这样不会浪费CPU时间。而线程的切换完全是通过操作系统去完成的，切换的时候一般会通过系统从用户态切换到内核态。这段话的重点是，线程是内核态的。 我们常见的代码逻辑都是被封装在一个个函数块里面。每次传递一个参数，这个函数就会从头到尾执行一遍，有对应的输出。如果在执行的过程中，发生了线程的抢占切换，那么当前线程就会保存函数当前的上下文信息（放到寄存器里面），去执行其他线程的逻辑。当这个线程重新执行时会根据之前保存的上下文信息继续执行。这段话的重点是，线程的切换需要保存函数的上下文信息。 而且现代操作系统一般都是抢占式的，所以多个线程在执行的时候在什么时候切换我们是无法控制的。所以，多线程编程时为了保证数据的准确性与安全性，我们经常需要加锁。这段话的重点是，线程的执行顺序我们无法控制，什么时候切换我们也几乎无法控制。 由于线程在运行时经常会由于IO阻塞（或者时钟阻塞）而放弃CPU，会导致我们的逻辑不能流畅的执行下去。所以，我们一般采用异步+回调的方式去执行代码。当线程与到阻塞时直接返回，继续执行下面的逻辑。同时注册一个回调函数，当内核数据准备好了之后再通知我们。这种写代码的方式其实不够直观，因为我们一般都习惯顺序执行的逻辑，一段代码能从头跑到尾那是再理想不过了。这段话的重点是，涉及到IO阻塞的多线程编程时，我们一般用异步+回调的方式来解决问题。 协程是用户态的，他是包含在线程里面的，简答来说你可以认为一个线程可以按照你的规则把自己的时间片分给多个协程去执行。 因为一个线程里面可能有多个协程，所以协程的执行需要切换，切换就需要保存当前的上下文信息（一组寄存器和调用堆栈，保存在自身的用户空间内），这样才能在再次执行的时候继续前面的工作。相比线程，协程要保存的东西都很少。 相比线程，协程的切换时机是可以控制的。我们可以告诉协程，代码执行到哪句的时候切换到哪个协程，这样就可以避免线程执行不确定性带来的安全问题，避免了各种锁机制带来的相关问题。 协程的代码看起来是同步的，不需要回调。比如说有两个协程，A协程执行到第3句就一定会切换到B协程的第4句，假如A与B里面都有循环，那展开来看其实就是A与B函数不断的顺序执行。这种感觉有点像并发，同样在一个线程上的A与B不断的切换去执行逻辑。 协程不过是一种用户级别的实现手段，他并不像线程那样有明确的概念与实体，更像是一个语言技巧。他的切换开销很小。 协程拥有自己的寄存器上下文和栈。协程调度切换时，将寄存器上下文和栈保存到其他地方，在切回来的时候，恢复先前保存的寄存器上下文和栈。因此：协程能保留上一次调用时的状态（即所有局部状态的一个特定组合），每次过程重入时，就相当于进入上一次调用的状态，换种说法：进入上一次离开时所处逻辑流的位置。在并发编程中，协程与线程类似，每个协程表示一个执行单元，有自己的本地数据，与其它协程共享全局数据和其它资源。目前主流语言基本上都选择了多线程作为并发设施，与线程相关的概念是抢占式多任务（Preemptive multitasking），而与协程相关的是协作式多任务。 不管是进程还是线程，每次阻塞、切换都需要陷入系统调用(system call)，先让CPU跑操作系统的调度程序，然后再由调度程序决定该跑哪一个进程(线程)。而且由于抢占式调度执行顺序无法确定的特点，使用线程时需要非常小心地处理同步问题，而协程完全不存在这个问题（事件驱动和异步程序也有同样的优点）。 协作式的任务，是要用户自己来负责任务的让出的。如果一个任务不主动让出，其他任务就不会得到调度。这是协程的一个弱点，但是好好的规划，这其实是一个可以变得很强大的优点。 总结一下上面的重点： 1.多线程处理，叫做抢占式多任务处理；多协程处理，叫做协作式多任务处理。 2.历史上是先有协程，但是因为它是非抢占式的，导致多任务时间片不能公平分享，所以后来全部废弃了协程改成抢占式的线程。 3.协程是用户态的，是包含在一个线程里面的多个执行单元。意味他是单线程处理的过程。（比如你的main函数比协程修饰的函数先停止，那么协程是没有执行完的）协程都没有参与多核CPU的并行处理。而线程是在多核 CPU上是受操作系统调度并行执行的。 4.由于协程可以在用户空间内切换上下文，不再需要陷入内核来做线程切换，避免了大量的用户空间和内核空间之间的数据拷贝，降低了CPU的消耗，从而避免了追求高并发时CPU早早到达瓶颈的窘境 。 5.协程本质还是单线程下处理多任务，单线程的瓶颈也是协程的瓶颈。我觉得协程最大的意义就是可以用同步方式编写异步代码 。 4. Go并发 goroutine是Go并行设计的核心。 一般会使用goroutine来处理并发任务 。goroutine是go语言中最为NB的设计，也是其魅力所在，goroutine的本质是协程，是实现并行计算的核心。它是处于异步方式运行，你不需要等它运行完成以后在执行以后的代码。 Goroutine是建立在线程之上的轻量级的抽象。它允许我们以非常低的代价在同一个地址空间中并行地执行多个函数或者方法。相比于线程，它的创建和销毁的代价要小很多，并且它的调度是独立于线程的。在Go中创建一个goroutine非常简单，使用“go”关键字即可： 1go hello(str) 先来看一个简单的例子： 1234567891011121314151617181920212223242526272829303132package mainimport ( "time")func Print() &#123; for i := 1; i &lt;= 5; i++ &#123; time.Sleep(100 * time.Millisecond) println(i) &#125;&#125;func HelloWorld() &#123; println("Hello world")&#125;func main() &#123; go Print() // 开启第一个goroutine go HelloWorld() // 开启第二个goroutine time.Sleep(2*time.Second) println("end")&#125;打印：Hello world12345end 4.1 Go中的CountDownLatch CountDownLatch是Java中的一个同步辅助类，在完成一组正在其他线程中执行的操作之前，它允许一个或多个线程一直等待。 在Go中可以使用sync包中的WaitGroup来实现一样的功能，WaitGroup 等待一组goroutinue执行完毕，主程序调用 Add 添加等待的goroutinue数量，每个goroutinue在执行结束时调用 Done ，此时等待队列数量减1，主程序通过Wait阻塞，直到等待队列为0。 123456789101112131415161718192021222324252627282930313233343536package mainimport ( "fmt" "sync")func cal(a int , b int ,n *sync.WaitGroup) &#123; c := a+b fmt.Printf("%d + %d = %d\n",a,b,c) defer n.Done() //goroutinue完成后, WaitGroup的计数-1&#125;func main() &#123; var go_sync sync.WaitGroup //声明一个WaitGroup变量 for i :=0 ; i&lt;10 ;i++&#123; go_sync.Add(1) // WaitGroup的计数加1 go cal(i,i+1,&amp;go_sync) &#125; go_sync.Wait() //等待所有goroutine执行完毕 println("主程序执行完毕")&#125;结果：0 + 1 = 11 + 2 = 39 + 10 = 193 + 4 = 74 + 5 = 92 + 3 = 55 + 6 = 116 + 7 = 137 + 8 = 158 + 9 = 17主程序执行完毕 4.2 goroutine之间的通讯–channel channel用于数据传递或数据共享，其本质是一个先进先出的队列，使用goroutine+channel进行数据通讯简单高效，同时也线程安全，多个goroutine可同时修改一个channel，不需要加锁。 channel可分为三种类型： 只读channel：只能读channel里面数据，不可写入 只写channel：只能写数据，不可读 一般channel：可读可写 注意，必须使用make 创建channel： 12c1 := make(chan int)c2 := make(chan string) channel通过操作符&lt;-来接收和发送数据 : 12c1 &lt;- str //发送数据str到c1newStr := &lt;- c1 //从str中接受数据并赋值给newStr 默认的，信道的存消息和取消息都是阻塞的 , 叫做无缓冲的信道。也就是说, 无缓冲的信道在取消息和存消息的时候都会挂起当前的goroutine，除非另一端已经准备好。 那么有缓存的channel是指在声明的时候指定该channel缓存的容量： 1ch := make(chan int, 10) 有缓存的 channel 类似一个阻塞队列(采用环形数组实现)。当缓存未满时，向 channel 中发送消息时不会阻塞，当缓存满时，发送操作将被阻塞，直到有其他 goroutine 从中读取消息；相应的，当 channel 中消息不为空时，读取消息不会出现阻塞，当 channel 为空时，读取操作会造成阻塞，直到有 goroutine 向 channel 中写入消息。 123456789101112ch := make(chan int, 3)// 读消息阻塞，因为channel为空&lt;- chch := make(chan int, 3)ch &lt;- 1ch &lt;- 2ch &lt;- 3// 存入消息阻塞，channel已满，未被读取ch &lt;- 4 channel的使用： 1.使用channel阻塞主线程，直到子goroutine完成才继续往下走。 1234567c := make(chan int)go func()&#123; //do something c &lt;- 1&#125;()doAnnotherThing()&lt;- c 2.消息传递 12345678910func test()&#123; intChan := make(chan int) go func() &#123; intChan &lt;- 1 &#125;() value := &lt;- intChan fmt.Println("value : ", value)&#125; 匿名函数中的操作产生一个值，将该值传递到主函数中去。 3.合并多个channel的输出 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657package main import ( "fmt" "time" ) func testMergeInput() &#123; input1 := make(chan int) input2 := make(chan int) output := make(chan int) //将 channel 1 和 2 中的数据输出到output中 go func(in1, in2 &lt;-chan int, out chan&lt;- int) &#123; for &#123; select &#123; case v := &lt;-in1: out &lt;- v case v := &lt;-in2: out &lt;- v &#125; &#125; &#125;(input1, input2, output) go func() &#123; for i := 0; i &lt; 10; i++ &#123; input1 &lt;- i time.Sleep(time.Millisecond * 100) &#125; &#125;() go func() &#123; for i := 20; i &lt; 30; i++ &#123; input2 &lt;- i time.Sleep(time.Millisecond * 100) &#125; &#125;() go func() &#123; for &#123; select &#123; case value := &lt;-output: fmt.Println("输出：", value) &#125; &#125; &#125;() time.Sleep(time.Second * 5) fmt.Println("主线程退出") &#125; func main()&#123; testMergeInput() &#125; 4.模拟生产者和消费者模式 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465package mainimport ( "fmt" "math/rand" "time")var( //同步控制模型，生产者模型 lockChan = make(chan int, 1) remainMoney = 1000)func testSynchronize() &#123; quit := make(chan bool, 2) go func() &#123; for i:=0; i&lt;10;i++&#123; money := (rand.Intn(12) + 1) * 100 go testSynchronize_expense(money) time.Sleep(time.Millisecond * time.Duration(rand.Intn(500))) &#125; quit &lt;- true &#125;() go func() &#123; for i:=0; i&lt;10; i++&#123; money := (rand.Intn(12) + 1) * 100 go testSynchronize_gain(money) time.Sleep(time.Millisecond * time.Duration(rand.Intn(500))) &#125; quit &lt;- true &#125;() &lt;- quit &lt;- quit fmt.Println("主程序退出")&#125;func testSynchronize_expense(money int) &#123; lockChan &lt;- 0 if(remainMoney &gt;= money)&#123; srcRemainMoney := remainMoney remainMoney -= money fmt.Printf("原来有%d, 花了%d，剩余%d\n", srcRemainMoney, money, remainMoney) &#125;else&#123; fmt.Printf("想消费%d钱不够了, 只剩%d\n", money, remainMoney) &#125; &lt;- lockChan&#125;func testSynchronize_gain(money int) &#123; lockChan &lt;- 0 srcRemainMoney := remainMoney remainMoney += money fmt.Printf("原来有%d, 赚了%d，剩余%d\n", srcRemainMoney, money, remainMoney) &lt;- lockChan&#125;func main()&#123; testSynchronize()&#125;]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>Go基础学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go中的配置文件读取的几种方式(14)]]></title>
    <url>%2Fposts%2F774afcc7.html</url>
    <content type="text"><![CDATA[日常开发中读取配置文件包含以下几种格式： json 格式字符串 K=V 键值对 xml 文件 yml 格式文件 toml 格式文件 前面两种书写简单，解析过程也比较简单。xml形式书写比较累赘，yml是树形结构，为简化配置而生，toml是一种有着自己语法规则的配置文件格式，我们一一来看使用方式，各位看官自行比较哪种更加实用。 1.读取json格式的文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107package mainimport ( "encoding/json" "fmt" "io/ioutil" "log" "sync")type Configs map[string]json.RawMessagevar configPath string = "c:/test.json"type MainConfig struct &#123; Port string `json:"port"` Address string `json:"address"`&#125;var conf *MainConfigvar confs Configsvar instanceOnce sync.Once//从配置文件中载入json字符串func LoadConfig(path string) (Configs, *MainConfig) &#123; buf, err := ioutil.ReadFile(path) if err != nil &#123; log.Panicln("load config conf failed: ", err) &#125; mainConfig := &amp;MainConfig&#123;&#125; err = json.Unmarshal(buf, mainConfig) if err != nil &#123; log.Panicln("decode config file failed:", string(buf), err) &#125; allConfigs := make(Configs, 0) err = json.Unmarshal(buf, &amp;allConfigs) if err != nil &#123; log.Panicln("decode config file failed:", string(buf), err) &#125; return allConfigs, mainConfig&#125;//初始化 可以运行多次func SetConfig(path string) &#123; allConfigs, mainConfig := LoadConfig(path) configPath = path conf = mainConfig confs = allConfigs&#125;// 初始化，只能运行一次func Init(path string) *MainConfig &#123; if conf != nil &amp;&amp; path != configPath &#123; log.Printf("the config is already initialized, oldPath=%s, path=%s", configPath, path) &#125; instanceOnce.Do(func() &#123; allConfigs, mainConfig := LoadConfig(path) configPath = path conf = mainConfig confs = allConfigs &#125;) return conf&#125;//初始化配置文件 为 struct 格式func Instance() *MainConfig &#123; if conf == nil &#123; Init(configPath) &#125; return conf&#125;//初始化配置文件 为 map格式func AllConfig() Configs &#123; if conf == nil &#123; Init(configPath) &#125; return confs&#125;//获取配置文件路径func ConfigPath() string &#123; return configPath&#125;//根据key获取对应的值，如果值为struct，则继续反序列化func (cfg Configs) GetConfig(key string, config interface&#123;&#125;) error &#123; c, ok := cfg[key] if ok &#123; return json.Unmarshal(c, config) &#125; else &#123; return fmt.Errorf("fail to get cfg with key: %s", key) &#125;&#125;func main() &#123; path := ConfigPath() fmt.Println("path: ",path) Init(path) value := confs["port"] fmt.Println(string(value))&#125; json格式文件内容： 1234&#123; "port": "7788", "address": "47.95.34.2"&#125; 运行结果： 12path: c:/test.json&quot;7788&quot; 2. 读取key=value类型的配置文件 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253package mainimport ( "bufio" "io" "os" "strings")//读取key=value类型的配置文件func InitConfig(path string) map[string]string &#123; config := make(map[string]string) f, err := os.Open(path) defer f.Close() if err != nil &#123; panic(err) &#125; r := bufio.NewReader(f) for &#123; b, _, err := r.ReadLine() if err != nil &#123; if err == io.EOF &#123; break &#125; panic(err) &#125; s := strings.TrimSpace(string(b)) index := strings.Index(s, "=") if index &lt; 0 &#123; continue &#125; key := strings.TrimSpace(s[:index]) if len(key) == 0 &#123; continue &#125; value := strings.TrimSpace(s[index+1:]) if len(value) == 0 &#123; continue &#125; config[key] = value &#125; return config&#125;func main() &#123; config := InitConfig("c:/1.txt") ip := config["ip"] port := config["port"] fmt.Println("ip=",string(ip)," port=",string(port))&#125; 配置文件类容： 12ip=127.0.0.1port=3344 运行结果： 1ip=127.0.0.1 port=3344 3. 读取yml格式文件 Java中SpringBoot支持使用yml格式的配置文件作为替代properties文件的一种方式。跟properties文件相比，好处就是层级目录，相同的前缀都在该前缀下，前缀只用写一次即可。Go也支持yml文件解析，只是麻烦的程度真的是，，，不想写！ 我们先定义一个yml文件： 12345678910port: 8080ip: 127.0.0.1host: www.baidu.comspring: redis: host: redis.dns.baidu.com port: 6379 dataBase: 0 timeout: 2000 解析代码如下： 1234567891011121314151617181920212223242526272829303132333435363738package utilsimport ( "fmt" "gopkg.in/yaml.v2" "io/ioutil")//解析yml文件type BaseInfo struct &#123; Port string `yaml:"port"` Ip string `yaml:"ip"` Host string `yaml:"host"` Spring RedisEntity `yaml:"spring"`&#125;type RedisEntity struct &#123; Redis RedisData `yaml:"redis"`&#125;type RedisData struct &#123; Host string `yaml:"host"` Port string `yaml:"port"` DataBase string `yaml:"dataBase"` Timeout string `yaml:"timeout"`&#125;func (c *BaseInfo) GetConf() *BaseInfo &#123; yamlFile, err := ioutil.ReadFile("c:/1.yml") if err != nil &#123; fmt.Println(err.Error()) &#125; err = yaml.Unmarshal(yamlFile, c) if err != nil &#123; fmt.Println(err.Error()) &#125; return c&#125; 解释一下：以上yml文件中是有三层目录结构，所以需要定义三个struct，每个struct分别包含每一层的字段。这样说你应该就能明白如何解析。 运行一下test方法来验证上面程序： 1234567891011121314package mainimport ( "fmt" "goProject/src/utils" "testing")func Test(t *testing.T) &#123; info := utils.BaseInfo&#123;&#125; conf := info.GetConf() fmt.Println(conf.Host)&#125; 可以看到将yml中的树形结构解析为BaseInfo对象。 4. 读取toml格式文件 仿佛我感觉这个比读取yml文件更为痛苦，因为在写toml文件的时候，还有一定的语法规则，仿佛在告诉你别停，继续学习。 TOML 的全称是Tom’s Obvious, Minimal Language，因为它的作者是 GitHub联合创始人Tom Preston-Werner 。 TOML 的目标是成为一个极简的配置文件格式。TOML 被设计成可以无歧义地被映射为哈希表的结构，从而可以被多种语言解析。 举个例子： 12345678910111213141516171819202122232425262728293031port=8080 [user]name="xiaoming"age=14sex=1[database]servers=["127.0.0.1","127.0.0.2","127.0.0.3"]connection_max=5000enabled=true[servers] # 你可以依照你的意愿缩进。使用空格或Tab。TOML不会在意。 [servers.a] ip="34.23.1.4" port=6379 [servers.b] ip="34.23.1.6" port=9921#嵌套[nest]data=[["n1","n2"],[1,2]]# 在数组里换行没有关系。names = [ "li", "wang"] 下面来解释一下toml格式的书写规范。 首先：TOML 是大小写敏感的。 常见的语法规则： 注释 使用 # 表示注释。 字符串 字符串以&quot;&quot;包裹，里面的字符必须是 UTF-8 格式。引号、反斜杠和控制字符（U+0000 到 U+001F）需要转义。 常用的转义序列： 123456789\b - backspace (U+0008)\t - tab (U+0009)\n - linefeed (U+000A)\f - form feed (U+000C)\r - carriage return (U+000D)\&quot; - quote (U+0022)\/ - slash (U+002F)\\ - backslash (U+005C)\uXXXX - unicode (U+XXXX) 布尔值 true false 日期 使用ISO8601格式日期： 12019-05-03T22：44：26Z 数组 数组使用方括号包裹。空格会被忽略。元素使用逗号分隔。注意，不允许混用数据类型。 12345[ 1, 2, 3 ][ &quot;red&quot;, &quot;yellow&quot;, &quot;green&quot; ][ [ 1, 2 ], [3, 4, 5] ][ [ 1, 2 ], [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;] ] # 这是可以的。[ 1, 2.0 ] # 注意：这是不行的。 数值类型 数值类型严格区分整数和浮点数。这两个不是一种类型。 字典对象 多个kv集合在一起组成字典对象。字典对象的名字用[]包含起来，单独作为一行。 解释一下这种格式对应的json格式： 123456[servers.a] ip=&quot;34.23.1.4&quot; port=6379 &#123; &quot;servers&quot;: &#123; &quot;a&quot;: &#123; &quot;ip&quot;: &quot;34.23.1.4&quot;,&quot;port&quot;:6379&#125;&#125;&#125; 我们写一个小程序来解析上面的toml文件： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152package utilsimport ( "fmt" "github.com/BurntSushi/toml" "io/ioutil" "os")type BaseData struct &#123; Db DataBase `toml:"dataBase"` Se Servers `toml:"servers"`&#125;type DataBase struct &#123; Servers []string `toml:"servers"` ConnectionMax int `toml:"connection_max"` Enabled bool `toml:"enabled"`&#125;type Servers struct &#123; A ServerEn `toml:"a"` B ServerEn `toml:"b"`&#125;type ServerEn struct &#123; IP string `toml:"ip"` Port int `toml:"port"`&#125;func ReadConf(fname string) (p *BaseData, err error) &#123; var ( fp *os.File fcontent []byte ) p = new(BaseData) if fp, err = os.Open(fname); err != nil &#123; fmt.Println("open error ", err) return &#125; if fcontent, err = ioutil.ReadAll(fp); err != nil &#123; fmt.Println("ReadAll error ", err) return &#125; if err = toml.Unmarshal(fcontent, p); err != nil &#123; fmt.Println("toml.Unmarshal error ", err) return &#125; return&#125; 注意看我定义的对象，也是逐层解析。 测试类： 123456789101112package mainimport ( "fmt" "goProject/src/utils" "testing")func Test(t *testing.T) &#123; p, _ := utils.ReadConf("c:/1.toml") fmt.Println(p)&#125;]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>Go基础学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go中的fmt几种输出的区别和格式化方式(13)]]></title>
    <url>%2Fposts%2Ff0360c8c.html</url>
    <content type="text"><![CDATA[在日常使用fmt包的过程中，各种眼花缭乱的print是否让你莫名的不知所措呢,更让你茫然的是各种格式化的占位符。。简直就是噩梦。今天就让我们来征服格式化输出，做一个会输出的Goer。 fmt.Print有几个变种： 12345Print: 输出到控制台,不接受任何格式化操作Println: 输出到控制台并换行Printf : 只可以打印出格式化的字符串。只可以直接输出字符串类型的变量（不可以输出别的类型）Sprintf：格式化并返回一个字符串而不带任何输出Fprintf：来格式化并输出到 io.Writers 而不是 os.Stdout 1. 通用的占位符 12345%v 值的默认格式。%+v 类似%v，但输出结构体时会添加字段名%#v 相应值的Go语法表示 %T 相应值的类型的Go语法表示 %% 百分号,字面上的%,非占位符含义 默认格式%v下，对于不同的数据类型，底层会去调用默认的格式化方式： 1234567bool: %t int, int8 etc.: %d uint, uint8 etc.: %d, %x if printed with %#vfloat32, complex64, etc: %gstring: %schan: %p pointer: %p 如果是复杂对象的话,按照如下规则进行打印： 1234struct: &#123;field0 field1 ...&#125; array, slice: [elem0 elem1 ...] maps: map[key1:value1 key2:value2] pointer to above: &amp;&#123;&#125;, &amp;[], &amp;map[] 示例: 123456789101112131415161718192021222324252627282930313233343536373839package mainimport ( "fmt" "strconv")type User struct &#123; Name string Age int&#125;func (User) GetUser(user User) string&#123; return user.Name + " " + strconv.Itoa(user.Age)&#125;func main() &#123; user := User&#123;"xiaoming", 13&#125; //Go默认形式 fmt.Printf("%v",user) fmt.Println() //类型+值对象 fmt.Printf("%#v",user) fmt.Println() //输出字段名和字段值形式 fmt.Printf("%+v",user) fmt.Println() //值类型的Go语法表示形式 fmt.Printf("%T",user) fmt.Println() fmt.Printf("%%")&#125;输出：&#123;xiaoming 13&#125;main.User&#123;Name:"xiaoming", Age:13&#125;&#123;Name:xiaoming Age:13&#125;main.User% 2. 常用类型 2.1 整数类型： 123456789%b 二进制表示 %c 相应Unicode码点所表示的字符 %d 十进制表示 %o 八进制表示 %q 单引号围绕的字符字面值，由Go语法安全地转义 %x 十六进制表示，字母形式为小写 a-f %X 十六进制表示，字母形式为大写 A-F %U Unicode格式：123，等同于 &quot;U+007B&quot; 示例： 12345678910111213141516171819202122232425262728293031323334package mainimport ( "fmt")func main() &#123; fmt.Printf("%b",123) fmt.Println() fmt.Printf("%c",123) fmt.Println() fmt.Printf("%d",123) fmt.Println() fmt.Printf("%0",123) fmt.Println() fmt.Printf("%q",123) fmt.Println() fmt.Printf("%x",123) fmt.Println() fmt.Printf("%X",123) fmt.Println() fmt.Printf("%U",123) fmt.Println()&#125;输出：1111011&#123;123%!(NOVERB)%!(EXTRA int=123)'&#123;'7b7BU+007B 2.2 浮点数 1234567891011%b 无小数部分、二进制指数的科学计数法，如-123456p-78； 参见strconv.FormatFloat %e 科学计数法，如-1234.456e+78 %E 科学计数法，如-1234.456E+78 %f 有小数部分但无指数部分，如123.456 %F 等价于%f %g 根据实际情况采用%e或%f格式（以获得更简洁、准确的输出） %e 科学计数法，例如 -1234.456e+78 %E 科学计数法，例如 -1234.456E+78 %f 有小数点而无指数，例如 123.456 %g 根据情况选择 %e 或 %f 以产生更紧凑的（无末尾的0）输出 %G 根据情况选择 %E 或 %f 以产生更紧凑的（无末尾的0）输出 示例： 12345678910111213141516171819202122232425262728package mainimport ( "fmt")func main() &#123; fmt.Printf("%b",12675757563.5345432567) fmt.Println() fmt.Printf("%e",12675757563.5345432567) fmt.Println() fmt.Printf("%E",12675757563.5345432567) fmt.Println() fmt.Printf("%f",12675757563.5345432567) fmt.Println() fmt.Printf("%g",12675757563.5345432567) fmt.Println() fmt.Printf("%G",12675757563.5345432567) fmt.Println()&#125;输出：6645747581470399p-191.267576e+101.267576E+1012675757563.5345441.2675757563534544e+101.2675757563534544E+10 2.3 布尔型 1%t true 或 false 2.4 字符串 1234%s 字符串或切片的无解译字节 %q 双引号围绕的字符串，由Go语法安全地转义 %x 十六进制，小写字母，每字节两个字符 %X 十六进制，大写字母，每字节两个字符 示例： 1234567891011121314151617181920212223package mainimport ( "fmt")func main() &#123; //user := User&#123;"xiaoming", 13&#125; fmt.Printf("%s","I'm a girl") fmt.Println() fmt.Printf("%q","I'm a girl") fmt.Println() fmt.Printf("%x","I'm a girl") fmt.Println() fmt.Printf("%X","I'm a girl") fmt.Println()&#125;输出：I'm a girl"I'm a girl"49276d2061206769726c49276D2061206769726C 2.5 指针 1%p 十六进制表示，前缀 0x 示例： 123456789101112131415package mainimport ( "fmt")func main() &#123; a := 1 b := &amp;a fmt.Printf("%p",b)&#125;输出：0xc00000c0a8指针的地址 2.6 其他标志 12345+ 总打印数值的正负号；对于%q（%+q）保证只输出ASCII编码的字符。 - 左对齐 # 备用格式：为八进制添加前导 0（%#o），为十六进制添加前导 0x（%#x）或0X（%#X），为 %p（%#p）去掉前导 0x；对于 %q，若 strconv.CanBackquote 返回 true，就会打印原始（即反引号围绕的）字符串；如果是可打印字符，%U（%#U）会写出该字符的Unicode编码形式（如字符 x 会被打印成 U+0078 &apos;x&apos;）。 &apos; &apos; （空格）为数值中省略的正负号留出空白（% d）；以十六进制（% x, % X）打印字符串或切片时，在字节之间用空格隔开 0 填充前导的0而非空格；对于数字，这会将填充移到正负号之后 示例： 12345678910111213141516171819202122func main() &#123; str := `duduud ffff nnnnn` fmt.Printf("%d",323) fmt.Println() fmt.Printf("%s",str) fmt.Println() fmt.Printf("%s %d","aaaa",10) fmt.Println() fmt.Printf("%s\n%d","aaaa",10)&#125;输出：323duduud ffff nnnnnaaaa 10aaaa10 2.7 格式化错误的提示 格式化错误．所有的错误都始于“%!”，有时紧跟着单个字符（占位符），并以小括号括住的描述结尾。 123func main() &#123; fmt.Printf("%s",2) //%%!s(int=2)&#125;]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>Go基础学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go中的反射reflect(12)]]></title>
    <url>%2Fposts%2F1a78a9f1.html</url>
    <content type="text"><![CDATA[前面我们在学习到struct结构体的时候，因为结构体中的字段首字母大写，而我们想把json文件映射到该结构体上时，需要在在结构体字段后面加上json标签，表明结构体字段和json字段的映射关系。这其中就用到了反射的方式去获取标签，取出该标签对应的json字段然后存储到结构体字段上。 Go语言中提供了反射的包为reflect，我们先来看一下常见的反射使用方式，在去做原理性说明。 在 reflect 包中，主要通过两个函数TypeOf() 和ValueOf()实现反射，TypeOf()获取到的结果是reflect.Type 类型，ValueOf()获取到的结果是reflect.Value类型。 1. 理解反射的类型（Type） reflect.TypeOf()返回的是Type类型，Type中包含了一个对象会有的相关信息，对象名，对象类型，对象的方法，对象中的属性等等。 reflect.Type中的方法： 12345678910111213141516171819202122232425262728293031323334func (t *rtype) String() string // 获取 t 类型的字符串描述，不要通过 String 来判断两种类型是否一致。func (t *rtype) Name() string // 获取 t 类型在其包中定义的名称，未命名类型则返回空字符串。func (t *rtype) PkgPath() string // 获取 t 类型所在包的名称，未命名类型则返回空字符串。func (t *rtype) Kind() reflect.Kind // 获取 t 类型的类别。func (t *rtype) Size() uintptr // 获取 t 类型的值在分配内存时的大小，功能和 unsafe.SizeOf 一样。func (t *rtype) Align() int // 获取 t 类型的值在分配内存时的字节对齐值。func (t *rtype) FieldAlign() int // 获取 t 类型的值作为结构体字段时的字节对齐值。func (t *rtype) NumMethod() int // 获取 t 类型的方法数量。func (t *rtype) NumField() int //返回一个struct 类型 的属性个数，如果非struct类型会抛异常func (t *rtype) Method() reflect.Method // 根据索引获取 t 类型的方法，如果方法不存在，则 panic。// 如果 t 是一个实际的类型，则返回值的 Type 和 Func 字段会列出接收者。// 如果 t 只是一个接口，则返回值的 Type 不列出接收者，Func 为空值。func (t *rtype) MethodByName(string) (reflect.Method, bool) // 根据名称获取 t 类型的方法。func (t *rtype) Implements(u reflect.Type) bool // 判断 t 类型是否实现了 u 接口。func (t *rtype) ConvertibleTo(u reflect.Type) bool // 判断 t 类型的值可否转换为 u 类型。func (t *rtype) AssignableTo(u reflect.Type) bool // 判断 t 类型的值可否赋值给 u 类型。func (t *rtype) Comparable() bool // 判断 t 类型的值可否进行比较操作//注意对于：数组、切片、映射、通道、指针、接口 func (t *rtype) Elem() reflect.Type // 获取元素类型、获取指针所指对象类型，获取接口的动态类型 有个方法是Elem()，获取元素类型、获取指针所指对象类型，获取接口的动态类型。对指针类型进行反射的时候，可以通过reflect.Elem()获取这个指针指向元素的类型。 12345678910111213141516171819202122232425262728package mainimport ( "fmt" "reflect" "strconv")type User struct &#123; Name string Age int&#125;func (User) GetUser(user User) string&#123; return user.Name + " " + strconv.Itoa(user.Age)&#125;func main() &#123; user := &amp;User&#123;"xiaoming", 13&#125; of := reflect.TypeOf(user) elem := reflect.TypeOf(user).Elem() fmt.Println(of.Kind(),of.Name()) fmt.Println(elem.Kind(),elem.Name())&#125;结果：ptr struct User 从上面的结果来看，TypeOf(user)的种类为ptr，Go中反射对所有的指针变量的种类都是ptr,但是指针变量的类型名称是空的。 通过Elem()方法可以得到指针指向的元素的类型和名称，得到User的类型为struct，名称为User。 1.1 通过反射获取结构体成员类型 1234567891011121314151617181920212223242526272829303132333435363738394041package mainimport ( "fmt" "reflect" "strconv")type User struct &#123; Name string Age int&#125;func (User) GetUser(user User) string&#123; return user.Name + " " + strconv.Itoa(user.Age)&#125;func main() &#123; user := User&#123;"xiaoming", 13&#125; //反射获取对象类型,字段类型 userType := reflect.TypeOf(user) fmt.Println(userType.Name(),userType.Kind()) for i := 0; i &lt; userType.NumField(); i++ &#123; fieldType := userType.Field(i) fmt.Printf("name: %v tag: '%v'\n", fieldType.Name, fieldType.Tag) &#125; if name, ok := userType.FieldByName("Name"); ok &#123; fmt.Println(name) &#125;&#125;输出信息：User structname: Name tag: ''name: Age tag: ''&#123;Name string 0 [0] false&#125; 在上面的代码中，NumField()方法获取结构体类型中的属性个数，通过Field(i)方法来获取结构体中的属性，返回的是StructField类型的结构体。该对象中包含如下信息： 123456789type StructField struct &#123; Name string // 字段名 PkgPath string // 字段路径 Type Type // 字段反射类型对象 Tag StructTag // 字段的结构体标签 Offset uintptr // 字段在结构体中的相对偏移 Index []int // Type.FieldByIndex中的返回的索引值 Anonymous bool // 是否为匿名字段&#125; 可以看到有我们关注的字段名，字段类型，还有tag类型等等。 2. 通过反射获取对象值 获取对象值通过ValueOf()方法来实现。 12345678910111213141516171819202122232425262728293031323334package mainimport ( "fmt" "reflect" "strconv")type User struct &#123; Name string Age int&#125;func (User) GetUser(user User) string&#123; return user.Name + " " + strconv.Itoa(user.Age)&#125;func main() &#123; user := User&#123;"xiaoming", 13&#125; //反射获取对象值 elem := reflect.ValueOf(user) fmt.Println(elem) for i := 0; i &lt; elem.NumField(); i++ &#123; field := elem.Field(i) i2 := field.Type() fmt.Println(i2.Name(), " ", field) &#125;&#125;输出：&#123;xiaoming 13&#125;string xiaomingint 13 2.1 创建对象和调用方法 12345678910111213141516171819202122232425262728293031323334353637383940414243package mainimport ( "fmt" "reflect" "strconv")type User struct &#123; Name string Age int&#125;func (User) GetUser(user User) string&#123; return user.Name + " " + strconv.Itoa(user.Age)&#125;func main() &#123; user1 := User&#123;"xiaoming", 13&#125; user := User&#123;&#125; //反射获取对象值 elem := reflect.TypeOf(user) //创建一个实例 value:= reflect.New(elem).Elem() value.Field(0).SetString("xiaohong") value.Field(1).SetInt(15) fmt.Println(value.Field(0),value.Field(1)) of := reflect.ValueOf(user) params := make([]reflect.Value,1) params[0] = reflect.ValueOf(user1) //调用方法，传递参数 call := of.Method(0).Call(params) fmt.Println(call)&#125;输出：xiaohong 15[xiaoming 13]]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>Go基础学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go中的interface学习(11)]]></title>
    <url>%2Fposts%2F36a6c519.html</url>
    <content type="text"><![CDATA[学过Java的同学都知道在Java中接口更像是一种规范，用接口定义了一组方法，下面实现这个接口的类只管按照写好的方法名和返回值去实现就好，内部如何实现是各个方法自己的事情，接口本身不关注。 另外Java中实现接口的类必须显式的声明实现了哪个接口： implement InterfaceName,仔细思考一下会有如下问题: 如果你修改了接口名，那么类也得跟着修改； 你必须先定义接口，才能去实现它； 一个类可以实现多个接口，Java中接口的设计更像是弥补继承的不足，如果你希望实现类再实现一个接口，那么的继续修改实现类。 带着以上几个问题，我们先看Go中如何去使用Interface，再去探讨理论的问题。 1.定义interface 在Go中接口是用type关键字定义，所以可以说interface是一种具有一组方法的类型，这些方法定义了interface的行为。当然，更突出的是Go允许不带任何方法的interface，这种类型的interface叫做empty interface。 下面的例子展示一个结构体实现了两个接口，但是一眼看上去你无法得知他实现了哪两个接口。 12345678910111213141516171819202122type Person interface &#123; GetAge() int GetName() string&#125;type Car interface &#123; GetAge() int GetName() string&#125;type Student struct &#123; age int name string&#125;func (s Student) GetAge() int &#123; return s.age&#125;func (s Student) GetName() string &#123; return s.name&#125; 上面有两个接口Person,Car。Stuent结构体有两个方法和这两个结构体中的方法一样，这就表示Student实现两这两个接口，当然你可以试一下将其中的一个方法注释掉。如果你用的GoLand 编辑器，你会发现编辑器左边的Student身上带着的上限的绿色箭头消失了，表示当前结构体没有实现任何方法。 综上：接口中有几个方法，实现者必须完全实现这些方法才表示当前实现的结构体或者自定义类型实现了这个接口。这一点跟Java中并无区别。 另外你是否也发现，其实你可以先去实现方法，在定义接口，因为接口跟实现之间完全没有耦合关系，接口是根据适用方的需求来定义的，完全不必要关心是否有其他地方定义过。 2.使用interface 先来看如下一段代码： 123456789101112131415161718192021222324252627282930313233343536package mainimport "fmt"type Get interface &#123; Get_name() string&#125;type Person struct &#123; age int16 name string&#125;type Student struct &#123; no int16 name string&#125;func (p Person) Get_name() string &#123; return p.name&#125;func (s Student) Get_name() string &#123; return s.name&#125;func Get_name(get Get) string &#123; return get.Get_name()&#125;func main() &#123; s := Student&#123;12345,"xiaoming"&#125; name := Get_name(s) fmt.Println(name)&#125; 上面定义了一个接口Get,分别定义了两个结构体都去实现了这个接口，那么如何去通过接口帮我们隐藏当前调用的哪个具体的实现呢，关键就在Get_name方法，这个方法的参数是接口本身，里面的实现你可以通过接口去调用他的任何方法，这些调用目前为止跟任何实现没有任何关系。 注意到下面我们在main方法中定义了一个Student类型的对象s，然后将s传入Get_name方法中，这就表示当前接口的实现者是Student，那么他走的就是Student相关的实现逻辑。即当前Get接口类型的变量get存储的是一个Student对象。 3. 如何判断 interface 变量存储的是哪种类型 现实开发中可能会有一种需求：因为Get_name是每个实现者自己去实现的方法，那么如果想在Get_name方法外面加点料，即需要判断当前来的实现者是哪种类型然后针对某种类型的实现者加点料，该如何做呢。 123456789func Get_name(get Get) string &#123; switch get.(type) &#123; case Person: fmt.Println("person") case Student: fmt.Println("student") &#125; return get.Get_name()&#125; 注意：interface.(type)方法只能在switch中实现，出了switch方法你就不能用.的方式调出来这个方法了。 .(type)表示当前我们要断言的类型。 4. 空的interface interface{} 是一个空的 interface 类型，根据前文的定义：一个类型如果实现了一个 interface 的所有方法就说该类型实现了这个 interface，空的 interface 没有方法，所以可以认为所有的类型都实现了 interface{}。如果定义一个函数参数是 interface{} 类型，这个函数应该可以接受任何类型作为它的参数。 123func doAnything(i interface&#123;&#125;)&#123; &#125; 这是一个很重要的特性。 4.1 使用空interface作为返回参数接收任一类型返回值 我们来看下面这个例子： 1234567891011121314151617181920212223242526272829type BaseInfo struct&#123; userId int userName string&#125;type StudentInfo struct&#123; BaseInfo Options []string&#125;type StaffInfo struct&#123; StudentInfo cardId string&#125;func checkData(id int) (interface&#123;&#125; , bool) &#123; data1 ,ok1 := CheckStaffData(id) // 检查员工信息是否正确，返回StaffInfo data2 ,ok2 := CheckStudentData(id) // 检查学生信息是否正确，返回StudentInfo if ok1 &#123; return data1,ok1 &#125; if ok2 &#123; return data2,ok2 &#125; return nil ,false&#125; 在checkData方法中我们使用了接口作为第一个返回值的接收参数。任何类型都可以。那么下一个问题来了，我们如何去解析用接口接收的这个返回吹呢？还记得上面用 switch interface.(type)判断的例子吧，所以代码可以这样写： 12345678910111213141516171819func getData() &#123; if data,ok := fetchQuestion(22); ok &#123; switch data.(type) &#123; case StaffInfo: fmt.Println("staffInfo") case StudentInfo: fmt.Println("student") case A: fmt.Println("A") case B: fmt.Println("B") ... ... ... &#125; &#125; &#125; 有没有发现这个switch很有可能会成为巨无霸。 在Java中是如何解决这个问题的呢？Java提供了继承和接口的方法，通过定义一个抽象类，或者一个接口，让每一个case的逻辑去实现这个抽象类或者接口，使用工厂模式去加载需要的实现类即可。每次有变动我们只用去更改工厂类，新增新的实现，getData方法可以不用变动。 那么在Go中如何实现呢？ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263type InfoType interface &#123; getType() int getData(s string)&#125;type BaseInfo struct&#123; userId int userName string returnType int8&#125;type StudentInfo struct&#123; BaseInfo Options []string&#125;type StaffInfo struct&#123; StudentInfo cardId string&#125;func (b BaseInfo) getType() int8 &#123; return b.returnType&#125;func (b BaseInfo) getData(s string) &#123; b.getData(s)&#125;func fetchQuestion(id int) (InfoType , bool) &#123; data1 ,ok1 := CheckStaffData(id) // 检查员工信息是否正确，返回StaffInfo data2 ,ok2 := CheckStudentData(id) // 检查学生信息是否正确，返回StudentInfo if ok1 &#123; return data1,ok1 &#125; if ok2 &#123; return data2,ok2 &#125; return nil ,false&#125;func getData() &#123; var s string if data,ok := fetchQuestion(22); ok &#123; switch data.getType() &#123; case A: s = A case B: s = B case C: s = C &#125; data.getData(s) &#125;&#125; 在上面代码中，把人员类型抽象成了一个接口，定义了两个方法： 获取类型； 根据类型选择不同的实现； BaseInfo和StudentInfo可以分别去实现这个接口。 最关键的是getData方法的修改，case中只用对type赋值即可，最后由getData方法根据type去调用不同的实现。简化了操作。 5. []interface{}的使用 上面刚说interface{}作为参数陈可以承接任何类型，那么[]interface{}作为参数是不是也可以承接任何类型的数组呢？先来试验一下。 12345678910111213141516package mainimport ("fmt")func getAll(vals []interface&#123;&#125;) &#123; for _, val := range vals &#123; fmt.Println(val) &#125;&#125;func main() &#123; names := []string&#123;"a", "b", "b"&#125; getAll(names)&#125; 如果你用的是GoLand，你会发现检查就报错，还没到编译期。 interface{}可以表示任一类型，不表示[]interface{}可以表示任意类型的数组，它只是表示这是一个数组，里面你想装啥都可以。 正确的使用[]interface{}的姿势是这样的，你需要new 一个interface类型的数组对象，将想要的元素塞进这个数组就可以。 123456789101112131415161718192021222324package mainimport ("fmt")func getAll(vals []interface&#123;&#125;) &#123; for _, val := range vals &#123; fmt.Println(val) &#125;&#125;func main() &#123; names := []string&#123;"a", "b", "b"&#125; c := 3 v := make([]interface&#123;&#125;,4) for i,value :=range names &#123; v[i] = value &#125; v[3] = c getAll(v) fmt.Println(v)&#125;]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>Go基础学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go中的命名规范(10)]]></title>
    <url>%2Fposts%2F29755826.html</url>
    <content type="text"><![CDATA[1.命名规范 1.1 Go是一门区分大小写的语言。 命名规则涉及变量、常量、全局函数、结构、接口、方法等的命名。 Go语言从语法层面进行了以下限定：任何需要对外暴露的名字必须以大写字母开头，不需要对外暴露的则应该以小写字母开头。 当命名（包括常量、变量、类型、函数名、结构字段等等）以一个大写字母开头，如：Analysize，那么使用这种形式的标识符的对象就可以被外部包的代码所使用（客户端程序需要先导入这个包），这被称为导出（像面向对象语言中的 public）； 命名如果以小写字母开头，则对包外是不可见的，但是他们在整个包的内部是可见并且可用的（像面向对象语言中的 private ） 1.2 包名称 保持package的名字和目录保持一致，尽量采取有意义的包名，简短，有意义，尽量和标准库不要冲突。包名应该为小写单词，不要使用下划线或者混合大小写。 12package domainpackage main 1.3 文件命名 尽量采取有意义的文件名，简短，有意义，应该为小写单词，使用下划线分隔各个单词。 1approve_service.go 1.4 结构体命名 采用驼峰命名法，首字母根据访问控制大写或者小写 struct 申明和初始化格式采用多行，例如下面： 12345type MainConfig struct &#123; Port string `json:"port"` Address string `json:"address"`&#125;config := MainConfig&#123;"1234", "123.221.134"&#125; 1.5 接口命名 命名规则基本和上面的结构体类型 单个函数的结构名以 “er” 作为后缀，例如 Reader , Writer 。 123type Reader interface &#123; Read(p []byte) (n int, err error)&#125; 1.6 变量命名 和结构体类似，变量名称一般遵循驼峰法，首字母根据访问控制原则大写或者小写，但遇到特有名词时，需要遵循以下规则： 如果变量为私有，且特有名词为首个单词，则使用小写，如 appService 若变量类型为 bool 类型，则名称应以 Has, Is, Can 或 Allow 开头 1234var isExist boolvar hasConflict boolvar canManage boolvar allowGitHook bool 1.7常量命名 常量均需使用全部大写字母组成，并使用下划线分词 1const APP_URL = &quot;https://www.baidu.com&quot; 如果是枚举类型的常量，需要先创建相应类型： 123456type Scheme stringconst ( HTTP Scheme = &quot;http&quot; HTTPS Scheme = &quot;https&quot;) 2. 错误处理 错误处理的原则就是不能丢弃任何有返回err的调用，不要使用 _ 丢弃，必须全部处理。接收到错误，要么返回err，或者使用log记录下来 尽早return：一旦有错误发生，马上返回 尽量不要使用panic，除非你知道你在做什么 错误描述如果是英文必须为小写，不需要标点结尾 采用独立的错误流进行处理 12345678910111213// 错误写法if err != nil &#123; // error handling&#125; else &#123; // normal code&#125;// 正确写法if err != nil &#123; // error handling return // or continue, etc.&#125;// normal code 3. 单元测试 单元测试文件名命名规范为 example_test.go 测试用例的函数名称必须以 Test 开头，例如：TestExample 每个重要的函数都要首先编写测试用例，测试用例和正规代码一起提交方便进行回归测试 。]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>Go基础学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go中的结构体(9)]]></title>
    <url>%2Fposts%2F83520626.html</url>
    <content type="text"><![CDATA[前面我们或多或少的都使用了结构体这种数据结构，本身结构体也有很多特性，我们一一来看。 结构体的作用是将一个或者多个任一类型的变量组合在一起的数据类型，类似于我们在Java中class的作用。在结构体重也可以嵌套结构体。结构体还可以有自己的方法。 1.定义结构体 我们先定义一个结构体： 结构体定义如下： 1234type 标识符 struct &#123; field1 type field2 type&#125; 例子： 123456type Staff struct &#123; UserId int16 UserName string Sex byte Age int8&#125; 2. 使用结构体 有三种方式可以使用结构体: 123var staff Staffstaff1 := new(Staff)staff2 := &amp;Staff&#123;&#125; 上面2和3的效果是一样的，返回的都是指向结构体的指针。 Go中的结构体不像class一样有构造函数，一般会使用上述第三种方式来构造出一个结构体对象，简称Go中的工厂模式： 123456789101112131415161718192021222324package mainimport "fmt"type Staff struct &#123; UserId int16 UserName string Sex byte Age int8&#125;func NewStaff(userId int16, userName string, sex byte, age int8) *Staff &#123; return &amp;Staff&#123; UserId:userId, UserName:userName, Sex:sex, Age:age, &#125;&#125;func main() &#123; staff := NewStaff(123,"xiaoming",byte(1),13) fmt.Println(staff)&#125; 3. 带标签的结构体 结构体中的字段除了有名字和类型外，还可以有一个可选的标签（tag）：它是一个附属于字段的字符串，可以是文档或其他的重要标记。 标签的内容不可以在一般的编程中使用，只有通过反射机制才能能获取它。 我们现在有这样一段程序：从json文件中读取json字符串，然后转为json对象： json文件内容： 1234&#123; &quot;port&quot;: &quot;7788&quot;, &quot;address&quot;: &quot;47.95.34.2&quot;&#125; 代码如下： 1234567891011121314151617181920212223242526272829303132333435package mainimport ( "encoding/json" "fmt" "io/ioutil" "log")type MainConfig struct &#123; port string address string&#125;func LoadConfig(path string) *MainConfig &#123; buf, err := ioutil.ReadFile(path) if err != nil &#123; log.Panicln("load config conf failed: ", err) &#125; mainConfig := &amp;MainConfig&#123;&#125; err = json.Unmarshal(buf, mainConfig) if err != nil &#123; log.Panicln("decode config file failed:", string(buf), err) &#125; fmt.Println(mainConfig.address,mainConfig.port) return mainConfig&#125;func main() &#123; LoadConfig("c:/test.json")&#125; 执行以上程序可以发现打印出来的结果是空的，原因是：Go开发规范认为：只有开头是大写字母的对象，方法才被认为是公开的，可以在包外访问，否则就是私有的，外部对象无法访问。 那我们定义结构体大写之后，但是想让结构体中的字段json格式化为小写应该怎么做呢？这时候就可以通过tag来指定： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546package mainimport ( "encoding/json" "fmt" "io/ioutil" "log")type MainConfig struct &#123; Port string `json:"port"` Address string `json:"address"`&#125;//反射获取字段中的tagfunc reflectTag(mg MainConfig) &#123; mgType := reflect.TypeOf(mg) tag0 := mgType.Field(0).Tag tag1 := mgType.Field(0).Tag fmt.Println(tag0,tag1)&#125;func LoadConfig(path string) *MainConfig &#123; buf, err := ioutil.ReadFile(path) if err != nil &#123; log.Panicln("load config conf failed: ", err) &#125; mainConfig := &amp;MainConfig&#123;&#125; err = json.Unmarshal(buf, mainConfig) if err != nil &#123; log.Panicln("decode config file failed:", string(buf), err) &#125; bytes, err := json.Marshal(mainConfig) fmt.Println(string(bytes)) return mainConfig&#125;func main() &#123; LoadConfig("c:/test.json") config := MainConfig&#123;"1234", "123.221.134"&#125; reflectTag(config)&#125;打印结果：&#123;"port":"7788","address":"47.95.34.2"&#125;json:"port" json:"port" 上面的代码是一段从json文件中读取配置文件的例子，也使用了反射机制来获取tag标签，使用 json.Unmarshal来反序列化，使用json.Marshal将对象序列化为json字符串。 4.方法 其实Go中任何自定义的类型都可以有方法，不仅仅是struct才有。除了指针和interface。 看一个自定义类型带方法的例子： 12345678910111213141516171819202122232425262728293031323334353637package mainimport "fmt"type MainConfig1 struct &#123; Port string `json:"port"` Address string `json:"address"`&#125;type Str stringfunc (s Str) Compact(str string,str1 string) string &#123; return str + str1&#125;func (s *Str) Compact1(str string,str1 string) Str &#123; return Str(str + str1)&#125;//mf相当于其他语言中的this，self表示当前对象本身func (mf MainConfig1) Compact2() Str &#123; return Str(mf.Port + "|" + mf.Address)&#125;func main() &#123; var s Str compact1 := s.Compact("a", "b") str := s.Compact1("c", "d") fmt.Println(compact1,str) var mf MainConfig1 mf.Port = "2" mf.Address = "3333" mf.Compact2()&#125; 声明 Str类型的s，通过s可以调用这两个方法。另外还记得Go的访问控制规范吧，首字母大写表示公共方法，小写表示私有，自定义类型的方法同样遵循这个原则，试着把 Compact1方法的首字母改为小写，你会发现通过 s.Compact1(&quot;c&quot;, &quot;d&quot;)找不到方法。 再看Compact2()方法的使用，使用当前接收者本身来获取属性，跟Java中的this关键字相似。 上面还有一个疑问点： 使用 Str 和使用 *Str作为接受者有什么不同呢？ 区别就在于：在接收者是指针时，方法可以改变接收者的值（或状态） 。 我们来改造一下Compact2()方法： 123456789101112func (mf MainConfig1) Compact2() Str &#123; mf.Address = "rrrr" return Str(mf.Port + "|" + mf.Address)&#125;var mf MainConfig1mf.Port = "2"mf.Address = "3333"compact2 := mf.Compact2()fmt.Println(compact2)bytes, _ := json.Marshal(mf)fmt.Println(string(bytes)) 输出的结果是： 122|rrrr&#123;&quot;port&quot;:&quot;2&quot;,&quot;address&quot;:&quot;3333&quot;&#125; 函数作用域内的Address值被改变了，但是实际上mf对象的属性值并没有被改变。 那么我们将receiver 改为指针试一下： 123456789101112func (mf *MainConfig1) Compact2() Str &#123; mf.Address = "rrrr" return Str(mf.Port + "|" + mf.Address)&#125;var mf MainConfig1mf.Port = "2"mf.Address = "3333"compact2 := mf.Compact2()fmt.Println(compact2)bytes, _ := json.Marshal(mf)fmt.Println(string(bytes)) 输出的结果是： 122|rrrr&#123;&quot;port&quot;:&quot;2&quot;,&quot;address&quot;:&quot;rrrr&quot;&#125; 会发现这次mf的属性值也会改变。因为本身我们使用的this对象就是 *MainConfig1自然可以改变对象内部的值。 5. 函数和方法的区别 函数将变量作为参数：Function1(recv) 方法在变量上被调用：recv.Method1() 在接收者是指针时，方法可以改变接收者的值（或状态） receiver_type 叫做 （接收者）基本类型，这个类型必须在和方法同样的包中被声明。 在 Go 中，（接收者）类型关联的方法不写在类型结构里面，就像类那样；耦合更加宽松；类型和方法之间的关联由接收者来建立。]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>Go基础学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go中的文件读写(8)]]></title>
    <url>%2Fposts%2Fb58bd9f6.html</url>
    <content type="text"><![CDATA[在 Go 语言中，文件使用指向 os.File 类型的指针来表示的，也叫做文件句柄 。我们来看一下os包的使用方式。 1.读取文件 os包提供了两种打开文件的方法： 12Open(name string) (*File, error)func OpenFile(name string, flag int, perm FileMode) (*File, error) 第一个方法是以只读的方式去打开文件，如果文件不存在或者程序没有足够的权限打开这个文件，Open函数会返回错误； 第二个方法会自由一些，你可以设置打开文件的方式，以及文件的操作权限，后面我们详细说明。 1234567891011121314151617181920212223242526272829303132package mainimport ( "fmt" "os")func main() &#123; file, e := os.Open("c:/1.txt") if e != nil &#123; fmt.Println(e) &#125; buf := make([]byte,1024) for &#123; len, _ := file.Read(buf) if len == 0 &#123; break &#125; fmt.Println(string(buf)) &#125; buf1 := make([]byte,1024) offset := 0 for &#123; len1, _ := file.ReadAt(buf1, int64(offset)) offset = offset + len1 if len1 == 0 &#123; break &#125; fmt.Println(string(buf1)) &#125; file.Close()&#125; 这是一个简单的打开文件读取数据的例子。 file.Read()方法是直接将文件内容读取到指定大小的byte数组中，由源码可知如果byte数组大于1G，那么默认一次最大可以读取1G大小的数据。 file.ReadAt()方法可以手动指定每次读取位置的偏移量。而不是默认设置。 我们再看OpenFile方法： 12345678910111213141516171819202122package mainimport ( "fmt" "os")func main() &#123; //以读写方式打开文件，如果不存在，则创建 openFile, e := os.OpenFile("c:/1.txt", os.O_RDWR|os.O_CREATE, 777) if e != nil &#123; fmt.Println(e) &#125; buf := make([]byte,1024) for &#123; len, _ := openFile.Read(buf) if len == 0 &#123; break &#125; fmt.Println(string(buf)) &#125; openFile.Close()&#125; OpenFile函数的第二个参数是文件的打开模式： 123456789101112const ( // Exactly one of O_RDONLY, O_WRONLY, or O_RDWR must be specified. O_RDONLY int = syscall.O_RDONLY // 只读模式 O_WRONLY int = syscall.O_WRONLY //只写模式 O_RDWR int = syscall.O_RDWR // 读写混合模式 // The remaining values may be or'ed in to control behavior. O_APPEND int = syscall.O_APPEND // 写模式的时候将数据附加到文件末尾 O_CREATE int = syscall.O_CREAT // 文件如果不存在就新建 O_EXCL int = syscall.O_EXCL // 和 O_CREATE模式一起使用, 文件必须不存在 O_SYNC int = syscall.O_SYNC //打开文件用于同步 I/O. O_TRUNC int = syscall.O_TRUNC // 打开文件时清空文件) 前面三种是文件打开模式，后面五种是打开文件之后相应的操作模式；前面三个你只能选择一个，后面可以多选，中间用&quot;|&quot;隔开。 OpenFile函数的第三个参数是文件的权限，跟linux文件权限一致： 123r ——&gt; 004w ——&gt; 002x ——&gt; 001 通常情况如果你只是读文件操作，权限是可以被忽略的，第三个参数可以传0。而在写文件的时候，就需要传666，以确保你有足够的权限执行写入。 2. 写入文件 上面我们用到了OpenFile，可以指定文件打开的方式，如果使用了只写或者读写模式，表示可以写文件。另外control模式选择的不同对你写文件的影响也是大有不同的,比如： os.O_RDWR|os.O_CREATE ： 文件不存在会新建文件，文件如果存在，会从文件开始处用新内容覆盖原始内容，(如果新内容只有5个字符，原始内容有10个，那么只有开始5个是新内容，后面5个还是以前的内容) os.O_RDWR|os.O_APPEND ： 本次写入的值会在文件末尾进行append操作，不会覆盖以前的内容。 os.O_RDWR|os.O_TRUNC ： 打开文件的时候先清空文件。 1234567891011121314151617package mainimport ( "fmt" "os")func main() &#123; openFile, e := os.OpenFile("c:/1.txt", os.O_RDWR|os.O_CREATE|os.O_TRUNC, 777) if e != nil &#123; fmt.Println(e) &#125; str := "overwrite to file" openFile.WriteString(str) openFile.Close()&#125; 上面是打开文件写内容，当然我们也可以创建文件写内容： 1234567891011121314151617181920212223242526272829package mainimport ( "bufio" "fmt" "os")func main() &#123; f, err := os.Create("c:/1.txt") if err != nil &#123; fmt.Println(err) return &#125; defer f.Close() content := map[string]string&#123; "hello": "111", "world": "222", "world1": "333", "world2": "444", &#125; bw := bufio.NewWriter(f) for k, v := range content &#123; bw.WriteString(k + ":" + v + "\n") &#125; bw.Flush()&#125; 3. 使用bufio读取文件 上面演示了按照字节流去读文件的方式，Go还提供了一个io缓冲区：bufio,可以先将文件读取到缓冲区中，然后从缓冲区读取数据： 1234567891011121314151617181920212223242526272829303132333435363738package mainimport ( "bufio" "fmt" "io" "os")func main() &#123; openFile, e := os.OpenFile("c:/1.txt", os.O_RDWR|os.O_CREATE, 777) if e != nil &#123; fmt.Println(e) &#125; reader := bufio.NewReader(openFile) //按行读取 for &#123; line, _, e := reader.ReadLine() if e == io.EOF &#123; break &#125; if e != nil &#123; fmt.Println(e) &#125; fmt.Println(string(line)) &#125; //按照指定分隔符读取 for&#123; s, e := reader.ReadString('\n') fmt.Println(s) if e == io.EOF &#123; break &#125; if e != nil &#123; fmt.Println(e) &#125; &#125; openFile.Close()&#125; 4. 读取压缩文件 compress包提供了读取压缩文件的功能，支持的压缩文件格式为：bzip2、flate、gzip、lzw 和 zlib。 archive包提供了tar，zip格式的压缩解压缩功能。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134package mainimport ( "archive/zip" "fmt" "io" "os" "strings")//压缩文件//files 文件数组，可以是不同dir下的文件或者文件夹//dest 压缩文件存放地址func Compress(files []*os.File, dest string) error &#123; d, _ := os.Create(dest) defer d.Close() w := zip.NewWriter(d) defer w.Close() for _, file := range files &#123; err := compress(file, "", w) if err != nil &#123; return err &#125; &#125; return nil&#125;func compress(file *os.File, prefix string, zw *zip.Writer) error &#123; info, err := file.Stat() if err != nil &#123; return err &#125; if info.IsDir() &#123; prefix = prefix + "/" + info.Name() fileInfos, err := file.Readdir(-1) if err != nil &#123; return err &#125; for _, fi := range fileInfos &#123; f, err := os.Open(file.Name() + "/" + fi.Name()) if err != nil &#123; return err &#125; err = compress(f, prefix, zw) if err != nil &#123; return err &#125; &#125; &#125; else &#123; header, err := zip.FileInfoHeader(info) header.Name = prefix + "/" + header.Name if err != nil &#123; return err &#125; writer, err := zw.CreateHeader(header) if err != nil &#123; return err &#125; _, err = io.Copy(writer, file) file.Close() if err != nil &#123; return err &#125; &#125; return nil&#125;//解压func DeCompress(zipFile, dest string) error &#123; reader, err := zip.OpenReader(zipFile) if err != nil &#123; return err &#125; defer reader.Close() for _, file := range reader.File &#123; rc, err := file.Open() if err != nil &#123; return err &#125; defer rc.Close() filename := dest + file.Name err = os.MkdirAll(getDir(filename), 0666) if err != nil &#123; return err &#125; w, err := os.Create(filename) if err != nil &#123; return err &#125; defer w.Close() _, err = io.Copy(w, rc) if err != nil &#123; return err &#125; w.Close() rc.Close() &#125; return nil&#125;func getDir(path string) string &#123; return subString(path, 0, strings.LastIndex(path, "/"))&#125;func subString(str string, start, end int) string &#123; rs := []rune(str) length := len(rs) if start &lt; 0 || start &gt; length &#123; panic("start is wrong") &#125; if end &lt; start || end &gt; length &#123; panic("end is wrong") &#125; return string(rs[start:end])&#125;func main() &#123; DeCompress("c:/1.zip","c:/") f1, err := os.Open("c:/1.txt") if err != nil &#123; fmt.Println(err) &#125; defer f1.Close() f2, err := os.Open("c:/2.txt") if err != nil &#123; fmt.Println(err) &#125; files := []*os.File&#123;f1,f2&#125; Compress(files,"c:/")&#125; 5.使用ioutil读写文件 ioutil包是Go官方给用户提供的一个io工具类，提供了一些封装好的方法让我们直接调用。 12345678910111213141516171819202122232425262728293031323334353637package mainimport ( "fmt" "io/ioutil")func main() &#123; //读取文件 bytes, e := ioutil.ReadFile("c:/1.txt") if e != nil &#123; fmt.Println(e) &#125; fmt.Println(string(bytes)) //读取文件夹 infos, e := ioutil.ReadDir("c:/1") if e != nil &#123; fmt.Println(e) &#125; for _,v := range infos &#123; fmt.Println(v.Name()) &#125; //向指定文件写入数据，如果文件不存在，则创建文件，写入数据之前清空文件 ioutil.WriteFile("c:/1.txt", []byte("xxxxxxxxx"), 666) //在当前目录下，创建一个以test为前缀的临时文件夹，并返回文件夹路径 name, e := ioutil.TempDir("c:/2", "tmp") fmt.Println(name) //在当前目录下，创建一个以test为前缀的文件，并以读写模式打开文件，并返回os.File指针 f, e := ioutil.TempFile("c:/2", "tmpFile") f.WriteString("tmp word") f.Close()&#125;]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>Go基础学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go包管理工具dep(7)]]></title>
    <url>%2Fposts%2F9f983d87.html</url>
    <content type="text"><![CDATA[dep是一个golang依赖管理工具，需要在Go 1.7及更高的版本中使用。 1. 安装 安装dep工具的方式有很多种，如果是mac电脑的话，只需要如下命令： 1brew install dep 对于Linux和类Unix系统而言，我们还可以使用如下方式安装dep： 1curl https://raw.githubusercontent.com/golang/dep/master/install.sh | sh 或者直接使用源码安装。 而对于windows电脑，可以使用命令行： 1go get -u github.com/golang/dep/cmd/dep 会自动下载到go_path/bin/目录下，你需要将go_path/bin 加入到环境变量中。 如果这种方式下载不下来，你也可以手动下载：点击这里，挑选适合你的版本，然后修改文件名为dep.exe放入go_path/bin/目录下。 2. 使用 安装完毕再命令行下敲入dep命令： 1234567891011121314151617181920$ depDep is a tool for managing dependencies for Go projects Usage: "dep [command]" Commands: init Set up a new Go project, or migrate an existing one status Report the status of the project's dependencies ensure Ensure a dependency is safely vendored in the project prune Pruning is now performed automatically by dep ensure. version Show the dep version information Examples: dep init set up a new project dep ensure install the project's dependencies dep ensure -update update the locked versions of all dependencies dep ensure -add github.com/pkg/errors add a dependency to the project Use "dep help [command]" for more information about a command. 如上： init命令用来初始化项目； status命令用来查看当前项目的依赖状态； ensure命令用来同步依赖包 3. 初始化 切换到你的工程目录下: 1cd $GOPATH/src/goProject 假设项目现在是空的，什么也没有,进行初始化: 1dep init -v 注意：因为墙的原因，，，不一定我们能成功的拉下来依赖，使用dep init 尽量带上-v： 1-v enable verbose logging (default: false) 如果半分钟没响应就ctrl+c中断，就可以看到fail的原因，dep遇到一些错误不会立马报错(特别是网络原因) 。 执行成功之后会生成两个文件 Gopkg.lock、Gopkg.toml和一个文件夹vendor Gopkg.toml文件记录着current project依赖项project的约束。 Gopkg.toml参数解释： [[constraint]]： 这个约束主要体现在到底要采用目标project的某个tag的版本（version），还是某个branch，或者是某个revision，这三个对于一个constraint只能选一个。 [[override]]:有时项目依赖比较复杂，经常会遇到依赖冲突导致 dep ensure 命令无法执行成功，这个时候使用 override 消除单个依赖关系上多个不可调和的constraint声明之间的分歧 [[required]]：列出了必须包含在Gopkg.lock中的一组包 [[ignored]]：列出dep静态分析源代码时忽略的一组包 [[prune]]：prune为依赖关系定义全局和每个项目的prune选项。 这些选项决定写入vendor/时丢弃哪些文件 unused-packages:修剪掉来自于目录中，但是没有出现在包导入图中的文件 non-go：修剪掉非.go文件 go-tests：修剪掉Go的测试文件 Gopkg.lock文件是工具生成的，你不用手工编辑 vendor文件里面存放current project的远程依赖的源代码 当需要指定目标project使用哪一个version时，可以在Gopkg.toml中添加。如，需要指定alice版本为0.8.4，在Gopkg.toml中添加： 123[[constraint]] name = "github.com/golang/dep" version = "=0.8.4" 然后执行 1dep ensure -update "github.com/golang/dep 在指定version的时候，如果指定semantic version，可选的符号有 =: 只选择对应version &gt;或&lt;: 大于(或小于)对应版本号 &gt;=或&lt;=: 大于等于(或小于等于)对应版本号 ~: ~1.2.3表示 &gt;=1.2.3,&lt;1.3.0 ^: ^1.2.3表示 &gt;1.2.3,&lt;2.0.0 不指定符号的话，默认为^符号。 有了包管理工具之后，好处还是挺多： 帮你锁住依赖版本，防止第三方包升级导致代码不兼容； 将项目的依赖都放在vendor下，就不用依赖gopath下的公共包了； 4. 添加依赖 依赖管理帮助 1`dep help ensure` 添加一条依赖 1`dep ensure -add github.com``/bitly/go-simplejson` 一次性添加多条依赖 1`dep ensure -add github.com``/pkg/errors` `github.com``/bitly/go-simplejson` 添加依赖指定依赖版本 1`dep ensure -add github.com``/bitly/go-simplejson``@=0.4.3` 添加后记住执行dep ensure确保同步 1`dep ensure -``v` 如果执行dep ensure出错，看下Gopkg.toml文件中是否同时配置了version，branch和revision。]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>Go基础学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go标准库--net/http学习(6)]]></title>
    <url>%2Fposts%2F57b0fe8.html</url>
    <content type="text"><![CDATA[Go中对网络的支持提供了标准库，net包提供了可移植的网络I/O接口，包括TCP/IP、UDP、域名解析和Unix域socket。 http包提供了HTTP客户端和服务端的实现。 一般我们用http肯定多一些，下面来看一下http的使用方式。 1. Post和Get请求的使用 创建一个最简单的get请求： 1234567891011121314151617package mainimport ( "fmt" "io/ioutil" "net/http")func main() &#123; resp, err := http.Get("http://www.baidu.com") if err != nil &#123; fmt.Print("err",err) &#125; closer := resp.Body bytes, err := ioutil.ReadAll(closer) fmt.Println(string(bytes))&#125; 执行程序可以获取百度首页的源码。 再看一个post请求的格式： 123456789101112131415161718192021package mainimport ( "bytes" "fmt" "io/ioutil" "net/http")func main() &#123; url := "http://www.baidu.com/comment/add" body := "&#123;\"userId\":223446,\"articleId\":443567,\"comment\":\"我是一条评论\"&#125;" response, err := http.Post(url, "•application/x-www-form-urlencoded", bytes.NewBuffer([]byte(body))) if err != nil &#123; fmt.Println("err",err) &#125; b1, err := ioutil.ReadAll(response.Body) fmt.Println(string(b1))&#125; 上面的post请求第一个参数是url地址,第二个参数是Content-Type ，第三个参数是post请求参数的字节码。 Http Header里的Content-Type一般有这三种： application/x-www-form-urlencoded：数据被编码为名称/值对。这是标准的编码格式。 multipart/form-data： 数据被编码为一条消息，页上的每个控件对应消息中的一个部分。 text/plain： 数据以纯文本形式(text/json/xml/html)进行编码，其中不含任何控件或格式字符。postman软件里标的是RAW。 网页中form的enctype属性为编码方式，常用有两种： application/x-www-form-urlencoded，默认编码方式 multipart/form-data Go还提供了另一种Post请求的api： 12345678func PostForm(url string, data url.Values) (resp *Response, err error) &#123; return DefaultClient.PostForm(url, data)&#125;//DefaultClient.PostForm(url, data)的实现如下：func (c *Client) PostForm(url string, data url.Values) (resp *Response, err error) &#123; return c.Post(url, "application/x-www-form-urlencoded", strings.NewReader(data.Encode()))&#125; PostForm()方法默认“application/x-www-form-urlencoded”的Content-Type类型，不用你手动去填写Content-Type了。 2. 如果想在请求中手动set header头信息该怎么做 http包提供了Header类型： 1type Header map[string][]string 用于请求头信息的获取和填充。 你可以自己定义Header： 123headers := http.Header&#123;"token": &#123;"fdase34532534fwr324brfh3urhf839hf349h"&#125;&#125;headers.Add("Accept-Charset","UTF-8")headers.Set("Host","www.baidu.com") 注意：Header是 map[string][]string类型的，value为字符数字。 3.NewRequest 和Client 在http包中也提供了一个叫做Client的结构体，它实现了Get,Post,等方法。并且还提供了一个默认的变量可以直接使用： 1var DefaultClient = &amp;Client&#123;&#125; NewRequest是一个方法： 1func NewRequest(method, url string, body io.Reader) (*Request, error)&#123;&#125; 第一个参数为请求类型，“GET”，“POST”，“PUT”，“DELETE”，等等。 如果body参数实现了io.Closer接口，Request返回值的Body 字段会被设置为body，并会被Client类型的Do、Post和PostFOrm方法以及Transport.RoundTrip方法关闭。 观看源码我们可以发现，因为NewRequest是一个通用方法，我们调用Get,Post其实是Go帮我们在底层传了相应的method去调用NewRequest， 所以最核心的http请求源码就是NewRequest。 另外，Client的Get和Post方实现里面去调用NewRequest，而http里面的Get和Post方法其实是分别调用Client的Get和Post方法的。 http的Post方法: 123func Post(url, contentType string, body io.Reader) (resp *Response, err error) &#123; return DefaultClient.Post(url, contentType, body)&#125; 上面使用了DefaultClient变量。 Client的Post方法： 12345678func (c *Client) Post(url, contentType string, body io.Reader) (resp *Response, err error) &#123; req, err := NewRequest("POST", url, body) if err != nil &#123; return nil, err &#125; req.Header.Set("Content-Type", contentType) return c.Do(req)&#125; 直接使用NewRequest方法。 4. 服务端的实现 前面说的这些发送Get或者Post请求，其实是相当于我们在模拟客户端的实现调用服务端接口。那么类比Java 的Controller层，使用Go应该如何实现呢？http.HandlerFuncWWE我们提供了路由注册功能。 1type HandlerFunc func(ResponseWriter, *Request) HandlerFunc type是一个适配器，通过类型转换让我们可以将普通的函数作为HTTP处理器使用。如果f是一个具有适当签名的函数，HandlerFunc(f)通过调用f实现了Handler接口（因为HandlerFunc实现了ServeHTTP函数），其实就是将函数f显示转换成HandlerFunc类型。 4.1 如何创建web服务端 123456789101112131415package mainimport ( "net/http")func SayHello(w http.ResponseWriter, req *http.Request) &#123; w.Write([]byte("Hello"))&#125;func main() &#123; http.HandleFunc("/hello", SayHello) http.ListenAndServe(":8080", nil)&#125; 首先调用Http.HandleFunc 按顺序做了几件事： 调用了DefaultServerMux的HandleFunc 调用了DefaultServerMux的Handle 往DefaultServeMux的map[string]muxEntry中增加对应的handler和路由规则 其次调用http.ListenAndServe(“:8080”, nil) 按顺序做了几件事情： 实例化Server 调用Server的ListenAndServe() 调用net.Listen(“tcp”, addr)监听端口 启动一个for循环，在循环体中Accept请求 对每个请求实例化一个Conn，并且开启一个goroutine为这个请求进行服务go c.serve() 读取每个请求的内容w, err := c.readRequest() 判断header是否为空，如果没有设置handler（这个例子就没有设置handler），handler就设置为DefaultServeMux 调用handler的ServeHttp 在这个例子中，下面就进入到DefaultServerMux.ServeHttp 根据request选择handler，并且进入到这个handler的ServeHTTP 1mux.handler(r).ServeHTTP(w, r) 选择handler： 12345A 判断是否有路由能满足这个request（循环遍历ServerMux的muxEntry）B 如果有路由满足，调用这个路由handler的ServeHttpC 如果没有路由满足，调用NotFoundHandler的ServeHttp 上面我们注意到： 1ListenAndServe(addr string, handler Handler) 其实是有两个参数的：当前监听端口号，事件处理器handler。 Handler接口的定义方式： 123type Handler interface &#123; ServeHTTP(ResponseWriter, *Request)&#125; 我们只需要实现这个接口就可以定义自己的handler，Go语言在自带包中已经帮我们提供了实现这个接口的公共方法： 123456type HandlerFunc func(ResponseWriter, *Request)// ServeHTTP calls f(w, r).func (f HandlerFunc) ServeHTTP(w ResponseWriter, r *Request) &#123; f(w, r)&#125; Go语言将func(ResponseWriter, *Request)这种类型的函数直接定义了类型HandlerFunc，而且还实现了ServeHTTP这个方法，但是这个方法本身并没有实现任何逻辑，需要我们自己来实现。 123456789101112131415package mainimport ( "net/http")func myHandler(w http.ResponseWriter, r *http.Request) &#123; w.Write([]byte("Hello World"))&#125;func main() &#123; http.HandleFunc("/hello", SayHello) http.ListenAndServe(":8080", http.HandlerFunc(myHandler))&#125; 类比的话，handler机制类似于Java SpringMVC中的Interceptor，是一个拦截器的性质。它发生在http.HandleFunc处理逻辑之前。 我们可以来实现一个小功能：只有指定referer头来的请求才能调用服务，否则返回403。 我们先定义一个结构体: 1234type SpecialRefer struct &#123; handler http.Handler referer string&#125; 包含两个对象：handler和自定义的referer。 因为我们需要将这个SpecialRefer实例化并传递给ListenAndServe这个方法，因此它必须实现ServeHTTP这个方法，所以在ServeHTTP里面可以直接定义我们用来实现中间件的逻辑。 1234567func (this *SpecialRefer) ServeHTTP(w http.ResponseWriter, r *http.Request) &#123; if r.Referer() == this.referer &#123; this.handler.ServeHTTP(w, r) &#125; else &#123; w.WriteHeader(403) &#125;&#125; 取出当前请求头中的referer信息，如果跟我们约定的不同则拦截请求。 完整代码如下： 1234567891011121314151617181920212223242526272829303132333435package mainimport ( "net/http")type SpecialRefer struct &#123; handler http.Handler referer string&#125;func (this *SpecialRefer) ServeHTTP(w http.ResponseWriter, r *http.Request) &#123; if r.Referer() == this.referer &#123; this.handler.ServeHTTP(w, r) &#125; else &#123; w.WriteHeader(403) &#125;&#125;func myHandler(w http.ResponseWriter, r *http.Request) &#123; w.Write([]byte("this is handler"))&#125;func hello(w http.ResponseWriter, r *http.Request) &#123; w.Write([]byte("hello"))&#125;func main() &#123; referer := &amp;SpecialRefer&#123; handler:http.HandlerFunc(myHandler), referer:"www.baidu.com", &#125; http.HandleFunc("/hello",hello) http.ListenAndServe(":8080", referer)&#125;]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>Go基础学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go连接MYSQL(5)]]></title>
    <url>%2Fposts%2F52c2bde.html</url>
    <content type="text"><![CDATA[Go原生提供了连接数据库操作的支持，在用 Golang进行开发的时候，如果需要在和数据库交互，则可以使用database/sql包。这是一个对关系型数据库的通用抽象，它提供了标准的、轻量的、面向行的接口。 在Go中访问数据库需要用到sql.DB接口：它可以创建语句(statement)和事务(transaction)，执行查询，获取结果。 使用数据库时，除了database/sql包本身，还需要引入想使用的特定数据库驱动。官方不提供实现，先下载第三方的实现，点击这里查看各种各样的实现版本。 本文测试数据库为mysql，使用的驱动为:github.com/go-sql-driver/mysql,需要引入的包为： 12"database/sql"_ "github.com/go-sql-driver/mysql" 解释一下导入包名前面的&quot;_&quot;作用： import 下划线（如：import _ github/demo）的作用：当导入一个包时，该包下的文件里所有init()函数都会被执行，然而，有些时候我们并不需要把整个包都导入进来，仅仅是是希望它执行init()函数而已。这个时候就可以使用 import _ 引用该包。 上面的mysql驱动中引入的就是mysql包中各个init()方法，你无法通过包名来调用包中的其他函数。导入时，驱动的初始化函数会调用sql.Register将自己注册在database/sql包的全局变量sql.drivers中，以便以后通过sql.Open访问。 执行数据库操作之前我们准备一张表： 12345678CREATE TABLE `user` ( `id` bigint(20) NOT NULL AUTO_INCREMENT, `name` varchar(45) DEFAULT '', `age` int(11) NOT NULL DEFAULT '0', `sex` tinyint(3) NOT NULL DEFAULT '0', `phone` varchar(45) NOT NULL DEFAULT '', PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=3 DEFAULT CHARSET=utf8; 1. 初始化数据库连接： 1234567891011DB, _ := sql.Open("mysql", "root:123456@tcp(127.0.0.1:3306)/test")//设置数据库最大连接数DB.SetConnMaxLifetime(100)//设置上数据库最大闲置连接数DB.SetMaxIdleConns(10)//验证连接if err := DB.Ping(); err != nil &#123; fmt.Println("open database fail") return&#125;fmt.Println("connnect success") sql.Open()中的数据库连接串格式为：&quot;用户名:密码@tcp(IP:端口)/数据库?charset=utf8&quot;。 DB的类型为:*sql.DB，有了DB之后我们就可以执行CRUD操作。Go将数据库操作分为两类：Query与Exec。两者的区别在于前者会返回结果，而后者不会。 Query表示查询，它会从数据库获取查询结果（一系列行，可能为空）。 Exec表示执行语句，它不会返回行。 此外还有两种常见的数据库操作模式： QueryRow表示只返回一行的查询，作为Query的一个常见特例。 Prepare表示准备一个需要多次使用的语句，供后续执行用。 2. 查询操作 1234567891011121314var user Userrows, e := DB.Query("select * from user where id in (1,2,3)")if e == nil &#123; errors.New("query incur error")&#125;for rows.Next()&#123; e := rows.Scan(user.sex, user.phone, user.name, user.id, user.age) if e != nil&#123; fmt.Println(json.Marshal(user)) &#125;&#125;rows.Close()//单行查询操作DB.QueryRow("select * from user where id=1").Scan(user.age, user.id, user.name, user.phone, user.sex) 整体工作流程如下： 使用db.Query()来发送查询到数据库，获取结果集Rows，并检查错误。 使用rows.Next()作为循环条件，迭代读取结果集。 使用rows.Scan从结果集中获取一行结果。 使用rows.Err()在退出迭代后检查错误。 使用rows.Close()关闭结果集，释放连接。 3. 增删改和Exec 通常不会约束你查询必须用Query，只是Query会返回结果集，而Exec不会返回。所以如果你执行的是增删改操作一般用Exec会好一些。Exec返回的结果是Result，Result接口允许获取执行结果的元数据: 123456type Result interface &#123; // 用于返回自增ID，并不是所有的关系型数据库都有这个功能。 LastInsertId() (int64, error) // 返回受影响的行数。 RowsAffected() (int64, error)&#125; 4. 准备查询 如果你现在想使用占位符的功能，where 的条件想以参数的形式传入，Go提供了db.Prepare语句来帮你绑定。准备查询的结果是一个准备好的语句（prepared statement），语句中可以包含执行时所需参数的占位符（即绑定值）。准备查询比拼字符串的方式好很多，它可以转义参数，避免SQL注入。同时，准备查询对于一些数据库也省去了解析和生成执行计划的开销，有利于性能。 占位符 PostgreSQL使用$N作为占位符，N是一个从1开始递增的整数，代表参数的位置，方便参数的重复使用。MySQL使用?作为占位符，SQLite两种占位符都可以，而Oracle则使用:param1的形式。 1234MySQL PostgreSQL Oracle===== ========== ======WHERE col = ? WHERE col = $1 WHERE col = :colVALUES(?, ?, ?) VALUES($1, $2, $3) VALUES(:val1, :val2, :val3) 123stmt, e := DB.Prepare("select * from user where id=?")query, e := stmt.Query(1)query.Scan() 5. 事务的使用 通过db.Begin()来开启一个事务，Begin方法会返回一个事务对象Tx。在结果变量Tx上调用Commit()或者Rollback()方法会提交或回滚变更，并关闭事务。在底层，Tx会从连接池中获得一个连接并在事务过程中保持对它的独占。事务对象Tx上的方法与数据库对象sql.DB的方法一一对应，例如Query,Exec等。事务对象也可以准备(prepare)查询，由事务创建的准备语句会显式绑定到创建它的事务。 12345678910111213141516171819//开启事务tx, err := DB.Begin()if err != nil &#123; fmt.Println("tx fail")&#125;//准备sql语句stmt, err := tx.Prepare("DELETE FROM user WHERE id = ?")if err != nil &#123; fmt.Println("Prepare fail") return false&#125;//设置参数以及执行sql语句res, err := stmt.Exec(user.id)if err != nil &#123; fmt.Println("Exec fail") return false&#125;//提交事务tx.Commit() 我们来一个完整的sql操作： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126package mainimport ( "database/sql" "encoding/json" "fmt" _ "github.com/go-sql-driver/mysql" "github.com/pkg/errors" "strings")//数据库配置const ( userName = "root" password = "123456" ip = "127.0.0.1" port = "3306" dbName = "test")//Db数据库连接池var DB *sql.DBtype User struct &#123; id int64 name string age int8 sex int8 phone string&#125;//注意方法名大写，就是publicfunc InitDB() &#123; //构建连接："用户名:密码@tcp(IP:端口)/数据库?charset=utf8" path := strings.Join([]string&#123;userName, ":", password, "@tcp(", ip, ":", port, ")/", dbName, "?charset=utf8"&#125;, "") //打开数据库,前者是驱动名，所以要导入： _ "github.com/go-sql-driver/mysql" DB, _ = sql.Open("mysql", path) //设置数据库最大连接数 DB.SetConnMaxLifetime(100) //设置上数据库最大闲置连接数 DB.SetMaxIdleConns(10) //验证连接 if err := DB.Ping(); err != nil &#123; fmt.Println("open database fail") return &#125; fmt.Println("connnect success")&#125;//查询操作func Query() &#123; var user User rows, e := DB.Query("select * from user where id in (1,2,3)") if e == nil &#123; errors.New("query incur error") &#125; for rows.Next() &#123; e := rows.Scan(user.sex, user.phone, user.name, user.id, user.age) if e != nil &#123; fmt.Println(json.Marshal(user)) &#125; &#125; rows.Close() DB.QueryRow("select * from user where id=1").Scan(user.age, user.id, user.name, user.phone, user.sex) stmt, e := DB.Prepare("select * from user where id=?") query, e := stmt.Query(1) query.Scan()&#125;func DeleteUser(user User) bool &#123; //开启事务 tx, err := DB.Begin() if err != nil &#123; fmt.Println("tx fail") &#125; //准备sql语句 stmt, err := tx.Prepare("DELETE FROM user WHERE id = ?") if err != nil &#123; fmt.Println("Prepare fail") return false &#125; //设置参数以及执行sql语句 res, err := stmt.Exec(user.id) if err != nil &#123; fmt.Println("Exec fail") return false &#125; //提交事务 tx.Commit() //获得上一个insert的id fmt.Println(res.LastInsertId()) return true&#125;func InsertUser(user User) bool &#123; //开启事务 tx, err := DB.Begin() if err != nil &#123; fmt.Println("tx fail") return false &#125; //准备sql语句 stmt, err := tx.Prepare("INSERT INTO user (`name`, `phone`) VALUES (?, ?)") if err != nil &#123; fmt.Println("Prepare fail") return false &#125; //将参数传递到sql语句中并且执行 res, err := stmt.Exec(user.name, user.phone) if err != nil &#123; fmt.Println("Exec fail") return false &#125; //将事务提交 tx.Commit() //获得上一个插入自增的id fmt.Println(res.LastInsertId()) return true&#125;func main() &#123; InitDB() Query() defer DB.Close()&#125;]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>Go基础学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go中的函数和闭包(4)]]></title>
    <url>%2Fposts%2F68e36528.html</url>
    <content type="text"><![CDATA[1. 函数参数和返回值的写法 如果有多个参数是同一个类型，可以简略写： 12345func testReturnFunc(v1,v2 int)(int,int) &#123; x1 := 2 * v1 x2 := 3 * v2 return x1,x2&#125; Go还支持命名返回值的方式。命名返回值作为结果形参（result parameters）被初始化为相应类型的零值，当需要返回的时候，我们只需要一条简单的不带参数的return语句 ： 12345func testReturnFunc1(v int)(x1,x2 int) &#123; x1 = 2 * v x2 = 3 * v return&#125; 函数返回的是x1和x2这两个值，如果x1,x2没有在函数中被赋值，那么会返回这两个参数的默认值。 2. 如何改变函数外部的变量 使用我们前面讲到的指针，得到一个外部参数的指针，将该指针作为参数传入函数中，这样对该指针赋值操作就相当于修改了该指针指向内存地址对应变量的值： 1234567891011121314func multiply(a,b int,reply *int) &#123; *reply = a *b&#125;func main() &#123; num := 0 reply := &amp;num multiply(3,5,reply) fmt.Println("return num :",*reply,num)&#125;输出：return num : 15 15 3. 将函数作为参数 函数可以作为其它函数的参数进行传递，然后在其它函数内调用执行，一般称之为回调 。 123456789101112131415161718package mainimport "fmt"func main() &#123; callback(4,add)&#125;func add(a,b int) &#123; fmt.Print(a+b)&#125;func callback(c int, f func(int, int)) &#123; f(c,c)&#125;输出：8 我理解这种回调的方式，使用场景在于多个方法之间形成调用链，下一个方法依赖上一个方法的某些值的时候，相当于callback是第一个方法，add是第二个方法，add方法需要依赖callback方法中的某些中间值。 4. 匿名函数—闭包 Go语言支持匿名函数，即函数可以像普通变量一样被传递或使用。 1i2 := func(x, y int) int &#123; return x + y &#125;(1,2) 如上，定义了一个匿名函数，包含两个参数x,y。返回x+y的结果。后面的()表示参数，参数是x=1,y=2。 当然你也可以先定义匿名函数，不适用，等你需要使用的时候，像调用函数一样传参数就可以： 12i2 := func(x, y int) int &#123; return x + y &#125;i3 := i2(x, y) 两种方式的区别就在于(x,y)参数放在哪里。 在Go语言中匿名函数和闭包是一个概念： 闭包是可以包含自由（未绑定到特定对象）变量的代码块，这些变量不在这个代码块内或者 任何全局上下文中定义，而是在定义代码块的环境中定义。要执行的代码块（由于自由变量包含 在代码块中，所以这些自由变量以及它们引用的对象没有被释放）为自由变量提供绑定的计算环 境（作用域）。 闭包的价值 闭包的价值在于可以作为函数对象或者匿名函数，对于类型系统而言，这意味着不仅要表示 数据还要表示代码。支持闭包的多数语言都将函数作为第一级对象，就是说这些函数可以存储到 变量中作为参数传递给其他函数，最重要的是能够被函数动态创建和返回。 Go语言中的闭包同样也会引用到函数外的变量。闭包的实现确保只要闭包还被使用，那么被闭包引用的变量会一直存在。 我们再看一个复杂一点的例子： 123456789101112131415func aa() &#123; a := 5 b := func()(func())&#123; c := 10 return func() &#123; fmt.Printf("a,c: %d,%d \n",a,c) a *= 3 &#125; &#125;() b() println(a)&#125;输出：a,c: 5,10 15 解释一下： 匿名函数的返回值是一个匿名函数，return返回的是一个匿名函数，注意没有加()，所以是用return接收。 然后在最外层是加了()的。所以将匿名函数的值给了b。注意了此时的b其实是一个函数。所以下面在使用的时候是b()。你可以尝试将匿名函数的最外层的()去掉，然后看一下b()打印的值是什么，在试一下b()()打印的值是什么。 另外，a在匿名函数内是可以引用的，但是你如果在匿名函数外引用c,你会发现找不到。并且在闭包内改变了a的值也是会作用到a真实的内存地址中的。]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>Go基础学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go中的指针(3)]]></title>
    <url>%2Fposts%2F102475d6.html</url>
    <content type="text"><![CDATA[学Java以来，让程序员忽略了指针和内存地址这些概念，Java帮我们封装了对象，简化了对象引用之间的关系。在Go语言中，又帮我们回忆起这些概念。 我们创建的每一个对象在内存中都有一个位置去存储，每个内存块都有一个地址表示当前位置，通常用十六进制表示，如0x24005676543。Go语言取地址的符号是&amp;。放在一个变量前使用就会返回相应变量的内存地址。 比如下面的代码： 12345var a = 3fmt.Printf("num is : %d, it's location in memory: %p\n", a, &amp;a)输出：num is : 3, it's location in memory: 0xc000062080 可以看到&amp;a输出了a的内存地址。 你可以将&amp;a赋值给一个变量,然后观察这个变量的类型，使用reflect.TypeOf()方法： 1234567var a = 3intP = &amp;ab := &amp;afmt.Println(reflect.TypeOf(b))输出：*int 可以看到&amp;a的返回类型是 *int。 我们可以将一个变量的内存地址放在一个叫做指针的特殊数据类型中，声明一个指针的方式如下: 1var p *int 然后我们可以使用p去指向一个内存地址： 1p = &amp;a 那么我们如何通过指针来获取对应内存地址的值呢，这也是有办法的。你可以在指针类型前面加上 * 号（前缀）来获取指针所指向的内容，这里的 * 号是一个类型更改器。使用一个指针引用一个值被称为间接引用。 1234var p *intvar a = 3p = &amp;afmt.Println(*p) *p打印的是a的值。 当一个指针被定义没有分配任何变量的时候，它的值为nil。 对于任何一个变量val，如下表达式都是正确的: 1var == *(&amp;var) 有如下你需要注意的是，你不能获取到一个常量或者一个没有赋值操作的文字的内存地址： 123const PI = 3.14piP := &amp;PI Cannot take the address of 'PI'p2 := &amp;3 Cannot take the address of '3' 在c语言中，指针是可以由程序员自行控制移动位置的，这无疑给程序增加了很多的不确定性，在Go中指针的概念其实更相当于是Java中的引用。它比Java中的引用高级之处在于： Java中的参数传递，传递的是值的拷贝，但是在Go中你可以使用指针来传递这个值，它直接指向了这个值的内存地址，并且只占用了4个或者8个字节。 Go 默认使用按值传递来传递参数，也就是传递参数的副本。函数接收参数副本之后，在使用变量的过程中可能对副本的值进行更改，但不会影响到原来的变量 。如果你希望函数可以直接修改参数的值，而不是对参数的副本进行操作，你需要将参数的地址（变量名前面添加&amp;符号，比如 &amp;variable）传递给函数，这就是按引用传递 。此时传递给函数的是一个指针 。]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>Go基础学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go中的字符串使用----strings和strconv(2)]]></title>
    <url>%2Fposts%2Ff86350d2.html</url>
    <content type="text"><![CDATA[Go中的字符串操作 字符串是工作中最常用的，值得我们专门的练习一下。在Go中使用strings包来操作字符串，这也是内置的包哈，不像Java中要么手写，要么引入common-lang 或者 别的第三方。另外涉及到字符串与其他类型之间的转换使用了strconv包来操作。 strings包 1. 字符串包含关系 1strings.Contains(s, substr string) bool 判断s串中是否包含substr串。 2. 判断子字符串或字符在父字符串中出现的位置（索引） 1strings.Index(s, str string) int 判断str字符串在s字符串中出现的第一个索引位置，-1 表示字符串 s 不包含字符串 str 1strings.LastIndex(s, str string) int LastIndex返回子字符串str在s中最后一次出现的索引位置。 如果要查询非ASCII编码的字符在父字符串中的位置，建议使用如下方法： 1strings.IndexRune(s string, r rune) int 3. 字符串替换 1strings.Replace(str, old, new, n) string 4. 前缀和后缀 1234判断字符串 s 是否以 prefix 开头strings.HasPrefix(s, prefix string) bool判断字符串 s 是否以 suffix 结尾strings.HasSuffix(s, suffix string) bool 5. 统计字符串出现次数 1strings.Count(s, str string) int 计算字符串 str 在字符串 s 中出现的非重叠次数 6. 重复字符串并返回一个新串 1strings.Repeat(s, count int) string 将s字符串重复count次，并返回新的字符串。 7. 修改字符串，替换某些字符 1234567剔除字符串开头和结尾的空白符号strings.TrimSpace(s)替换指定的字符strings.Trim(s, "\n")剔除开头或者结尾的字符串，该方法只在开头或者结尾剔除指定的字符strings.TrimLeft(a,"")strings.TrimRight(a,"") 8. 分割字符串 1strings.Fields(s) 会将字符串以“”空白字符作为分割线，分为字符数组。如果字符串只包含空白符号，则返回一个长度为 0 的 slice。 1strings.Split(s, sep) 自定义分割符号来对指定字符串进行分割 ,返回字符数组。 8. 修改字符串大小写 1234将字符串中的字符全部转为小写strings.ToLower(s) string将字符串中的字符全部转为大写strings.ToUpper(s) string 9. 拼接slice到字符串 1strings.Join(sl []string, sep string) string 1234c := [4]string&#123;"a","b",":","c"&#125;join := strings.Join(d, "")输出：ab:c Join 用于将元素类型为 string 的 slice 使用分割符号来拼接组成一个字符串 。 strconv-字符串与其它类型的转换 包strconv主要实现对字符串和基本数据类型之间的转换。基本数据类型包括：布尔、整型（包括有/无符号、二进制、八进制、十进制和十六进制）和浮点型等。 1.字符串和整型变量之间的转换 123func ParseInt(s string, base int, bitSize int) (i int64, err error)func ParseUint(s string, base int, bitSize int) (n uint64, err error)func Atoi(s string) (i int, err error) 将字符串解析为整数，ParseInt 支持正负号，ParseUint 不支持正负号。 base 表示进位制（2 到 36），如果 base 为 0，则根据字符串前缀判断，前缀 0x 表示 16 进制，前缀 0 表示 8 进制，否则是 10 进制。 bitSize 表示结果的位宽（包括符号位），0 表示最大位宽，取值 0、8、16、32 和 64 分别代表 int、int8、int16、int32 和 int64。 Atoi 是 ParseInt 的便捷版，内部通过调用 ParseInt(s, 10, 0) 来实现的。 在 ParseInt/ParseUint 的实现中，如果字符串表示的整数超过了 bitSize 参数能够表示的范围，则会返回 ErrRange，同时会返回 bitSize 能够表示的最大或最小值。 2.整型转为字符串类型 123func FormatUint(i uint64, base int) string // 无符号整型转字符串func FormatInt(i int64, base int) string // 有符号整型转字符串func Itoa(i int) string 其中，Itoa 内部直接调用 FormatInt(i, 10) 实现的。base 参数可以取 2~36（0-9，a-z）。 区别sprintf和itoa : 除了使用strconv包之外，我们的fmt也是可以将int打印为string： 1fm := fmt.Sprintf(&quot;%d&quot;, 111) 那么这两种方式该如何选择呢，其实我们可以看一下他们的效率： 123456789101112131415161718192021222324func testTime() &#123; now := time.Now() for i :=0;i &lt; 100000;i++&#123; fmt.Printf("%d",i) &#125; fmt.Println("=====================") fmt.Println(time.Now().Sub(now)) newTime := time.Now() for i :=0;i &lt; 100000;i++&#123; strconv.Itoa(i) &#125; fmt.Println("=====================") fmt.Println(time.Now().Sub(newTime))&#125;func main() &#123; testTime()&#125;效果还是很明显：162ms=====================3ms 另外Go跟Java中不同的是通过 s := string(77)这种方式得到的不是“77”而是77对应的ascii码值，M。 3.字符串和布尔值之间的转换 12345678// 接受 1, t, T, TRUE, true, True, 0, f, F, FALSE, false, False 等字符串；// 其他形式的字符串会返回错误func ParseBool(str string) (value bool, err error)// 直接返回 "true" 或 "false"func FormatBool(b bool) string// 将 "true" 或 "false" append 到 dst 中// 这里用了一个 append 函数对于字符串的特殊形式：append(dst, "true"...)func AppendBool(dst []byte, b bool) 4.字符串和浮点数之间的转换 123456789101112131415161718192021222324252627// FormatFloat 将浮点数 f 转换为字符串形式// f：要转换的浮点数// fmt：格式标记（b、e、E、f、g、G）// prec：精度（数字部分的长度，不包括指数部分）// bitSize：指定浮点类型（32:float32、64:float64），结果会据此进行舍入。//// 格式标记：// 'b' (-ddddp±ddd，二进制指数)// 'e' (-d.dddde±dd，十进制指数)// 'E' (-d.ddddE±dd，十进制指数)// 'f' (-ddd.dddd，没有指数)// 'g' ('e':大指数，'f':其它情况)// 'G' ('E':大指数，'f':其它情况)//// 如果格式标记为 'e'，'E'和'f'，则 prec 表示小数点后的数字位数// 如果格式标记为 'g'，'G'，则 prec 表示总的数字位数（整数部分+小数部分）// 参考格式化输入输出中的旗标和精度说明func FormatFloat(f float64, fmt byte, prec, bitSize int) string// 将字符串解析为浮点数，使用 IEEE754 规范进行舍入。// bigSize 取值有 32 和 64 两种，表示转换结果的精度。 // 如果有语法错误，则 err.Error = ErrSyntax// 如果结果超出范围，则返回 ±Inf，err.Error = ErrRangefunc ParseFloat(s string, bitSize int) (float64, error)func AppendFloat(dst []byte, f float64, fmt byte, prec int, bitSize int) 示例： 1234567891011121314151617181920212223v := 3.1415926535s32 := strconv.FormatFloat(v, 'E', -1, 32)fmt.Printf("%T, %v\n", s32, s32)s64 := strconv.FormatFloat(v, 'E', -1, 64)fmt.Printf("%T, %v\n", s64, s64)s := "0.12345678901234567890"f, err := strconv.ParseFloat(s, 32)fmt.Println(f, err) fmt.Println(float32(f), err)f, err = strconv.ParseFloat(s, 64)fmt.Println(f, err)结果：string, 3.1415927E+00string, 3.1415926535E+000.12345679104328156 &lt;nil&gt;0.12345679 &lt;nil&gt;0.12345678901234568 &lt;nil&gt;]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>Go基础学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go基础语法学习(1)]]></title>
    <url>%2Fposts%2F42334b44.html</url>
    <content type="text"><![CDATA[Go语言基础 Go是一门类似C的编译型语言，但是它的编译速度非常快。这门语言的关键字总共也就二十五个，比英文字母还少一个，这对于我们的学习来说就简单了很多。先让我们看一眼这些关键字都长什么样： 下面列举了 Go 代码中会使用到的 25 个关键字或保留字： break default func interface select case defer go map struct chan else goto package switch const fallthrough if range type continue for import return var Go程序设计的一些规则 Go之所以会那么简洁，是因为它有一些默认的行为： 大写字母开头的变量是可导出的，也就是其它包可以读取的，是公有变量；小写字母开头的就是不可导出的，是私有变量。 大写字母开头的函数也是一样，相当于class中的带public关键词的公有函数；小写字母开头的就是有private关键词的私有函数。 1. 变量、常量、Go内置类型 1.1 变量 Go语言里面定义变量有多种方式。 使用var关键字是Go最基本的定义变量方式，与C语言不同的是Go把变量类型放在变量名后面： 12//定义一个名称为“valName”，类型为"type"的变量var valName type 定义多个变量 12//定义三个类型都是“type”的变量var vname1, vname2, vname3 type 定义变量并初始化值 12//初始化“vName”的变量为“value”值，类型是“type”var vName type = value 同时初始化多个变量 12345/* 定义三个类型都是"type"的变量,并且分别初始化为相应的值 vname1为v1，vname2为v2，vname3为v3*/var vname1, vname2, vname3 type= v1, v2, v3 你是不是觉得上面这样的定义有点繁琐？没关系，因为Go语言的设计者也发现了，有一种写法可以让它变得简单一点。我们可以直接忽略类型声明，那么上面的代码变成这样了： 123456/* 定义三个变量，它们分别初始化为相应的值 vname1为v1，vname2为v2，vname3为v3 然后Go会根据其相应值的类型来帮你初始化它们*/var vname1, vname2, vname3 = v1, v2, v3 你觉得上面的还是有些繁琐？好吧，我也觉得。让我们继续简化： 123456/* 定义三个变量，它们分别初始化为相应的值 vname1为v1，vname2为v2，vname3为v3 编译器会根据初始化的值自动推导出相应的类型*/vname1, vname2, vname3 := v1, v2, v3 :=这个符号直接取代了var和type,这种形式叫做简短声明。不过它有一个限制，那就是它只能用在函数内部；在函数外部使用则会无法编译通过，所以一般用var方式来定义全局变量。 _（下划线）是个特殊的变量名，任何赋予它的值都会被丢弃。在这个例子中，我们将值2赋予b，并同时丢弃1： 1_, b := 1, 2 Go对于已声明但未使用的变量会在编译阶段报错，比如下面的代码就会产生一个错误：声明了i但未使用: 12345package mainfunc main() &#123; var i int&#125; 1.2 常量 所谓常量，也就是在程序编译阶段就确定下来的值，而程序在运行时无法改变该值。在Go程序中，常量可定义为数值、布尔值或字符串等类型。 它的语法如下： 123const constantName = value//如果需要，也可以明确指定常量的类型：const Pi float32 = 3.1415926 下面是一些常量声明的例子： 123const PI = 3.1415926const MaxThread = 10const Prefix = &quot;so_&quot; 1.3 内置基础类型 Go 语言按类别有以下几种数据类型： 布尔型 --&gt; 在Go中，布尔值的类型为bool,值只可以是常量 true 或者 false。一个简单的例子：var b bool = true； 整数型 --&gt; 整型 int 和浮点型 float，Go 语言支持整型和浮点型数字，并且原生支持复数，其中位的运算采用补码； 字符串 --&gt; 字符串就是一串固定长度的字符连接起来的字符序列。Go的字符串是由单个字节连接起来的。Go语言的字符串的字节使用UTF-8编码标识Unicode文本； 派生型 --&gt; (a) 指针类型（Pointer） (b) 数组类型 © 结构化类型(struct) (d) 联合体类型 (union) (e) 函数类型 (f ) 切片类型 (g) 接口类型（interface） (h) Map 类型 (i) Channel 类型 Boolean 12345678//示例代码var isActive bool // 全局变量声明var enabled, disabled = true, false // 忽略类型的声明func test() &#123; var available bool // 一般声明 valid := false // 简短声明 available = true // 赋值操作&#125; 数值类型 整数类型有无符号和带符号两种。Go同时支持int和uint，这两种类型的长度相同，但具体长度取决于不同编译器的实现。Go里面也有直接定义好位数的类型：rune, int8, int16, int32, int64和byte, uint8, uint16, uint32, uint64。其中rune是int32的别称，byte是uint8的别称。 需要注意的一点是，这些类型的变量之间不允许互相赋值或操作，不然会在编译时引起编译器报错。 如下的代码会产生错误：invalid operation: a + b (mismatched types int8 and int32) var a int8 var b int32 c:=a + b 另外，尽管int的长度是32 bit, 但int 与 int32并不可以互用。 浮点数的类型有float32和float64两种（没有float类型），默认是float64。 这就是全部吗？No！Go还支持复数。它的默认类型是complex128（64位实数+64位虚数）。如果需要小一些的，也有complex64(32位实数+32位虚数)。复数的形式为RE + IMi，其中RE是实数部分，IM是虚数部分，而最后的i是虚数单位。下面是一个使用复数的例子： 123var c complex64 = 5+5i//output: (5+5i)fmt.Printf("Value is: %v", c) 字符串 Go中的字符串都是采用UTF-8字符集编码,字符串是用一对双引号（&quot;&quot;）或反引号（）括起来定义，它的类型是 string。 123456789func test(a,b int) &#123; str := "hello world" m := "haha" result := str + m println(result) multStr := `hello world` println(multStr)&#125; 在Go中字符串是不可变的，例如下面的代码编译时会报错：cannot assign to s[0] 12var s string = "hello"s[0] = 'k' 如果你想要修改一个字符串怎么办呢： 12345s := "hello"c := []byte(s)c[0] = 'c's1 := string(c)println(s1) Go中可以使用 + 操作符来连接两个字符串： 1234str := "hello world"m := "haha"result := str + mprintln(result) 如果要声明一个多行的字符串怎么办？可以通过`` 来声明： 123multStr := `hello world`goprintln(multStr) 括起的字符串为Raw字符串，即字符串在代码中的形式就是打印时的形式，它没有字符转义，换行也将原样输出。例如本例中会输出： 12hello world 错误类型 Go内置有一个error类型，专门用来处理错误信息，Go的package里面还专门有一个包errors来处理错误： 123456func error() &#123; e := errors.New("this is a error demo") if e != nil &#123; println(e) &#125;&#125; 1.4 iota枚举 这个关键字用来声明enum的时候采用，它默认开始值是0，const中每增加一行加1： 1234567891011121314151617181920212223242526272829303132package mainconst ( x = iota // x == 0 y z w )const ( h, i, j = iota, iota, iota //h=0,i=0,j=0 iota在同一行值相同)const ( l = iota //a=0 m = &quot;B&quot; n = iota //2 o, p, q = iota, iota, iota //3,3,3 r = iota //4)func main() &#123; println(x,y,z,w) println(h,i,j) println(l,m,n,o,p,q,r)&#125;结果：0 1 2 30 0 00 B 2 3 3 3 4 1.5 array，slice，map array array就是数组，定义方式如下： 1var arr [n]type 在[n]type中，n表示数组的长度，type表示存储元素的类型。对数组的操作和其它语言类似，都是通过[]来进行读取或赋值： 123var arr [10]int // 声明了一个int类型的数组arr[0] = 1 // 数组下标是从0开始的arr[1] = 2 // 赋值操作 由于长度也是数组类型的一部分，因此[3]int与[4]int是不同的类型，数组也就不能改变长度。数组之间的赋值是值的赋值，即当把一个数组作为参数传入函数的时候，传入的其实是该数组的副本，而不是它的指针。如果要使用指针，那么就需要用到后面介绍的slice类型了。 数组可以使用另一种:=来声明: 12345a := [3]int&#123;1, 2, 3&#125; // 声明了一个长度为3的int数组b := [10]int&#123;1, 2, 3&#125; // 声明了一个长度为10的int数组，其中前三个元素初始化为1、2、3，其它默认为0c := [...]int&#123;4, 5, 6&#125; // 可以省略长度而采用`...`的方式，Go会自动根据元素个数来计算长度 Go支持嵌套数组，即多维数组。比如下面的代码就声明了一个二维数组： 123456// 声明了一个二维数组，该数组以两个数组作为元素，其中每个数组中又有4个int类型的元素doubleArray := [2][4]int&#123;[4]int&#123;1, 2, 3, 4&#125;, [4]int&#123;5, 6, 7, 8&#125;&#125;// 上面的声明可以简化，直接忽略内部的类型easyArray := [2][4]int&#123;&#123;1, 2, 3, 4&#125;, &#123;5, 6, 7, 8&#125;&#125; 注意： [2][4] int表示的是一个int型数组，数组内有两个数组，每个数组有四个元素组成。 slice 知道python数组的就知道slice，跟python的实现是一样的。 slice有一些简便的操作 slice的默认开始位置是0，ar[:n]等价于ar[0:n] slice的第二个序列默认是数组的长度，ar[n:]等价于ar[n:len(ar)] 如果从一个数组里面直接获取slice，可以这样ar[:]，因为默认第一个序列是0，第二个是数组的长度，即等价于ar[0:len(ar)] 下面有一些关于slice的示例： 12345678910111213141516// 声明一个数组var array = [10]byte&#123;'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j'&#125;// 声明两个slicevar aSlice, bSlice []byte// 演示一些简便操作aSlice = array[:3] // 等价于aSlice = array[0:3] aSlice包含元素: a,b,caSlice = array[5:] // 等价于aSlice = array[5:10] aSlice包含元素: f,g,h,i,jaSlice = array[:] // 等价于aSlice = array[0:10] 这样aSlice包含了全部的元素// 从slice中获取sliceaSlice = array[3:7] // aSlice包含元素: d,e,f,g，len=4，cap=7bSlice = aSlice[1:3] // bSlice 包含aSlice[1], aSlice[2] 也就是含有: e,fbSlice = aSlice[:3] // bSlice 包含 aSlice[0], aSlice[1], aSlice[2] 也就是含有: d,e,fbSlice = aSlice[0:5] // 对slice的slice可以在cap范围内扩展，此时bSlice包含：d,e,f,g,hbSlice = aSlice[:] // bSlice包含所有aSlice的元素: d,e,f,g 重要：slice是引用类型，所以当引用改变其中元素的值时，其它的所有引用都会改变该值，例如上面的aSlice和bSlice，如果修改了aSlice中元素的值，那么bSlice相对应的值也会改变。 对于slice有几个有用的内置函数： len 获取slice的长度 cap 获取slice的最大容量 append 向slice里面追加一个或者多个元素，然后返回一个和slice一样类型的slice copy 函数copy从源slice的src中复制元素到目标dst，并且返回复制的元素的个数 1234567891011121314151617181920212223242526//1.基于数组创建数组切片var array = [10]int&#123;1, 2, 3, 4, 5, 6, 7, 8, 9, 10&#125;var slice = array[1:7] //array[startIndex:endIndex] 不包含endIndex//2.直接创建数组切片slice2 := make([]int, 5, 10)//3.直接创建并初始化数组切片slice3 := []int&#123;1, 2, 3, 4, 5, 6&#125;//4.基于数组切片创建数组切片slice5 := slice3[:4]println(slice5)//5.遍历数组切片for i, v := range slice3 &#123;println(i, v)&#125;//6.len()和cap()var len = len(slice2) //数组切片的长度var cap = cap(slice) //数组切片的容量println("len(slice2) =", len)println("cap(slice) =", cap)//7.append() 会生成新的数组切片slice4 := append(slice2, 6, 7, 8)slice4 = append(slice4, slice3...)println(slice4)//8.copy() 如果进行操作的两个数组切片元素个数不一致，将会按照个数较小的数组切片进行复制copy(slice2, slice3) //将slice3的前五个元素复制给slice2println(slice2, slice3) map map就是Java中的Map，python中的字典。它的格式为 map[keyType]valueType 。 1234fruit := map[string]int&#123;"apple":5,"orange":7,"pineapple":3&#125;println(fruit)var appleCount = fruit["apple"]println(appleCount) 使用map过程中需要注意的几点： map是无序的，每次打印出来的map都会不一样，它不能通过index获取，而必须通过key获取 map的长度是不固定的，也就是和slice一样，也是一种引用类型 内置的len函数同样适用于map，返回map拥有的key的数量 map的值可以很方便的修改，通过numbers[&quot;one&quot;]=11可以很容易的把key为one的字典值改为11 map和其他基本型别不同，它不是thread-safe，在多个go-routine存取时，必须使用mutex lock机制 map的初始化可以通过key:val的方式初始化值，同时map内置有判断是否存在key的方式 1.6 make,new操作 make用于内建类型（map、slice 和channel）的内存分配。new用于各种类型的内存分配。 内建函数new本质上说跟其它语言中的同名函数功能一样：new(T)分配了零值填充的T类型的内存空间，并且返回其地址，即一个*T类型的值。用Go的术语说，它返回了一个指针，指向新分配的类型T的零值。有一点非常重要： new返回指针。 内建函数make(T, args)与new(T)有着不同的功能，make只能创建slice、map和channel，并且返回一个有初始值(非零)的T类型，而不是*T。本质来讲，导致这三个类型有所不同的原因是指向数据结构的引用在使用前必须被初始化。例如，一个slice，是一个包含指向数据（内部array）的指针、长度和容量的三项描述符；在这些项目被初始化之前，slice为nil。对于slice、map和channel来说，make初始化了内部的数据结构，填充适当的值。 make返回初始化后的（非零）值。 对于不同的数据类型，零值的意义是完全不一样的。比如，对于bool类型，零值为false；int的零值为0；string的零值是空字符串： 12345678910b := new(bool)println(*b)i := new(int)println(*i)s := new(string)println(*s)输出：false0 注意：上面最后string的输出是空值。 1.7 零值 关于“零值”，所指并非是空值，而是一种“变量未填充前”的默认值，通常为0。 此处罗列 部分类型 的 “零值” 1234567891011int 0int8 0int32 0int64 0uint 0x0rune 0 //rune的实际类型是 int32byte 0x0 // byte的实际类型是 uint8float32 0 //长度为 4 bytefloat64 0 //长度为 8 bytebool falsestring "" 1.8 一些技巧 1.分组声明 在Go语言中，同时声明多个常量、变量，或者导入多个包时，可采用分组的方式进行声明。 例如下面的代码： 12345678910import "log"import "net/http"import "strings"const a = 3const b = 2const c = 4var str = "aa"var prefix = "abc_" 可以写成如下分组形式： 12345678910111213141516import ( "log" "net/http" "strings")const( a = 2, b = 2, c = 4)var( str = "aa" prefix = "abc_") Go程序设计的一些规则 Go之所以会那么简洁，是因为它有一些默认的行为： 大写字母开头的变量是可导出的，也就是其它包可以读取的，是公有变量；小写字母开头的就是不可导出的，是私有变量。 大写字母开头的函数也是一样，相当于class中的带public关键词的公有函数；小写字母开头的就是有private关键词的私有函数。 2. 流程和函数 Go中流程控制分三大类：条件判断，循环控制和无条件跳转。 2.1 流程 if Go里面if条件判断语句中不需要括号，如下代码所示 ： 12345if x &gt; 80&#123; println("better")&#125; else &#123; println("good")&#125; Go的if还有一个强大的地方就是条件判断语句里面允许声明一个变量，这个变量的作用域只能在该条件逻辑块内，其他地方就不起作用了，如下所示 12345678910if countScore := getCountScore(); countScore &gt;= 80 &#123; println("better")&#125; else if countScore &gt;= 60 &#123; println("good")&#125; else &#123; println("e......")&#125;//这里打印countScore是找不到这个变量的println(countScore) goto Go有goto语句——请明智地使用它。用goto跳转到必须在当前函数内定义的标签。例如假设这样一个循环： 123456789101112a := 4b := 5if a &gt; b &#123; println(a * b)&#125; else &#123; Ding: a = a+12 b = a + (32/4) if a &lt; b &#123; goto Ding &#125;&#125; 注意：标签名是大小写敏感的 for Go里面的for除了基本的循环外，还可以读取slice和map的数据： 1234sum := 0for i:=0;i&lt;10;i++ &#123; sum += i&#125; for配合range可以用于读取slice和map的数据： 1234fruit := map[string]int&#123;"apple":5,"orange":7,"pineapple":3&#125;for k,v := range fruit&#123; println(k,v)&#125; 小插曲： &quot;_&quot;的使用 由于 Go 支持 “多值返回”, 而对于“声明而未被调用”的变量, 编译器会报错, 在这种情况下, 可以使用_来丢弃不需要的返回值 例如 1234fruit := map[string]int&#123;&quot;apple&quot;:5,&quot;orange&quot;:7,&quot;pineapple&quot;:3&#125;for _,v := range fruit&#123; println(v)&#125; &quot;_&quot;相当于占位符的作用，这个位置必须要有一个值来接收，但是这个值又没有用，可以用 “ _”来占着位置。 switch 跟别的语言中的switch别无他致： 123456789var sex byteswitch sex &#123;case 0: println("男生")case 1: println("女生")default: println("纳尼。。。。。")&#125; 2.2 函数 函数是Go里面的核心设计，它通过关键字func来声明，它的格式如下： 12345func funcName(input1 type1, input2 type2) (output1 type1, output2 type2) &#123; //这里是处理逻辑代码 //返回多个值 return value1, value2&#125; 上面的代码我们看出: 关键字func用来声明一个函数funcName 函数可以有一个或者多个参数，每个参数后面带有类型，通过,分隔 函数可以返回多个值 上面返回值声明了两个变量output1和output2，如果你不想声明也可以，直接就两个类型 如果只有一个返回值且不声明返回值变量，那么你可以省略 包括返回值 的括号 如果没有返回值，那么就直接省略最后的返回信息 如果有返回值， 那么必须在函数的外层添加return语句 来看一个最简单的函数： 1234567891011121314151617package mainfunc add(a,b int) map[] &#123; var resultMap = map[string]int&#123;&#125; resultMap["add"] = a + b resultMap["multi"] = a * b resultMap["subtract"] = a - b resultMap["division"] = a / b return resultMap&#125;func main() &#123; a := 3 b := 4 count := add(a, b) println(count)&#125; 多个返回值： Go中函数可以有多个返回值，这个比java强悍100倍： 123456789101112package mainfunc add(a,b int) (int,int) &#123; return a+b,a-b&#125;func main() &#123; a := 3 b := 4 add,sub := add(a, b) println(add,sub)&#125; 可变参数： 跟Java中差不多吧： 1func myFunc(arg ...int) &#123;&#125; defer defer是golang的一个特色功能，被称为“延迟调用函数”。当外部函数返回后执行defer。类似于其他语言的 try… catch … finally… 中的finally，当然差别还是明显的。 释放占用资源： 12345678910111213141516func test() error &#123; file, err := os.Open("path") if err != nil &#123; return err &#125; //放在判断err状态之后 defer file.Close() //todo //... return nil //defer执行时机&#125; 异常处理： 1234567891011121314151617181920func test2() &#123; defer func() &#123; if err := recover(); err != nil &#123; println(err) &#125; &#125;() file, err := os.Open("path") if err != nil &#123; panic(err) &#125; defer file.Close() //todo //... return //defer执行时机&#125; 日志输出： 1234567891011121314func test3() &#123; t1 := time.Now() defer func() &#123; println("耗时: %f s", time.Now().Sub(t1).Seconds()) &#125;() //todo //... return //defer执行时机&#125; 3. struct类型 Java中我们会去声明一些bean对象，里面包含字段和属性，在Go中可以声明一个struct类型的实体，在这个实体中声明一些属性，但是不可以在里面定义func。 1234type person struct &#123; name string age int&#125; 使用方式： 123456789101112131415161718192021package maintype person struct &#123; name string age int&#125;func main() &#123; var p person p.age = 13 p.name = &quot;xiaoming&quot; println(p.name,p.age) p1 := person&#123;&quot;xiaoming&quot;,13&#125; println(p1.name,p1.age) p2 := person&#123;age:13,name:&quot;xiaoming&quot;&#125; println(p2.name,p2.age) &#125; Go中的继承—匿名字段 上面我们在定义struct的时候，里面的属性都是字段名和类型一一对应的。实际上Go也支持只提供类型而不写字段名的方式，也就是匿名字段。 当匿名字段是一个struct的时候，那么这个struct所拥有的全部字段都被隐式地引入了当前定义的这个struct。 1234567891011121314151617181920package maintype PersonOther struct &#123; phone string address string&#125;type Person struct &#123; PersonOther name string age int&#125;func main() &#123; p1 := Person&#123;PersonOther&#123;"13242342123","xxxxxxx"&#125;,"xiaoming",13&#125; println(p1.address,p1.phone,p1.name,p1.age)&#125; 可以看到在p1中是可以看到address和phone属性的，这就跟Java中的继承一样。 既然说到继承，那么肯定会有这样的一种情况，就是在Person和PersonOther中都定义过了phone属性，当我们用p1去获取的时候到底获取的是哪个对象的phone属性呢？ Go里面很简单的解决了这个问题，最外层的优先访问，也就是当你通过Person.phone访问的时候，是访问student里面的字段，而不是PersonOther里面的字段。 当前如果你想访问父类中的phone也不是不可以，Go还保留着父类中的对象呢，你可以这样取出来： 1parentPhone := p1.PersonOther.phone 在Java中如果是这样的话，父类的同名字段就被子类覆写了，取不出来。 4. 也谈谈面向对象编程----多态 在Java中我们经常这样做： 定义一个关于计算面积的接口； 定义一个计算圆面积的类实现接口； 定义一个正方形计算面积的类实现接口； 定义一个计算长方形面积的类实现接口； … 这样我们就抽象出来一套统一的计算面积的方案由一个接口把持，需要哪个面积计算就调用相关的实现。 在Go中同样也有这样的概念，但是实现方式确是不同。基于上面的原因所以就有了method的概念，method是附属在一个给定的类型上的，他的语法和函数的声明语法几乎一样，只是在func后面增加了一个receiver(也就是method所依从的主体)。 method的语法如下： 1func (r ReceiverType) funcName(parameters) (results) 看一个具体的示例： 12345678910111213141516171819202122232425package maintype Circle struct &#123; redius float64&#125;type Square struct &#123; width,height float64&#125;func (c Circle) area() float64 &#123; return c.redius * c.redius * 3.14&#125;func (s Square) area() float64 &#123; return s.height * s.width&#125;func main() &#123; c := Circle&#123;3.55&#125; s := Square&#123;3,5&#125; println(c.area()) println(s.area())&#125; 可以看到调用method通过Circle示例访问就像访问struct里面的字段一样。]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>Go基础学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hbase计数器]]></title>
    <url>%2Fposts%2Fe2d63c84.html</url>
    <content type="text"><![CDATA[如果在HBase中使用某一行的值进行Put操作进行计数器功能，为了保证原子性操作，必然会导致一个客户端对计数器所在行的资源占有，如果在大量进行计数器操作时，则会占有大量资源，并且一旦某一客户端崩溃，将会使得其他客户端进入长时间等待。HBase中定义了一个计数器来完成用户的技术操作，并且防止资源占有问题，并且也保证其原子性。 在HBase中，HBase将某一列作为计数器来使用，因此创建计数器与创建行是相同的，因此创建计数器时不需要特定的创建流程，因为HBase的列具有动态添加的特性，使得计数器跟列具有相同的特性：动态添加。在第一次使用时计数器（实质为列）隐藏的进行了创建。 计数器增加值是增加一个long值，其增加的值也有负有正，不同的数据进行增加时有不同的效果： 大于0：增加计数器的值 等于0：不更改计数器的值 小于0：减少计数器的值 需要注意的是，计数器的值增加的是一个long类型的整数，而不是一个字符串，有时候增减一个字符串会发现结果很意外。 shell命令如下： 12345678910111213141516171819hbase(main):007:0&gt; incr 'ns1:calllogs','24,18000696806,20170220210151,0,1533','f1:callee'COUNTER VALUE = 10 row(s) in 0.0780 secondshbase(main):008:0&gt; incr 'ns1:calllogs','24,18000696806,20170220210151,0,1533','f1:callee'COUNTER VALUE = 20 row(s) in 0.0540 secondshbase(main):009:0&gt; incr 'ns1:calllogs','24,18000696806,20170220210151,0,1533','f1:callee'COUNTER VALUE = 30 row(s) in 0.0200 secondshbase(main):010:0&gt; incr 'ns1:calllogs','24,18000696806,20170220210151,0,1533','f1:callee'COUNTER VALUE = 40 row(s) in 0.0190 seconds获取计数器：hbase(main):011:0&gt; get_counter 'ns1:calllogs','24,18000696806,20170220210151,0,1533','f1:callee'COUNTER VALUE = 4 java代码set计数器： 计数器分为单列计数器和多列计数器，如字面意思，单列指的是可以给某一行的某一列计数，多列指的是可以给某一行的多列同时计数。 示例表： 1234567891011create &apos;user_info&apos;,&apos;base_info&apos;,&apos;extra_info&apos;put &apos;user_info&apos;,&apos;1&apos;,&apos;base_info:user_name&apos;,&apos;xiaoming&apos;put &apos;user_info&apos;,&apos;2&apos;,&apos;base_info:sex&apos;,&apos;1&apos;put &apos;user_info&apos;,&apos;3&apos;,&apos;base_info:age&apos;,&apos;12&apos;put &apos;user_info&apos;,&apos;4&apos;,&apos;base_info:user_name&apos;,&apos;xiaoli&apos;put &apos;user_info&apos;,&apos;5&apos;,&apos;base_info:sex&apos;,&apos;1&apos;put &apos;user_info&apos;,&apos;6&apos;,&apos;base_info:age&apos;,&apos;14&apos;put &apos;user_info&apos;,&apos;7&apos;,&apos;base_info:user_name&apos;,&apos;xiaohong&apos;put &apos;user_info&apos;,&apos;8&apos;,&apos;base_info:sex&apos;,&apos;2&apos;put &apos;user_info&apos;,&apos;9&apos;,&apos;base_info:age&apos;,&apos;15&apos; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465package cn.edu.hust.demo1.hbase;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.hbase.*;import org.apache.hadoop.hbase.client.*;import org.apache.hadoop.hbase.filter.*;import org.apache.hadoop.hbase.util.Bytes;import org.junit.Before;import org.junit.Test;import java.io.IOException;import java.util.ArrayList;import java.util.Arrays;import java.util.List;public class HbaseApiTest &#123; static Configuration conf = null; static HBaseAdmin admin = null; static Connection connection = null; static Table table = null; @Before public void init() throws IOException &#123; //创建conf对象 conf = HBaseConfiguration.create(); //通过连接工厂创建连接对象 connection = ConnectionFactory.createConnection(conf); //通过连接查询tableName对象 TableName tname = TableName.valueOf("user_info"); //获得table table = connection.getTable(tname); &#125; /** * 单计数器 * @throws IOException */ @Test public void singleCounterTest() throws IOException &#123; long counter1 = table.incrementColumnValue(Bytes.toBytes("1"), Bytes.toBytes("base_info"), Bytes.toBytes("sex"), 1); // 一次增加1 System.out.println("counter1 = " + counter1); &#125; /** * 多计数器 * @throws IOException */ @Test public void MulticounterTest() throws IOException &#123; Increment incr = new Increment(Bytes.toBytes("1")); incr.addColumn(Bytes.toBytes("base_info"), Bytes.toBytes("sex"), 1); incr.addColumn(Bytes.toBytes("base_info"), Bytes.toBytes("name"), 2); Result increment = table.increment(incr); Cell[] cells = increment.rawCells(); for(Cell cell : cells) &#123; System.out.print(Arrays.toString(cell.getRow()) + " "); System.out.print(new String(cell.getFamily()) + ":"); System.out.print(new String(cell.getQualifier()) + " = "); System.out.print(new String(cell.getValue())); &#125; &#125;&#125;]]></content>
      <categories>
        <category>大数据学习</category>
        <category>Hbase</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hbase协处理器]]></title>
    <url>%2Fposts%2F2e2f423b.html</url>
    <content type="text"><![CDATA[为什么引入coprocessors 前面我们知道Hbase在客户端读取数据的时候可以通过扫描器和过滤器来加速获取数据和筛选不必要的数据以减少数据返回的量，提提高数据获取准确性。但是这些操作毕竟是在客户端来完成的，网络传输和处理还是有一定的时间损耗。所以Hbase也引入了一种功能—协处理器：允许在服务端执行用户程序来管理数据。 HBase作为列族数据库最经常被人诟病的特性包括：无法轻易建立“二级索引”，难以执行求和、计数、排序等操作。比如，在旧版本的(&lt;0.92)Hbase中，统计数据表的总行数，需要使用Counter方法，执行一次MapReduce Job才能得到。虽然HBase在数据存储层中集成了MapReduce，能够有效用于数据表的分布式计算。然而在很多情况下，做一些简单的相加或者聚合计算的时候，如果直接将计算过程放置在server端，能够减少通讯开销，从而获得很好的性能提升。于是，HBase在0.92之后引入了协处理器(coprocessors)，实现一些激动人心的新特性：能够轻易建立二次索引、复杂过滤器(谓词下推)以及访问控制等。 协处理器允许用户在region服务器上运行自己的代码，更准确的说是允许用户执行region级的操作，并且可以使用与RDBMS中的触发器类似的功能。在客户端，用户不用关系操作具体在哪里执行，HBase的分布式框架会帮助用户把这些工作变得透明。有点类似MapReduce。 使用协处理器的好处显而易见，可以将运算放到server端，减少通信开销的同时还能有效的提升性能。 coprocessors类型 协处理的两种类型： 系统协处理器：可以全局导入region server上的所有数据表 表协处理器：用户可以指定一张表使用协处理器 协处理器框架为了更好支持其行为的灵活性，提供了两个不同方面的插件。 观察者（observer），类似于关系数据库的触发器（主动触发），在一些特定事件发生的时候被执行。这些事件包括一些用户产生的事件，也包括服务器端内部产生的事件。 终端（endpoint），动态的终端有点像存储过程。除了事件处理之外还需要将用户自定义操作添加到服务器端。用户代码可以被部署到管理数据的服务器端。endpoint通过添加一些远程调用来扩展RPC协议。 Observer 观察者的设计意图是允许用户通过插入代码来重载协处理器框架的upcall方法，而具体的事情触发的callback方法由HBase的核心代码来指定。协处理器框架处理所有的callback调用细节，协处理器自身只需要插入添加或者改变的功能。 Observer提供的接口： RegionObserver：提供客户端的数据操作事件钩子：Get、Put、Delete和Scan等 WALObserver：提供WAL相关操作钩子 MasterObserver：提供DDL类型的操作钩子，如创建、删除、修改数据表等。 这些接口可以同时使用在同一个地方，按照不同的优先级顺序执行，用户可以任意基于协处理器实现负责的HBase功能层，HBase有很多种事件可以触发观察者方法，这些事件与方法从HBase0.92版本起，都会集成在HBase中。不过这些API可能会由于各种原因有所改动，不同版本的接口改动比较大。 endpoint endpoint是HBase的一种通用扩展。当endpoint安装在集群上时，它扩展了HBase的RPC协议，对客户端应用开放了新方法。就像observer一样，endpoint在RegionServer上执行，紧挨着你的数据。 endpoint协处理类似于其他数据库引擎中的存储过程。从客户端看，调用一个endpoint协处理器类似于调用其他HBase命令，只是其功能建立在定义协处理器的定制代码上，通常先创建请求对象，然后把它传给HtableInterface在集群上执行，最后收集结果。 总结 Observer 允许集群在正常的客户端操作过程中可以有不同的行为表现； Endpoint 允许扩展集群的能力，对客户端应用开放新的运算命令； observer 类似于 RDBMS 中的触发器，主要在服务端工作； endpoint 类似于 RDBMS 中的存储过程，主要在 client 端工作； observer 可以实现权限管理、优先级设置、监控、 ddl 控制、 二级索引等功能； endpoint 可以实现 min、 max、 avg、 sum、 distinct、 group by 等功能； coprocessors 示例 Observer协处理器的实现： 假设我们有这样的需求： 12345678假定某个表有A和B两个列--------------------------------便于后续描述我们称之为coprocessor_table1. 当我们向A列插入数据的时候通过协处理器像B列也插入数据。2.在读取数据的时候只允许客户端读取B列数据而不能读取A列数据。换句话说A列是只写 B列是只读的。（为了简单起见，用户在读取数据的时候需要制定列名）3. A列值必须是整数，换句话说B列值也自然都是整数4.当删除操作的时候不能指定删除B列5.当删除A列的时候同时需要删除B列6.对于其他列的删除不做检查 HBase API中有BaseRegionObserver，这个类已经帮助我们实现了大部分的默认实现，我们只要专注于业务上的方法重载即可。 根据上述需求和代码框架，具体逻辑实现如下。 在插入需要做检查所以重写了prePut方法 在删除前需要做检查所以重写了preDelete方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112public class MyRegionObserver extends BaseRegionObserver &#123; private static final Log LOG = LogFactory.getLog(MyRegionObserver.class); private RegionCoprocessorEnvironment env = null; // 设定只有F族下的列才能被操作，且A列只写，B列只读。的语言 private static final String FAMAILLY_NAME = "F"; private static final String ONLY_PUT_COL = "A"; private static final String ONLY_READ_COL = "B"; // 协处理器是运行于region中的，每一个region都会加载协处理器 // 这个方法会在regionserver打开region时候执行（还没有真正打开） @Override public void start(CoprocessorEnvironment e) throws IOException &#123; env = (RegionCoprocessorEnvironment) e; &#125; // 这个方法会在regionserver关闭region时候执行（还没有真正关闭） @Override public void stop(CoprocessorEnvironment e) throws IOException &#123; // nothing to do here &#125; /** * 需求 1.不允许插入B列 2.只能插入A列 3.插入的数据必须为整数 4.插入A列的时候自动插入B列 */ @Override public void prePut(final ObserverContext&lt;RegionCoprocessorEnvironment&gt; e, final Put put, final WALEdit edit, final Durability durability) throws IOException &#123; // 首先查看单个put中是否有对只读列有写操作 List&lt;Cell&gt; cells = put.get(Bytes.toBytes(FAMAILLY_NAME), Bytes.toBytes(ONLY_READ_COL)); if (cells != null &amp;&amp; cells.size() != 0) &#123; LOG.warn("User is not allowed to write read_only col."); throw new IOException("User is not allowed to write read_only col."); &#125; // 检查A列 cells = put.get(Bytes.toBytes(FAMAILLY_NAME), Bytes.toBytes(ONLY_PUT_COL)); if (cells == null || cells.size() == 0) &#123; // 当不存在对于A列的操作的时候则不做任何的处理，直接放行即可 LOG.info("No A col operation, just do it."); return; &#125; // 当A列存在的情况下在进行值得检查，查看是否插入了整数 byte[] aValue = null; for (Cell cell : cells) &#123; try &#123; aValue = CellUtil.cloneValue(cell); LOG.warn("aValue = " + Bytes.toString(aValue)); Integer.valueOf(Bytes.toString(aValue)); &#125; catch (Exception e1) &#123; LOG.warn("Can not put un number value to A col."); throw new IOException("Can not put un number value to A col."); &#125; &#125; // 当一切都ok的时候再去构建B列的值，因为按照需求，插入A列的时候需要同时插入B列 LOG.info("B col also been put value!"); put.addColumn(Bytes.toBytes(FAMAILLY_NAME), Bytes.toBytes(ONLY_READ_COL), aValue); &#125; /** * 需求 1.不能删除B列 2.只能删除A列 3.删除A列的时候需要一并删除B列 */ @Override public void preDelete( final ObserverContext&lt;RegionCoprocessorEnvironment&gt; e, final Delete delete, final WALEdit edit, final Durability durability) throws IOException &#123; // 首先查看是否对于B列进行了指定删除 List&lt;Cell&gt; cells = delete.getFamilyCellMap().get( Bytes.toBytes(FAMAILLY_NAME)); if (cells == null || cells.size() == 0) &#123; // 如果客户端没有针对于FAMAILLY_NAME列族的操作则不用关心，让其继续操作即可。 LOG.info("NO F famally operation ,just do it."); return; &#125; // 开始检查F列族内的操作情况 byte[] qualifierName = null; boolean aDeleteFlg = false; for (Cell cell : cells) &#123; qualifierName = CellUtil.cloneQualifier(cell); // 检查是否对B列进行了删除，这个是不允许的 if (Arrays.equals(qualifierName, Bytes.toBytes(ONLY_READ_COL))) &#123; LOG.info("Can not delete read only B col."); throw new IOException("Can not delete read only B col."); &#125; // 检查是否存在对于A队列的删除 if (Arrays.equals(qualifierName, Bytes.toBytes(ONLY_PUT_COL))) &#123; LOG.info("there is A col in delete operation!"); aDeleteFlg = true; &#125; &#125; // 如果对于A列有删除，则需要对B列也要删除 if (aDeleteFlg) &#123; LOG.info("B col also been deleted!"); delete.addColumn(Bytes.toBytes(FAMAILLY_NAME), Bytes.toBytes(ONLY_READ_COL)); &#125; &#125;&#125; 部署方式： 完成实现后需要将协处理器类打包成jar文件，对于协处理器的加载通常有三种方法： 1.配置文件加载：即通过hbase-site.xml文件配置加载，一般这样的协处理器是系统级别的，全局的协处理器，如权限控制等检查。 2.shell加载：可以通过alter命令来对表进行scheme进行修改来加载协处理器。 3.通过API代码实现：即通过API的方式来加载协处理器。 上述加载方法中，1，3都需要将协处理器jar文件放到集群的hbase的classpath中。而2方法只需要将jar文件上传至集群环境的hdfs即可。其中第一种方式应该慎用，这种方式会影响所有的region。使用这种方式去添加的coprocessor最好是经过审核，确定所有的region都使用该操作。 先说一下方式1的使用方式： 修改hbase-site.xml文件，添加如下内容： 123456789101112&lt;property&gt; &lt;name&gt;hbase.coprocessor.region.classes&lt;/name&gt; &lt;value&gt;coprocessor.RegionObserverExample, coprocessor.AnotherCoprocessor&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.coprocessor.master.classes&lt;/name&gt; &lt;value&gt;coprocessor.MasterObserverExample&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.coprocessor.wal.classes&lt;/name&gt; &lt;value&gt;coprocessor.WALObserverExample, bar.foo.MyWALObserver&lt;/value&gt;&lt;/property&gt; 然后将包含上述类的jar包入到hbase的lib目录中，重启Hbase即可。 下面我们先通过方式2来加载coprocessor。 这种方式适合开发人员使用，只会影响特写的表或者region。这种方式有可能导致开发人员滥用coprocessor，从而使得hbase集群负载过高，因此建议回收建表权限，只能由集群管理人员建表，并在建表时指定coprocessor。 这种方式无须重启集群，从而达到热加载的目的。 将包含上述类的jar包放到某个hdfs路径，当然也可以直接放在本地目录，但要保证每台hbase服务器都有这个类。 1.建表： 1234hbase(main):001:0&gt; create 'coprocessor_table','F'0 row(s) in 2.7570 seconds=&gt; Hbase::Table - coprocessor_table 2.通过alter命令将协处理器加载到表中： 1hbase(main):003:0&gt; alter 'coprocessor_table' , METHOD =&gt;'table_att','coprocessor'=&gt;'hdfs:///data/hadoop.jar|cn.edu.hust.demo1.hbase.MyRegionObserver|333' 注意jar包的权限，如果hbase用户不能读取这个jar包，会导致enable时失败。 简单解释下这个命令。这条命令在一个表的table_att中添加了一个新的属性“Coprocessor”。使用的时候Coprocessor会尝试从这个表的table_attr中读取这个属性的信息。这个属性的值用管道符“|”分成了四部分： 文件路径： 文件路径中需要包含Coprocessor的实现，并且对所有的RegionServer都是可达的。这个路径可以是每个RegionServer的本地磁盘路径，也可以是HDFS上的一个路径。通常建议是将Coprocessor实现存储到HDFS。HBASE-14548允许使用一个路径中包含的所有的jar，或者是在路径中使用通配符来指定某些jar，比如：hdfs://:/user// 或者 hdfs://:/user//*.jar。需要注意的是如果是用路径来指定要加载的Coprocessor，这个路径下的所有jar文件都会被加载，不过该路径下的子目录中的jar不会被加载。另外，如果要用路径指定Coprocessor时，就不要再使用通配符了。这些特性在Java API中也得到了支持。 类名： Coprocessor的全限定类名。 优先级： 一个整数。HBase将会使用优先级来决定在同一个位置配置的所有Observer Coprocessor的执行顺序。这个位置可以留白，这样HBase将会分配一个默认的优先级。 参数（可选的）： 这些值会被传递给要使用的Coprocessor实现。这个项是可选的。 如果是在存在这张表的情况下，为了不影响业务，你需要先卸载这张表，再去装载coprocessor。 1hbase(main):001:0&gt; disable 'coprocessor_table' 加载完coprocessor之后，再使用enable命令恢复： 1hbase(main):001:0&gt; enable 'coprocessor_table' 3.检查协处理器的加载： 1234567hbase(main):004:0&gt; desc 'coprocessor_table'Table coprocessor_table is ENABLEDcoprocessor_table, &#123;TABLE_ATTRIBUTES =&gt; &#123;coprocessor$1 =&gt; 'hdfs:///data/hadoop.jar|cn.edu.hust.demo1.hbase.MyRegionObserver|1001'&#125;COLUMN FAMILIES DESCRIPTION&#123;NAME =&gt; 'F', BLOOMFILTER =&gt; 'ROW', VERSIONS =&gt; '1', IN_MEMORY =&gt; 'false', KEEP_DELETED_CELLS =&gt; 'FALSE', DATA_BLOCK_ENCODING =&gt; 'NONE', TTL =&gt; 'FOREVER', COMPRESSION =&gt; 'NONE', MIN_VERSIONS =&gt; '0', BLOCKCACHE =&gt; 'true', BLOCKSIZE =&gt; '65536', REPLICATION_SCOPE =&gt; '0'&#125;1 row(s) in 0.1700 seconds 4.正常插入A列： 123456789101112hbase(main):005:0&gt; scan 'coprocessor_table'ROW COLUMN+CELL0 row(s) in 0.2460 secondshbase(main):006:0&gt; put 'coprocessor_table','row1','F:A',1230 row(s) in 0.4360 secondshbase(main):007:0&gt; scan 'coprocessor_table'ROW COLUMN+CELL row1 column=F:A, timestamp=1547839937236, value=123 row1 column=F:B, timestamp=1547839937236, value=1231 row(s) in 0.0350 seconds 可以看到B列自动被插入了。 5.插入A列，但是值不为整数： 1234hbase(main):008:0&gt; put 'coprocessor_table','row1','F:A','cc'ERROR: Failed 1 action: IOException: 1 time, servers with issues: hadoopslaver1,16020,1547042618202 可以看到，插入失败。 6.插入B列： 123hbase(main):010:0* put 'coprocessor_table','row1','F:B',123ERROR: Failed 1 action: IOException: 1 time, servers with issues: hadoopslaver1,16020,1547042618202 同样也插入失败了。 7.删除B列： 123hbase(main):011:0&gt; delete 'coprocessor_table','row1','F:B'ERROR: java.io.IOException: Can not delete read only B col. 删除失败。 8.删除A列： 123456789101112hbase(main):012:0&gt; scan 'coprocessor_table'ROW COLUMN+CELL row1 column=F:A, timestamp=1547839937236, value=123 row1 column=F:B, timestamp=1547839937236, value=1231 row(s) in 0.0240 secondshbase(main):013:0&gt; delete 'coprocessor_table','row1','F:A'0 row(s) in 0.1050 secondshbase(main):014:0&gt; scan 'coprocessor_table'ROW COLUMN+CELL0 row(s) in 0.0260 seconds A和B列同时被删除了。 以上验证符合我们的预期。 如果该coprocessor只是为了某一次任务使用，使用完毕我们可以卸载该coprocessor： 1hbase(main):012:0&gt; alter 'coprocessor_table', METHOD =&gt; 'table_att_unset', NAME =&gt; 'coprocessor$1' coprocessor$1的含义是： 你可以在一个表中添加多个coprocessor，形式如下： 1alter 'coprocessor_table' , METHOD =&gt;'table_att','coprocessor$1'=&gt;'|org.apache.hadoop.hbase.coprocessor.AggregateImplementation||', 'coprocessor$2' =&gt; '|org.apache.hadoop.hbase.coprocessor.BaseWALObserver||' 默认只有一个的情况下写coprocessor即可，所以coprocessor$1取的是第一个coprocessor。 上面说了1，2两种方式如何加载coprocessor，那么第三种通过api的方式也提一下： 通过的方法： 123456789101112131415161718TableName tableName = TableName.valueOf("users");String path = "hdfs://&lt;namenode&gt;:&lt;port&gt;/user/&lt;hadoop-user&gt;/coprocessor.jar";Configuration conf = HBaseConfiguration.create();Connection connection = ConnectionFactory.createConnection(conf);Admin admin = connection.getAdmin();admin.disableTable(tableName);HTableDescriptor hTableDescriptor = new HTableDescriptor(tableName);HColumnDescriptor columnFamily1 = new HColumnDescriptor("personalDet");columnFamily1.setMaxVersions(3);hTableDescriptor.addFamily(columnFamily1);HColumnDescriptor columnFamily2 = new HColumnDescriptor("salaryDet");columnFamily2.setMaxVersions(3);hTableDescriptor.addFamily(columnFamily2);hTableDescriptor.setValue("COPROCESSOR$1", path + "|"+ RegionObserverExample.class.getCanonicalName() + "|"+ Coprocessor.PRIORITY_USER);admin.modifyTable(tableName, hTableDescriptor);admin.enableTable(tableName); HBase 0.96+ 的版本提供了更高级的API： 1234567891011121314151617TableName tableName = TableName.valueOf("users");Path path = new Path("hdfs://&lt;namenode&gt;:&lt;port&gt;/user/&lt;hadoop-user&gt;/coprocessor.jar");Configuration conf = HBaseConfiguration.create();Connection connection = ConnectionFactory.createConnection(conf);Admin admin = connection.getAdmin();admin.disableTable(tableName);HTableDescriptor hTableDescriptor = new HTableDescriptor(tableName);HColumnDescriptor columnFamily1 = new HColumnDescriptor("personalDet");columnFamily1.setMaxVersions(3);hTableDescriptor.addFamily(columnFamily1);HColumnDescriptor columnFamily2 = new HColumnDescriptor("salaryDet");columnFamily2.setMaxVersions(3);hTableDescriptor.addFamily(columnFamily2);hTableDescriptor.addCoprocessor(RegionObserverExample.class.getCanonicalName(), path,Coprocessor.PRIORITY_USER, null);admin.modifyTable(tableName, hTableDescriptor);admin.enableTable(tableName); EndPoint处理器实现 与Observer类型不同的是，Endpoint协处理器需要与服务区直接通信，服务端是对于Protobuf Service的实现，所以两者直接会有一个机遇protocl的RPC接口，客户端和服务端都需要进行基于接口的代码逻辑实现。 如前所述，Endpoint类比于数据库中的存储过程，他触发服务端的基于region的同步运行，再将各个结果在客户端搜集后归并计算。特点类似于传统的MR框架，在服务端map在客户端Reduce。相对于Observer来说开发难度大一点。 和Observer一样，在完成实现前先来了解需求 ： 1可以统计表coprocessor_demo1下某字段字段的值总和 先定义表对象： 12345create 'coprocessor_demo1',&#123;NAME=&gt;'cf'&#125;put 'coprocessor_demo1','1','cf:value',332put 'coprocessor_demo1','2','cf:value',44put 'coprocessor_demo1','3','cf:value',12 因为客户端和服务端之间存在RPC通信，所以两者间需要确定接口，HBase的协处理器是通过Protobuf协议来实现数据交换的，所以需要通过Protobuf来定义接口。 先定义消息体,Sum.proto: 123456789101112131415161718syntax = "proto2";option java_package = "cn.edu.hust.demo1.hbase.coprocessor.server";option java_outer_classname = "Sum";option java_generic_services = true;option java_generate_equals_and_hash = true;option optimize_for = SPEED;message SumRequest &#123; required string family = 1; required string column = 2; &#125;message SumResponse &#123; required int64 sum = 1 [default = 0];&#125;service SumService &#123; rpc getSum(SumRequest) returns (SumResponse);&#125; 然后将其编译为java类： 1protoc --java_out=E:\protobuf\include Sum.proto 以下为编译的java类： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697989910010110210310410510610710810911011111211311411511611711811912012112212312412512612712812913013113213313413513613713813914014114214314414514614714814915015115215315415515615715815916016116216316416516616716816917017117217317417517617717817918018118218318418518618718818919019119219319419519619719819920020120220320420520620720820921021121221321421521621721821922022122222322422522622722822923023123223323423523623723823924024124224324424524624724824925025125225325425525625725825926026126226326426526626726826927027127227327427527627727827928028128228328428528628728828929029129229329429529629729829930030130230330430530630730830931031131231331431531631731831932032132232332432532632732832933033133233333433533633733833934034134234334434534634734834935035135235335435535635735835936036136236336436536636736836937037137237337437537637737837938038138238338438538638738838939039139239339439539639739839940040140240340440540640740840941041141241341441541641741841942042142242342442542642742842943043143243343443543643743843944044144244344444544644744844945045145245345445545645745845946046146246346446546646746846947047147247347447547647747847948048148248348448548648748848949049149249349449549649749849950050150250350450550650750850951051151251351451551651751851952052152252352452552652752852953053153253353453553653753853954054154254354454554654754854955055155255355455555655755855956056156256356456556656756856957057157257357457557657757857958058158258358458558658758858959059159259359459559659759859960060160260360460560660760860961061161261361461561661761861962062162262362462562662762862963063163263363463563663763863964064164264364464564664764864965065165265365465565665765865966066166266366466566666766866967067167267367467567667767867968068168268368468568668768868969069169269369469569669769869970070170270370470570670770870971071171271371471571671771871972072172272372472572672772872973073173273373473573673773873974074174274374474574674774874975075175275375475575675775875976076176276376476576676776876977077177277377477577677777877978078178278378478578678778878979079179279379479579679779879980080180280380480580680780880981081181281381481581681781881982082182282382482582682782882983083183283383483583683783883984084184284384484584684784884985085185285385485585685785885986086186286386486586686786886987087187287387487587687787887988088188288388488588688788888989089189289389489589689789889990090190290390490590690790890991091191291391491591691791891992092192292392492592692792892993093193293393493593693793893994094194294394494594694794894995095195295395495595695795895996096196296396496596696796896997097197297397497597697797897998098198298398498598698798898999099199299399499599699799899910001001100210031004100510061007100810091010101110121013101410151016101710181019102010211022102310241025102610271028102910301031103210331034103510361037103810391040104110421043104410451046104710481049105010511052105310541055105610571058105910601061106210631064106510661067106810691070107110721073107410751076107710781079108010811082108310841085108610871088108910901091109210931094109510961097109810991100110111021103110411051106110711081109111011111112111311141115111611171118111911201121112211231124112511261127112811291130113111321133113411351136113711381139114011411142114311441145114611471148114911501151115211531154115511561157115811591160116111621163116411651166116711681169117011711172117311741175117611771178117911801181118211831184118511861187118811891190119111921193119411951196119711981199120012011202120312041205120612071208120912101211121212131214121512161217121812191220122112221223122412251226122712281229123012311232123312341235123612371238123912401241124212431244124512461247124812491250125112521253125412551256125712581259126012611262126312641265126612671268126912701271127212731274127512761277127812791280128112821283128412851286128712881289129012911292129312941295129612971298129913001301130213031304130513061307130813091310131113121313131413151316131713181319132013211322132313241325132613271328132913301331133213331334133513361337133813391340134113421343134413451346134713481349135013511352135313541355135613571358135913601361136213631364136513661367136813691370137113721373137413751376137713781379138013811382138313841385138613871388138913901391139213931394139513961397139813991400140114021403140414051406140714081409141014111412141314141415141614171418141914201421142214231424142514261427142814291430// Generated by the protocol buffer compiler. DO NOT EDIT!// source: Sum.protopackage cn.edu.hust.demo1.hbase.coprocessor.server;// Generated by the protocol buffer compiler. DO NOT EDIT!// source: sumcode.protopublic final class Sum &#123; private Sum() &#123;&#125; public static void registerAllExtensions( com.google.protobuf.ExtensionRegistry registry) &#123; &#125; public interface SumRequestOrBuilder extends com.google.protobuf.MessageOrBuilder &#123; // required string family = 1; /** * &lt;code&gt;required string family = 1;&lt;/code&gt; */ boolean hasFamily(); /** * &lt;code&gt;required string family = 1;&lt;/code&gt; */ java.lang.String getFamily(); /** * &lt;code&gt;required string family = 1;&lt;/code&gt; */ com.google.protobuf.ByteString getFamilyBytes(); // required string column = 2; /** * &lt;code&gt;required string column = 2;&lt;/code&gt; */ boolean hasColumn(); /** * &lt;code&gt;required string column = 2;&lt;/code&gt; */ java.lang.String getColumn(); /** * &lt;code&gt;required string column = 2;&lt;/code&gt; */ com.google.protobuf.ByteString getColumnBytes(); &#125; /** * Protobuf type &#123;@code SumRequest&#125; */ public static final class SumRequest extends com.google.protobuf.GeneratedMessage implements SumRequestOrBuilder &#123; // Use SumRequest.newBuilder() to construct. private SumRequest(com.google.protobuf.GeneratedMessage.Builder&lt;?&gt; builder) &#123; super(builder); this.unknownFields = builder.getUnknownFields(); &#125; private SumRequest(boolean noInit) &#123; this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); &#125; private static final SumRequest defaultInstance; public static SumRequest getDefaultInstance() &#123; return defaultInstance; &#125; public SumRequest getDefaultInstanceForType() &#123; return defaultInstance; &#125; private final com.google.protobuf.UnknownFieldSet unknownFields; @java.lang.Override public final com.google.protobuf.UnknownFieldSet getUnknownFields() &#123; return this.unknownFields; &#125; private SumRequest( com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException &#123; initFields(); int mutable_bitField0_ = 0; com.google.protobuf.UnknownFieldSet.Builder unknownFields = com.google.protobuf.UnknownFieldSet.newBuilder(); try &#123; boolean done = false; while (!done) &#123; int tag = input.readTag(); switch (tag) &#123; case 0: done = true; break; default: &#123; if (!parseUnknownField(input, unknownFields, extensionRegistry, tag)) &#123; done = true; &#125; break; &#125; case 10: &#123; bitField0_ |= 0x00000001; family_ = input.readBytes(); break; &#125; case 18: &#123; bitField0_ |= 0x00000002; column_ = input.readBytes(); break; &#125; &#125; &#125; &#125; catch (com.google.protobuf.InvalidProtocolBufferException e) &#123; throw e.setUnfinishedMessage(this); &#125; catch (java.io.IOException e) &#123; throw new com.google.protobuf.InvalidProtocolBufferException( e.getMessage()).setUnfinishedMessage(this); &#125; finally &#123; this.unknownFields = unknownFields.build(); makeExtensionsImmutable(); &#125; &#125; public static final com.google.protobuf.Descriptors.Descriptor getDescriptor() &#123; return cn.edu.hust.demo1.hbase.coprocessor.server.Sum.internal_static_SumRequest_descriptor; &#125; protected com.google.protobuf.GeneratedMessage.FieldAccessorTable internalGetFieldAccessorTable() &#123; return cn.edu.hust.demo1.hbase.coprocessor.server.Sum.internal_static_SumRequest_fieldAccessorTable .ensureFieldAccessorsInitialized( cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumRequest.class, cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumRequest.Builder.class); &#125; public static com.google.protobuf.Parser&lt;SumRequest&gt; PARSER = new com.google.protobuf.AbstractParser&lt;SumRequest&gt;() &#123; public SumRequest parsePartialFrom( com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException &#123; return new SumRequest(input, extensionRegistry); &#125; &#125;; @java.lang.Override public com.google.protobuf.Parser&lt;SumRequest&gt; getParserForType() &#123; return PARSER; &#125; private int bitField0_; // required string family = 1; public static final int FAMILY_FIELD_NUMBER = 1; private java.lang.Object family_; /** * &lt;code&gt;required string family = 1;&lt;/code&gt; */ public boolean hasFamily() &#123; return ((bitField0_ &amp; 0x00000001) == 0x00000001); &#125; /** * &lt;code&gt;required string family = 1;&lt;/code&gt; */ public java.lang.String getFamily() &#123; java.lang.Object ref = family_; if (ref instanceof java.lang.String) &#123; return (java.lang.String) ref; &#125; else &#123; com.google.protobuf.ByteString bs = (com.google.protobuf.ByteString) ref; java.lang.String s = bs.toStringUtf8(); if (bs.isValidUtf8()) &#123; family_ = s; &#125; return s; &#125; &#125; /** * &lt;code&gt;required string family = 1;&lt;/code&gt; */ public com.google.protobuf.ByteString getFamilyBytes() &#123; java.lang.Object ref = family_; if (ref instanceof java.lang.String) &#123; com.google.protobuf.ByteString b = com.google.protobuf.ByteString.copyFromUtf8( (java.lang.String) ref); family_ = b; return b; &#125; else &#123; return (com.google.protobuf.ByteString) ref; &#125; &#125; // required string column = 2; public static final int COLUMN_FIELD_NUMBER = 2; private java.lang.Object column_; /** * &lt;code&gt;required string column = 2;&lt;/code&gt; */ public boolean hasColumn() &#123; return ((bitField0_ &amp; 0x00000002) == 0x00000002); &#125; /** * &lt;code&gt;required string column = 2;&lt;/code&gt; */ public java.lang.String getColumn() &#123; java.lang.Object ref = column_; if (ref instanceof java.lang.String) &#123; return (java.lang.String) ref; &#125; else &#123; com.google.protobuf.ByteString bs = (com.google.protobuf.ByteString) ref; java.lang.String s = bs.toStringUtf8(); if (bs.isValidUtf8()) &#123; column_ = s; &#125; return s; &#125; &#125; /** * &lt;code&gt;required string column = 2;&lt;/code&gt; */ public com.google.protobuf.ByteString getColumnBytes() &#123; java.lang.Object ref = column_; if (ref instanceof java.lang.String) &#123; com.google.protobuf.ByteString b = com.google.protobuf.ByteString.copyFromUtf8( (java.lang.String) ref); column_ = b; return b; &#125; else &#123; return (com.google.protobuf.ByteString) ref; &#125; &#125; private void initFields() &#123; family_ = ""; column_ = ""; &#125; private byte memoizedIsInitialized = -1; public final boolean isInitialized() &#123; byte isInitialized = memoizedIsInitialized; if (isInitialized != -1) return isInitialized == 1; if (!hasFamily()) &#123; memoizedIsInitialized = 0; return false; &#125; if (!hasColumn()) &#123; memoizedIsInitialized = 0; return false; &#125; memoizedIsInitialized = 1; return true; &#125; public void writeTo(com.google.protobuf.CodedOutputStream output) throws java.io.IOException &#123; getSerializedSize(); if (((bitField0_ &amp; 0x00000001) == 0x00000001)) &#123; output.writeBytes(1, getFamilyBytes()); &#125; if (((bitField0_ &amp; 0x00000002) == 0x00000002)) &#123; output.writeBytes(2, getColumnBytes()); &#125; getUnknownFields().writeTo(output); &#125; private int memoizedSerializedSize = -1; public int getSerializedSize() &#123; int size = memoizedSerializedSize; if (size != -1) return size; size = 0; if (((bitField0_ &amp; 0x00000001) == 0x00000001)) &#123; size += com.google.protobuf.CodedOutputStream .computeBytesSize(1, getFamilyBytes()); &#125; if (((bitField0_ &amp; 0x00000002) == 0x00000002)) &#123; size += com.google.protobuf.CodedOutputStream .computeBytesSize(2, getColumnBytes()); &#125; size += getUnknownFields().getSerializedSize(); memoizedSerializedSize = size; return size; &#125; private static final long serialVersionUID = 0L; @java.lang.Override protected java.lang.Object writeReplace() throws java.io.ObjectStreamException &#123; return super.writeReplace(); &#125; @java.lang.Override public boolean equals(final java.lang.Object obj) &#123; if (obj == this) &#123; return true; &#125; if (!(obj instanceof cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumRequest)) &#123; return super.equals(obj); &#125; cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumRequest other = (cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumRequest) obj; boolean result = true; result = result &amp;&amp; (hasFamily() == other.hasFamily()); if (hasFamily()) &#123; result = result &amp;&amp; getFamily() .equals(other.getFamily()); &#125; result = result &amp;&amp; (hasColumn() == other.hasColumn()); if (hasColumn()) &#123; result = result &amp;&amp; getColumn() .equals(other.getColumn()); &#125; result = result &amp;&amp; getUnknownFields().equals(other.getUnknownFields()); return result; &#125; private int memoizedHashCode = 0; @java.lang.Override public int hashCode() &#123; if (memoizedHashCode != 0) &#123; return memoizedHashCode; &#125; int hash = 41; hash = (19 * hash) + getDescriptorForType().hashCode(); if (hasFamily()) &#123; hash = (37 * hash) + FAMILY_FIELD_NUMBER; hash = (53 * hash) + getFamily().hashCode(); &#125; if (hasColumn()) &#123; hash = (37 * hash) + COLUMN_FIELD_NUMBER; hash = (53 * hash) + getColumn().hashCode(); &#125; hash = (29 * hash) + getUnknownFields().hashCode(); memoizedHashCode = hash; return hash; &#125; public static cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumRequest parseFrom( com.google.protobuf.ByteString data) throws com.google.protobuf.InvalidProtocolBufferException &#123; return PARSER.parseFrom(data); &#125; public static cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumRequest parseFrom( com.google.protobuf.ByteString data, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException &#123; return PARSER.parseFrom(data, extensionRegistry); &#125; public static cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumRequest parseFrom(byte[] data) throws com.google.protobuf.InvalidProtocolBufferException &#123; return PARSER.parseFrom(data); &#125; public static cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumRequest parseFrom( byte[] data, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException &#123; return PARSER.parseFrom(data, extensionRegistry); &#125; public static cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumRequest parseFrom(java.io.InputStream input) throws java.io.IOException &#123; return PARSER.parseFrom(input); &#125; public static cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumRequest parseFrom( java.io.InputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException &#123; return PARSER.parseFrom(input, extensionRegistry); &#125; public static cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumRequest parseDelimitedFrom(java.io.InputStream input) throws java.io.IOException &#123; return PARSER.parseDelimitedFrom(input); &#125; public static cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumRequest parseDelimitedFrom( java.io.InputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException &#123; return PARSER.parseDelimitedFrom(input, extensionRegistry); &#125; public static cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumRequest parseFrom( com.google.protobuf.CodedInputStream input) throws java.io.IOException &#123; return PARSER.parseFrom(input); &#125; public static cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumRequest parseFrom( com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException &#123; return PARSER.parseFrom(input, extensionRegistry); &#125; public static Builder newBuilder() &#123; return Builder.create(); &#125; public Builder newBuilderForType() &#123; return newBuilder(); &#125; public static Builder newBuilder(cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumRequest prototype) &#123; return newBuilder().mergeFrom(prototype); &#125; public Builder toBuilder() &#123; return newBuilder(this); &#125; @java.lang.Override protected Builder newBuilderForType( com.google.protobuf.GeneratedMessage.BuilderParent parent) &#123; Builder builder = new Builder(parent); return builder; &#125; /** * Protobuf type &#123;@code SumRequest&#125; */ public static final class Builder extends com.google.protobuf.GeneratedMessage.Builder&lt;Builder&gt; implements cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumRequestOrBuilder &#123; public static final com.google.protobuf.Descriptors.Descriptor getDescriptor() &#123; return cn.edu.hust.demo1.hbase.coprocessor.server.Sum.internal_static_SumRequest_descriptor; &#125; protected com.google.protobuf.GeneratedMessage.FieldAccessorTable internalGetFieldAccessorTable() &#123; return cn.edu.hust.demo1.hbase.coprocessor.server.Sum.internal_static_SumRequest_fieldAccessorTable .ensureFieldAccessorsInitialized( cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumRequest.class, cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumRequest.Builder.class); &#125; // Construct using cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumRequest.newBuilder() private Builder() &#123; maybeForceBuilderInitialization(); &#125; private Builder( com.google.protobuf.GeneratedMessage.BuilderParent parent) &#123; super(parent); maybeForceBuilderInitialization(); &#125; private void maybeForceBuilderInitialization() &#123; if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) &#123; &#125; &#125; private static Builder create() &#123; return new Builder(); &#125; public Builder clear() &#123; super.clear(); family_ = ""; bitField0_ = (bitField0_ &amp; ~0x00000001); column_ = ""; bitField0_ = (bitField0_ &amp; ~0x00000002); return this; &#125; public Builder clone() &#123; return create().mergeFrom(buildPartial()); &#125; public com.google.protobuf.Descriptors.Descriptor getDescriptorForType() &#123; return cn.edu.hust.demo1.hbase.coprocessor.server.Sum.internal_static_SumRequest_descriptor; &#125; public cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumRequest getDefaultInstanceForType() &#123; return cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumRequest.getDefaultInstance(); &#125; public cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumRequest build() &#123; cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumRequest result = buildPartial(); if (!result.isInitialized()) &#123; throw newUninitializedMessageException(result); &#125; return result; &#125; public cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumRequest buildPartial() &#123; cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumRequest result = new cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumRequest(this); int from_bitField0_ = bitField0_; int to_bitField0_ = 0; if (((from_bitField0_ &amp; 0x00000001) == 0x00000001)) &#123; to_bitField0_ |= 0x00000001; &#125; result.family_ = family_; if (((from_bitField0_ &amp; 0x00000002) == 0x00000002)) &#123; to_bitField0_ |= 0x00000002; &#125; result.column_ = column_; result.bitField0_ = to_bitField0_; onBuilt(); return result; &#125; public Builder mergeFrom(com.google.protobuf.Message other) &#123; if (other instanceof cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumRequest) &#123; return mergeFrom((cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumRequest)other); &#125; else &#123; super.mergeFrom(other); return this; &#125; &#125; public Builder mergeFrom(cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumRequest other) &#123; if (other == cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumRequest.getDefaultInstance()) return this; if (other.hasFamily()) &#123; bitField0_ |= 0x00000001; family_ = other.family_; onChanged(); &#125; if (other.hasColumn()) &#123; bitField0_ |= 0x00000002; column_ = other.column_; onChanged(); &#125; this.mergeUnknownFields(other.getUnknownFields()); return this; &#125; public final boolean isInitialized() &#123; if (!hasFamily()) &#123; return false; &#125; if (!hasColumn()) &#123; return false; &#125; return true; &#125; public Builder mergeFrom( com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException &#123; cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumRequest parsedMessage = null; try &#123; parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry); &#125; catch (com.google.protobuf.InvalidProtocolBufferException e) &#123; parsedMessage = (cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumRequest) e.getUnfinishedMessage(); throw e; &#125; finally &#123; if (parsedMessage != null) &#123; mergeFrom(parsedMessage); &#125; &#125; return this; &#125; private int bitField0_; // required string family = 1; private java.lang.Object family_ = ""; /** * &lt;code&gt;required string family = 1;&lt;/code&gt; */ public boolean hasFamily() &#123; return ((bitField0_ &amp; 0x00000001) == 0x00000001); &#125; /** * &lt;code&gt;required string family = 1;&lt;/code&gt; */ public java.lang.String getFamily() &#123; java.lang.Object ref = family_; if (!(ref instanceof java.lang.String)) &#123; java.lang.String s = ((com.google.protobuf.ByteString) ref) .toStringUtf8(); family_ = s; return s; &#125; else &#123; return (java.lang.String) ref; &#125; &#125; /** * &lt;code&gt;required string family = 1;&lt;/code&gt; */ public com.google.protobuf.ByteString getFamilyBytes() &#123; java.lang.Object ref = family_; if (ref instanceof String) &#123; com.google.protobuf.ByteString b = com.google.protobuf.ByteString.copyFromUtf8( (java.lang.String) ref); family_ = b; return b; &#125; else &#123; return (com.google.protobuf.ByteString) ref; &#125; &#125; /** * &lt;code&gt;required string family = 1;&lt;/code&gt; */ public Builder setFamily( java.lang.String value) &#123; if (value == null) &#123; throw new NullPointerException(); &#125; bitField0_ |= 0x00000001; family_ = value; onChanged(); return this; &#125; /** * &lt;code&gt;required string family = 1;&lt;/code&gt; */ public Builder clearFamily() &#123; bitField0_ = (bitField0_ &amp; ~0x00000001); family_ = getDefaultInstance().getFamily(); onChanged(); return this; &#125; /** * &lt;code&gt;required string family = 1;&lt;/code&gt; */ public Builder setFamilyBytes( com.google.protobuf.ByteString value) &#123; if (value == null) &#123; throw new NullPointerException(); &#125; bitField0_ |= 0x00000001; family_ = value; onChanged(); return this; &#125; // required string column = 2; private java.lang.Object column_ = ""; /** * &lt;code&gt;required string column = 2;&lt;/code&gt; */ public boolean hasColumn() &#123; return ((bitField0_ &amp; 0x00000002) == 0x00000002); &#125; /** * &lt;code&gt;required string column = 2;&lt;/code&gt; */ public java.lang.String getColumn() &#123; java.lang.Object ref = column_; if (!(ref instanceof java.lang.String)) &#123; java.lang.String s = ((com.google.protobuf.ByteString) ref) .toStringUtf8(); column_ = s; return s; &#125; else &#123; return (java.lang.String) ref; &#125; &#125; /** * &lt;code&gt;required string column = 2;&lt;/code&gt; */ public com.google.protobuf.ByteString getColumnBytes() &#123; java.lang.Object ref = column_; if (ref instanceof String) &#123; com.google.protobuf.ByteString b = com.google.protobuf.ByteString.copyFromUtf8( (java.lang.String) ref); column_ = b; return b; &#125; else &#123; return (com.google.protobuf.ByteString) ref; &#125; &#125; /** * &lt;code&gt;required string column = 2;&lt;/code&gt; */ public Builder setColumn( java.lang.String value) &#123; if (value == null) &#123; throw new NullPointerException(); &#125; bitField0_ |= 0x00000002; column_ = value; onChanged(); return this; &#125; /** * &lt;code&gt;required string column = 2;&lt;/code&gt; */ public Builder clearColumn() &#123; bitField0_ = (bitField0_ &amp; ~0x00000002); column_ = getDefaultInstance().getColumn(); onChanged(); return this; &#125; /** * &lt;code&gt;required string column = 2;&lt;/code&gt; */ public Builder setColumnBytes( com.google.protobuf.ByteString value) &#123; if (value == null) &#123; throw new NullPointerException(); &#125; bitField0_ |= 0x00000002; column_ = value; onChanged(); return this; &#125; // @@protoc_insertion_point(builder_scope:SumRequest) &#125; static &#123; defaultInstance = new SumRequest(true); defaultInstance.initFields(); &#125; // @@protoc_insertion_point(class_scope:SumRequest) &#125; public interface SumResponseOrBuilder extends com.google.protobuf.MessageOrBuilder &#123; // required int64 sum = 1 [default = 0]; /** * &lt;code&gt;required int64 sum = 1 [default = 0];&lt;/code&gt; */ boolean hasSum(); /** * &lt;code&gt;required int64 sum = 1 [default = 0];&lt;/code&gt; */ long getSum(); &#125; /** * Protobuf type &#123;@code SumResponse&#125; */ public static final class SumResponse extends com.google.protobuf.GeneratedMessage implements SumResponseOrBuilder &#123; // Use SumResponse.newBuilder() to construct. private SumResponse(com.google.protobuf.GeneratedMessage.Builder&lt;?&gt; builder) &#123; super(builder); this.unknownFields = builder.getUnknownFields(); &#125; private SumResponse(boolean noInit) &#123; this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); &#125; private static final SumResponse defaultInstance; public static SumResponse getDefaultInstance() &#123; return defaultInstance; &#125; public SumResponse getDefaultInstanceForType() &#123; return defaultInstance; &#125; private final com.google.protobuf.UnknownFieldSet unknownFields; @java.lang.Override public final com.google.protobuf.UnknownFieldSet getUnknownFields() &#123; return this.unknownFields; &#125; private SumResponse( com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException &#123; initFields(); int mutable_bitField0_ = 0; com.google.protobuf.UnknownFieldSet.Builder unknownFields = com.google.protobuf.UnknownFieldSet.newBuilder(); try &#123; boolean done = false; while (!done) &#123; int tag = input.readTag(); switch (tag) &#123; case 0: done = true; break; default: &#123; if (!parseUnknownField(input, unknownFields, extensionRegistry, tag)) &#123; done = true; &#125; break; &#125; case 8: &#123; bitField0_ |= 0x00000001; sum_ = input.readInt64(); break; &#125; &#125; &#125; &#125; catch (com.google.protobuf.InvalidProtocolBufferException e) &#123; throw e.setUnfinishedMessage(this); &#125; catch (java.io.IOException e) &#123; throw new com.google.protobuf.InvalidProtocolBufferException( e.getMessage()).setUnfinishedMessage(this); &#125; finally &#123; this.unknownFields = unknownFields.build(); makeExtensionsImmutable(); &#125; &#125; public static final com.google.protobuf.Descriptors.Descriptor getDescriptor() &#123; return cn.edu.hust.demo1.hbase.coprocessor.server.Sum.internal_static_SumResponse_descriptor; &#125; protected com.google.protobuf.GeneratedMessage.FieldAccessorTable internalGetFieldAccessorTable() &#123; return cn.edu.hust.demo1.hbase.coprocessor.server.Sum.internal_static_SumResponse_fieldAccessorTable .ensureFieldAccessorsInitialized( cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumResponse.class, cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumResponse.Builder.class); &#125; public static com.google.protobuf.Parser&lt;SumResponse&gt; PARSER = new com.google.protobuf.AbstractParser&lt;SumResponse&gt;() &#123; public SumResponse parsePartialFrom( com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException &#123; return new SumResponse(input, extensionRegistry); &#125; &#125;; @java.lang.Override public com.google.protobuf.Parser&lt;SumResponse&gt; getParserForType() &#123; return PARSER; &#125; private int bitField0_; // required int64 sum = 1 [default = 0]; public static final int SUM_FIELD_NUMBER = 1; private long sum_; /** * &lt;code&gt;required int64 sum = 1 [default = 0];&lt;/code&gt; */ public boolean hasSum() &#123; return ((bitField0_ &amp; 0x00000001) == 0x00000001); &#125; /** * &lt;code&gt;required int64 sum = 1 [default = 0];&lt;/code&gt; */ public long getSum() &#123; return sum_; &#125; private void initFields() &#123; sum_ = 0L; &#125; private byte memoizedIsInitialized = -1; public final boolean isInitialized() &#123; byte isInitialized = memoizedIsInitialized; if (isInitialized != -1) return isInitialized == 1; if (!hasSum()) &#123; memoizedIsInitialized = 0; return false; &#125; memoizedIsInitialized = 1; return true; &#125; public void writeTo(com.google.protobuf.CodedOutputStream output) throws java.io.IOException &#123; getSerializedSize(); if (((bitField0_ &amp; 0x00000001) == 0x00000001)) &#123; output.writeInt64(1, sum_); &#125; getUnknownFields().writeTo(output); &#125; private int memoizedSerializedSize = -1; public int getSerializedSize() &#123; int size = memoizedSerializedSize; if (size != -1) return size; size = 0; if (((bitField0_ &amp; 0x00000001) == 0x00000001)) &#123; size += com.google.protobuf.CodedOutputStream .computeInt64Size(1, sum_); &#125; size += getUnknownFields().getSerializedSize(); memoizedSerializedSize = size; return size; &#125; private static final long serialVersionUID = 0L; @java.lang.Override protected java.lang.Object writeReplace() throws java.io.ObjectStreamException &#123; return super.writeReplace(); &#125; @java.lang.Override public boolean equals(final java.lang.Object obj) &#123; if (obj == this) &#123; return true; &#125; if (!(obj instanceof cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumResponse)) &#123; return super.equals(obj); &#125; cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumResponse other = (cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumResponse) obj; boolean result = true; result = result &amp;&amp; (hasSum() == other.hasSum()); if (hasSum()) &#123; result = result &amp;&amp; (getSum() == other.getSum()); &#125; result = result &amp;&amp; getUnknownFields().equals(other.getUnknownFields()); return result; &#125; private int memoizedHashCode = 0; @java.lang.Override public int hashCode() &#123; if (memoizedHashCode != 0) &#123; return memoizedHashCode; &#125; int hash = 41; hash = (19 * hash) + getDescriptorForType().hashCode(); if (hasSum()) &#123; hash = (37 * hash) + SUM_FIELD_NUMBER; hash = (53 * hash) + hashLong(getSum()); &#125; hash = (29 * hash) + getUnknownFields().hashCode(); memoizedHashCode = hash; return hash; &#125; public static cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumResponse parseFrom( com.google.protobuf.ByteString data) throws com.google.protobuf.InvalidProtocolBufferException &#123; return PARSER.parseFrom(data); &#125; public static cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumResponse parseFrom( com.google.protobuf.ByteString data, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException &#123; return PARSER.parseFrom(data, extensionRegistry); &#125; public static cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumResponse parseFrom(byte[] data) throws com.google.protobuf.InvalidProtocolBufferException &#123; return PARSER.parseFrom(data); &#125; public static cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumResponse parseFrom( byte[] data, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException &#123; return PARSER.parseFrom(data, extensionRegistry); &#125; public static cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumResponse parseFrom(java.io.InputStream input) throws java.io.IOException &#123; return PARSER.parseFrom(input); &#125; public static cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumResponse parseFrom( java.io.InputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException &#123; return PARSER.parseFrom(input, extensionRegistry); &#125; public static cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumResponse parseDelimitedFrom(java.io.InputStream input) throws java.io.IOException &#123; return PARSER.parseDelimitedFrom(input); &#125; public static cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumResponse parseDelimitedFrom( java.io.InputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException &#123; return PARSER.parseDelimitedFrom(input, extensionRegistry); &#125; public static cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumResponse parseFrom( com.google.protobuf.CodedInputStream input) throws java.io.IOException &#123; return PARSER.parseFrom(input); &#125; public static cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumResponse parseFrom( com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException &#123; return PARSER.parseFrom(input, extensionRegistry); &#125; public static Builder newBuilder() &#123; return Builder.create(); &#125; public Builder newBuilderForType() &#123; return newBuilder(); &#125; public static Builder newBuilder(cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumResponse prototype) &#123; return newBuilder().mergeFrom(prototype); &#125; public Builder toBuilder() &#123; return newBuilder(this); &#125; @java.lang.Override protected Builder newBuilderForType( com.google.protobuf.GeneratedMessage.BuilderParent parent) &#123; Builder builder = new Builder(parent); return builder; &#125; /** * Protobuf type &#123;@code SumResponse&#125; */ public static final class Builder extends com.google.protobuf.GeneratedMessage.Builder&lt;Builder&gt; implements cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumResponseOrBuilder &#123; public static final com.google.protobuf.Descriptors.Descriptor getDescriptor() &#123; return cn.edu.hust.demo1.hbase.coprocessor.server.Sum.internal_static_SumResponse_descriptor; &#125; protected com.google.protobuf.GeneratedMessage.FieldAccessorTable internalGetFieldAccessorTable() &#123; return cn.edu.hust.demo1.hbase.coprocessor.server.Sum.internal_static_SumResponse_fieldAccessorTable .ensureFieldAccessorsInitialized( cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumResponse.class, cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumResponse.Builder.class); &#125; // Construct using cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumResponse.newBuilder() private Builder() &#123; maybeForceBuilderInitialization(); &#125; private Builder( com.google.protobuf.GeneratedMessage.BuilderParent parent) &#123; super(parent); maybeForceBuilderInitialization(); &#125; private void maybeForceBuilderInitialization() &#123; if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) &#123; &#125; &#125; private static Builder create() &#123; return new Builder(); &#125; public Builder clear() &#123; super.clear(); sum_ = 0L; bitField0_ = (bitField0_ &amp; ~0x00000001); return this; &#125; public Builder clone() &#123; return create().mergeFrom(buildPartial()); &#125; public com.google.protobuf.Descriptors.Descriptor getDescriptorForType() &#123; return cn.edu.hust.demo1.hbase.coprocessor.server.Sum.internal_static_SumResponse_descriptor; &#125; public cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumResponse getDefaultInstanceForType() &#123; return cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumResponse.getDefaultInstance(); &#125; public cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumResponse build() &#123; cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumResponse result = buildPartial(); if (!result.isInitialized()) &#123; throw newUninitializedMessageException(result); &#125; return result; &#125; public cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumResponse buildPartial() &#123; cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumResponse result = new cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumResponse(this); int from_bitField0_ = bitField0_; int to_bitField0_ = 0; if (((from_bitField0_ &amp; 0x00000001) == 0x00000001)) &#123; to_bitField0_ |= 0x00000001; &#125; result.sum_ = sum_; result.bitField0_ = to_bitField0_; onBuilt(); return result; &#125; public Builder mergeFrom(com.google.protobuf.Message other) &#123; if (other instanceof cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumResponse) &#123; return mergeFrom((cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumResponse)other); &#125; else &#123; super.mergeFrom(other); return this; &#125; &#125; public Builder mergeFrom(cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumResponse other) &#123; if (other == cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumResponse.getDefaultInstance()) return this; if (other.hasSum()) &#123; setSum(other.getSum()); &#125; this.mergeUnknownFields(other.getUnknownFields()); return this; &#125; public final boolean isInitialized() &#123; if (!hasSum()) &#123; return false; &#125; return true; &#125; public Builder mergeFrom( com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException &#123; cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumResponse parsedMessage = null; try &#123; parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry); &#125; catch (com.google.protobuf.InvalidProtocolBufferException e) &#123; parsedMessage = (cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumResponse) e.getUnfinishedMessage(); throw e; &#125; finally &#123; if (parsedMessage != null) &#123; mergeFrom(parsedMessage); &#125; &#125; return this; &#125; private int bitField0_; // required int64 sum = 1 [default = 0]; private long sum_ ; /** * &lt;code&gt;required int64 sum = 1 [default = 0];&lt;/code&gt; */ public boolean hasSum() &#123; return ((bitField0_ &amp; 0x00000001) == 0x00000001); &#125; /** * &lt;code&gt;required int64 sum = 1 [default = 0];&lt;/code&gt; */ public long getSum() &#123; return sum_; &#125; /** * &lt;code&gt;required int64 sum = 1 [default = 0];&lt;/code&gt; */ public Builder setSum(long value) &#123; bitField0_ |= 0x00000001; sum_ = value; onChanged(); return this; &#125; /** * &lt;code&gt;required int64 sum = 1 [default = 0];&lt;/code&gt; */ public Builder clearSum() &#123; bitField0_ = (bitField0_ &amp; ~0x00000001); sum_ = 0L; onChanged(); return this; &#125; // @@protoc_insertion_point(builder_scope:SumResponse) &#125; static &#123; defaultInstance = new SumResponse(true); defaultInstance.initFields(); &#125; // @@protoc_insertion_point(class_scope:SumResponse) &#125; /** * Protobuf service &#123;@code SumService&#125; */ public static abstract class SumService implements com.google.protobuf.Service &#123; protected SumService() &#123;&#125; public interface Interface &#123; /** * &lt;code&gt;rpc getSum(.SumRequest) returns (.SumResponse);&lt;/code&gt; */ public abstract void getSum( com.google.protobuf.RpcController controller, cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumRequest request, com.google.protobuf.RpcCallback&lt;cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumResponse&gt; done); &#125; public static com.google.protobuf.Service newReflectiveService( final Interface impl) &#123; return new SumService() &#123; @java.lang.Override public void getSum( com.google.protobuf.RpcController controller, cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumRequest request, com.google.protobuf.RpcCallback&lt;cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumResponse&gt; done) &#123; impl.getSum(controller, request, done); &#125; &#125;; &#125; public static com.google.protobuf.BlockingService newReflectiveBlockingService(final BlockingInterface impl) &#123; return new com.google.protobuf.BlockingService() &#123; public final com.google.protobuf.Descriptors.ServiceDescriptor getDescriptorForType() &#123; return getDescriptor(); &#125; public final com.google.protobuf.Message callBlockingMethod( com.google.protobuf.Descriptors.MethodDescriptor method, com.google.protobuf.RpcController controller, com.google.protobuf.Message request) throws com.google.protobuf.ServiceException &#123; if (method.getService() != getDescriptor()) &#123; throw new java.lang.IllegalArgumentException( "Service.callBlockingMethod() given method descriptor for " + "wrong service type."); &#125; switch(method.getIndex()) &#123; case 0: return impl.getSum(controller, (cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumRequest)request); default: throw new java.lang.AssertionError("Can't get here."); &#125; &#125; public final com.google.protobuf.Message getRequestPrototype( com.google.protobuf.Descriptors.MethodDescriptor method) &#123; if (method.getService() != getDescriptor()) &#123; throw new java.lang.IllegalArgumentException( "Service.getRequestPrototype() given method " + "descriptor for wrong service type."); &#125; switch(method.getIndex()) &#123; case 0: return cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumRequest.getDefaultInstance(); default: throw new java.lang.AssertionError("Can't get here."); &#125; &#125; public final com.google.protobuf.Message getResponsePrototype( com.google.protobuf.Descriptors.MethodDescriptor method) &#123; if (method.getService() != getDescriptor()) &#123; throw new java.lang.IllegalArgumentException( "Service.getResponsePrototype() given method " + "descriptor for wrong service type."); &#125; switch(method.getIndex()) &#123; case 0: return cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumResponse.getDefaultInstance(); default: throw new java.lang.AssertionError("Can't get here."); &#125; &#125; &#125;; &#125; /** * &lt;code&gt;rpc getSum(.SumRequest) returns (.SumResponse);&lt;/code&gt; */ public abstract void getSum( com.google.protobuf.RpcController controller, cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumRequest request, com.google.protobuf.RpcCallback&lt;cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumResponse&gt; done); public static final com.google.protobuf.Descriptors.ServiceDescriptor getDescriptor() &#123; return cn.edu.hust.demo1.hbase.coprocessor.server.Sum.getDescriptor().getServices().get(0); &#125; public final com.google.protobuf.Descriptors.ServiceDescriptor getDescriptorForType() &#123; return getDescriptor(); &#125; public final void callMethod( com.google.protobuf.Descriptors.MethodDescriptor method, com.google.protobuf.RpcController controller, com.google.protobuf.Message request, com.google.protobuf.RpcCallback&lt; com.google.protobuf.Message&gt; done) &#123; if (method.getService() != getDescriptor()) &#123; throw new java.lang.IllegalArgumentException( "Service.callMethod() given method descriptor for wrong " + "service type."); &#125; switch(method.getIndex()) &#123; case 0: this.getSum(controller, (cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumRequest)request, com.google.protobuf.RpcUtil.&lt;cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumResponse&gt;specializeCallback( done)); return; default: throw new java.lang.AssertionError("Can't get here."); &#125; &#125; public final com.google.protobuf.Message getRequestPrototype( com.google.protobuf.Descriptors.MethodDescriptor method) &#123; if (method.getService() != getDescriptor()) &#123; throw new java.lang.IllegalArgumentException( "Service.getRequestPrototype() given method " + "descriptor for wrong service type."); &#125; switch(method.getIndex()) &#123; case 0: return cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumRequest.getDefaultInstance(); default: throw new java.lang.AssertionError("Can't get here."); &#125; &#125; public final com.google.protobuf.Message getResponsePrototype( com.google.protobuf.Descriptors.MethodDescriptor method) &#123; if (method.getService() != getDescriptor()) &#123; throw new java.lang.IllegalArgumentException( "Service.getResponsePrototype() given method " + "descriptor for wrong service type."); &#125; switch(method.getIndex()) &#123; case 0: return cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumResponse.getDefaultInstance(); default: throw new java.lang.AssertionError("Can't get here."); &#125; &#125; public static Stub newStub( com.google.protobuf.RpcChannel channel) &#123; return new Stub(channel); &#125; public static final class Stub extends cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumService implements Interface &#123; private Stub(com.google.protobuf.RpcChannel channel) &#123; this.channel = channel; &#125; private final com.google.protobuf.RpcChannel channel; public com.google.protobuf.RpcChannel getChannel() &#123; return channel; &#125; public void getSum( com.google.protobuf.RpcController controller, cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumRequest request, com.google.protobuf.RpcCallback&lt;cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumResponse&gt; done) &#123; channel.callMethod( getDescriptor().getMethods().get(0), controller, request, cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumResponse.getDefaultInstance(), com.google.protobuf.RpcUtil.generalizeCallback( done, cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumResponse.class, cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumResponse.getDefaultInstance())); &#125; &#125; public static BlockingInterface newBlockingStub( com.google.protobuf.BlockingRpcChannel channel) &#123; return new BlockingStub(channel); &#125; public interface BlockingInterface &#123; public cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumResponse getSum( com.google.protobuf.RpcController controller, cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumRequest request) throws com.google.protobuf.ServiceException; &#125; private static final class BlockingStub implements BlockingInterface &#123; private BlockingStub(com.google.protobuf.BlockingRpcChannel channel) &#123; this.channel = channel; &#125; private final com.google.protobuf.BlockingRpcChannel channel; public cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumResponse getSum( com.google.protobuf.RpcController controller, cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumRequest request) throws com.google.protobuf.ServiceException &#123; return (cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumResponse) channel.callBlockingMethod( getDescriptor().getMethods().get(0), controller, request, cn.edu.hust.demo1.hbase.coprocessor.server.Sum.SumResponse.getDefaultInstance()); &#125; &#125; // @@protoc_insertion_point(class_scope:SumService) &#125; private static com.google.protobuf.Descriptors.Descriptor internal_static_SumRequest_descriptor; private static com.google.protobuf.GeneratedMessage.FieldAccessorTable internal_static_SumRequest_fieldAccessorTable; private static com.google.protobuf.Descriptors.Descriptor internal_static_SumResponse_descriptor; private static com.google.protobuf.GeneratedMessage.FieldAccessorTable internal_static_SumResponse_fieldAccessorTable; public static com.google.protobuf.Descriptors.FileDescriptor getDescriptor() &#123; return descriptor; &#125; private static com.google.protobuf.Descriptors.FileDescriptor descriptor; static &#123; java.lang.String[] descriptorData = &#123; "\n\rsumcode.proto\",\n\nSumRequest\022\016\n\006family\030" + "\001 \002(\t\022\016\n\006column\030\002 \002(\t\"\035\n\013SumResponse\022\016\n\003" + "sum\030\001 \002(\003:\001021\n\nSumService\022#\n\006getSum\022\013.S" + "umRequest\032\014.SumResponseB \n\021com.endpoint." + "testB\003SumH\001\210\001\001\240\001\001" &#125;; com.google.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner assigner = new com.google.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner() &#123; public com.google.protobuf.ExtensionRegistry assignDescriptors( com.google.protobuf.Descriptors.FileDescriptor root) &#123; descriptor = root; internal_static_SumRequest_descriptor = getDescriptor().getMessageTypes().get(0); internal_static_SumRequest_fieldAccessorTable = new com.google.protobuf.GeneratedMessage.FieldAccessorTable( internal_static_SumRequest_descriptor, new java.lang.String[] &#123; "Family", "Column", &#125;); internal_static_SumResponse_descriptor = getDescriptor().getMessageTypes().get(1); internal_static_SumResponse_fieldAccessorTable = new com.google.protobuf.GeneratedMessage.FieldAccessorTable( internal_static_SumResponse_descriptor, new java.lang.String[] &#123; "Sum", &#125;); return null; &#125; &#125;; com.google.protobuf.Descriptors.FileDescriptor .internalBuildGeneratedFileFrom(descriptorData, new com.google.protobuf.Descriptors.FileDescriptor[] &#123; &#125;, assigner); &#125; // @@protoc_insertion_point(outer_class_scope)&#125; Endpoint的基本框架代码我剥离出来如下：需要知道的是每一个Endpoint的协处理器是运行在单个region上的，所以每一个结算结果都是对于单个region的数据范围的计算结果。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public class Endpoint类名称 extends protol类.服务接口名称 implements Coprocessor, CoprocessorService &#123; // 单个region的上下文环境信息 private RegionCoprocessorEnvironment envi; // rpc服务，返回本身即可，因为此类实例就是一个服务实现 @Override public Service getService() &#123; return this; &#125; // 协处理器是运行于region中的，每一个region都会加载协处理器 // 这个方法会在regionserver打开region时候执行（还没有真正打开） @Override public void start(CoprocessorEnvironment env) throws IOException &#123;// 需要检查当前环境是否在region上 if (env instanceof RegionCoprocessorEnvironment) &#123; this.envi = (RegionCoprocessorEnvironment) env; &#125; else &#123; throw new CoprocessorException("Must be loaded on a table region!"); &#125; &#125; // 这个方法会在regionserver关闭region时候执行（还没有真正关闭） @Override public void stop(CoprocessorEnvironment env) throws IOException &#123;// nothing to do &#125; // 服务端（每一个region上）的接口实现方法 // 第一个参数是固定的，其余的request参数和response参数是proto接口文件中指明的。 @Override public void 服务接口方法(RpcController controller, 服务接口参数 request, RpcCallback&lt;服务接口response&gt; done) &#123;// 单个region上的计算结果值，根据需要实现 int result = 0; // 定义返回response Endpoint类名称.返回response.Builder responseBuilder = Endpoint类名称.返回response .newBuilder(); // 计算结果值逻辑代码 // 返回结果值 responseBuilder.setResult(result); done.run(responseBuilder.build()); return; &#125;&#125; 所以根据本次需求，填充的代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889package cn.edu.hust.demo1.hbase.coprocessor.server;import java.io.IOException;import java.util.ArrayList;import java.util.List;import com.google.protobuf.RpcCallback;import org.apache.hadoop.hbase.Cell;import org.apache.hadoop.hbase.CellUtil;import org.apache.hadoop.hbase.Coprocessor;import org.apache.hadoop.hbase.CoprocessorEnvironment;import org.apache.hadoop.hbase.client.Scan;import org.apache.hadoop.hbase.coprocessor.CoprocessorException;import org.apache.hadoop.hbase.coprocessor.CoprocessorService;import org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment;import org.apache.hadoop.hbase.protobuf.ResponseConverter;import org.apache.hadoop.hbase.regionserver.InternalScanner;import org.apache.hadoop.hbase.util.Bytes;import com.google.protobuf.RpcController;import com.google.protobuf.Service;public class SumEndPoint extends Sum.SumService implements Coprocessor,CoprocessorService&#123; private RegionCoprocessorEnvironment env; // 定义环境 @Override public Service getService() &#123; return this; &#125; @Override public void start(CoprocessorEnvironment env) throws IOException &#123; if (env instanceof RegionCoprocessorEnvironment) &#123; this.env = (RegionCoprocessorEnvironment)env; &#125; else &#123; throw new CoprocessorException("no load region"); &#125; &#125; @Override public void stop(CoprocessorEnvironment env) throws IOException &#123; &#125; @Override public void getSum(RpcController controller, Sum.SumRequest request, RpcCallback&lt;Sum.SumResponse&gt; done) &#123; // 设置扫描对象 Scan scan = new Scan(); scan.addFamily(Bytes.toBytes(request.getFamily())); scan.addColumn(Bytes.toBytes(request.getFamily()), Bytes.toBytes(request.getColumn())); // 定义变量 Sum.SumResponse response = null; InternalScanner scanner = null; // 扫描每个region，取值后求和 try &#123; scanner = env.getRegion().getScanner(scan); List&lt;Cell&gt; results = new ArrayList&lt;Cell&gt;(); boolean hasMore = false; Long sum = 0L; do &#123; hasMore = scanner.next(results); for (Cell cell : results) &#123; sum += Long.parseLong(new String(CellUtil.cloneValue(cell))); &#125; results.clear(); &#125; while (hasMore); // 设置返回结果 response = Sum.SumResponse.newBuilder().setSum(sum).build(); &#125; catch (IOException e) &#123; ResponseConverter.setControllerException(controller, e); &#125; finally &#123; if (scanner != null) &#123; try &#123; scanner.close(); &#125; catch (IOException e) &#123; //e.printStackTrace(); &#125; &#125; &#125; // 将rpc结果返回给客户端 done.run(response); &#125;&#125; 以上就是服务端程序，然后可以将其打包上传至服务端，EndPoint类和protobuf类都需要打包上传至hdfs。 然后在我们的表中指定该coprocessor： 1alter'coprocessor_demo1',METHOD =&gt;'table_att','coprocessor' =&gt;'hdfs:///data/hadoop.jar|cn.edu.hust.demo1.hbase.coprocessor.server.SumEndPoint|100' 可以查看一下是否指定成功： 1234567hbase(main):012:0&gt; desc 'coprocessor_demo1'Table coprocessor_demo1 is ENABLEDcoprocessor_demo1, &#123;TABLE_ATTRIBUTES =&gt; &#123;coprocessor$1 =&gt; 'hdfs:///data/hadoop.jar|cn.edu.hust.demo1.hbase.coprocessor.server.SumEndPoint|100'&#125;COLUMN FAMILIES DESCRIPTION&#123;NAME =&gt; 'cf', BLOOMFILTER =&gt; 'ROW', VERSIONS =&gt; '1', IN_MEMORY =&gt; 'false', KEEP_DELETED_CELLS =&gt; 'FALSE', DATA_BLOCK_ENCODING =&gt; 'NONE', TTL =&gt; 'FOREVER', COMPRESSION =&gt; 'NONE', MIN_VERSIONS =&gt; '0', BLOCKCACHE =&gt; 'true', BLOCKSIZE =&gt; '65536', REPLICATION_SCOPE =&gt; '0'&#125;1 row(s) in 2.2380 seconds 下面我们写客户端的测试程序： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475package cn.edu.hust.demo1.hbase.coprocessor.client;import cn.edu.hust.demo1.hbase.coprocessor.server.Sum;import com.google.protobuf.ServiceException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.hbase.*;import org.apache.hadoop.hbase.client.*;import org.apache.hadoop.hbase.client.coprocessor.Batch;import org.apache.hadoop.hbase.ipc.BlockingRpcCallback;import org.junit.Before;import org.junit.Test;import java.io.IOException;import java.util.Map;public class HbaseApiTest &#123; static Configuration conf = null; static HBaseAdmin admin = null; static Connection connection = null; static Table table = null; @Before public void init() throws IOException &#123; //创建conf对象 conf = HBaseConfiguration.create(); //通过连接工厂创建连接对象 connection = ConnectionFactory.createConnection(conf); //通过连接查询tableName对象 TableName tname = TableName.valueOf("coprocessor_demo1"); //获得table table = connection.getTable(tname); &#125; @Test public void put() throws Exception &#123; long sum = 0L; // 设置请求对象 final Sum.SumRequest request = Sum.SumRequest.newBuilder().setFamily("cf").setColumn("value").build(); try &#123; // 获得返回值 Map&lt;byte[], Long&gt; result = table.coprocessorService(Sum.SumService.class, null, null, new Batch.Call&lt;Sum.SumService, Long&gt;() &#123; @Override public Long call(Sum.SumService service) throws IOException &#123; BlockingRpcCallback&lt;Sum.SumResponse&gt; rpcCallback = new BlockingRpcCallback&lt;Sum.SumResponse&gt;(); service.getSum(null, request, rpcCallback); Sum.SumResponse response = (Sum.SumResponse) rpcCallback.get(); return response.hasSum() ? response.getSum() : 0L; &#125; &#125;); // 将返回值进行迭代相加 for (Long v : result.values()) &#123; sum += v; &#125; // 结果输出 System.out.println("sum: " + sum); &#125; catch (ServiceException e) &#123; e.printStackTrace(); &#125;catch (Throwable e) &#123; e.printStackTrace(); &#125; table.close(); connection.close(); &#125;&#125; 上面的程序是将每一个region中的返回值叠加，获取sum，打印的结果为： 123sum: 388Process finished with exit code 0]]></content>
      <categories>
        <category>大数据学习</category>
        <category>Hbase</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hbase扫描器和过滤器]]></title>
    <url>%2Fposts%2Faf1f3985.html</url>
    <content type="text"><![CDATA[1. Hbase扫描器 Hbase在扫描数据的时候，使用scanner表扫描器。 HTable通过一个Scan实例，调用getScanner(scan)来获取扫描器。可以配置扫描起止位以及其他的过滤条件。 通过迭代器返回查询结果，使用起来虽然不是很方便，不过并不复杂。 但是这里有一点可能被忽略的地方，就是返回的scanner迭代器，每次调用next的获取下一条记录的时候，默认配置下会访问一次RegionServer。这在网络不是很好的情况下，对性能的影响是很大的，建议配置扫描器缓存。 扫描器缓存 Hbase.client.scanner.caching配置项可以设置Hbase scanner一次从服务端抓取的数据条数，默认情况下一次一条。通过将其设置成一个合理的值，可以减少scan过程中next()的时间开销，代价是scanner需要通过客户端的内存来维持这些被cache的行记录。 有三个地方可以对其进行配置： 1、在Hbase的conf配置文件中进行配置。 2、通过调用HTable.setScannerCaching(int scannerCaching)进行配置。 3、通过调用Scan.setCaching(int caching)进行配置。 三者的优先级从上到下依次升高。 Scan类拥有以下构造器： 123456public Scan()public Scan(byte [] startRow)public Scan(byte [] startRow, byte [] stopRow)public Scan(byte [] startRow, Filter filter)public Scan(Get get)public Scan(Scan scan) 用户可以选择性的提供startRow参数，来定义扫描读取Hbase表的起始行键，即行键不是必须指定的。同时可选stopRow参数来限定读取到何处停止。 其实行包括在内，而终止行不包含在内。一般区间表示为[startRow,stopRow) 扫描操作有一个特点：用户提供的参数不必精确匹配两行。扫描会匹配相等或者大于给定的起始行的行键。如果没有显示地指定起始行，它会从表的起始位置开始获取数据。 当遇到了与设置的终止行相同或者大于终止行的行键时，扫描也会终止。如果没有指定终止键，会扫描到表尾。 另一个可选的参数叫做过滤器(filter),可直接指向Filter实例。尽管Scan实例通常由空白构造器构造，但其所有可选参数都有对应的getter方法和setter方法。 建Scan实例后，用户可能还要给它增加更多限制条件。这种情况下，用户仍然可以使用空白 参数的扫描，它可以读取整个表格，包括所有列族以及它们的所有列。可以用多种方法限制要读取的数据： 123456public Scan addFamily(byte [] family) // 方法限制返回数据的列族public Scan addColumn(byte [] family, byte [] qualifier) // 方法限制返回的列Scan setTimeRange(long minStamp,long maxStamp) // 设置时间范围Scan setTimeStamp(long timestamp) // 设置时间戳Scan setMaxVersions() // 设置最大版本数Scan setMaxVersions(int maxVersions) // 设置最大版本数 示例代码： 1234567891011121314151617181920212223242526@Testpublic void testScan() throws IOException &#123; Connection conn = ConnectionFactory.createConnection(); Table table = conn.getTable(TableName.valueOf("student")); Scan scan = new Scan(); scan.withStartRow(Bytes.toBytes("223110")).withStopRow(Bytes.toBytes("225019")); scan.addFamily(Bytes.toBytes("base_info")); ResultScanner scanner = table.getScanner(scan); Iterator&lt;Result&gt; results = scanner.iterator(); while (results.hasNext())&#123; Result r = results.next(); String rowId = Bytes.toString(r.getRow()); Cell cId = r.getColumnLatestCell(Bytes.toBytes("base_info"),Bytes.toBytes("id")); Cell cName = r.getColumnLatestCell(Bytes.toBytes("base_info"),Bytes.toBytes("name")); Cell cAge = r.getColumnLatestCell(Bytes.toBytes("base_info"),Bytes.toBytes("age")); int id = Bytes.toInt(CellUtil.cloneValue(cId)); String name = Bytes.toString(CellUtil.cloneValue(cName)); int age = Bytes.toInt(CellUtil.cloneValue(cAge)); System.out.println("-----------------------------"); System.out.println(rowId + "," + id + "," + age + "," + name); &#125; scanner.close(); table.close(); conn.close();&#125; 扫描器租约 要确保尽早释放扫描器对象，一个打开的扫描器会占用不少的服务端资源，累计多了会占用大量的堆空间。当使用完ResultScanner之后调用它的close()方法，同时当把close()方法放到try/finally块中，以保证其在迭代获取数据过程中出现异常和错误时，仍然能执行close()。 设置扫描器缓存 每一个next()调用都会为每一行数据生成一个单独的RPC请求，即使使用next(int nbRows)方法也是如此，因为该方法仅仅是在客户端循环地调用next()方法。很显然，当单元格数据较少时，这样做的性能不会很好。因此，如果一次RPC请求可以获取多行数据，这样更有意义。这样的方法可以由扫描器的缓存实现，默认情况下，这个缓存是关闭的。 Scan类中提供了设置缓存的方法如下： 1234public Scan setCacheBlocks(boolean cacheBlocks) // 设置是否应用缓存块来进行扫描public boolean getCacheBlocks() // 查看是否支持块缓存public Scan setCaching(int caching) // 设置扫描器的缓存行数public int getCaching() // 获取扫描器中的缓存行数 用户需要少量的RPC请求次数和客户端以及服务器的内存消耗找到平衡点。很多时候，设置扫描器缓存可以提高性能，不过设置的太高就会产生不良的影响：每次调用next()将会占用更长的时间，因为要获取更多的文件并传输到客户端，如果返回给客户端的数据超出了其堆的大小，程序就会终止并抛出OutOfMemoryException异常。 扫描器demo 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172package com.rickiyang.Hbase;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.Hbase.Cell;import org.apache.hadoop.Hbase.HbaseConfiguration;import org.apache.hadoop.Hbase.TableName;import org.apache.hadoop.Hbase.client.*;import org.apache.hadoop.Hbase.util.Bytes;import org.junit.Before;import org.junit.Test;import java.io.IOException;/** * @Author yangyue * @Date Created in 下午5:58 2019/1/19 * @Modified by: * @Description: **/public class HbaseTest &#123; static Configuration conf = null; static HbaseAdmin admin = null; static Connection connection = null; static Table table = null; @Before public void init() throws IOException &#123; //创建conf对象 conf = HbaseConfiguration.create(); //通过连接工厂创建连接对象 connection = ConnectionFactory.createConnection(conf); //通过连接查询tableName对象 TableName tname = TableName.valueOf("ns1:t1"); //获得table table = connection.getTable(tname); &#125; @Test public void testGetMultipleRecords() throws IOException &#123; Scan scan = new Scan(); // 这里的Row设置的是RowKey！而不是行号 scan.withStartRow(Bytes.toBytes(1)).withStopRow(Bytes.toBytes(10)); //设置扫描缓存 scan.setCaching(1000); ResultScanner scanner = table.getScanner(scan); printResultScanner(scanner); &#125; public void printResultScanner(ResultScanner scanner) &#123; for (Result row : scanner) &#123; // Hbase中一个RowKey会包含[1,n]条记录，所以需要循环 System.out.println("\n RowKey:" + String.valueOf(Bytes.toInt(row.getRow()))); printRow(row); &#125; &#125; public void printRow(Result row) &#123; for (Cell cell : row.rawCells()) &#123; System.out.print(String.valueOf(Bytes.toInt(cell.getRow())) + " "); System.out.print(new String(cell.getFamily()) + ":"); System.out.print(new String(cell.getQualifier()) + " = "); System.out.print(new String(cell.getValue())); System.out.print(" Timestamp = " + cell.getTimestamp()); &#125; &#125; &#125; 2. Hbase过滤器 1、使用过滤器可以提高操作表的效率， Hbase中两种数据读取函数get()和scan()都支持过滤器，支持直接访问和通过指定起止行键来访问，但是缺少细粒度的筛选功能（如基于正则表达式对行键或值进行筛选的功能）。 2、可以使用预定义好的过滤器或者是实现自定义过滤器。 3、过滤器在客户端创建，通过RPC传送到服务器端，在服务器端执行过滤操作，把数据返回给客户端。 2.1 过滤器分类 Comparision Filters(比较过滤器) RowFilter 行过滤器 行键过滤器，一般来讲，执行 Scan 使用 startRow/stopRow 方式比较好，而 RowFilter 过滤器也可以完成对某一行的过滤。 FamilyFilter 列族过滤器 QualifierFilter 列名过滤器 ValueFilter 值过滤器 值过滤器用户筛选某个特定值的单元格。与RegexStringComparator配合使用，可以使用功能强大的表达式来进行筛选。 DependentColumnFilter 参考列过滤器 该过滤器有两个参数 —— 列族和列修饰。 尝试找到该列所在的每一行，并返回该行具有相同时间戳的全部键值对。如果某一行不包含指定的列，则该行的任何键值对都不返回。 Dedicated Filters(专用过滤器) SingleColumnValueFilter 用一列的值决定是否一行数据是否被过滤 SingleColumnValueExcludeFilter 单列排除过滤器，该过滤器继承SingleColumnValueFilter，排除当前列。 PrefixFilter 前缀过滤器。筛选出具有特点前缀的行键的数据。扫描操作以字典序查找，当遇到比前缀大的行时，扫描结束。PrefixFilter对get()方法作用不大。前缀过滤器只针对行键。 PageFilter 分页过滤器。可以使用这个过滤器对结果按行分页。当用户创建PageFilter的实例的时候，指定了pageSize，这个参数可以控制每页返回的行数。 需要注意的是，该过滤器并不能保证返回的结果行数小于等于指定的页面行数，因为过滤器是分别作用到各个region server的，它只能保证当前region返回的结果行数不超过指定页面行数。 KeyOnlyFilter 行健过滤器。只返回每行的行键，不返回值。对于之关注于行键的应用常见来说非常合适，不返回值，可以减少传递到客户端的数据量，能起到一定的优化作用。 FirstKeyOnlyFilter 首次行健过滤器。返回结果中首次匹配关键字的列。 TimestampsFilter 时间戳过滤器。使用时间戳过滤器可以对扫描结果中对版本进行细粒度的控制。 RandomRowFilter 随机行过滤器。随机行过滤器可以让结果中包含随机行。 该过滤器是随机选择一行的过滤器。参数 chance 是一个浮点值，介于 0.1 和 1.0 之间。 ColumnRangeFilter 基于列范围（不是行范围）过滤数据。 可用于获得一个范围的列，例如，如果你的一行中有百万个列，但是你只希望查看列名为aaa到fff的范围; 该方法从 Hbase 0.92 版本开始引入; 一个列名是可以出现在多个列族中的，该过滤器将返回所有列族中匹配的列。 Decorating Filters(附加过滤器) SkipFilter 跳转过滤器。与ValueFilter结合使用，如果发现一行中的某一列不符合条件，那么整行都会被过滤掉。 WhileMatchFilters 全匹配过滤器。如果你想想要在遇到某种条件数据之前的数据时，就可以使用这个过滤器，当遇到不符合设定条件的数据的时候，整个扫描也结束了。 自定义过滤器 可以通过实现Filter接口或者直接竭诚FilterBase类来实现自定义过滤器。 做法 : 继承 FilterBase，然后打成 jar 放到 $Hbase_HOEM/lib 目录下去（注意：需要重启 Hbase 集群） 说过滤器之前先说一些概念， 在过滤器中会置入比较器，通过比较器来实现各种效果，Hbase提供的比较器有： 1234567BinaryComparator 匹配完整字节数组BinaryPrefixComparator 匹配字节数组前缀BitComparator 通过BitwiseOp类提供的按位与（AND），或（OR），异或（XOR）操作执行位级比较LongComparatorNullComparator 不做匹配，只判断当前值是不是null RegexStringComparator 正则表达式匹配SubstringComparator 子串匹配 Hbase支持过滤器链，多个过滤器顺序执行，FilterList类可以添加多个过滤器对象，多个过滤器之间有两种关系： 12MUST_PASS_ALL, 数据必须满足所有的过滤器，有一个不满足则返回空MUST_PASS_ONE; 满足任意一个就可以 Hbase过滤器中提供的比较符有： 1234567LESS 匹配小于设定值的值LESS_OR_EQUAL 匹配小于或等于设定值的值EQUAL 匹配等于设定值的值NOT_EQUAL 匹配大于设定值不相同的值GREATER_OR_EQUAL 匹配大于或等于设定值的值GREATER 匹配大于设定值的值NOT_OP 排除一切值 我们先从比较器的维度列举： 1234567891011121314151617181920212223242526//构建一个FilterList，依次通过每个filter，Operator可选FilterList filterList = new FilterList(FilterList.Operator.MUST_PASS_ALL);Scan scan = new Scan();//正则过滤器RegexStringComparator comp = new RegexStringComparator("132.*"); // 以 132 开头的手机号SingleColumnValueFilter filter = new SingleColumnValueFilter(Bytes.toBytes("family"), Bytes.toBytes("qualifier"), CompareFilter.CompareOp.EQUAL, comp);filterList.addFilter(filter);//子串过滤器,大小写不敏感SubstringComparator comp1 = new SubstringComparator("2239"); // 查找包含 2239 的字符串SingleColumnValueFilter filter1 = new SingleColumnValueFilter(Bytes.toBytes("family"), Bytes.toBytes("qualifier"), CompareFilter.CompareOp.EQUAL, comp1);filterList.addFilter(filter1);//前缀过滤器BinaryPrefixComparator comp2 = new BinaryPrefixComparator(Bytes.toBytes("138")); //查找前缀为138开头的手机号SingleColumnValueFilter filter2 = new SingleColumnValueFilter(Bytes.toBytes("family"), Bytes.toBytes("qualifier"), CompareFilter.CompareOp.EQUAL, comp2);filterList.addFilter(filter2);//二进制比较器，用于按字典顺序比较 Byte 数据值//过滤出 表中为223567的列族，过滤结果为该列族下的所有行BinaryComparator comp3 = new BinaryComparator(Bytes.toBytes("223567"));FamilyFilter filter3 = new FamilyFilter(CompareFilter.CompareOp.EQUAL, comp3);filterList.addFilter(filter3);scan.setFilter(filterList); 一般来说RegexStringComparator，SubstringComparator基本可以满足日常需求，BinaryComparator，BinaryPrefixComparator用的比较少。 下面我们从过滤器的维度列举： 列值过滤器 12345678910111213//构建一个FilterList，依次通过每个filter，Operator可选FilterList filterList = new FilterList(FilterList.Operator.MUST_PASS_ALL);Scan scan = new Scan();//列值过滤器SingleColumnValueFilter filter = new SingleColumnValueFilter(Bytes.toBytes("family"), Bytes.toBytes("qualifier"), CompareFilter.CompareOp.EQUAL, Bytes.toBytes("value"));filterList.addFilter(filter);//跟 SingleColumnValueFilter 功能一样，只是不查询出该列的值。SingleColumnValueExcludeFilter filter1 = new SingleColumnValueExcludeFilter(Bytes.toBytes("family"), Bytes.toBytes("qualifier"), CompareFilter.CompareOp.EQUAL, Bytes.toBytes("value"));filterList.addFilter(filter1);scan.setFilter(filterList); 键值元数据过滤器 Hbase 采用键值对保存内部数据，键值元数据过滤器评估一行的键**(ColumnFamily：Qualifiers)**是否存在。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051//构建一个FilterList，依次通过每个filter，Operator可选FilterList filterList = new FilterList(FilterList.Operator.MUST_PASS_ALL);Scan scan = new Scan();//过滤列族FamilyFilter filter2 = new FamilyFilter(CompareFilter.CompareOp.EQUAL, new BinaryComparator(Bytes.toBytes("my-family"))); // 列族为 my-familyfilterList.addFilter(filter2);/** * 过滤列名 * 构造函数中第二个参数是ByteArrayComparable类型,所有的比较器都可以使用 */QualifierFilter filter3 = new QualifierFilter(CompareFilter.CompareOp.EQUAL, new BinaryComparator(Bytes.toBytes("my-column"))); // 列名为 my-columnfilterList.addFilter(filter3);//列名（Qualifier）前缀匹配过滤ColumnPrefixFilter filter4 = new ColumnPrefixFilter(Bytes.toBytes("columnPrefix")); // 前缀为 columnPrefixfilterList.addFilter(filter4);//同时过滤多个列名（Qualifier）前缀byte[][] prefixes = new byte[][]&#123;Bytes.toBytes("prefix-1"), Bytes.toBytes("prefix-2")&#125;;MultipleColumnPrefixFilter filter5 = new MultipleColumnPrefixFilter(prefixes);filterList.addFilter(filter5);byte[] startColumn = Bytes.toBytes("A");byte[] endColumn = Bytes.toBytes("D");//返回所有列中从A到D打头的范围的数据，实际返回类似CREATOR、CREATE_TIME、CHANNEL_CODE等列的数据ColumnRangeFilter filter6 = new ColumnRangeFilter(startColumn, true, endColumn, true);filterList.addFilter(filter6);/** *该过滤器有两个参数 —— 列族和列修饰。 尝试找到该列所在的每一行，并返回该行具有相同时间戳的全部键值对。如果某一行不包含指定的列，则该行的任何键值对都不返回。 * 该过滤器还可以有一个可选布尔参数 —— dropDependentColumn. 如果为true, 从属的列不返回。 * 该过滤器还可以有两个可选参数 —— 一个比较操作符和一个值比较器，用于列族和修饰的进一步检查。如果从属的列找到，其值还必须通过值检查，然后就是时间戳必须考虑。 * */DependentColumnFilter filter7 = new DependentColumnFilter(Bytes.toBytes("family"), Bytes.toBytes("qualifier"), true, CompareFilter.CompareOp.EQUAL, new BinaryPrefixComparator(Bytes.toBytes("value")));filterList.addFilter(filter7);//行键过滤器RowFilter filter8 = new RowFilter(CompareFilter.CompareOp.EQUAL, new BinaryComparator(Bytes.toBytes("rowName")));filterList.addFilter(filter8);//随机行过滤器float chance = 0.5f;RandomRowFilter filter9 = new RandomRowFilter(chance); // change 在 0.1 ~ 1.0 之间的浮点值filterList.addFilter(filter9);scan.setFilter(filterList); 功能过滤器 PageFilter 指定页面行数，返回对应行数的结果集。 需要注意的是，该过滤器并不能保证返回的结果行数小于等于指定的页面行数，因为过滤器是分别作用到各个region server的，它只能保证当前region返回的结果行数不超过指定页面行数。 1234567891011Scan scan = new Scan();PageFilter filter = new PageFilter(10L);scan.setFilter(filter);ResultScanner rs = table.getScanner(scan);for (Result r : rs) &#123; for (Cell cell : r.rawCells()) &#123; System.out.println("Rowkey : " + Bytes.toString(r.getRow()) + " Familiy:Quilifier : " + Bytes.toString(CellUtil.cloneQualifier(cell)) + "value: " + Bytes.toString(CellUtil.cloneValue(cell)) + "Time: " + cell.getTimestamp()); &#125;&#125; 由于该过滤器并不能保证返回的结果行数小于等于指定的页面行数，所以更好的返回指定行数的办法是ResultScanner.next(int nbRows) ，即： 12345678for (Result r : rs.next(5)) &#123; for (Cell cell : r.rawCells()) &#123; System.out.println("Rowkey : " + Bytes.toString(r.getRow()) + " Familiy:Quilifier : " + Bytes.toString(CellUtil.cloneQualifier(cell)) + "value: " + Bytes.toString(CellUtil.cloneValue(cell)) + "Time: " + cell.getTimestamp()); &#125;&#125; FirstKeyOnlyFilter 该过滤器仅仅返回每一行中的第一个cell的值，可以用于高效的执行行数统计操作。 123Scan scan = new Scan();FirstKeyOnlyFilter fkof = new FirstKeyOnlyFilter();scan.setFilter(fkof); KeyOnlyFilter 只查询每行键值对中有 “键” 元数据信息，不显示值，可以提升扫描的效率。 123Scan scan = new Scan();KeyOnlyFilter filter = new KeyOnlyFilter(); // 只查询每行键值对中有 "键" 元数据信息，不显示值，可以提升扫描的效率scan.setFilter(filter); InclusiveStopFilter 常规的 Scan 包含 start-row 但不包含 stop-row，如果使用该过滤器便可以包含 stop-row。 123Scan scan = new Scan();InclusiveStopFilter filter = new InclusiveStopFilter(Bytes.toBytes("stopRowKey"));scan.setFilter(filter); SkipFilter 根据整行中的每个列来做过滤，只要存在一列不满足条件，整行都被过滤掉。 例如，如果一行中的所有列代表的是不同物品的重量，则真实场景下这些数值都必须大于零，我们希望将那些包含任意列值为0的行都过滤掉。 在这个情况下，我们结合ValueFilter和SkipFilter共同实现该目的： 1scan.setFilter(new SkipFilter(new ValueFilter(CompareOp.NOT_EQUAL,new BinaryComparator(Bytes.toBytes(0))));]]></content>
      <categories>
        <category>大数据学习</category>
        <category>Hbase</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hbase region的预分区]]></title>
    <url>%2Fposts%2F5aeefc82.html</url>
    <content type="text"><![CDATA[本文讨论一下Hbase region的分割。具体来说，会讨论为什么要预分区，要不要预分区和如何分区。 1. 为什么要预分区 我们知道region是Hbase表的基本单位，region又是由一个或者多个store组成，每个store保存一个columns family；每个store又由一个memStore和一个或者多个storeFile组成。memeStore存储在内存中，storeFile存储在HDFS中。 12345table region... store... memStore storeFile... 通常一个Hbase表由多个region构成，这些region分布在多个regionserver上。也就是说，region是在regionserver中插入和查询数据时负载均衡的物理机制。一张Hbase表在刚刚创建的时候，默认只有一个region。所以很有可能关于这张表的请求都被路由到同一个region server，不管集群中有多少region server。这也就是为什么Hbase表在刚刚创建的阶段不能充分利用整个集群的吞吐量的原因。 Hbase表在创建之初只有一个region的原因是因为Hbase本身不知道如何在rowkey集中创建分割点。分割点高度依赖于rowkey的设计。Hbase提供了client端工具用于region分割，而不是让用户猜测或者导致故障。用户可以通过pre-splitting的流程，可以创建一个已经有多个region的新表。因为pre-splitting可以让你在表创建之初可以在集群中进行负载均衡，所以如果你已经知道插入数据的rowkey分布，就应该应用它。但是，pre-splitting创建region也有风险：由于数据分布不均匀，或者存在热点行或者较大的行，不能均匀地分担负载。如果最初的一组分割点选择得不好，那么最终可能会出现异构的负载分布，这反过来又会限制集群性能。 大致总结一下为什么要进行region的预分区： Hbase table 对 client 的服务是region级别的，region落在regionserver上。 Hbase 会对单个 region 的大小以及单 region 中文件数量进行限制。超过了限制就会进行 split，这样就解决了 mysql等关系型数据库普遍存在的 单表只能无限 append 的问题。 Hbase 中split一般比较快速的完成，但是这会成为业务的潜在风险，比如业务高峰发生了自动的split，一两秒服务质量比较低可能就会造成服务影响。 所以综合1和2，建议大家在Hbase创建表时进行预分区，预先指定分区数量。 同时，如果一个节点有过多的读写操作，还会造成热点问题，Hbase可能会因为Zookeeper的连接超时而关掉该节点，将表预分割为固定数量的区域(region)也是一个好的解决方法，并且通过rowkey设计将这些区域均匀地分布在所有的服务器(regionserver)上。 2. 如何分区 RegionSplitter提供三个用于Pre-splitting的工具：HexStringSplit、UniformSplit、SplitAlgorithm。 HexStringSplit row key是十六进制的字符串(hexadecimal ASCII)作为前缀的时候，HexHash(prefix)作为row key的前缀，其中Hexhash为最终得到十六进制字符串的hash算法。 123456789#方式一create 'tableName', &#123; NAME =&gt; 'info', COMPRESSION =&gt; 'snappy' &#125;, &#123;NUMREGIONS =&gt; 60, SPLITALGO =&gt; 'HexStringSplit'&#125;#方式二在Hbase shell 下执行：Hbase org.apache.hadoop.Hbase.util.RegionSplitter mytable HexStringSplit -c 10 -f f1-c 10 的意思为，最终的region数目为10个；-f f1为创建一个那么为f1的 column family. UniformSplit row key是字节数组arbitrary bytes的时候，某个Hbase的表查询只是以随机查询为主，可以用UniformSplit的方式进行，按照原始byte值（从0x00~0xFF）右边以00填充。以这种方式分区的表在插入的时候需要对rowkey进行一个技巧性的改造， 比如原来的rowkey为rawStr，则需要对其取hashCode，然后进行按照比特位反转后放在最初rowkey串的前面。 举个例子： 1create 'table_name', 'info',&#123;NUMREGIONS =&gt; 15, SPLITALGO =&gt; 'UniformSplit'&#125; rowKey设计： 12345678910111213141516171819202122public static void main(String[] args) throws Exception &#123; Configuration conf = HbaseConfiguration.create(); conf.set("Hbase.zookeeper.quorum", "hadoop1"); conf.set("Hbase.zookeeper.property.clientPort", "2181"); HConnection connection = HConnectionManager.createConnection(conf); HTableInterface table = connection.getTable("huanggang"); for (int i=1; i&lt; 6553500; i++) &#123; byte[] rowKey = Bytes.add(Bytes.toBytes(Integer.reverse(Integer.valueOf(Integer.valueOf(i).hashCode()))), Bytes.toBytes(i)); System.out.println(rowKey); Put put = new Put(rowKey); put.add("f".getBytes(), "col1".getBytes(), Bytes.toBytes(new Random().nextInt(10000))); put.add("f".getBytes(), "col2".getBytes(), Bytes.toBytes(new Random().nextInt(10000))); put.add("f".getBytes(), "col3".getBytes(), Bytes.toBytes(new Random().nextInt(10000))); put.add("f".getBytes(), "col4".getBytes(), Bytes.toBytes(new Random().nextInt(10000))); put.add("f".getBytes(), "col5".getBytes(), Bytes.toBytes(new Random().nextInt(10000))); put.add("f".getBytes(), "col6".getBytes(), Bytes.toBytes(new Random().nextInt(10000))); table.put(put); &#125; table.flushCommits(); table.close(); &#125; &#125; SplitAlgorithm SplitAlgorithm是一个接口，需要开发人员自己实现相应的分隔策略。 待定 以上为Hbase自带的分区工具，另外我们还可以自定义分区： 1234567891011Examples: Hbase&gt; create 'ns1:t1', 'f1', SPLITS =&gt; ['10', '20', '30', '40'] Hbase&gt; create 't1', 'f1', SPLITS =&gt; ['10', '20', '30', '40'] Hbase&gt; create 't1', 'f1', SPLITS_FILE =&gt; 'splits.txt', OWNER =&gt; 'johndoe' Hbase&gt; create 't1', &#123;NAME =&gt; 'f1', VERSIONS =&gt; 5&#125;, METADATA =&gt; &#123; 'mykey' =&gt; 'myvalue' &#125; Hbase&gt; # Optionally pre-split the table into NUMREGIONS, using Hbase&gt; # SPLITALGO ("HexStringSplit", "UniformSplit" or classname) Hbase&gt; create 't1', 'f1', &#123;NUMREGIONS =&gt; 15, SPLITALGO =&gt; 'HexStringSplit'&#125; Hbase&gt; create 't1', 'f1', &#123;NUMREGIONS =&gt; 15, SPLITALGO =&gt; 'HexStringSplit', CONFIGURATION =&gt; &#123;'Hbase.hregion.scan.loadColumnFamiliesOnDemand' =&gt; 'true'&#125;&#125; Hbase&gt; create 't1', &#123;NAME =&gt; 'f1'&#125;, &#123;NAME =&gt; 'if1', LOCAL_INDEX=&gt;'COMBINE_INDEX|INDEXED=f1:q1:8|rowKey:rowKey:10,UPDATE=true'&#125; 以上为Hbase自带的示例，可以看到如果分区比较少的情况下，通过SPLITS关键字指定即可，如果分区很多，可以通过文件的形式指定。比如我们想划分10个region，key如下： 123456789100001|0002|0003|0004|0005|0006|0007|0008|0009|0010| 然后创建表： 1create 'split_table_test', 'cf', &#123;SPLITS_FILE =&gt; '/data/region_split_info.txt'&#125; 假如我还想对Hbase表做一个SNAPPY压缩，应该怎么写呢？ 1create 'split_table_test',&#123;NAME =&gt;'cf', COMPRESSION =&gt; 'SNAPPY'&#125;, &#123;SPLITS_FILE =&gt; 'region_split_info.txt'&#125; 这里注意，一定要将分区的参数指定单独用一个大括号扩起来，因为分区是针对全表，而不是针对某一个column family。 3. RowKey的设计原则 上面说到region的预分区，主要围绕rowKey的设计来做，那么rowKey应该如何设计呢。我们知道Hbase是三维有序存储的，通过rowkey（行键），column key（column family和qualifier）和TimeStamp（时间戳）这个三个维度可以对Hbase中的数据进行快速定位。 Hbase中rowkey可以唯一标识一行记录，在Hbase查询的时候，有以下几种方式： 通过get方式，指定rowkey获取唯一一条记录 通过scan方式，设置startRow和stopRow参数进行范围匹配 全表扫描，即直接扫描整张表中所有行记录 3.1 rowkey长度原则 rowkey是一个二进制码流，可以是任意字符串，最大长度 64kb ，实际应用中一般为10-100bytes，以 byte[] 形式保存，一般设计成定长。 建议越短越好，不要超过16个字节，原因如下： 数据的持久化文件HFile中是按照KeyValue存储的，如果rowkey过长，比如超过100字节，1000w行数据，光rowkey就要占用100*1000w=10亿个字节，将近1G数据，这样会极大影响HFile的存储效率； MemStore将缓存部分数据到内存，如果rowkey字段过长，内存的有效利用率就会降低，系统不能缓存更多的数据，这样会降低检索效率。 目前操作系统都是64位系统，内存8字节对齐，控制在16个字节，8字节的整数倍利用了操作系统的最佳特性。 3.2 rowkey散列原则 如果rowkey按照时间戳的方式递增，不要将时间放在二进制码的前面，建议将rowkey的高位作为散列字段，由程序随机生成，低位放时间字段，这样将提高数据均衡分布在每个RegionServer，以实现负载均衡的几率。如果没有散列字段，首字段直接是时间信息，所有的数据都会集中在一个RegionServer上，这样在数据检索的时候负载会集中在个别的RegionServer上，造成热点问题，会降低查询效率。 3.3 rowkey唯一原则 必须在设计上保证其唯一性，rowkey是按照字典顺序排序存储的，因此，设计rowkey的时候，要充分利用这个排序的特点，将经常读取的数据存储到一块，将最近可能会被访问的数据放到一块。 3.4 什么是热点 Hbase中的行是按照rowkey的字典顺序排序的，这种设计优化了scan操作，可以将相关的行以及会被一起读取的行存取在临近位置，便于scan。然而糟糕的rowkey设计是热点的源头。 热点发生在大量的client直接访问集群的一个或极少数个节点（访问可能是读，写或者其他操作）。大量访问会使热点region所在的单个机器超出自身承受能力，引起性能下降甚至region不可用，这也会影响同一个RegionServer上的其他region，由于主机无法服务其他region的请求。 设计良好的数据访问模式以使集群被充分，均衡的利用。 为了避免写热点，设计rowkey使得不同行在同一个region，但是在更多数据情况下，数据应该被写入集群的多个region，而不是一个。 下面是一些常见的避免热点的方法以及它们的优缺点： 加盐 这里所说的加盐不是密码学中的加盐，而是在rowkey的前面增加随机数，具体就是给rowkey分配一个随机前缀以使得它和之前的rowkey的开头不同。分配的前缀种类数量应该和你想使用数据分散到不同的region的数量一致。加盐之后的rowkey就会根据随机生成的前缀分散到各个region上，以避免热点。 哈希 哈希会使同一行永远用一个前缀加盐。哈希也可以使负载分散到整个集群，但是读却是可以预测的。使用确定的哈希可以让客户端重构完整的rowkey，可以使用get操作准确获取某一个行数据 反转 第三种防止热点的方法时反转固定长度或者数字格式的rowkey。这样可以使得rowkey中经常改变的部分（最没有意义的部分）放在前面。这样可以有效的随机rowkey，但是牺牲了rowkey的有序性。 反转rowkey的例子以手机号为rowkey，可以将手机号反转后的字符串作为rowkey，这样的就避免了以手机号那样比较固定开头导致热点问题 时间戳反转 一个常见的数据处理问题是快速获取数据的最近版本，使用反转的时间戳作为rowkey的一部分对这个问题十分有用，可以用 Long.Max_Value - timestamp 追加到key的末尾，例如 [key][reverse_timestamp] , [key] 的最新值可以通过scan [key]获得[key]的第一条记录，因为Hbase中rowkey是有序的，第一条记录是最后录入的数据。 比如需要保存一个用户的操作记录，按照操作时间倒序排序，在设计rowkey的时候，可以这样设计 [userId反转][Long.Max_Value - timestamp]，在查询用户的所有操作记录数据的时候，直接指定反转后的userId，startRow是[userId反转][000000000000],stopRow是[userId反转][Long.Max_Value - timestamp] 如果需要查询某段时间的操作记录，startRow是[user反转][Long.Max_Value - 起始时间]，stopRow是[userId反转][Long.Max_Value - 结束时间] 其他一些建议 尽量减少行和列的大小在Hbase中，value永远和它的key一起传输的。当具体的值在系统间传输时，它的rowkey，列名，时间戳也会一起传输。如果你的rowkey和列名很大，甚至可以和具体的值相比较，那么你将会遇到一些有趣的问题。Hbase storefiles中的索引（有助于随机访问）最终占据了Hbase分配的大量内存，因为具体的值和它的key很大。可以增加block大小使得storefiles索引再更大的时间间隔增加，或者修改表的模式以减小rowkey和列名的大小。压缩也有助于更大的索引。 列族尽可能越短越好，最好是一个字符。 冗长的属性名虽然可读性好，但是更短的属性名存储在Hbase中会更好。]]></content>
      <categories>
        <category>大数据学习</category>
        <category>Hbase</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux时间配置同步]]></title>
    <url>%2Fposts%2Ff1a04f3f.html</url>
    <content type="text"><![CDATA[在家里玩虚拟机，不时发现因为多台机器时间不同步问题导致Hbase无法使用，所以查询了一下linux配置同步互联网时间。 在系统中查看时间可以使用date命令： 12[hadoop@hadoopmaster conf]$ date2019年 01月 08日 星期二 00:12:52 CST date显示的是系统时间，那如何让硬件，比如CPU也去同步这个时间呢，linux提供了hwclock命令： 12345#显示硬件时间[root@hadoopmaster conf]# hwclock2019年01月08日 星期二 00时15分54秒 -0.756066 seconds#将系统时间写入到硬件中去[root@hadoopmaster conf]# hwclock -w linux提供了同步互联网时间的工具：ntpdate。ntpdate是一个linux时间同步工具，安装ntpdate： 1yum install -y ntpdate 使用ntpdate同时时间： 输入: 12[root@hadoopmaster conf]# ntpdate ntp1.aliyun.com 8 Jan 00:19:41 ntpdate[24074]: adjust time server 120.25.115.20 offset -0.014956 sec 这是阿里云提供的授时中心，还有一些可以用的： time.nuri.net 同步完成以后，将时间写入硬件中: hwclock -w 执行以后你可以查看系统时间是否是当前最新时间：date。 以上就是更新系统时间的方法，当然，如果我们想省事，可以写个定时任务去拉最新时间： 1crontab -e 打开定时任务列表，添加我们的定时任务： 1* */1 * * * root ntpdate ntp1.aliyun.com;hwclock -w 上面的定时任务表示：每一小时去阿里云授时中心同步一次，然后写入硬件。当然你也可以将频次调的更快一些。]]></content>
      <categories>
        <category>linux系统维护</category>
      </categories>
      <tags>
        <tag>系统维护</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[html转图片-wkhtmltopdf使用]]></title>
    <url>%2Fposts%2F4ceb55a2.html</url>
    <content type="text"><![CDATA[目前java将html转为图片的可用方法不是很多，有如下工具可以使用： html2image: 坑太多，不会加载第三方的css样式； cssbox：使用起来相对会好一些，但是如果网页内样式比较多的情况下，部分样式会乱掉；其次，如果网页内包含图片，图片会损失很多细节； 1wkhtmltopdf：本来是设计用于html转pdf的，但是也可以转图片。这个使用起来各方面良好，唯一不好的地方就是他是一个软件，必须的安装才能使用。 因为目前使用的是wkhtmltopdf这个方案，下面就介绍这个方案的使用方式吧。 1.安装 官网地址如下： https://wkhtmltopdf.org/ 需要去其官网下载对应环境的软件，支持windows，linux，mac。 特别注意： linux一定要注意系统版本，是什么系统一定要安装对应系统的包，否则gg。centos一定注意区分是6还是7，安装对应的包。 查看系统版本： 12345678910111213141516[root@hadoop /data/service]# cat /etc/os-releaseNAME="CentOS Linux"VERSION="7 (Core)"ID="centos"ID_LIKE="rhel fedora"VERSION_ID="7"PRETTY_NAME="CentOS Linux 7 (Core)"ANSI_COLOR="0;31"CPE_NAME="cpe:/o:centos:centos:7"HOME_URL="https://www.centos.org/"BUG_REPORT_URL="https://bugs.centos.org/" CENTOS_MANTISBT_PROJECT="CentOS-7"CENTOS_MANTISBT_PROJECT_VERSION="7"REDHAT_SUPPORT_PRODUCT="centos"REDHAT_SUPPORT_PRODUCT_VERSION="7" 2. 下载对应的包去安装 wkhtmltox-0.12.5-1.centos7.x86_64.rpm 安装： rpm -ivh wkhtmltox-0.12.5-1.centos7.x86_64.rpm 然后你会发现如下： 1234567891011[root@hadoop ~]# rpm -ivh wkhtmltox-0.12.5-1.centos7.x86_64.rpm错误：依赖检测失败： fontconfig 被 wkhtmltox-1:0.12.5-1.centos7.x86_64 需要 libX11 被 wkhtmltox-1:0.12.5-1.centos7.x86_64 需要 libXext 被 wkhtmltox-1:0.12.5-1.centos7.x86_64 需要 libXrender 被 wkhtmltox-1:0.12.5-1.centos7.x86_64 需要 libjpeg 被 wkhtmltox-1:0.12.5-1.centos7.x86_64 需要 libpng 被 wkhtmltox-1:0.12.5-1.centos7.x86_64 需要 xorg-x11-fonts-75dpi 被 wkhtmltox-1:0.12.5-1.centos7.x86_64 需要 xorg-x11-fonts-Type1 被 wkhtmltox-1:0.12.5-1.centos7.x86_64 需要[root@hadoop ~]# 缺少相关包，所以你需要将缺失的包补充完整。当然机器不同，包缺失的情况也不一样，随机应变。 安装包使用如下命令：yum install packageName 所有的依赖安装完成以后，接着执行rpm命令应该是可以的，安装完毕你可以检测是否可用，使用如下命令看是否存在： 1234567891011121314151617181920212223242526272829303132333435363738394041[root@hadoop ]# wkhtmltoimageYou need to specify at least one input file, and exactly one output fileUse - for stdin or stdout Name: wkhtmltoimage 0.12.5 (with patched qt) Synopsis: wkhtmltoimage [OPTIONS]... &lt;input file&gt; &lt;output file&gt; Description: Converts an HTML page into an image, General Options: --crop-h &lt;int&gt; Set height for cropping --crop-w &lt;int&gt; Set width for cropping --crop-x &lt;int&gt; Set x coordinate for cropping --crop-y &lt;int&gt; Set y coordinate for cropping -H, --extended-help Display more extensive help, detailing less common command switches -f, --format &lt;format&gt; Output file format --height &lt;int&gt; Set screen height (default is calculated from page content) (default 0) -h, --help Display help --license Output license information and exit --log-level &lt;level&gt; Set log level to: none, error, warn or info (default info) --quality &lt;int&gt; Output image quality (between 0 and 100) (default 94) -q, --quiet Be less verbose, maintained for backwards compatibility; Same as using --log-level none -V, --version Output version information and exit --width &lt;int&gt; Set screen width, note that this is used only as a guide line. Use --disable-smart-width to make it strict. (default 1024) Contact: If you experience bugs or want to request new features please visit &lt;https://github.com/wkhtmltopdf/wkhtmltopdf/issues&gt; 这样说明已经 安装成功了。该命令是将网页生成图片的命令。 你可以直接测试一下： 1wkhtmltoimage https://guagua-workplatform.guazi.com/jobEmail.html /data/www/account/aaa.jpg 去/data/www/目录下看是否生成照片。 3.使用java调用 接下来我们写java代码吧。其实java代码也只是使用exec去调用linux命令，所以代码很简单： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970import org.apache.commons.lang3.StringUtils;import org.jsoup.Jsoup;import org.jsoup.nodes.Document;import org.jsoup.nodes.Element;import org.jsoup.select.Elements; import java.io.IOException;import java.nio.file.Files;import java.nio.file.Paths;import java.util.Date;import java.util.Random; /** * @Author yangyue * @Date Created in 上午8:28 2018/12/31 * @Modified by: * @Description: 使用wkhtmltopdf将html生成图片 * ------------------------------ * * api请参见：https://wkhtmltopdf.org/ * * * *******使用之前需要安装该软件******* * * * ------------------------------ **/public class CustomwkhtmltopdfmlToPicUtil &#123; private static final String OS_NAME = "os.name"; private static final String LINUX = "Linux"; private static final String MAC = "Mac OS X"; private static final String EXEC_SUFFIX = "wkhtmltoimage --width 851 --quality 95 "; private static final String EMAIL_TEMPLATE_URL = "http://www.baidu.com"; /** * 生成照片 * @param fileName * @return */ private static boolean generatePic(String fileName)&#123; try &#123; String picPath = Constant.PIC_ADDRESS + fileName + Constant.PIC_SUFFIX; String command = getCommand(EMAIL_TEMPLATE_URL, picPath); Process process = Runtime.getRuntime().exec(command); process.waitFor(); //这个调用比较关键，就是等当前命令执行完成后再往下执行 LoggerUtils.info("---执行完成-----"); &#125; catch (Throwable e) &#123; LoggerUtils.error("生成照片出错，&#123;&#125;",e); return false; &#125; return true; &#125; /** * 拼装系统命令字符串 * @param sourceFilePath * @param targetFilePath * @return */ private static String getCommand(String sourceFilePath, String targetFilePath) &#123; String system = System.getProperty(OS_NAME); if (system.contains(LINUX) || system.contains(MAC)) &#123; return EXEC_SUFFIX + sourceFilePath + " " + targetFilePath; &#125; else if(system.contains("Windows")) &#123; return WINDOWS_PATH + sourceFilePath + " " + targetFilePath; &#125; return StringUtils.EMPTY; &#125;&#125; 你可以执行这段java代码，会在你指定的目录生成网页对应的图片。 4.使用参数来做更多 我们注意到上面其实是在执行命令：“wkhtmltoimage --width 851 --quality 95 ” 上面的&quot;–width&quot;是表示截图的宽，“–quality”表示图片的质量，默认是94%。 如果你还需要指定别的参数，那么你可以参看官方给的wiki： https://wkhtmltopdf.org/usage/wkhtmltopdf.txt 5.遇到乱码怎么办 以上就是wkhtmltopdf的使用，当然可能你还会遇到别的问题，比如部署在服务器上的时候生成的照片是乱码。生成的图片中所有的中文全部都显示为 方块 这种乱码，一般会有两种可能： 1.查看你需要转图的网页字体： 。如果不是utf-8你可以将其改为utf-8; 2.服务器没有安装中文字体,这个也好确认，执行命令： 12345678910111213141516171819202122232425fc-list[root@hadoop /data/service]# fc-list/usr/share/fonts/dejavu/DejaVuSansCondensed-Oblique.ttf: DejaVu Sans,DejaVu Sans Condensed:style=Condensed Oblique,Oblique/usr/share/fonts/dejavu/DejaVuSansCondensed-Bold.ttf: DejaVu Sans,DejaVu Sans Condensed:style=Condensed Bold,Bold/usr/share/fonts/zh_CN/MSYH.TTF: Microsoft YaHei:style=Regular,Normal/usr/share/X11/fonts/Type1/c0611bt_.pfb: Courier 10 Pitch:style=Bold Italic/usr/share/X11/fonts/Type1/UTBI____.pfa: Utopia:style=Bold Italic/usr/share/X11/fonts/Type1/c0419bt_.pfb: Courier 10 Pitch:style=Regular/usr/share/fonts/dejavu/DejaVuSans.ttf: DejaVu Sans:style=Book/usr/share/X11/fonts/Type1/c0648bt_.pfb: Bitstream Charter:style=Regular/usr/share/fonts/dejavu/DejaVuSans-Bold.ttf: DejaVu Sans:style=Bold/usr/share/X11/fonts/Type1/cursor.pfa: Cursor:style=Regular/usr/share/X11/fonts/Type1/UTB_____.pfa: Utopia:style=Bold/usr/share/X11/fonts/Type1/c0583bt_.pfb: Courier 10 Pitch:style=Bold/usr/share/X11/fonts/Type1/UTI_____.pfa: Utopia:style=Italic/usr/share/X11/fonts/Type1/c0582bt_.pfb: Courier 10 Pitch:style=Italic/usr/share/fonts/dejavu/DejaVuSansCondensed.ttf: DejaVu Sans,DejaVu Sans Condensed:style=Condensed,Book/usr/share/fonts/dejavu/DejaVuSans-ExtraLight.ttf: DejaVu Sans,DejaVu Sans Light:style=ExtraLight/usr/share/fonts/dejavu/DejaVuSansCondensed-BoldOblique.ttf: DejaVu Sans,DejaVu Sans Condensed:style=Condensed Bold Oblique,Bold Oblique/usr/share/X11/fonts/Type1/c0633bt_.pfb: Bitstream Charter:style=Bold Italic/usr/share/X11/fonts/Type1/c0649bt_.pfb: Bitstream Charter:style=Italic/usr/share/fonts/dejavu/DejaVuSans-Oblique.ttf: DejaVu Sans:style=Oblique/usr/share/X11/fonts/Type1/c0632bt_.pfb: Bitstream Charter:style=Bold/usr/share/fonts/dejavu/DejaVuSans-BoldOblique.ttf: DejaVu Sans:style=Bold Oblique/usr/share/X11/fonts/Type1/UTRG____.pfa: Utopia:style=Regular 会显示你已经安装的字体，如果网页没有问题，你可以尝试安装中文字体，以微软雅黑字体为例：msyh.ttf 。自行下载，将该字体文件上传至服务器。 1234567安装中文字体： 0、查看目前安装字体：fc-list 1、下载所需字体,例如msyh.ttf 2、mkdir /usr/share/fonts/zh_CN 3、mv msyh.ttf /usr/share/fonts/zh_CN 4、执行fc-cache -fv 5、查看是否安装成功：fc-list，查看是已安装 稍后你再执行java程序，应该就正常。]]></content>
      <categories>
        <category>java开发工具类</category>
      </categories>
      <tags>
        <tag>工具类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 查看文件夹大小，磁盘剩余空间（du/df）]]></title>
    <url>%2Fposts%2F24a4bcf2.html</url>
    <content type="text"><![CDATA[du查看目录大小，df查看磁盘使用情况。 du （1）基本功能 递归查看文件夹下所有文件的大小 （2）常用参数： -h, –human-readable 以可读性较好的方式显示尺寸(例如：1K 234M 2G) -s, –summarize 只分别计算命令列中每个参数所占的总用量 （3）其它参数说明： -a, –all 输出所有文件的磁盘用量，不仅仅是目录 –apparent-size 显示表面用量，而并非是磁盘用量；虽然表面用量通常会小一些，但有时它会因为稀疏文件间的”洞”、内部碎片、非直接引用的块等原因而变大。 -B, –block-size=大小 使用指定字节数的块 -b, –bytes 等于–apparent-size –block-size=1 -c, –total 显示总计信息 -D, –dereference-args 解除命令行中列出的符号连接 –files0-from=F 计算文件F 中以NUL 结尾的文件名对应占用的磁盘空间如果F 的值是”-“，则从标准输入读入文件名 -H 等于–dereference-args (-D) -h, –human-readable 以可读性较好的方式显示尺寸(例如：1K 234M 2G) –si 类似-h，但在计算时使用1000 为基底而非1024 -k 等于–block-size=1K -l, –count-links 如果是硬连接，就多次计算其尺寸 -m 等于–block-size=1M -L, –dereference 找出任何符号链接指示的真正目的地 -P, –no-dereference 不跟随任何符号链接(默认) -0, –null 将每个空行视作0 字节而非换行符 -S, –separate-dirs 不包括子目录的占用量 -s, –summarize 只分别计算命令列中每个参数所占的总用量 -x, –one-file-system 跳过处于不同文件系统之上的目录 -X, –exclude-from=文件 排除与指定文件中描述的模式相符的文件 –exclude=PATTERN 排除与PATTERN 中描述的模式相符的文件 –max-depth=N 显示目录总计(与–all 一起使用计算文件) 当N 为指定数值时计算深度为N； –max-depth=0 等于–summarize –time 显示目录或该目录子目录下所有文件的最后修改时间 –time=WORD 显示WORD 时间，而非修改时间：atime，access，use，ctime 或status –time-style=样式 按照指定样式显示时间(样式解释规则同”date”命令)： full-iso，long-iso，iso，+FORMAT –help 显示此帮助信息并退出 –version 显示版本信息并退出 df df -hl 查看磁盘剩余空间 df -h 查看每个根路径的分区大小 du -sh [目录名] 返回该目录的大小 du -sm [文件夹] 返回该文件夹总M数 区别 du是面向文件的命令，只计算被文件占用的空间，不计算文件系统 metadata 占用的空间。 df则是基于文件系统总体来计算，通过文件系统中未分配空间来确定系统中已经分配空间的大小。df命令可以获取硬盘占用了多少空间，还剩下多少空间，它也可以显示所有文件系统对i节点和磁盘块的使用情况。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tomcat部署和日志维护]]></title>
    <url>%2Fposts%2Fa996d8cf.html</url>
    <content type="text"><![CDATA[一、解压上传的tomcat包 二、修改tomcat bin目录下的catalina.sh文件。 在JAVA_OPTS=&quot;$JAVA_OPTS -Djava.protocol.handler.pkgs=org.apache.catalina.webresources&quot;下面增加如下一行数据： JAVA_OPTS=&quot;$JAVA_OPTS -Xms5120m -Xmx5120m -XX:+UseG1GC -XX:+PrintGCDetails -XX:ParallelGCThreads=15 -XX:ConcGCThreads=4&quot; tomcat内存根据机器具体内存情况进行分配，8G的机器JVM可以配置5G内存，4G的机器可以配置2G内存 三、catalina.out日志按日期切割 vim /etc/logrotate.d/tomcat 创建配置文件，文件内容如下，其中tomcat-app1是部署工程的tomcat的名字，这个名字可以根据实际工程名字情况进行修改 12345678/data/service/tomcat-app1/logs/catalina.out&#123; copytruncate daily dateformat .%Y-%m-%d.log rotate 3 missingok dateext compress&#125; 执行logrotate -f /etc/logrotate.d/tomcat检查catalina.out文件是否已经清零，并且生成了类似的catalina.out.2018-01-19.gz文件 如果查看.gz文件内容，可以使用zcat命令 四. 修改tomcat线程数，编辑server.xml： 1.取消56,57行注释，并修改线程数： &lt;Executor name=&quot;tomcatThreadPool&quot; namePrefix=&quot;catalina-exec-&quot; maxThreads=&quot;250&quot; minSpareThreads=&quot;30&quot;/&gt; (瞬时流量波动比较大的服务，这个minSpareThreads要设置大一些，比如100.否则会出现ListenOverflows,导致请求得不到处理) 2.注释68,70行： 3.取消73,76行注释，并增加相应配置（如果一个服务器上部署多个不同服务，注意服务的启动端口一定不要重复） &lt;Connector executor=&quot;tomcatThreadPool&quot; port=&quot;8080&quot; protocol=&quot;HTTP/1.1&quot; connectionTimeout=&quot;20000&quot; acceptCount=&quot;800&quot; enableLookups=&quot;false&quot; compression=&quot;on&quot; compressionMinSize=&quot;2048&quot; compressableMimeType=&quot;text/csv,text/html,text/xml,text/css,text/plain,text/javascript,application/javasc ript,application/x-javascript,application/json,application/xml&quot; maxConnections=&quot;20000&quot; redirectPort=&quot;8443&quot; /&gt; 如果一个服务器上部署多个不同服务，注意服务之间的shutdown端口和AJP协议端口一定不要重复 最后配置上access日志的处理时间：（%D表示处理时间） 五. 配置日志压缩和定时删除： 应用日志定期打包压缩脚本： cron_gz.sh 1234567#!/bin/bashcd /data/service_logs/app1date=`date -d -2day +%Y%m%di |awk &apos;BEGIN&#123;FS=&quot;&quot;&#125;&#123;print $1$2$3$4&quot;-&quot;$5$6&quot;-&quot;$7$8&#125;&apos;`hour=`date -d &quot;1 hour ago&quot; +&quot;%Y-%m-%d-%H&quot;`gzip $(ls |grep $date | grep -v &apos;info&apos;)gzip $(ls | grep $hour)find /data/service_logs/app/ -mtime +2 -name &quot;*.log.gz&quot; -exec rm -rf &#123;&#125; \;]]></content>
      <categories>
        <category>shell</category>
      </categories>
      <tags>
        <tag>工具类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Log4j日志追踪中加入tracedId]]></title>
    <url>%2Fposts%2Ff8249caa.html</url>
    <content type="text"><![CDATA[Java应用开发过程中，每一次请求从头到尾流转的过程如何通过一个唯一标识标记处来整个调用链路上的所有日志记录，对于我们去追踪当前次请求发生的问题是非常有益的。在我们的应用中一般使用Log4J去收集日志，Log4J中提供了MDC类来存放对象，MDC中的对象，可以被同一线程中执行的代码访问到，当前线程的子线程会继承其父线程中MDC的内容。 MDC 介绍 MDC（Mapped Diagnostic Context，映射调试上下文）是 log4j 和 logback 提供的一种方便在多线程条件下记录日志的功能。MDC 可以看成是一个与当前线程绑定的Map，可以往其中添加键值对。MDC 中包含的内容可以被同一线程中执行的代码所访问。当前线程的子线程会继承其父线程中的 MDC 的内容。当需要记录日志时，只需要从 MDC 中获取所需的信息即可。MDC 的内容则由程序在适当的时候保存进去。对于一个 Web 应用来说，通常是在请求被处理的最开始保存这些数据。 在 log4j 和 logback 的取值方式为： 1%X&#123;traceid&#125; 实现： 在进入真正的业务逻辑代码之前，我需要生成traceId，那就可以在拦截器中实现。 1234567891011121314151617181920212223242526272829303132333435363738394041import java.util.UUID;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;import com.guazi.org.monitor.MonitorConfig;import org.apache.commons.lang3.StringUtils;import org.slf4j.MDC;import org.springframework.web.servlet.HandlerInterceptor;import org.springframework.web.servlet.ModelAndView;/** * 拦截所有请求，在请求的开始阶段放入生成的traceId，在请求处理结束后 * 将MDC中的traceId清理掉 * @author yangyue */public class TraceInterceptor implements HandlerInterceptor&#123; @Override public void afterCompletion(HttpServletRequest request, HttpServletResponse reponse, Object arg2, Exception arg3) throws Exception &#123; MDC.clear(); &#125; @Override public void postHandle(HttpServletRequest request, HttpServletResponse response, Object arg2, ModelAndView arg3) throws Exception &#123; &#125; @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object arg2) throws Exception &#123; String traceId = UUID.randomUUID().toString(); if(StringUtils.isNotBlank(traceId))&#123; MDC.put("traceId", traceId); &#125; MonitorConfig.VISIT.count(); return true; &#125;&#125; 完整的Log4J日志文件配置如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;Configuration status="DEBUG"&gt; &lt;Appenders&gt; &lt;Console name="myConsole" target="SYSTEM_OUT"&gt; &lt;PatternLayout pattern="[%d&#123;MM-dd HH:mm:ss,SSS&#125; %-5p] [traceId:%X&#123;traceId&#125;] [%t] %c&#123;2.&#125; - %m%n%ex" /&gt; &lt;/Console&gt; &lt;RollingFile name="infoAppender" fileName="/data/service_logs/hadoop/info.log" filePattern="/data/service_logs/hadoop/info.log.%d&#123;yyyy-MM-dd-HH&#125;.log"&gt; &lt;PatternLayout&gt; &lt;Pattern&gt;[%d&#123;MM-dd HH:mm:ss SSS&#125; %-5level] [traceId:%X&#123;traceId&#125;] [%t] %c - %m%n%ex&lt;/Pattern&gt; &lt;/PatternLayout&gt; &lt;ThresholdFilter level="info" onMatch="ACCEPT" onMismatch="DENY"/&gt; &lt;Policies&gt; &lt;TimeBasedTriggeringPolicy/&gt; &lt;/Policies&gt; &lt;/RollingFile&gt; &lt;RollingFile name="errorAppender" fileName="/data/service_logs/hadoop/error.log" filePattern="/data/service_logs/hadoop/error.log.%d&#123;yyyy-MM-dd&#125;.log"&gt; &lt;PatternLayout&gt; &lt;Pattern&gt;[%d&#123;MM-dd HH:mm:ss SSS&#125; %-5level] [traceId:%X&#123;traceId&#125;] [%t] %c - %m%n%ex&lt;/Pattern&gt; &lt;/PatternLayout&gt; &lt;ThresholdFilter level="error" onMatch="ACCEPT" onMismatch="DENY"/&gt; &lt;Policies&gt; &lt;TimeBasedTriggeringPolicy/&gt; &lt;/Policies&gt; &lt;/RollingFile&gt; &lt;RollingFile name="remoteAppender" fileName="/data/service_logs/hadoop/process_remote.log" filePattern="/data/service_logs/hadoop/process_remote.log.%d&#123;yyyy-MM-dd&#125;.log"&gt; &lt;PatternLayout&gt; &lt;Pattern&gt;[%d&#123;MM-dd HH:mm:ss SSS&#125; %-5level] [traceId:%X&#123;traceId&#125;] [%t] %c - %m%n%ex&lt;/Pattern&gt; &lt;/PatternLayout&gt; &lt;ThresholdFilter level="info" onMatch="ACCEPT" onMismatch="DENY"/&gt; &lt;Policies&gt; &lt;TimeBasedTriggeringPolicy/&gt; &lt;/Policies&gt; &lt;/RollingFile&gt; &lt;/Appenders&gt; &lt;Loggers&gt; &lt;Root level="info"&gt; &lt;AppenderRef ref="infoAppender"/&gt; &lt;/Root&gt; &lt;logger name="info.do" level="INFO" additivity="false"&gt; &lt;appender-ref ref="infoAppender"/&gt; &lt;/logger&gt; &lt;logger name="error.do" level="ERROR" additivity="false"&gt; &lt;appender-ref ref="errorAppender"/&gt; &lt;/logger&gt; &lt;logger name="remote.do" level="INFO" additivity="false"&gt; &lt;appender-ref ref="remoteAppender"/&gt; &lt;/logger&gt; &lt;/Loggers&gt;&lt;/Configuration&gt; 另外要说的是，MDC中的内容只是在当前线程中可见，如果你再当前线程中新开了子线程，或者在当前线程中使用了线程池，MDC中的内容对于他们而言都是不可见的。这时候就需要你去手动做透传。另一种方法就是你重新包装一下线程池类，将tracedId作为参数传入。]]></content>
      <categories>
        <category>java开发工具类</category>
      </categories>
      <tags>
        <tag>工具类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tomcat重启脚本]]></title>
    <url>%2Fposts%2Fadc3306d.html</url>
    <content type="text"><![CDATA[写了一个tomcat服务重启的脚本，因为挂载nginx，所有需要摘除healthCheck.html,服务重启完毕之后挂上ng。另外在重启过程中去检查服务是否启动成功， 如果不成功则不挂载ng。 tomcat重启脚本： #!/bin/sh #检查服务状态，判断是否可以挂到ng dudgeMountHealthCheckFile() { for i in `seq 1 10` do echo 'i=' $i httpStatus=$(curl -I -s -w &quot;%{http_code}\n&quot; -o /dev/null 127.0.0.1:8080/index.jsp) if [ $httpStatus -ne 200 ] then sleep 20 echo 'htppCode is: ' $httpStatus else echo 'htppCode is: ' $httpStatus touch /data/service/tomcat/webapps/ROOT/healthcheck.html echo 'add healthcheck.html' break fi done } rm -rf /data/service/tomcat/webapps/ROOT/healthcheck.html echo 'remove healthcheck.html' sleep 15 p='/data/service/tomcat' tomcatpath=${p}'/bin' echo 'operate restart tomcat: '$tomcatpath pid=`ps aux | grep $tomcatpath | grep -v grep | grep -v retomcat | awk '{print $2}'` echo 'exist pid:'$pid if [ -n &quot;$pid&quot; ] then { echo ===========shutdown================ $tomcatpath'/shutdown.sh' sleep 5 pid=`ps aux | grep $tomcatpath | grep -v grep | grep -v retomcat | awk '{print $2}'` if [ -n &quot;$pid&quot; ] then { sleep 3 echo ========kill tomcat begin============== echo $pid kill -9 $pid echo ========kill tomcat end============== } fi sleep 3 echo ===========startup.sh============== $tomcatpath'/startup.sh' sleep 10 dudgeMountHealthCheckFile } else echo ===========startup.sh============== $tomcatpath'/startup.sh' sleep 10 dudgeMountHealthCheckFile fi]]></content>
      <categories>
        <category>shell</category>
      </categories>
      <tags>
        <tag>工具类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用table标签生成页面出现大量空白的解决办法]]></title>
    <url>%2Fposts%2Fe4afcbeb.html</url>
    <content type="text"><![CDATA[写博客的时候，就发现Hexo中插入HTML表格会留出大量空白。如下所示： 姓名 性别 xiaoming 1 虽然markdown语法也是可以生成表格的，但是写的时候特别麻烦，所以我一直是直接插入的HTML表格的，仅仅是因为简单高效。 后来在hexo的issue中发现这个问题，issue,有两种解决办法： 解决方法1 将代码改为紧凑模式，修改代码如下 姓名性别xiaoming1 也就是说代码标签之间不要留白，全部改为紧贴着的。 姓名性别xiaoming1 我一般都是使用tablesgenerator来生成表格的。 解决方法2(推荐) 123&#123;% raw %&#125;html tags &amp; content&#123;% endraw %&#125; 姓名 性别 xiaoming 1]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java重试工具类:spring-retry和guava-retryer]]></title>
    <url>%2Fposts%2F73e3dbd9.html</url>
    <content type="text"><![CDATA[日常开发中经常遇到调用外部接口失败的情况，这时候我们需要去设置失败重试机制，正常情况我们的重试机制是：如果出错了，一般是网络抖动或者延迟的情况，设置重试一次，或者几次，可能是如下方案： try-catch-redo简单重试模式： 12345try&#123; doSomething();&#125; catch &#123; redo();&#125; 如果想定制什么时候重试，重试几次，就需要我们自己定义重试策略，那么我们的代码就稍微复杂一些，可能是如下方案： try-catch-redo-retry strategy策略重试模式： 12345try&#123; doSomething();&#125; catch &#123; redo(retryTime,interval);&#125; 如上是我们一般的处理方案，由上我们大致可以看出如果想要实现一个优雅的重试方案，需要我们详细的去考虑重试机制，重试策略，重试失败措施，重试代码如何做到不侵入业务代码等等。所以，这样的一个小功能我们也是可以做成很有意思的小组件的。现代分布式系统中系统调用如此频繁，重试机制也是大家开发中的重复性劳动，所以这种不必要的代码已经有人给我们写好了，分别是Java中的Spring-Retry和Guava-Retrying。 其中Spring-Retry是基于Throwable类型的重试机制，即针对可捕获异常执行重试策略，并提供相应的回滚策略；而Guava-Retrying提供了更为丰富的重试源定义，譬如多个异常或者多个返回值。 Spring-Retry: 引入jar包： 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.retry&lt;/groupId&gt; &lt;artifactId&gt;spring-retry&lt;/artifactId&gt; &lt;version&gt;1.2.2.RELEASE&lt;/version&gt;&lt;/dependency&gt; 通过构造一个重试模板来执行重试策略，在重试模板中可以设置重试次数，重试间隔和回退策略。我们通过一段代码来看一下： 123456789101112131415161718192021222324252627282930313233343536public void retryDoSomething(final Map&lt;String, Object&gt; map) throws Exception &#123; // 构建重试模板实例 RetryTemplate retryTemplate = new RetryTemplate(); // 设置重试策略，主要设置重试次数 SimpleRetryPolicy policy = new SimpleRetryPolicy(3, Collections.singletonMap(Exception.class, true)); // 设置重试回退操作策略，主要设置重试间隔时间 FixedBackOffPolicy fixedBackOffPolicy = new FixedBackOffPolicy(); fixedBackOffPolicy.setBackOffPeriod(100); retryTemplate.setRetryPolicy(policy); retryTemplate.setBackOffPolicy(fixedBackOffPolicy); // 通过RetryCallback 重试回调实例包装正常逻辑逻辑，第一次执行和重试执行执行的都是这段逻辑 final RetryCallback&lt;Object, Exception&gt; retryCallback = new RetryCallback&lt;Object, Exception&gt;() &#123; //RetryContext 重试操作上下文约定，统一spring-try包装 @Override public Object doWithRetry(RetryContext context) throws Exception &#123; System.out.println("do some thing"); Exception e = doSomething(map); System.out.println(context.getRetryCount()); throw e;//这个点特别注意，重试的根源通过Exception返回 &#125; &#125;; // 通过RecoveryCallback 重试流程正常结束或者达到重试上限后的退出恢复操作实例 final RecoveryCallback&lt;Object&gt; recoveryCallback = new RecoveryCallback&lt;Object&gt;() &#123; @Override public Object recover(RetryContext context) throws Exception &#123; System.out.println("do recory operation"); return null; &#125; &#125;; try &#123; // 由retryTemplate 执行execute方法开始逻辑执行 retryTemplate.execute(retryCallback, recoveryCallback); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; spring-retry是通过RetryTemplate的execute方法来执行重试代码的，execute方法是线程安全的，实现中他将retry的餐参数对象保存在ThreadLocal中： 123RetryContext context = this.open(retryPolicy, state);//ThreadLocal中保存context对象RetrySynchronizationManager.register(context); 以上代码需要注意的是重试的执行逻辑是在RetryCallback类中实现的，触发重试逻辑是在doWithRetry方法中抛出泛型异常。这就意味着如果你的代码并没有异常抛出但是仍然需要在某种返回值的条件下触发重试逻辑的时候，你需要手动的判断该返回值然后抛出异常。这个操作有点强人所难。 所以在此基础之上，guava-retryer重新做了设计，在支持重试次数和重试频度的基础上，能够兼容支持多个异常或者自定义实体对象的重试源定义。 引入jar： 12345&lt;dependency&gt; &lt;groupId&gt;com.github.rholder&lt;/groupId&gt; &lt;artifactId&gt;guava-retrying&lt;/artifactId&gt; &lt;version&gt;2.0.0&lt;/version&gt;&lt;/dependency&gt; 代码如下： 1234567891011121314151617181920212223242526public LedgerResult requestRestry(NrRequestParams nrRequestParams) &#123; Retryer&lt;LedgerResult&gt; retryer = RetryerBuilder.&lt;LedgerResult&gt;newBuilder() .retryIfExceptionOfType(BizException.class) .retryIfException() .retryIfResult(result -&gt; result == null) .withWaitStrategy(WaitStrategies.fixedWait(5, TimeUnit.MINUTES)) .withStopStrategy(StopStrategies.stopAfterAttempt(10)) .withRetryListener(new RetryListener() &#123; @Override public &lt;V&gt; void onRetry(Attempt&lt;V&gt; attempt) &#123; long attemptNumber = attempt.getAttemptNumber(); if (attemptNumber &gt; 1) &#123; logger.info("&lt;=====重试次数:&#123;&#125;,params:&#123;&#125;", attemptNumber - 1, nrRequestParams); &#125; &#125; &#125;) .build(); try &#123; return retryer.call(() -&gt; requestLedger(nrRequestParams)); &#125; catch (ExecutionException e) &#123; logger.error("&lt;&lt;&lt;&lt;&lt;=====重试失败====参数：&#123;&#125;", nrRequestParams, e); &#125; catch (RetryException e) &#123; logger.error("&lt;&lt;&lt;&lt;&lt;=====重试失败====参数：&#123;&#125;", nrRequestParams, e); &#125; return null; &#125; 通过RetryerBuilder来构造工厂对象，设置重试策略，比如支持特定异常类型，支持检测返回值类型判定，一个全局的监控方法，更加优雅的重试策略和停止策略设置等等。 guava的api对于用户来说更加的友好，大家会选择哪一个用于日常开发不言自明。]]></content>
      <categories>
        <category>java开发工具类</category>
      </categories>
      <tags>
        <tag>工具类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive高级操作]]></title>
    <url>%2Fposts%2F92142135.html</url>
    <content type="text"><![CDATA[1.复杂数据类型表创建 有如下建表语句： 1234567891011create table cdt(id int, name string, work_location array&lt;string&gt;, piaofang map&lt;string,bigint&gt;, address struct&lt;location:string,zipcode:int,phone:string,value:int&gt;) row format delimited fields terminated by "\t" collection items terminated by "," map keys terminated by ":" lines terminated by "\n"; 数据如下： 12311 huangbo guangzhou,xianggang,shenzhen a1:30,a2:20,a3:100 beijing,112233,13522334455,50022 xuzheng xianggang b2:50,b3:40 tianjin,223344,13644556677,60033 wangbaoqiang beijing,zhejinag c1:200 chongqinjg,334455,15622334455,20 建表以后导入数据： 12345678910hive&gt; load data local inpath "/usr/local/hive/user.txt" into table cdt;Loading data to table t1.cdtOKTime taken: 2.861 secondshive&gt; select * from cdt;OK11 huangbo [] &#123;"guangzhou":null,"xianggang":null,"shenzhen":null&#125; &#123;"location":"a1:30","zipcode":null,"phone":"a3:100","value":null&#125;22 xuzheng [] &#123;"xianggang":null&#125; &#123;"location":"b2:50","zipcode":null,"phone":null,"value":null&#125;33 wangbaoqiang ["beijing","zhejinag"] &#123;"c1":200&#125; &#123;"location":"","zipcode":null,"phone":null,"value":null&#125;Time taken: 0.841 seconds, Fetched: 3 row(s) 查询Array类型的第一个数据： 123456hive&gt; select work_location[0] from cdt;OKNULLNULLbeijingTime taken: 157.351 seconds, Fetched: 3 row(s) 查询map类型的某一个key： 123456hive&gt; select piaofang['c1'] from cdt;OKNULLNULL200Time taken: 4.907 seconds, Fetched: 3 row(s) 查询struct类型中的某一个字段： 12345678910111213hive&gt; select address from cdt;OK&#123;"location":"a1:30","zipcode":null,"phone":"a3:100","value":null&#125;&#123;"location":"b2:50","zipcode":null,"phone":null,"value":null&#125;&#123;"location":"","zipcode":null,"phone":null,"value":null&#125;Time taken: 0.853 seconds, Fetched: 3 row(s)hive&gt; select address.location from cdt;OKa1:30b2:50Time taken: 10.742 seconds, Fetched: 3 row(s) 2.视图 和关系型数据库一样，Hive 也提供了视图的功能，不过请注意，Hive 的视图和关系型数据库的数据还是有很大的区别： （1）只有逻辑视图，没有物化视图； （2）视图只能查询，不能 Load/Insert/Update/Delete 数据； （3）视图在创建时候，只是保存了一份元数据，当查询视图的时候，才开始执行视图对应的 那些子查询 1.创建视图 123hive&gt; create view view_users as select * from users;OKTime taken: 4.517 seconds 2.查看视图 12345678910hive&gt; show views;OKview_usersTime taken: 0.094 seconds, Fetched: 1 row(s)hive&gt; desc view_users;OKname stringage intsex intTime taken: 0.133 seconds, Fetched: 3 row(s) 3.视图的使用 跟普通表的查询一样： 123456789101112131415161718hive&gt; select * from view_users;OK小明 12 1小红 14 0小紫 11 0小李 15 1小豆 13 0&apos;李隆基&apos; 12 1&apos;王阳明&apos; 23 1&apos;李逵&apos; 33 1&apos;欧阳拉拉&apos; 12 1&apos;杨过&apos; 54 1&apos;小龙女&apos; 12 0&apos;周芷若&apos; 32 0&apos;李小龙&apos; 32 1&apos;陈龙&apos; 62 1&apos;范冰冰&apos; 42 0Time taken: 1.15 seconds, Fetched: 15 row(s) 删除视图： 123hive&gt; drop view view_users;OKTime taken: 2.962 seconds 3.函数 hive的内置函数有很多，并且很有用，查看所有函数： 1234567891011121314151617hive&gt; drop view view_users;OKTime taken: 2.962 secondshive&gt; show functions;OK!!=$sum0%&amp;...xpath_shortxpath_stringyear|~Time taken: 0.087 seconds, Fetched: 271 row(s) 一共是271个内置函数。 查看函数的详细说明: 1234hive&gt; desc function to_date;OKto_date(expr) - Extracts the date part of the date or datetime expression exprTime taken: 0.12 seconds, Fetched: 1 row(s) hive除了提供内置函数功能以外，还支持自定义函数，当业务比较复杂，用户可以将自定义的函数导入hive库中使用。hive提供了三种类型的自定义函数接口： UDF （user-defined function）作用于单个数据行，产生一个数据行作为输出。（数学函数，字 符串函数） UDAF （用户定义聚集函数 User- Defined Aggregation Funcation）：接收多个输入数据行，并产 生一个输出数据行。（count，max） UDTF （表格生成函数 User-Defined Table Functions）：接收一行输入，输出多行（explode） 自定义函数： 下面我们来写一个自定义UDF函数，函数的作用是将存储在数据库中的数字映射为对应的中文： 看一下你所使用的hive版本引入对应版本的jar包： 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.hive&lt;/groupId&gt; &lt;artifactId&gt;hive-exec&lt;/artifactId&gt; &lt;version&gt;2.3.2&lt;/version&gt;&lt;/dependency&gt; 编写UDF函数： 123456789101112131415161718import org.apache.hadoop.hive.ql.exec.UDF;public class SexUDF extends UDF &#123; public String evaluate(Integer sex) &#123; if (sex == null) &#123; return ""; &#125; if (sex == 0) &#123; return "女"; &#125; else if (sex == 1) &#123; return "男"; &#125; else &#123; return "未知"; &#125; &#125;&#125; 将这个类打成jar包传入服务器，在hive中引入jar包，然后创建一个自定义函数，你就可以使用这个函数： 12345678910111213141516171819hive&gt; add jar /usr/local/hive/hadoop.jar;Added [/usr/local/hive/hadoop.jar] to class pathAdded resources: [/usr/local/hive/hadoop.jar]hive&gt; create temporary function sexUDF as 'cn.edu.hust.demo1.hive.SexUDF';OKTime taken: 0.514 secondshive&gt; select sexUDF(1);OK男Time taken: 2.415 seconds, Fetched: 1 row(s)hive&gt; select sexUDF(0);OK女Time taken: 0.522 seconds, Fetched: 1 row(s)hive&gt; select sexUDF(2);OK未知Time taken: 0.265 seconds, Fetched: 1 row(s)hive&gt;]]></content>
      <categories>
        <category>大数据学习</category>
        <category>hive</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive SQL练习]]></title>
    <url>%2Fposts%2Fcd8871b9.html</url>
    <content type="text"><![CDATA[1.库操作相关 语法： CREATE (DATABASE|SCHEMA) [IF NOT EXISTS] database_name [COMMENT database_comment] //关于数据块的描述 [LOCATION hdfs_path] //指定数据库在HDFS上的存储位置 [WITH DBPROPERTIES (property_name=property_value, …)]; //指定数据块属性 1.1 创建库相关 创建库语句： 12345678910hive&gt; create database t1;OKTime taken: 16.029 secondshive&gt; show databases;OKdefaultmydb2t1Time taken: 0.743 seconds, Fetched: 3 row(s)hive&gt; 创建库的时候检查库是否存在： 123hive&gt; create database if not exists t1;OKTime taken: 0.16 seconds 创建库带注释： 1234567hive&gt; create database if not exists t2 comment 'hive is ok';OKTime taken: 0.636 secondshive&gt; desc database t2;OKt2 hive is ok hdfs://ns1/user/hive/warehouse/t2.db hadoop USERTime taken: 3.94 seconds, Fetched: 1 row(s) 1.2 查看库相关 展示所有库： 1234567hive&gt; show databases;OKdefaultmydb2t1t2Time taken: 0.049 seconds, Fetched: 4 row(s) 展示库详细信息： desc database [extended] dbname; 1234hive&gt; desc database extended t2;OKt2 hive is ok hdfs://ns1/user/hive/warehouse/t2.db hadoop USERTime taken: 0.421 seconds, Fetched: 1 row(s) 查看当前正在使用的库： 1234hive&gt; select current_database();OKdefaultTime taken: 31.444 seconds, Fetched: 1 row(s) 查看创建库的详细语句： 12345678hive&gt; show create database t2;OKCREATE DATABASE `t2`COMMENT 'hive is ok'LOCATION 'hdfs://ns1/user/hive/warehouse/t2.db'Time taken: 0.926 seconds, Fetched: 5 row(s) 1.3 删除库相关 默认情况下hive不允许删除包含表的数据库，有两种办法解决这个问题： 1.手动删除数据库下所有的表，然后删除库； 2.使用cascade关键字。 drop database if exists dbname cascade; 删除不含表的数据库： 12345678910111213hive&gt; show tables in t2;OKTime taken: 27.882 secondshive&gt; drop database t2;OKTime taken: 78.301 secondshive&gt; &gt; show databases;OKdefaultmydb2t1Time taken: 0.799 seconds, Fetched: 3 row(s) 删除含表的数据库： 123456789101112131415161718hive&gt; show databases;OKdefaultmydb2t1Time taken: 1.435 seconds, Fetched: 3 row(s)hive&gt; show tables in t1;OKusersTime taken: 0.209 seconds, Fetched: 1 row(s)hive&gt; drop database if exists t1 cascade;OKTime taken: 40.105 secondshive&gt; show databases;OKdefaultmydb2Time taken: 0.066 seconds, Fetched: 2 row(s) 1.4 切换库 use database_name; 2.表操作 2.1 创建表 CREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name [(col_name data_type [COMMENT col_comment], …)] [COMMENT table_comment] [PARTITIONED BY (col_name data_type [COMMENT col_comment], …)] [CLUSTERED BY (col_name, col_name, …) [SORTED BY (col_name [ASC|DESC], …)] INTO num_buckets BUCKETS] [ROW FORMAT row_format] [STORED AS file_format] [LOCATION hdfs_path] 关键字说明： EXTERNAL ：关键字可以让用户创建一个外部表，在建表的同时指定一个指向实际数据的路径（LOCATION） LIKE: 允许用户复制现有的表结构，但是不复制数据 COMMENT: 可以为表与字段增加描述 PARTITIONED BY : 指定分区 ROW FORMAT : DELIMITED [FIELDS TERMINATED BY char][COLLECTION ITEMS TERMINATED BY char] MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char] | SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, …)] 用户在建表的时候可以自定义 SerDe 或者使用自带的 SerDe。如果没有指定 ROW FORMAT 或者 ROW FORMAT DELIMITED，将会使用自带的 SerDe。在建表的时候，用户还需要为表指定列，用户在指定表的列的同时也会指定自定义的 SerDe，Hive 通过 SerDe 确定表的具体的列的数据。 STORED AS : SEQUENCEFILE //序列化文件 | TEXTFILE //普通的文本文件格式 | RCFILE //行列存储相结合的文件 | INPUTFORMAT input_format_classname OUTPUTFORMAT output_format_classname //自定义文件格式 如果文件数据是纯文本，可以使用 STORED AS TEXTFILE。如果数据需要压缩，使用 STORED AS SEQUENCE 。 LOCATION : 指定表在HDFS的存储路径 如果一份数据已经存储在HDFS上，并且要被多个用户或者客户端使用，最好创建外部表;反之，最好创建内部表。 如果不指定，就按照默认的规则存储在默认的仓库路径中。 1.创建一张默认的内部表 123456789hive&gt; create table users (name string, age int, sex int) row format delimited fields terminated by &quot;,&quot;;OKTime taken: 13.577 secondshive&gt; desc users;OKname stringage intsex intTime taken: 1.12 seconds, Fetched: 3 row(s) 2.外部表: 12345678hive&gt; create external table user_bank_info_ext(id int,bank_no string,bank_name string) row format delimited fields terminated by "," location "/usr/local/hive/data/user_bank";OKTime taken: 1.049 secondshive&gt; show tables;OKuser_bank_info_extusersTime taken: 0.397 seconds, Fetched: 2 row(s) 3.分区表： 123456789hive&gt; create external table goods_sale_book(id int,goods_id int,purchaser_id int) partitioned by (city_id string) row format delimited fields terminated by "," location "/usr/local/hive/data/goods_book";OKTime taken: 0.639 secondshive&gt; show tables;OKgoods_sale_bookuser_bank_info_extusersTime taken: 0.118 seconds, Fetched: 3 row(s) 添加分区： 1alter table goods_sale_book add partition(city_id=111); 4.桶表： 12345678910hive&gt; create external table books (id int,name string,isbn string,author string,warehouse_id int) clustered by (id) sorted by (warehouse_id,id) into 4 buckets row format delimited fields terminated by &quot;,&quot; location &quot;/usr/local/hive/data/books&quot;;OKTime taken: 78.249 secondshive&gt; show tables;OKbooksgoods_sale_bookuser_bank_info_extusersTime taken: 4.183 seconds, Fetched: 4 row(s) 5.查询另一个表的结果作为一张新表 1234567891011121314151617181920212223242526272829303132hive&gt; load data local inpath &quot;/usr/local/hive/user.txt&quot; into table users;Loading data to table t1.usersOKTime taken: 5.979 secondshive&gt; select * from users;OK小明 12 1小红 14 0小紫 11 0小李 15 1小豆 13 0&apos;李隆基&apos; 12 1&apos;王阳明&apos; 23 1&apos;李逵&apos; 33 1&apos;欧阳拉拉&apos; 12 1&apos;杨过&apos; 54 1&apos;小龙女&apos; 12 0&apos;周芷若&apos; 32 0&apos;李小龙&apos; 32 1&apos;陈龙&apos; 62 1&apos;范冰冰&apos; 42 0Time taken: 6.565 seconds, Fetched: 15 row(s)hive&gt; create table users_bak as select * from users where sex=0;hive&gt; select * from users_bak;OK小红 14 0小紫 11 0小豆 13 0&apos;小龙女&apos; 12 0&apos;周芷若&apos; 32 0&apos;范冰冰&apos; 42 0Time taken: 2.185 seconds, Fetched: 6 row(s) 6.复制表结构 123hive&gt; create table users_copy like users;OKTime taken: 3.016 seconds 注意： 如果在table的前面没有加external关键字，那么复制出来的新表。无论如何都是内部表 如果在table的前面有加external关键字，那么复制出来的新表。无论如何都是外部表 2.2 修改表 1.修改表名 12hive&gt; alter table user_copy rename to new_user_copy;FAILED: SemanticException [Error 10001]: Table not found t1.user_copy 2.新增字段 12345678910hive&gt; alter table users_copy add columns(mobile_phone string);OKTime taken: 2.344 secondshive&gt; desc users_copy;OKname stringage intsex intmobile_phone stringTime taken: 1.515 seconds, Fetched: 4 row(s)]]></content>
      <categories>
        <category>大数据学习</category>
        <category>hive</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hiveserver2使用以及java连接方式]]></title>
    <url>%2Fposts%2Fd946acf8.html</url>
    <content type="text"><![CDATA[我们知道hive的数据是存储在HDFS上的，hive只是充当一个客户端，实际上是去HDFS执行mapReduce命令。在我们第一次安装hive的时候，介绍过使用bin目录下的&quot;hive&quot;命令去启动hive客户端程序： 那么生产环境下我们肯定不能在服务器上使用命令行的方式去操作hive，在hive的bin目录下提供了hiveserver2命令，HiveServer2(HS2)是一种能使客户端执行Hive查询的服务。 HiveServer2是HiveServer1的改进版，HiveServer1已经被废弃。HiveServer2可以支持多客户端并发和身份认证。旨在为开放API客户端（如JDBC和ODBC）提供更好的支持。 HiveServer2单进程运行，提供组合服务，包括基于Thrift的Hive服务（TCP或HTTP）和用于Web UI的Jetty Web服务器。 Hiveserver2提供了客户端连接hive的服务，用户并不是直接连接的，bin目录下还有另外一个命令，beeline。它是基于SQLLine CLI的JDBC客户端 。 Beeline支持嵌入模式(embedded mode)和远程模式(remote mode)。在嵌入式模式下，运行嵌入式的Hive(类似Hive CLI)，而远程模式可以通过Thrift连接到独立的HiveServer2进程上。从Hive 0.14版本开始，Beeline使用HiveServer2工作时，它也会从HiveServer2输出日志信息到STDERR。 1.hiveserver2服务配置 下面我们来配置通过beeline连接hiveserver2服务，在远程连接hive服务。 1.1 修改hadoop的hdfs-site.xml文件： 加入一条配置信息，启用webhdfs,因为我们需要远程访问HDFS： 1234&lt;property&gt; &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt; 1.2 修改hadoop的core-site.xml文件： 需要允许hadoop集群可以被任一代理用户访问： 12345678&lt;property&gt; &lt;name&gt;hadoop.proxyuser.hadoop.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.hadoop.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt; 修改完成之后同步两个文件至集群所有机器，然后重启集群。 1.3 hadoop重启完成之后，我们可以开始启动hiveserver2服务，输入命令：hvieserver2,然后jps一下会多出一个进程： 当然这种方式启动hiveserver2服务，你关闭窗口服务就挂了，我们可以使用后台的方式来启动： 12345nohup hiveserver2 1&gt;/usr/local/hive/log/hiveserver.log 2&gt;/usr/local/hive/log/hiveserver.err &amp;或者：nohup hiveserver2 1&gt;/dev/null 2&gt;/dev/null &amp;或者：nohup hiveserver2 &gt;/dev/null 2&gt;&amp;1 &amp; 以上3个命令是等价的，第一个表示记录日志，第二个和第三个不记录日志。 其中第一个命令中的1 和 2 的含义分别是： 1：表示标准输出日志 2：表示错误日志输出，如果你没有配置错误输出目录，则会在当前工作目录下生成目录，默认日志名叫做：nohup.xxx 命令学习：nohup 命令：如果你正在运行一个进程，而且你觉得在退出帐户时该进程还不会结束， 那么可以使用 nohup 命令。该命令可以在你退出帐户/关闭终端之后继续运行相应的进程。 nohup 就是不挂起的意思(no hang up)。 该命令的一般形式为：nohup command &amp; 1.4 启动完毕hiveserver2服务之后，我们可以开始启用beeline客户端去连接： 输入beeline命令之后需要你手动的输入连接串： 1!connect jdbc:hive2://hadoopmaster:10000 -n root 当然我们也直接将连接串拼接在beeline命令后面： 1beeline -u jdbc:hive2://hadoopmaster:10000 -n root --color=true --silent=false 1.5 以下为beeline常用的参数解释： 1234567891011121314151617181920212223242526272829303132The Beeline CLI 支持以下命令行参数: Option Description --autoCommit=[true/false] ---进入一个自动提交模式：beeline --autoCommit=true --autosave=[true/false] ---进入一个自动保存模式：beeline --autosave=true --color=[true/false] ---显示用到的颜色：beeline --color=true --delimiterForDSV= DELIMITER ---分隔值输出格式的分隔符。默认是“|”字符。 --fastConnect=[true/false] ---在连接时，跳过组建表等对象：beeline --fastConnect=false --force=[true/false] ---是否强制运行脚本：beeline--force=true --headerInterval=ROWS ---输出的表间隔格式，默认是100: beeline --headerInterval=50 --help ---帮助 beeline --help --hiveconf property=value ---设置属性值，以防被hive.conf.restricted.list重置：beeline --hiveconf prop1=value1 --hivevar name=value ---设置变量名：beeline --hivevar var1=value1 --incremental=[true/false] ---输出增量 --isolation=LEVEL ---设置事务隔离级别：beeline --isolation=TRANSACTION_SERIALIZABLE --maxColumnWidth=MAXCOLWIDTH ---设置字符串列的最大宽度：beeline --maxColumnWidth=25 --maxWidth=MAXWIDTH ---设置截断数据的最大宽度：beeline --maxWidth=150 --nullemptystring=[true/false] ---打印空字符串：beeline --nullemptystring=false --numberFormat=[pattern] ---数字使用DecimalFormat：beeline --numberFormat="#,###,##0.00" --outputformat=[table/vertical/csv/tsv/dsv/csv2/tsv2] ---输出格式：beeline --outputformat=tsv --showHeader=[true/false] ---显示查询结果的列名：beeline --showHeader=false --showNestedErrs=[true/false] ---显示嵌套错误：beeline --showNestedErrs=true --showWarnings=[true/false] ---显示警告：beeline --showWarnings=true --silent=[true/false] ---减少显示的信息量：beeline --silent=true --truncateTable=[true/false] ---是否在客户端截断表的列 --verbose=[true/false] ---显示详细错误信息和调试信息：beeline --verbose=true -d &lt;driver class&gt; ---使用一个驱动类：beeline -d driver_class -e &lt;query&gt; ---使用一个查询语句：beeline -e "query_string" -f &lt;file&gt; ---加载一个文件：beeline -f filepath 多个文件用-e file1 -e file2 -n &lt;username&gt; ---加载一个用户名：beeline -n valid_user -p &lt;password&gt; ---加载一个密码：beeline -p valid_password 使用java连接hiveserver2服务 既然hiveserver2就是为了提供远程连接hive服务的功能，我们直接使用java来操作一下，首先必须的启动hiveserver2服务。 2. 基于java的hive访问 如下所示工程基于maven搭建，需要引入如下jar包： 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.hive&lt;/groupId&gt; &lt;artifactId&gt;hive-jdbc&lt;/artifactId&gt; &lt;version&gt;2.3.2&lt;/version&gt;&lt;/dependency&gt; 123456789101112131415161718192021222324252627282930313233343536package cn.edu.hust.demo1.hive;import java.sql.*;public class HiveClient &#123; private static String driverName = "org.apache.hive.jdbc.HiveDriver"; /** * @param args * @throws SQLException */ public static void main(String[] args) throws SQLException &#123; try &#123; Class.forName(driverName); &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); System.exit(1); &#125; Connection con = DriverManager.getConnection("jdbc:hive2://hadoopmaster:10000/default", "hadoop", "123456"); Statement stmt = con.createStatement(); String sql = "select * from t2"; System.out.println("Running: " + sql); ResultSet res = stmt.executeQuery(sql); while (res.next()) &#123; System.out.println(res.getString(1)+ " " +res.getString(2) + " " + res.getString(3)); &#125; String insertSql = "insert into t2 (id,name,age) values(4,\"xiaocun\",13)"; stmt.execute(insertSql); System.out.println("-----------------------"); System.out.println("Running: " + sql); ResultSet res1 = stmt.executeQuery(sql); while (res1.next()) &#123; System.out.println(res1.getString(1)+ " " +res1.getString(2) + " " + res1.getString(3)); &#125; &#125;&#125; 以上程序执行完毕结果如下： 12345678910111213Running: select * from t21 xiaoming 122 xiaohong 133 xiaohei 15-----------------------Running: select * from t21 xiaoming 122 xiaohong 133 xiaohei 154 xiaocun 13Process finished with exit code 0]]></content>
      <categories>
        <category>大数据学习</category>
        <category>hive</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive元数据存储-MYSQL数据库表]]></title>
    <url>%2Fposts%2Fdfd44a4f.html</url>
    <content type="text"><![CDATA[hive默认本地存储是将元数据存储在内嵌的Derby数据库中，但是生产环境中没有人会用本地模式，一般remote模式使用MYSQL数据库，我们在hive-site.xml中也是配置的MYSQL数据库来存储元数据，下面就来看一下在MYSQL中是如何存储这些元数据信息的。 在本地我们连接MYSQL可以看到hive元数据存储创建的表还真的不少： 1.hive表和视图相关的元数据表 主要有TBLS、TABLE_PARAMS、TBL_PRIVS，这三张表通过TBL_ID关联。 TBLS：存储Hive表、视图、索引表的基本信息 TABLE_PARAMS：该表存储表/视图的属性信息 TBL_PRIVS： 该表存储表/视图的授权信息 TBLS表： 表字段 说明 示例数据 TBL_ID 表ID 1 CREATE_TIME 创建时间 1546336624 DB_ID 数据库ID 2，对应DBS中的DB_ID LAST_ACCESS_TIME 上次访问时间 1546336624 OWNER 所有者 hadoop RETENTION 保留字段 0 SD_ID 序列化配置信息 12，对应SDS表中的SD_ID TBL_NAME 表名 t2 TBL_TYPE 表类型 MANAGED_TABLE、EXTERNAL_TABLE、INDEX_TABLE、VIRTUAL_VIEW VIEW_EXPANDED_TEXT 视图的详细HQL语句 select name,age from t2 VIEW_ORIGINAL_TEXT 视图的原始HQL语句 select * from t2 TABLE_PARAMS表： 表字段 说明 示例数据 TBL_ID 表ID 1 PARAM_KEY 属性名 totalSize、numFiles、EXTERNAL PARAM_VALUE 属性值 1536169128、1536170730、TRUE TBL_PRIVS表： 表字段 说明 示例数据 TBL_GRANT_ID 授权ID 1 CREATE_TIME 授权时间 1536169128 GRANT_OPTION 描述信息 GRANTOR 授权执行用户 hadoop GRANTOR_TYPE 授权者类型 USER PRINCIPAL_NAME 被授权用户 root PRINCIPAL_TYPE 被授权用户类型 USER TBL_PRIV 权限 Select、Alter TBL_ID 表ID 1，对应TBLS表中的TBL_ID 2.hive表字段相关的表： hive会将所有表的字段信息都维护到这张表中：COLUMNS_V2 。 表字段 说明 示例数据 CD_ID 字段信息ID 1 COMMENT 字段注释 COLUMN_NAME 字段名 name TYPE_NAME 字段类型 string INTEGER_IDX 字段顺序 3 3.hive表分区相关的表： 主要有PARTITIONS、PARTITION_KEYS、PARTITION_KEY_VALS、PARTITION_PARAMS ： PARTITIONS： 该表存储表分区的基本信息 ； PARTITION_KEYS：该表存储分区的字段信息； PARTITION_KEY_VALS：该表存储分区字段值； PARTITION_PARAMS ：该表存储分区的属性信息 。 PARTITIONS表： 表字段 说明 示例数据 PART_ID 分区ID 1 CREATE_TIME 分区创建时间 1536169128 LAST_ACCESS_TIME 最后一次访问时间 1636163126 PART_NAME 分区名 pt=2018-04-15 SD_ID 分区存储ID 33 TBL_ID 表ID 1 PARTITION_KEYS表： 表字段 说明 示例数据 TBL_ID 表ID 1 PKEY_COMMENT 分区字段说明 PKEY_NAME 分区字段名 area_id PKEY_TYPE 分区字段类型 int INTEGER_IDX 分区字段顺序 1 PARTITION_KEY_VALS表： 表字段 说明 示例数据 PART_ID 分区ID 1 PART_KEY_VAL 分区字段值 2018-04-20 INTEGER_IDX 分区字段值顺序 0 PARTITION_PARAMS 表： 表字段 说明 示例数据 PART_ID 分区ID 1 PARAM_KEY 分区属性名 numFiles、numRows PARAM_VALUE 分区属性值 33、554222112 4. hive文件存储相关的表： 主要有SDS、SD_PARAMS、SERDES、SERDE_PARAMS 。 SDS:该表保存文件存储的基本信息，如INPUT_FORMAT、OUTPUT_FORMAT、是否压缩等 ; SD_PARAMS:该表存储Hive存储的属性信息，在创建表时候使用 ; SERDES: 该表存储序列化使用的类信息 ; SERDE_PARAMS : 该表存储序列化的一些属性、格式信息,比如：行、列分隔符 。 SDS表： 表字段 说明 示例数据 SD_ID 存储信息ID 1 CD_ID 字段信息ID 1，对应CDS表 INPUT_FORMAT 文件输入格式 org.apache.hadoop.mapred.TextInputFormat IS_COMPRESSED 是否压缩 0 IS_STOREDASSUBDIRECTORIES 是否以子目录存储 0 LOCATION HDFS路径 hdfs://hadoopmaster:9000/user/hive/warehouse/t2 NUM_BUCKETS 分桶数量 3 OUTPUT_FORMAT 文件输出格式 org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat SERDE_ID 序列化类ID 3，对应SERDES表 SD_PARAMS表： 表字段 说明 示例数据 SD_ID 存储配置ID 1 PARAM_KEY 存储属性名 serialization.format PARAM_VALUE 存储属性值 1 SERDES表： 表字段 说明 示例数据 SERDE_ID 序列化类配置ID 1 PARAM_KEY 属性名 PARAM_VALUE 属性值 org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe SERDE_PARAMS 表： 表字段 说明 示例数据 SERDE_ID 序列化类配置ID 1 PARAM_KEY 属性名 serialization.format PARAM_VALUE 属性值 1 5.hive数据库相关基本信息 VERSION,DBS,DATABASE_PARAMS,主要描述hive版本以及hive用户相关信息。 VERSION ：查询版本信息 ； DBS：存储Hive中所有数据库的基本信息； DATABASE_PARAMS：存储数据库的相关参数，在CREATE DATABASE时候用 。 VERSION 表： 表字段 说明 示例数据 VER_ID ID主键 1 SCHEMA_VERSION Hive版本 2.3.0 VERSION_COMMENT 版本说明 Hive release version 2.3.0 DBS表： 表字段 说明 示例数据 DB_ID 数据库ID 2 DESC 数据库描述 测试库 DB_LOCATION_URI 数据库HDFS路径 hdfs://hadoopmaster:9000/user/hive/warehouse NAME 数据库名 default OWNER_NAME 数据库所有者用户名 public OWNER_TYPE 所有者角色 ROLE DATABASE_PARAMS表： 表字段 说明 示例数据 DB_ID 数据库ID PARAM_KEY 参数名 PARAM_VALUE 参数值 6.其他一些不常用的表： DB_PRIVS：数据库权限信息表。通过GRANT语句对数据库授权后，将会在这里存储; FUNCS：用户注册的函数信息; FUNC_RU：用户注册函数的资源信息; IDXS：索引表，存储Hive索引相关的元数据; INDEX_PARAMS：索引相关的属性信息; PART_PRIVS：分区的授权信息; PART_COL_STATS：分区字段的统计信息; PART_COL_PRIVS：分区字段的权限信息; TAB_COL_STATS：表字段的统计信息,使用ANALYZE语句对表字段分析后记录在这里; TBL_COL_PRIVS：表字段的授权信息]]></content>
      <categories>
        <category>大数据学习</category>
        <category>hive</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[序列化--Avro]]></title>
    <url>%2Fposts%2F62981c04.html</url>
    <content type="text"><![CDATA[avro Avro是一种序列化的方法。常见的序列化有Java的Serializable,google的protobuf，以及Doug Cutting的Avro。 Avro的Schema Avro的Schema用JSON表示。Schema定义了简单数据类型和复杂数据类型。 Avro基本数据类型 类型 含义 null 没有值 boolean 布尔值 int 32位有符号整数 long 64位有符号整数 float 单精度（32位）的IEEE 754浮点数 double 双精度（64位）的IEEE 754浮点数 bytes 8位无符号字节序列 string 字符串 基本类型没有属性，基本类型的名字也就是类型的名字，比如： {&quot;type&quot;: &quot;string&quot;} ​ 复杂类型 Avro提供了6种复杂类型。分别是Record，Enum，Array，Map，Union和Fixed。 ​ Record Record类型使用的类型名字是 “record”，还支持其它属性的设置： name：record类型的名字(必填) namespace：命名空间(可选) doc：这个类型的文档说明(可选) aliases：record类型的别名，是个字符串数组(可选) fields：record类型中的字段，是个对象数组(必填)。每个字段需要以下属性： name：字段名字(必填) doc：字段说明文档(可选) type：一个schema的json对象或者一个类型名字(必填) default：默认值(可选) order：排序(可选)，只有3个值ascending(默认)，descending或ignore aliases：别名，字符串数组(可选) 一个Record类型例子，定义一个元素类型是Long的链表： { &quot;type&quot;: &quot;record&quot;, &quot;name&quot;: &quot;LongList&quot;, &quot;aliases&quot;: [&quot;LinkedLongs&quot;], // old name for this &quot;fields&quot; : [ {&quot;name&quot;: &quot;value&quot;, &quot;type&quot;: &quot;long&quot;}, // each element has a long {&quot;name&quot;: &quot;next&quot;, &quot;type&quot;: [&quot;null&quot;, &quot;LongList&quot;]} // optional next element ] } ​ Enum 枚举类型的类型名字是”enum”，还支持其它属性的设置： name：枚举类型的名字(必填) namespace：命名空间(可选) aliases：字符串数组，别名(可选) doc：说明文档(可选) symbols：字符串数组，所有的枚举值(必填)，不允许重复数据。 一个枚举类型的例子： { &quot;type&quot;: &quot;enum&quot;, &quot;name&quot;: &quot;Suit&quot;, &quot;symbols&quot; : [&quot;SPADES&quot;, &quot;HEARTS&quot;, &quot;DIAMONDS&quot;, &quot;CLUBS&quot;] } Array 数组类型的类型名字是”array”并且只支持一个属性： items：数组元素的schema 一个数组例子： {&quot;type&quot;: &quot;array&quot;, &quot;items&quot;: &quot;string&quot;} Map Map类型的类型名字是”map”并且只支持一个属性： values：map值的schema Map的key必须是字符串。 一个Map例子： {&quot;type&quot;: &quot;map&quot;, &quot;values&quot;: &quot;long&quot;} Union 组合类型，表示各种类型的组合，使用数组进行组合。比如[“null”, “string”]表示类型可以为null或者string。 组合类型的默认值是看组合类型的第一个元素，因此如果一个组合类型包括null类型，那么null类型一般都会放在第一个位置，这样子的话这个组合类型的默认值就是null。 组合类型中不允许同一种类型的元素的个数不会超过1个，除了record，fixed和enum。比如组合类中有2个array类型或者2个map类型，这是不允许的。 组合类型不允许嵌套组合类型。 Fixed 混合类型的类型名字是fixed，支持以下属性： name：名字(必填) namespace：命名空间(可选) aliases：字符串数组，别名(可选) size：一个整数，表示每个值的字节数(必填) 比如16个字节数的fixed类型例子如下： {&quot;type&quot;: &quot;fixed&quot;, &quot;size&quot;: 16, &quot;name&quot;: &quot;md5&quot;} Avro序列化方式 Avro使用Json来声明Schema，这个预先定义的模式作为通信两端数据协议，在网络传输前后对目标数据进行二进制处理。 定义schema 首先定义一个schema，user.avsc： {&quot;namespace&quot;: &quot;com.rickiyang.avro&quot;, &quot;type&quot;: &quot;record&quot;, &quot;name&quot;: &quot;User&quot;, &quot;fields&quot;: [ {&quot;name&quot;: &quot;name&quot;, &quot;type&quot;: &quot;string&quot;}, {&quot;name&quot;: &quot;age&quot;, &quot;type&quot;: [&quot;int&quot;, &quot;null&quot;]}, {&quot;name&quot;: &quot;sex&quot;, &quot;type&quot;: [&quot;int&quot;, &quot;null&quot;]} ] } 上面schema中使用的是record类型，里面的各个字段上面都有解释。 编译schema 根据schema生成相应的java类对象，用户set值。 编译schema有两种方式，一种是通过maven插件生成，一种是命令行的方式。 命令行方式： 需要额外的jar包：avro-tools-1.8.1.jar,(https://mvnrepository.com/artifact/org.apache.avro/avro-maven-plugin) 将该jar包和user.avsc文件放在同一目录下，执行如下命令： java -jar avro-tools-1.8.1.jar compile schema user.avsc . 注意最后有个“.”,表示在当前目录下生成java文件。 执行完之后在当前目录下就会生成User类。接下来我们就可以使用刚才定义的对象去set值了。关于maven插件的方式就不多做说明，请参考这一篇博文(https://www.baeldung.com/java-apache-avro)，在你的IDE中安装maven插件，set source path 和 target path，然后执行“mvn clean install” 即可。 序列化和反序列化 上面编译schema生成java对象，我们拿到java对象可以去set相应的值。当然也有一种不通过使用生成的User对象的方式去序列化，我们一一介绍。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123import org.apache.avro.Schema;import org.apache.avro.file.DataFileReader;import org.apache.avro.file.DataFileWriter;import org.apache.avro.generic.GenericData;import org.apache.avro.generic.GenericDatumReader;import org.apache.avro.generic.GenericDatumWriter;import org.apache.avro.generic.GenericRecord;import org.apache.avro.io.DatumReader;import org.apache.avro.io.DatumWriter;import org.apache.avro.specific.SpecificDatumReader;import org.apache.avro.specific.SpecificDatumWriter;import java.io.File;import java.io.IOException;/** * @Author yangyue * @Date Created in 上午10:20 2018/11/21 * @Modified by: * @Description: **/public class Serialization &#123; public static void main(String[] args) &#123; encode(); decode(); nonCoding(); nonCodingDecode(); &#125; /** * 序列化 */ public static void encode() &#123; try &#123; User user1 = new User(); user1.setName("Tom"); user1.setAge(7); user1.setSex(1); DatumWriter&lt;User&gt; userDatumWriter = new SpecificDatumWriter&lt;&gt;(User.class); DataFileWriter&lt;User&gt; dataFileWriter = new DataFileWriter&lt;&gt;(userDatumWriter); dataFileWriter.create(user1.getSchema(), new File("user.avro")); dataFileWriter.append(user1); dataFileWriter.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;​ /** * 反序列化 */ public static void decode() &#123; try &#123; DatumReader&lt;User&gt; userDatumReader = new SpecificDatumReader&lt;&gt;(User.class); DataFileReader&lt;User&gt; dataFileReader = new DataFileReader&lt;&gt;(new File("user.avro"), userDatumReader); User user = null; while (dataFileReader.hasNext()) &#123; user = dataFileReader.next(user); System.out.println(user); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; /** * 不使用对象的序列化 */ public static void nonCoding() &#123; try &#123; ClassLoader classLoader = Serialization.class.getClassLoader(); String avscFilePath = classLoader.getClass().getResource("/user.avsc").getPath(); Schema schema = new Schema.Parser().parse(new File(avscFilePath)); GenericRecord user1 = new GenericData.Record(schema); user1.put("name", "Tony"); user1.put("age", 18); GenericRecord user2 = new GenericData.Record(schema); user2.put("name", "Ben"); user2.put("age", 3); user2.put("sex", 1); File file = new File("user.avro"); DatumWriter&lt;GenericRecord&gt; datumWriter = new GenericDatumWriter&lt;&gt;(schema); DataFileWriter&lt;GenericRecord&gt; dataFileWriter = new DataFileWriter&lt;&gt;(datumWriter); dataFileWriter.create(schema, file); dataFileWriter.append(user1); dataFileWriter.append(user2); dataFileWriter.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; /** * 不使用对象的反序列化 */ public static void nonCodingDecode() &#123; try &#123; ClassLoader classLoader = Serialization.class.getClassLoader(); String avscFilePath = classLoader.getClass().getResource("/user.avsc").getPath(); Schema schema = new Schema.Parser().parse(new File(avscFilePath)); File file = new File("user.avro"); DatumReader&lt;GenericRecord&gt; datumReader = new GenericDatumReader&lt;&gt;(schema); DataFileReader&lt;GenericRecord&gt; dataFileReader = new DataFileReader&lt;&gt;(file, datumReader); GenericRecord user = null; while (dataFileReader.hasNext()) &#123; user = dataFileReader.next(user); System.out.println(user); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 以上为序列化和反序列化的两种方式，注意到序列化的文件都输出到根目录下的user.avro这个文件中，输出序列化文件，使用于写数据到HDFS中存储使用。 Avro实战 Kafka与Avro 上面已经生成User类，我们可以用该对象去发送avro消息。 引进pom： 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &lt;version&gt;0.10.2.1&lt;/version&gt;&lt;/dependency&gt; 创建Consumer: 1234567891011121314151617181920212223242526272829303132333435363738394041424344/** * @Author yangyue * @Date Created in 下午4:17 2018/11/23 * @Modified by: * @Description: **/import org.apache.avro.specific.SpecificRecordBase;import org.apache.kafka.clients.consumer.ConsumerConfig;import org.apache.kafka.clients.consumer.ConsumerRecord;import org.apache.kafka.clients.consumer.ConsumerRecords;import org.apache.kafka.clients.consumer.KafkaConsumer;import org.apache.kafka.common.serialization.StringDeserializer;import java.util.Collections;import java.util.List;import java.util.Properties;import java.util.stream.Collectors;import java.util.stream.StreamSupport;public class Consumer&lt;T extends SpecificRecordBase&gt; &#123; private KafkaConsumer&lt;String, T&gt; consumer = new KafkaConsumer&lt;&gt;(getProperties()); public List&lt;T&gt; receive(Topic topic) &#123;// TopicPartition partition = new TopicPartition(topic.topicName, 0); consumer.subscribe(Collections.singletonList(topic.topicName));// consumer.assign(Collections.singletonList(partition)); ConsumerRecords&lt;String, T&gt; records = consumer.poll(10); return StreamSupport.stream(records.spliterator(), false) .map(ConsumerRecord::value).collect(Collectors.toList()); &#125; private Properties getProperties() &#123; Properties props = new Properties(); props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092"); props.put(ConsumerConfig.GROUP_ID_CONFIG, "DemoConsumer"); props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName()); props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, AvroDeserializer.class.getName()); return props; &#125;&#125; Avro 对象的反序列化类 AvroDeserializer: 1234567891011121314151617181920212223242526272829303132333435363738394041/** * @Author yangyue * @Date Created in 下午4:17 2018/11/23 * @Modified by: * @Description: **/import com.sun.xml.internal.ws.encoding.soap.DeserializationException;import org.apache.avro.io.BinaryDecoder;import org.apache.avro.io.DatumReader;import org.apache.avro.io.DecoderFactory;import org.apache.avro.specific.SpecificDatumReader;import org.apache.avro.specific.SpecificRecordBase;import org.apache.kafka.common.serialization.Deserializer;import java.io.ByteArrayInputStream;import java.io.IOException;import java.util.Map;public class AvroDeserializer&lt;T extends SpecificRecordBase&gt; implements Deserializer&lt;T&gt; &#123; @Override public void configure(Map&lt;String, ?&gt; configs, boolean isKey) &#123; &#125; @Override public T deserialize(String topic, byte[] data) &#123; DatumReader&lt;T&gt; userDatumReader = new SpecificDatumReader&lt;&gt;(Topic.matchFor(topic).topicType.getSchema()); BinaryDecoder binaryEncoder = DecoderFactory.get().directBinaryDecoder(new ByteArrayInputStream(data), null); try &#123; return userDatumReader.read(null, binaryEncoder); &#125; catch (IOException e) &#123; throw new DeserializationException(e.getMessage()); &#125; &#125; @Override public void close() &#123; &#125;&#125; 创建Consumer： 1234567891011121314151617181920212223242526272829303132333435363738394041424344/** * @Author yangyue * @Date Created in 下午4:17 2018/11/23 * @Modified by: * @Description: **/import org.apache.avro.specific.SpecificRecordBase;import org.apache.kafka.clients.consumer.ConsumerConfig;import org.apache.kafka.clients.consumer.ConsumerRecord;import org.apache.kafka.clients.consumer.ConsumerRecords;import org.apache.kafka.clients.consumer.KafkaConsumer;import org.apache.kafka.common.serialization.StringDeserializer;import java.util.Collections;import java.util.List;import java.util.Properties;import java.util.stream.Collectors;import java.util.stream.StreamSupport;public class Consumer&lt;T extends SpecificRecordBase&gt; &#123; private KafkaConsumer&lt;String, T&gt; consumer = new KafkaConsumer&lt;&gt;(getProperties()); public List&lt;T&gt; receive(Topic topic) &#123;// TopicPartition partition = new TopicPartition(topic.topicName, 0); consumer.subscribe(Collections.singletonList(topic.topicName));// consumer.assign(Collections.singletonList(partition)); ConsumerRecords&lt;String, T&gt; records = consumer.poll(10); return StreamSupport.stream(records.spliterator(), false) .map(ConsumerRecord::value).collect(Collectors.toList()); &#125; private Properties getProperties() &#123; Properties props = new Properties(); props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092"); props.put(ConsumerConfig.GROUP_ID_CONFIG, "DemoConsumer"); props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName()); props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, AvroDeserializer.class.getName()); return props; &#125;&#125; ​ 创建序列化avro对象的解析器AvroSerializer： 12345678910111213141516171819202122232425262728293031323334353637383940414243/** * @Author yangyue * @Date Created in 下午4:16 2018/11/23 * @Modified by: * @Description: **/import org.apache.avro.io.BinaryEncoder;import org.apache.avro.io.DatumWriter;import org.apache.avro.io.EncoderFactory;import org.apache.avro.specific.SpecificDatumWriter;import org.apache.avro.specific.SpecificRecordBase;import org.apache.kafka.common.errors.SerializationException;import org.apache.kafka.common.serialization.Serializer;import java.io.ByteArrayOutputStream;import java.io.IOException;import java.util.Map;public class AvroSerializer&lt;T extends SpecificRecordBase&gt; implements Serializer&lt;T&gt;&#123; @Override public void configure(Map&lt;String, ?&gt; configs, boolean isKey) &#123; &#125; @Override public byte[] serialize(String topic, T data) &#123; DatumWriter&lt;T&gt; userDatumWriter = new SpecificDatumWriter&lt;&gt;(data.getSchema()); ByteArrayOutputStream outputStream = new ByteArrayOutputStream(); BinaryEncoder binaryEncoder = EncoderFactory.get().directBinaryEncoder(outputStream, null); try &#123; userDatumWriter.write(data, binaryEncoder); &#125; catch (IOException e) &#123; throw new SerializationException(e.getMessage()); &#125; return outputStream.toByteArray(); &#125; @Override public void close() &#123; &#125;&#125; 定义一个topic： 1234567891011121314151617181920212223242526272829/** * @Author yangyue * @Date Created in 下午4:17 2018/11/23 * @Modified by: * @Description: **/import com.rickiyang.avro.User;import org.apache.avro.specific.SpecificRecordBase;import java.util.EnumSet;public enum Topic &#123; USER("user-info-topic", new User()); public final String topicName; public final SpecificRecordBase topicType; Topic(String topicName, SpecificRecordBase topicType) &#123; this.topicName = topicName; this.topicType = topicType; &#125; public static Topic matchFor(String topicName) &#123; return EnumSet.allOf(Topic.class).stream() .filter(topic -&gt; topic.topicName.equals(topicName)) .findFirst() .orElse(null); &#125;&#125; 接下来我们可以运行一下： 12345678910111213141516171819202122232425262728293031323334353637383940414243import com.rickiyang.avro.User;import java.util.List;import java.util.Scanner;/** * @Author yangyue * @Date Created in 下午4:18 2018/11/23 * @Modified by: * @Description: **/public class KafkaTest &#123; public static void main(String[] args) &#123; Producer&lt;User&gt; producer = new Producer&lt;&gt;(); Consumer&lt;User&gt; consumer = new Consumer&lt;&gt;(); System.out.println("Please input 'send', 'receive', or 'exit'"); Scanner scanner = new Scanner(System.in); while (scanner.hasNext()) &#123; String input = scanner.next(); switch (input) &#123; case "send": producer.sendData(Topic.USER, new User("xiaoming", 12, 1)); break; case "receive": List&lt;User&gt; users = consumer.receive(Topic.USER); if (users.isEmpty()) &#123; System.out.println("Received nothing"); &#125; else &#123; users.forEach(user -&gt; System.out.println("Received user: " + user)); &#125; break; case "exit": System.exit(0); break; default: System.out.println("Please input 'send', 'receive', or 'exit'"); &#125; &#125; &#125;&#125;]]></content>
      <categories>
        <category>kafka</category>
      </categories>
      <tags>
        <tag>学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hbase和HDFS之间互相操作数据]]></title>
    <url>%2Fposts%2Fd23070f0.html</url>
    <content type="text"><![CDATA[1.读取HDFS中的数据存入Hbase 现在在HDFS中有一个user.txt文件，格式如下: 1234567891011121314151617181920225694,林碧万,1,32,10000001224478,于震海,1,24,13060007224458,杨海兵,1,25,10054005224340,周旺沛,1,21,10034076225691,孙庆江,1,27,10000154224282,汪鹏名,1,22,10000001224285,郑博,1,26,10000154225699,孙天宇,2,23,10054005225708,徐飞,1,24,10054005225703,李风广,1,21,10000001224462,武静静,2,27,10000154225710,曾轲,1,28,13060007225715,刘通,1,26,10054005223720,赛林林,1,26,10000154224489,文俊龙,1,29,13060007225050,陈奔,1,26,10000154223728,吴迪,2,22,10054005225066,杨铭,1,24,10000001223745,刘靖文,1,26,10000154223755,杨佳鹏,1,24,10032049 上面字段为别为：userid，name，gender，部门。 将数据存入HDFS，在HDFS中新建目录，/data，传入数据： 1hadoop dfs -put /usr/local/user.txt /data/ 将hdfs中的数据写入Hbase,在Hbase中建表： 1create 'users','info' 下面写我们的hadoop代码，读取hadoop中的数据传入到Hbase。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.conf.Configured;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.Hbase.HbaseConfiguration;import org.apache.hadoop.Hbase.client.Put;import org.apache.hadoop.Hbase.mapreduce.TableMapReduceUtil;import org.apache.hadoop.Hbase.mapreduce.TableReducer;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import org.apache.hadoop.util.Tool;import org.apache.hadoop.util.ToolRunner;import java.io.IOException;/** * 从hdfs中读取数据插入到Hbase */public class ReadHDFSDataToHbaseMR extends Configured implements Tool &#123; public static void main(String[] args) throws Exception &#123; int run = ToolRunner.run(new ReadHDFSDataToHbaseMR(), args); System.exit(run); &#125; @Override public int run(String[] arg0) throws Exception &#123; Configuration conf = HbaseConfiguration.create(); conf.set("fs.defaultFS", "hdfs://hadoopmaster:9000/"); conf.set("Hbase.zookeeper.quorum", "hadoopmaster:2181,hadoopslaver1:2181,hadoopslaver2:2181"); System.setProperty("HADOOP_USER_NAME", "hadoop"); FileSystem fs = FileSystem.get(conf); Job job = Job.getInstance(conf); job.setJarByClass(ReadHDFSDataToHbaseMR.class); job.setMapperClass(HDFSToHbaseMapper.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(NullWritable.class); TableMapReduceUtil.initTableReducerJob("users", HDFSToHbaseReducer.class, job, null, null, null, null, false); job.setOutputKeyClass(NullWritable.class); job.setOutputValueClass(Put.class); Path inputPath = new Path("/data/"); Path outputPath = new Path("/users/output/"); if (fs.exists(outputPath)) &#123; fs.delete(outputPath, true); &#125; FileInputFormat.addInputPath(job, inputPath); FileOutputFormat.setOutputPath(job, outputPath); boolean isDone = job.waitForCompletion(true); return isDone ? 0 : 1; &#125; public static class HDFSToHbaseMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt; &#123; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; context.write(value, NullWritable.get()); &#125; &#125; /** * reduce处理的数据给到Hbase，组装Hbase的column * 225050,陈奔,1,34,10000154 */ public static class HDFSToHbaseReducer extends TableReducer&lt;Text, NullWritable, NullWritable&gt; &#123; @Override protected void reduce(Text key, Iterable&lt;NullWritable&gt; values, Context context) throws IOException, InterruptedException &#123; String[] split = key.toString().split(","); Put put = new Put(split[0].getBytes()); put.addColumn("info".getBytes(), "name".getBytes(), split[1].getBytes()); put.addColumn("info".getBytes(), "sex".getBytes(), split[2].getBytes()); put.addColumn("info".getBytes(), "age".getBytes(), split[3].getBytes()); put.addColumn("info".getBytes(), "department".getBytes(), split[4].getBytes()); context.write(NullWritable.get(), put); &#125; &#125;&#125; 写入结果： 123456789101112131415161718Hbase(main):018:0&gt; scan 'users'ROW COLUMN+CELL 223720 column=info:age, timestamp=1547044148652, value=26 223720 column=info:department, timestamp=1547044148652, value=10000154 223720 column=info:name, timestamp=1547044148652, value=\xE8\xB5\x9B\xE6\x9E\x97\xE6\x9E\x97 223720 column=info:sex, timestamp=1547044148652, value=1 223728 column=info:age, timestamp=1547044148652, value=22 223728 column=info:department, timestamp=1547044148652, value=10054005 223728 column=info:name, timestamp=1547044148652, value=\xE5\x90\xB4\xE8\xBF\xAA ... ... ... 225710 column=info:sex, timestamp=1547044148652, value=1 225715 column=info:age, timestamp=1547044148652, value=26 225715 column=info:department, timestamp=1547044148652, value=10054005 225715 column=info:name, timestamp=1547044148652, value=\xE5\x88\x98\xE9\x80\x9A 225715 column=info:sex, timestamp=1547044148652, value=1 20 row(s) in 1.2050 seconds scan的结果可以看到中文的姓名在Hbase中其实是转为ascii编码的16进制形式存储的。如果你想看这个值是什么，你可以使用Hbase的客户端，比如Hue，另外shell中支持直接查看，使用echo命令即可： 1echo -e '\xE5\x88\x98\xE9\x80\x9A' 2.读取Hbase数据存HDFS 将Hbase中的数据读取出来做运算，然后存入hdfs。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.conf.Configured;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.Hbase.Cell;import org.apache.hadoop.Hbase.CellUtil;import org.apache.hadoop.Hbase.HbaseConfiguration;import org.apache.hadoop.Hbase.client.Result;import org.apache.hadoop.Hbase.client.Scan;import org.apache.hadoop.Hbase.io.ImmutableBytesWritable;import org.apache.hadoop.Hbase.mapreduce.TableMapReduceUtil;import org.apache.hadoop.Hbase.mapreduce.TableMapper;import org.apache.hadoop.Hbase.util.Bytes;import org.apache.hadoop.io.DoubleWritable;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import org.apache.hadoop.util.Tool;import org.apache.hadoop.util.ToolRunner;import java.io.IOException;import java.util.List;/** * 从Hbase中取出数据，计算平均年龄存入hdfs */public class ReadHbaseDataToHDFS extends Configured implements Tool &#123; public static void main(String[] args) throws Exception &#123; int run = ToolRunner.run(new ReadHbaseDataToHDFS(), args); System.exit(run); &#125; @Override public int run(String[] arg0) throws Exception &#123; Configuration conf = HbaseConfiguration.create(); conf.set("fs.defaultFS", "hdfs://hadoopmaster:9000/"); conf.set("Hbase.zookeeper.quorum", "hadoopmaster:2181,hadoopslaver1:2181,hadoopslaver2:2181"); System.setProperty("HADOOP_USER_NAME", "hadoop"); FileSystem fs = FileSystem.get(conf); Job job = Job.getInstance(conf); job.setJarByClass(ReadHbaseDataToHDFS.class); // 取对业务有用的数据 info,age Scan scan = new Scan(); scan.addColumn("info".getBytes(), "age".getBytes()); TableMapReduceUtil.initTableMapperJob( "student".getBytes(), // 指定表名 scan, // 指定扫描数据的条件 HbaseToHDFSMapper.class, // 指定mapper class Text.class, // outputKeyClass mapper阶段的输出的key的类型 IntWritable.class, // outputValueClass mapper阶段的输出的value的类型 job, // job对象 false ); job.setReducerClass(HbaseToHDFSReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(DoubleWritable.class); Path outputPath = new Path("/data/"); if (fs.exists(outputPath)) &#123; fs.delete(outputPath, true); &#125; FileOutputFormat.setOutputPath(job, outputPath); boolean isDone = job.waitForCompletion(true); return isDone ? 0 : 1; &#125; public static class HbaseToHDFSMapper extends TableMapper&lt;Text, IntWritable&gt; &#123; Text outKey = new Text("age"); IntWritable outValue = new IntWritable(); // key是Hbase中的行键 // value是Hbase中的所行键的所有数据 @Override protected void map(ImmutableBytesWritable key, Result value, Context context) throws IOException, InterruptedException &#123; boolean isContainsColumn = value.containsColumn("info".getBytes(), "age".getBytes()); if (isContainsColumn) &#123; List&lt;Cell&gt; listCells = value.getColumnCells("info".getBytes(), "age".getBytes()); System.out.println("listCells:\t" + listCells); Cell cell = listCells.get(0); System.out.println("cells:\t" + cell); byte[] cloneValue = CellUtil.cloneValue(cell); String ageValue = Bytes.toString(cloneValue); outValue.set(Integer.parseInt(ageValue)); context.write(outKey, outValue); &#125; &#125; &#125; public static class HbaseToHDFSReducer extends Reducer&lt;Text, IntWritable, Text, DoubleWritable&gt; &#123; DoubleWritable outValue = new DoubleWritable(); @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123; int count = 0; int sum = 0; for (IntWritable value : values) &#123; count++; sum += value.get(); &#125; double avgAge = sum * 1.0 / count; outValue.set(avgAge); context.write(key, outValue); &#125; &#125;&#125; 运行程序可以去hdfs中查看结果 ： 123456789[hadoop@hadoopmaster hadoop]$ hdfs dfs -ls /data19/01/08 23:56:41 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableFound 2 items-rw-r--r-- 3 hadoop supergroup 0 2019-01-08 23:55 /data/_SUCCESS-rw-r--r-- 3 hadoop supergroup 0 2019-01-08 23:55 /data/part-r-00000[hadoop@hadoopmaster hadoop]$ hdfs dfs -cat /data/part-r-0000019/01/09 00:01:06 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableage 25.15]]></content>
      <categories>
        <category>大数据学习</category>
        <category>Hbase</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hbase基本API操作]]></title>
    <url>%2Fposts%2F106a4f35.html</url>
    <content type="text"><![CDATA[前面讲过Hbase的shell操作，那么在开发过程中我们如何用代码去操作Hbase呢，下面说一下关于Java操作Hbase的使用方式，以下代码使用IDEA搭建环境和运行。 工程目录结构如下，主要是引入core-site.xml和Hbase-site.xml文件。 core-site.xml如下： 12345678&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoopmaster:9000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; Hbase-site.xml如下，将你自己服务器中Hbase配置文件拷贝过来就行： 12345678910111213141516171819202122232425262728293031323334353637383940&lt;?xml version="1.0"?&gt;&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;Hbase.master&lt;/name&gt; &lt;value&gt;hadoopmaster:60000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;Hbase.master.maxclockskew&lt;/name&gt; &lt;value&gt;180000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;Hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://hadoopmaster:9000/Hbase&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;Hbase.tmp.dir&lt;/name&gt; &lt;value&gt;/usr/local/Hbase/tmp/Hbase&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;Hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;Hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;hadoopmaster,hadoopslaver1,hadoopslaver2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;Hbase.zookeeper.property.dataDir&lt;/name&gt; &lt;value&gt;/usr/local/zookeeper&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 然后我们就可以编写测试类：HbaseApiTest 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111package cn.edu.hust.demo1.Hbase;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.Hbase.*;import org.apache.hadoop.Hbase.client.*;import org.apache.hadoop.Hbase.filter.*;import org.apache.hadoop.Hbase.util.Bytes;import org.junit.Before;import org.junit.Test;import java.io.IOException;import java.util.ArrayList;import java.util.List;/** * @Author yangyue * @Date Created in 下午7:57 2018/10/15 * @Modified by: * @Description: **/public class HbaseApiTest &#123; static Configuration conf = null; static HbaseAdmin admin = null; static Connection connection = null; static Table table = null; @Before public void init() throws IOException &#123; //创建conf对象 conf = HbaseConfiguration.create(); //通过连接工厂创建连接对象 connection = ConnectionFactory.createConnection(conf); //通过连接查询tableName对象 TableName tname = TableName.valueOf("user_info"); //获得table table = connection.getTable(tname); &#125; @Test public void put() throws Exception &#123; //通过bytes工具类创建字节数组(将字符串) byte[] rowid = Bytes.toBytes("1"); //创建put对象 Put put = new Put(rowid); byte[] f1 = Bytes.toBytes("base_info"); byte[] id = Bytes.toBytes("sex") ; byte[] value = Bytes.toBytes("0"); put.addColumn(f1,id,value); //执行插入 table.put(put); &#125; @Test public void get() throws Exception &#123; //通过bytes工具类创建字节数组(将字符串) Get get = new Get(Bytes.toBytes("1")); Result r = table.get(get); byte[] idvalue = r.getValue(Bytes.toBytes("base_info"),Bytes.toBytes("name")); System.out.println(Bytes.toString(idvalue)); &#125; @Test public void testDeleteTable() throws IOException &#123; admin.disableTable("base_info"); admin.deleteTable("base_info"); &#125; @Test public void testInsertSingleRecord() throws IOException &#123; Put rk1 = new Put(Bytes.toBytes("2")); rk1.addColumn(Bytes.toBytes("base_info"), Bytes.toBytes("name"), Bytes.toBytes("NikoBelic")); table.put(rk1); &#125; @Test public void testInsertMultipleRecords() throws IOException &#123; List&lt;Put&gt; rowKeyList = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; 10; i++) &#123; Put rk = new Put(Bytes.toBytes(i+"")); rk.addColumn(Bytes.toBytes("base_info"), Bytes.toBytes("name"), Bytes.toBytes("Name" + i)); rk.addColumn(Bytes.toBytes("base_info"), Bytes.toBytes("sex"), Bytes.toBytes("sex" + i)); rowKeyList.add(rk); &#125; table.put(rowKeyList); &#125; @Test public void testDeleteRecord() throws IOException &#123; Delete delete = new Delete(Bytes.toBytes(2)); table.delete(delete); &#125; @Test public void testGetSingleRecord() throws IOException &#123; Get get = new Get(Bytes.toBytes(4)); Result result = table.get(get); printRow(result); &#125;&#125; 以上代码讲过运行没有问题，可以在Hbase shell 中查看结果,当然，运行的前提是你有一张user_info表： 123456789101112131415161718192021222324252627282930313233343536Hbase(main):005:0&gt; create 'user_info','base_info','extra_info'0 row(s) in 3.3010 secondsHbase(main):021:0&gt; get 'user_info',1COLUMN CELL base_info:name timestamp=1546873586767, value=xiaoming base_info:sex timestamp=1546876032203, value=01 row(s) in 0.0730 secondsHbase(main):022:0&gt; get 'user_info',2COLUMN CELL base_info:name timestamp=1546876130497, value=NikoBelic base_info:sex timestamp=1546875961805, value=01 row(s) in 0.0530 secondsHbase(main):033:0&gt; scan 'user_info'ROW COLUMN+CELL 0 column=base_info:name, timestamp=1546876537680, value=Name0 0 column=base_info:sex, timestamp=1546876537680, value=sex0 1 column=base_info:name, timestamp=1546876537680, value=Name1 1 column=base_info:sex, timestamp=1546876537680, value=sex1 2 column=base_info:name, timestamp=1546876537680, value=Name2 2 column=base_info:sex, timestamp=1546876537680, value=sex2 3 column=base_info:name, timestamp=1546876537680, value=Name3 3 column=base_info:sex, timestamp=1546876537680, value=sex3 4 column=base_info:name, timestamp=1546876537680, value=Name4 4 column=base_info:sex, timestamp=1546876537680, value=sex4 5 column=base_info:name, timestamp=1546876537680, value=Name5 5 column=base_info:sex, timestamp=1546876537680, value=sex5 6 column=base_info:name, timestamp=1546876537680, value=Name6 6 column=base_info:sex, timestamp=1546876537680, value=sex6 7 column=base_info:name, timestamp=1546876537680, value=Name7 7 column=base_info:sex, timestamp=1546876537680, value=sex7 8 column=base_info:name, timestamp=1546876537680, value=Name8 8 column=base_info:sex, timestamp=1546876537680, value=sex8 9 column=base_info:name, timestamp=1546876537680, value=Name9 9 column=base_info:sex, timestamp=1546876537680, value=sex910 row(s) in 0.2310 seconds]]></content>
      <categories>
        <category>大数据学习</category>
        <category>Hbase</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hbase初识]]></title>
    <url>%2Fposts%2Fe64b3d3a.html</url>
    <content type="text"><![CDATA[Hbase基本知识学习 数据在 Hbase 中的排布（逻辑上） Row-Key Value（CF、Qualifier、Version 1 info{'姓': '张'，'名':'三'} pwd{'密码': '111'} 2 Info{'姓': '李'} 我们可以看出，在Hbase中首先会有 Column Family 的概念，简称为CF。CF 一般用于将相关的列（Column）组合起来。在物理上 Hbase其实是按 CF 存储的，只是按照 Row-key 将相关 CF 中的列关联起来。 数据在Hbase中的物理分布如下： Row-Key CF:Column-Key 时间戳 Cell Value 1 info:fn 123456789 三 1 Info:fn 123456789 张 2 Info:fn 123456789 四 2 Info:fn 123456789 李 如果要用很短的一句话总结 Hbase，我们可以认为 Hbase 就是一个有序的多维 Map，其中每一个 Row-key 映射了许多数据，这些数据存储在 CF 中的 Column。我们可以用下图来表示这句话。 Hbase是一个面向列的数据库，在表中它由行排序。 表模式定义只能列族，也就是键值对。 一个表有多个列族以及每一个列族可以有任意数量的列。 后续列的值连续地存储在磁盘上。 表中的每个单元格值都具有时间戳。在一个Hbase中： 表是行的集合。 行是列族的集合。 列族是列的集合。 列是键值对的集合。 Row key row key 是用来检索记录的主键； row key 可以使用任意字符串； 存储时，数据按照row key 的字典顺序存储； 设计key时，要充分考虑排序存储这个特性，将经常一起读取的行存储在一起。 Column Family Hbase中的每个列都归属于某个列族； 列族是schema的一部分(列并不是),必须在使用表之前定义； 列表都以列族为前缀，例如：course:math,course:English都属于course这个列族。 Column 属于某一个columnfamily。通过列族:单元格修饰符访问，可以具体到某个列。可以把单元格修饰符认为是实际的列名。 只要有列族存在，客户端随时可以把列添加到列族。 Cell 由 rowKey + columnFamily + version 唯一确定的单元； 字节码形式存储。cell中的数据是没有类型的，全部是字节码(Byte Array)形式存储。 TimeStamp 每个cell都保存着一份数据的多个版本； 版本通过时间戳来索引，时间戳的类型是64类整型； 时间戳可以由Hbase(在数据写入时自动)赋值,此时时间戳是精确到毫秒的当前系统时间。时间戳也可以由客户显式赋值。如果应用程序要避免数据版本冲突,就必须自己生成具有唯一性的时间戳。每个cell中,不同版本的数据按照时间倒序排序,即最新的数据排在最前面； 为了避免数据存在过多版本造成的的管理(包括存贮和索引)负担,Hbase提供了两种数据版本回收方式。一是保存数据的最后n个版本,二是保存最近一段时间内的版本（比如最近七天）。用户可以针对每个列族进行设置。 Region Hbase自动把表水平划分成多个区域(region)，每个region会保存一个表里面某段连续的数据。 每个表一开始只有一个region，随着数据不断插入表，region不断增大，当增大到一个阀值的时候，region就会等分会两个新的region。 当table中的行不断增多，就会有越来越多的region。这样一张完整的表被保存在多个Region 上。 Region是Hbase中“分布式存储”和“负载均衡”的最小单元。最小单元就表示不同的region可以分布在不同的Region server上。但一个Region是不会拆分到多个server上的。 Hbase存储和HDFS存储的关系: HDFS: Client -&gt; NameNode -&gt; DataNode Hbase: Client -&gt; HMaster -&gt; HRegionServer -&gt; Zookeeper(元数据) -&gt; HDFS(数据文件) 每条数据线缓存到HRegionServer的内存中,当达到Block大小时再写入HDFS。 Hbase中只有表结构和列族,其他都是数据。 1个列族中有多个列,被存储到1个文件中。 Hbase中不能删除数据,如果对一个相同的行键插入数据（不同时间戳）,就认为是修改。 habse命令： 名称 命令行表达式 创建表 create ‘表名’,’列族1’,’列族2’,’列族N’ 查看所有表 list 描述表 describe ‘表名’ 判断表存在 exists ‘表名’ 判断是否禁用启用表 is_enabled/is_disable ‘表名’ 添加记录 put ‘表名’,’rowKey’,’列族:列’,’值’ 查看记录rowkey下的所有数据 get ‘表名’,’rowkey’ 查看表中记录总数 count 表名 获取某个列族 get ‘表名’,’rowkey’,’列族’ 获取某个列族的某个列 get ‘表名’,’rowkey’,’列族’,’列’ 删除记录 delete ‘表名’,’行名’,’列族:列’ 删除整行 deleteall ‘表名’,’rowkey’ 删除一张表 1. disable ‘表名’ 2. drop ‘表名’ 清空表 truncate ‘表名’ 查看所有记录 scan ‘表名’ 查看某个表某个列中的所有数据 scan ‘表名’,{COLUMNS=&gt; ‘列族名:列名’} 更新记录 就是重写一遍，进行覆盖，Hbase没有修改，都是追加 ZK在Hbase中的功能： 保存HMaster的地址和 backup-master 地址 HMaster的功能： 管理HRegionServer 做CRUD的节点 管理HRegionServer中的表分配 保存 表-ROOT- 的地址 （Hbase默认的根表，检索表） HRegionServer列表 表的CRUD数据 和HDFS交互，读写数据 Hbase体系结构 数据模型 Hbase以表的形式存储数据。表由行和列组成。行划分为若干个列族。 最基本的单位是列（column），一列或者多列组成一行（row），并且由唯一的行键（row key）来确定存储。一个表中有很多行，每一列可能有多个版本，在每一个单元格（Cell）中存储了不同的值。 Hbase的行与行之间是有序的，按照row key的字典序进行排序，行键是唯一的，在一个表里只出现一次，否则就是在更新同一行，行键可以是任意的字节数组。一行由若干列组成，其中的某些列又可以构成一个列族（column family），一个列族的所有列存储在同一个底层的存储文件里，这个文件称之为HFile。 列族需要在创建表的时候就定义好，数量也不宜过多。列族名必须由可打印字符组成，创建表的时候不需要定义好列。对列的引用格式通常为family：qualifier，qualifier也可以是任意的字节数组。同一个列族里qualifier的名称应该唯一，否则就是在更新同一列，列的数量没有限制，可以有数百万个。列值也没有类型和长度限定。Hbase会对row key的长度做检查，默认应该小于65536。 Hbase的存取模式如下（表，行键，列族，列，时间戳）-&gt; 值。即一个表中的某一行键的某一列族的某一列的某一个版本的值唯一。 行数据的存取操作是原子的，可以读取任意数目的列。目前还不支持跨行事务和跨表事务。 同一列族下的数据压缩在一起，访问控制磁盘和内存都在列族层面进行。 架构 Hbase包含3个重要组件：Zookeeper、HMaster和HRegionServer。 Zookeeper Zookeeper为整个Hbase提供协助服务，包括： 存放整个Hbase集群的元数据以及集群的状态信息。 实现HMaster主从节点的failover。 ZooKeeper为Hbase集群提供协调服务，它管理着HMaster和HRegionServer的状态(available/alive等)，并且会在它们宕机时通知给HMaster，从而HMaster可以实现HMaster之间的failover，或对宕机的HRegionServer中的HRegion集合的修复(将它们分配给其他的HRegionServer)。ZooKeeper集群本身使用一致性协议(PAXOS协议)保证每个节点状态的一致性。 HMaster HMaster主要用于监控和操作集群中的所有HRegionServer。HMaster没有单点问题，Hbase中可以启动多个HMaster，通过Zookeeper的MasterElection机制保证总有一个Master在运行。 管理所有的HRegionServer，告诉其需要维护哪些HRegion，并监控所有HRegionServer的运行状态。当一个新的HRegionServer登录到HMaster时，HMaster会告诉它等待分配数据；而当某个HRegion死机时，HMaster会把它负责的所有HRegion标记为未分配，然后再把它们分配到其他HRegionServer中。HMaster没有单点问题，Hbase可以启动多个HMaster，通过Zookeeper的选举机制保证集群中总有一个HMaster运行，从而提高了集群的可用性。 HMaster主要负责Table和Region的管理工作： 管理用户对表的增删改查操作。 管理HRegionServer的负载均衡，调整Region分布。 Region Split后，负责新Region的分布。 在HRegionServer停机后，负责失效HRegionServer上Region迁移。 HRegionServer HRegionServer是Hbase中最核心的模块，主要负责响应用户I/O请求，向HDFS文件系统读写数据。Hbase中的所有数据从底层来说一般都是保存在HDFS中的，用户通过一系列HRegionServer获取这些数据。集群一个节点上一般只运行一个HRegionServer，且每一个区段的HRegion只会被一个HRegionServer维护。HRegionServer主要负责响应用户I/O请求，向HDFS文件系统读写数据，是Hbase中最核心的模块。HRegionServer内部管理了一系列HRegion对象，每个HRegion对应了逻辑表中的一个连续数据段。HRegion由多个HStore组成，每个HStore对应了逻辑表中的一个列族的存储，可以看出每个列族其实就是一个集中的存储单元。因此，为了提高操作效率，最好将具备共同I/O特性的列放在一个列族中。 存放和管理本地HRegion。 读写HDFS，管理Table中的数据。 Client直接通过HRegionServer读写数据（从HMaster中获取元数据，找到RowKey所在的HRegion/HRegionServer后）。 另外，Hbase架构中还包括以下几个部分： HRegion 当表的大小超过预设值的时候，Hbase会自动将表划分为不同的区域，每个区域包含表中所有行的一个子集。对用户来说，每个表是一堆数据的集合，靠主键（RowKey）来区分。从物理上来说，一张表被拆分成了多块，每一块就是一个HRegion。我们用表名+开始/结束主键，来区分每一个HRegion，一个HRegion会保存一个表中某段连续的数据，一张完整的表数据是保存在多个HRegion中的。 HStore 它是Hbase存储的核心，由MemStore和StoreFiles两部分组成。MemStore是内存缓冲区，用户写入的数据首先会放入MemStore，当MemStore满了以后会Flush成一个StoreFile（底层实现是HFile），当StoreFile的文件数量增长到一定阈值后，会触发Compact合并操作，将多个StoreFiles合并成一个StoreFile，合并过程中会进行版本合并和数据删除操作。因此，可以看出Hbase其实只有增加数据，所有的更新和删除操作都是在后续的Compact过程中进行的，这样使得用户的写操作只要进入内存就可以立即返回，保证了HbaseI/O的高性能。当StoreFiles Compact后，会逐步形成越来越大的StoreFile，当单个StoreFile大小超过一定阈值后，会触发Split操作，同时把当前的HRegion Split成2个HRegion，父HRegion会下线，新分出的2个子HRegion会被HMaster分配到相应的HRegionServer，使得原先1个HRegion的负载压力分流到2个HRegion上。 HLog 每个HRegionServer中都有一个HLog对象，它是一个实现了Write Ahead Log的预写日志类。在每次用户操作将数据写入MemStore的时候，也会写一份数据到HLog文件中，HLog文件会定期滚动刷新，并删除旧的文件（已持久化到StoreFile中的数据）。当HMaster通过Zookeeper感知到某个HRegionServer意外终止时，HMaster首先会处理遗留的 HLog文件，将其中不同HRegion的HLog数据进行拆分，分别放到相应HRegion的目录下，然后再将失效的HRegion重新分配，领取到这些HRegion的HRegionServer在加载 HRegion的过程中，会发现有历史HLog需要处理，因此会Replay HLog中的数据到MemStore中，然后Flush到StoreFiles，完成数据恢复。 写数据流程： Client向HRegionServer发送写请求 HRegionServer将数据写到Hlog(write ahead log)。为了数据的持久化和恢复 HRegionServer将数据写到内存(memstore) 反馈Client写成功 数据Flush过程 当memstore数据达到阀值（默认64M），将数据刷到硬盘，将内存中的数据删除，同时删除HLog中的历史数据。 将数据存储到HDFS中 在Hlog中做标记点 数据合并过程 当数据块达到4块，HMaster将数据加载到本地，进行合并 当合并的数据超过256M，进行拆分，将拆分后的region分配给不同的HRegionServer管理 当HRegionServer宕机后，将HRegionServer上的Hlog拆分，然后分配给不同的HRegionServer加载，修改 .META. 注意：Hlog会同步到HDFS Hbase的读流程 通过ZK和 -ROOT- .META. 表定位HRegionServer 数据从内存和硬盘合并后返回给Client 数据块会缓存 HMaster的职责 管理用户对Table的CRUD操作 记录Region在哪台HRegionServer上 在Region Split后，负责新Region的分配 新机器加入时，管理HRegionServer的负载均衡，调整Region分布 在HRegionServer宕机后，负责失效HRegionServer上的Regions迁移 HRegionServer的职责 HRegionServer主要负责响应用户的IO请求，向HDFS文件系统中读写数据，是Hbase中最核心的模块 HRegionServer管理了很多table的分区，也就是Region Client的职责 Hbase Client 使用Hbase的RPC机制与HMaster和HRegionServer进行通信 1. 管理类操作： Client与HMaster进行RPC 2. 数据读写类操作：Client与HRegionServer进行RPC Hbase为什么能实现快速的查询？ 如果快速查询（从磁盘读数据），Hbase是根据rowkey查询的，只要能快速的定位rowkey，就能实现快速的查询，主要是以下因素： Hbase是可划分成多个region，可以简单的理解为关系型数据库的多个分区。 键是排好序了的。 按列存储的。 首先，能快速找到行所在的region(分区)，假设表有10亿条记录，占空间1TB，分裂成了500个region，1个region占2个G。最多读取2G的记录，就能找到对应记录； 其次，是按列存储的，其实是列族，假设分为3个列族，每个列族就是666M，如果要查询的东西在其中1个列族上，1个列族包含1个或者多个HStoreFile，假设一个HStoreFile是128M，该列族包含5个HStoreFile在磁盘上. 剩下的在内存中。 再次，是排好序了的，要的记录有可能在最前面，也有可能在最后面，假设在中间，我们只需遍历2.5个HStoreFile共300M。 最后，每个HStoreFile(HFile的封装)，是以键值对（key-value）方式存储，只要遍历一个个数据块中的key的位置，并判断符合条件可以了。一般key是有限的长度，假设跟value是1:19（忽略HFile上其它块），最终只需要15M就可获取的对应的记录，按照磁盘的访问100M/S，只需0.15秒。加上块缓存机制（LRU原则），会取得更高的效率。 Hbase的实时查询 实时查询，可以认为是从内存中查询，一般响应时间在1秒内。Hbase的机制是数据先写入到内存中，当数据量达到一定的量（如128M），再写入磁盘中，在内存中，是不进行数据的更新或合并操作的，只增加数据，这使得用户的写操作只要进入内存中就可以立即返回，保证了Hbase I/O的高性能。 实时查询即反映当前时间的数据，可以认为这些数据始终是在内存的，保证了数据的实时响应。]]></content>
      <categories>
        <category>大数据学习</category>
        <category>Hbase</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hbase基本shell命令]]></title>
    <url>%2Fposts%2Febc35787.html</url>
    <content type="text"><![CDATA[启动Hbase，进入shell，退出shell： start-Hbase.sh Hbase shell quit 进入shell之后遇到命令不知道怎么使用的话，使用’help’命令： help的使用方式： help '命令' #命令需要加引号 需要注意的事： 不知道为什么Hbase的shell界面输入字符之后如果想回退按backspace是不行的，我使用的shell客户端是Xshell，同时按住：ctrl+space+backspace可以。 1.创建表： create '表名', '列族名1','列族名2','列族名N' 2.查看表： 3.判断表是否启用或禁用： 4.添加表数据： put '表名','rowKey','列族:列','值' 注意到表名后面的rowkey，其实就相当于是行号的概念。 5.查看rowkey下的所有数据： get '表名','rowKey' 6.查看表中的记录总数： count '表名' 7.获取某行数据： get '表名','rowkey' 8.获取某个列族和获取列族的某个列： get '表名','rowkey','列族' get '表名','rowkey','列族：列' 9.删除记录(某一行中的某一个值)： delete '表名','rowkey','列族：列' 10.删除整行： deleteall '表名','rowkey' 11.删除表： 先要屏蔽该表，才能对该表进行删除 第一步 disable ‘表名’ ，第二步 drop '表名' 12.清空表： truncate '表名' 13.查看表中某个列的所有数据： scan &quot;表名&quot;,{COLUMNS=&gt;'列族名:列名'}]]></content>
      <categories>
        <category>大数据学习</category>
        <category>Hbase</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据倾斜的解决方案]]></title>
    <url>%2Fposts%2Fd4452255.html</url>
    <content type="text"><![CDATA[1.数据倾斜是怎么出现的 在执行shuffle操作的时候，是按照key，来进行values的数据的输出、拉取和聚合的。同一个key的values，一定是分配到一个reduce task进行处理的。多个key对应的values，比如一共是100万。可能某个key对应了98万数据，被分配到一个task上去面去执行，另外两个task，可能各分配到了1万数据，可能是数百个key，对应的1万条数据，这样就会出现数据倾斜问题。 这两个task，各分配到了1万数据，可能同时在10分钟内都运行完了。另外一个task有98万条，98 * 10 = 980分钟 = 16.3个小时。 大家看，本来另外两个task很快就运行完毕了（10分钟），但是由于一个拖后腿的家伙，第三个task，要16.3个小时才能运行完，就导致整个spark作业，也得16.3个小时才能运行完。所以一旦出现数据倾斜后果是很可怕的。解决数据倾斜问题类似与性能调优哈。 2.数据倾斜发生的表象 绝大多数task执行得都非常快，但个别task执行极慢(你要用client模式，standalone client，yarn client，本地机器一执行spark-submit脚本，就会开始打印log）。比如，总共有1000个task，997个task都在1分钟之内执行完了，但是剩余两三个task却要一两个小时。这种情况很常见。 原本能够正常执行的Spark作业，某天突然报出OOM（内存溢出）异常，观察异常栈，是我们写的业务代码造成的。这种情况比较少见。 3.定位数据倾斜出现的原因与出现问题的位置 出现数据倾斜的原因，基本只可能是因为发生了shuffle操作，在shuffle的过程中，出现了数据倾斜的问题。因为某个或者某些key对应的数据，远远的高于其他的key。 数据倾斜只会发生在shuffle过程中。这里给大家罗列一些常用的并且可能会触发shuffle操作的算子：distinct、groupByKey、reduceByKey、aggregateByKey、join、cogroup、repartition等。出现数据倾斜时，可能就是你的代码中使用了这些算子中的某一个所导致的； 看log； log一般会报是在你的哪一行代码，导致了OOM异常。或者看log，看看是执行到了第几个stage。spark代码，是怎么划分成一个一个的stage的。哪一个stage生成的task特别慢，就能够自己用肉眼去对你的spark代码进行stage的划分，就能够通过stage定位到你的代码，到底哪里发生了数据倾斜。 如果是用yarn-client模式提交，那么本地是直接可以看到log的，可以在log中找到当前运行到了第几个stage；如果是用yarn-cluster模式提交，则可以通过Spark Web UI来查看当前运行到了第几个stage。此外，无论是使用yarn-client模式还是yarn-cluster模式，我们都可以在Spark Web UI上深入看一下当前这个stage各个task分配的数据量，从而进一步确定是不是task分配的数据不均匀导致了数据倾斜。 知道数据倾斜发生在哪一个stage之后，接着我们就需要根据stage划分原理，推算出来发生倾斜的那个stage对应代码中的哪一部分，这部分代码中肯定会有一个shuffle类算子。精准推算stage与代码的对应关系，需要对Spark的源码有深入的理解，这里我们可以介绍一个相对简单实用的推算方法：只要看到Spark代码中出现了一个shuffle类算子或者是Spark SQL的SQL语句中出现了会导致shuffle的语句（比如group by语句），那么就可以判定，以那个地方为界限划分出了前后两个stage。 这里我们就以Spark最基础的入门程序——单词计数来举例，如何用最简单的方法大致推算出一个stage对应的代码。如下示例，在整个代码中，只有一个reduceByKey是会发生shuffle的算子，因此就可以认为，以这个算子为界限，会划分出前后两个stage。 stage0，主要是执行从textFile到map操作，以及执行shuffle write操作。shuffle write操作，我们可以简单理解为对pairs RDD中的数据进行分区操作，每个task处理的数据中，相同的key会写入同一个磁盘文件内。 stage1，主要是执行从reduceByKey到collect操作，stage1的各个task一开始运行，就会首先执行shuffle read操作。执行shuffle read操作的task，会从stage0的各个task所在节点拉取属于自己处理的那些key，然后对同一个key进行全局性的聚合或join等操作，在这里就是对key的value值进行累加。stage1在执行完reduceByKey算子之后，就计算出了最终的wordCounts RDD，然后会执行collect算子，将所有数据拉取到Driver上，供我们遍历和打印输出。 123456789val conf = new SparkConf()val sc = new SparkContext(conf)val lines = sc.textFile("hdfs://...")val words = lines.flatMap(_.split(" "))val pairs = words.map((_, 1))val wordCounts = pairs.reduceByKey(_ + _)wordCounts.collect().foreach(println(_)) 通过对单词计数程序的分析，希望能够让大家了解最基本的stage划分的原理，以及stage划分后shuffle操作是如何在两个stage的边界处执行的。然后我们就知道如何快速定位出发生数据倾斜的stage对应代码的哪一个部分了。比如我们在Spark Web UI或者本地log中发现，stage1的某几个task执行得特别慢，判定stage1出现了数据倾斜，那么就可以回到代码中定位出stage1主要包括了reduceByKey这个shuffle类算子，此时基本就可以确定是由educeByKey算子导致的数据倾斜问题。比如某个单词出现了100万次，其他单词才出现10次，那么stage1的某个task就要处理100万数据，整个stage的速度就会被这个task拖慢。 4.查看导致数据倾斜的key的数据分布情况 知道了数据倾斜发生在哪里之后，通常需要分析一下那个执行了shuffle操作并且导致了数据倾斜的RDD/Hive表，查看一下其中key的分布情况。这主要是为之后选择哪一种技术方案提供依据。针对不同的key分布与不同的shuffle算子组合起来的各种情况，可能需要选择不同的技术方案来解决。 此时根据你执行操作的情况不同，可以有很多种查看key分布的方式： 如果是Spark SQL中的group by、join语句导致的数据倾斜，那么就查询一下SQL中使用的表的key分布情况。 如果是对Spark RDD执行shuffle算子导致的数据倾斜，那么可以在Spark作业中加入查看key分布的代码，比如RDD.countByKey()。然后对统计出来的各个key出现的次数，collect/take到客户端打印一下，就可以看到key的分布情况。 举例来说，对于上面所说的单词计数程序，如果确定了是stage1的reduceByKey算子导致了数据倾斜，那么就应该看看进行reduceByKey操作的RDD中的key分布情况，在这个例子中指的就是pairs RDD。如下示例，我们可以先对pairs采样10%的样本数据，然后使用countByKey算子统计出每个key出现的次数，最后在客户端遍历和打印样本数据中各个key的出现次数。 123val sampledPairs = pairs.sample(false, 0.1)val sampledWordCounts = sampledPairs.countByKey()sampledWordCounts.foreach(println(_)) 5.数据倾斜的解决方案 5.1 方案一：使用Hive ETL预处理数据 方案适用场景：导致数据倾斜的是Hive表。如果该Hive表中的数据本身很不均匀（比如某个key对应了100万数据，其他key才对应了10条数据），而且业务场景需要频繁使用Spark对Hive表执行某个分析操作，那么比较适合使用这种技术方案。 方案实现思路：此时可以评估一下，是否可以通过Hive来进行数据预处理（即通过Hive ETL预先对数据按照key进行聚合，或者是预先和其他表进行join），然后在Spark作业中针对的数据源就不是原来的Hive表了，而是预处理后的Hive表。此时由于数据已经预先进行过聚合或join操作了，那么在Spark作业中也就不需要使用原先的shuffle类算子执行这类操作了。 方案实现原理：这种方案从根源上解决了数据倾斜，因为彻底避免了在Spark中执行shuffle类算子，那么肯定就不会有数据倾斜的问题了。但是这里也要提醒一下大家，这种方式属于治标不治本。因为毕竟数据本身就存在分布不均匀的问题，所以Hive ETL中进行group by或者join等shuffle操作时，还是会出现数据倾斜，导致Hive ETL的速度很慢。我们只是把数据倾斜的发生提前到了Hive ETL中，避免Spark程序发生数据倾斜而已。 方案优点：实现起来简单便捷，效果还非常好，完全规避掉了数据倾斜，Spark作业的性能会大幅度提升。 方案缺点：治标不治本，Hive ETL中还是会发生数据倾斜。 方案实践经验：在一些Java系统与Spark结合使用的项目中，会出现Java代码频繁调用Spark作业的场景，而且对Spark作业的执行性能要求很高，就比较适合使用这种方案。将数据倾斜提前到上游的Hive ETL，每天仅执行一次，只有那一次是比较慢的，而之后每次Java调用Spark作业时，执行速度都会很快，能够提供更好的用户体验。 5.2 方案二：过滤少数导致倾斜的key 方案适用场景：如果发现导致倾斜的key就少数几个，而且对计算本身的影响并不大的话，那么很适合使用这种方案。比如99%的key就对应10条数据，但是只有一个key对应了100万数据，从而导致了数据倾斜。 方案实现思路：如果我们判断那少数几个数据量特别多的key，对作业的执行和计算结果不是特别重要的话，那么干脆就直接过滤掉那少数几个key。比如，在Spark SQL中可以使用where子句过滤掉这些key或者在Spark Core中对RDD执行filter算子过滤掉这些key。如果需要每次作业执行时，动态判定哪些key的数据量最多然后再进行过滤，那么可以使用sample算子对RDD进行采样，然后计算出每个key的数量，取数据量最多的key过滤掉即可。 方案实现原理：将导致数据倾斜的key给过滤掉之后，这些key就不会参与计算了，自然不可能产生数据倾斜。 方案优点：实现简单，而且效果也很好，可以完全规避掉数据倾斜。 方案缺点：适用场景不多，大多数情况下，导致倾斜的key还是很多的，并不是只有少数几个。 方案实践经验：在项目中我们也采用过这种方案解决数据倾斜。有一次发现某一天Spark作业在运行的时候突然OOM了，追查之后发现，是Hive表中的某一个key在那天数据异常，导致数据量暴增。因此就采取每次执行前先进行采样，计算出样本中数据量最大的几个key之后，直接在程序中将那些key给过滤掉。 5.3 方案三：提高shuffle操作的并行度 方案适用场景：如果我们必须要对数据倾斜迎难而上，那么建议优先使用这种方案，因为这是处理数据倾斜最简单的一种方案。 方案实现思路：在对RDD执行shuffle算子时，给shuffle算子传入一个参数，比如reduceByKey(1000)，该参数就设置了这个shuffle算子执行时shuffle read task的数量。对于Spark SQL中的shuffle类语句，比如group by、join等，需要设置一个参数，即spark.sql.shuffle.partitions，该参数代表了shuffle read task的并行度，该值默认是200，对于很多场景来说都有点过小。 方案实现原理：增加shuffle read task的数量，可以让原本分配给一个task的多个key分配给多个task，从而让每个task处理比原来更少的数据。举例来说，如果原本有5个key，每个key对应10条数据，这5个key都是分配给一个task的，那么这个task就要处理50条数据。而增加了shuffle read task以后，每个task就分配到一个key，即每个task就处理10条数据，那么自然每个task的执行时间都会变短了。 方案优点：实现起来比较简单，可以有效缓解和减轻数据倾斜的影响。 方案缺点：只是缓解了数据倾斜而已，没有彻底根除问题，根据实践经验来看，其效果有限。 方案实践经验：该方案通常无法彻底解决数据倾斜，因为如果出现一些极端情况，比如某个key对应的数据量有100万，那么无论你的task数量增加到多少，这个对应着100万数据的key肯定还是会分配到一个task中去处理，因此注定还是会发生数据倾斜的。所以这种方案只能说是在发现数据倾斜时尝试使用的第一种手段，尝试去用最简单的方法缓解数据倾斜而已，或者是和其他方案结合起来使用。 5.4 方案四：两阶段聚合（局部聚合+全局聚合） 方案适用场景：对RDD执行reduceByKey等聚合类shuffle算子或者在Spark SQL中使用group by语句进行分组聚合时，比较适用这种方案。 方案实现思路：这个方案的核心实现思路就是进行两阶段聚合。第一次是局部聚合，先给每个key都打上一个随机数，比如10以内的随机数，此时原先一样的key就变成不一样的了，比如(hello, 1) (hello, 1) (hello, 1) (hello, 1)，就会变成(1_hello, 1) (1_hello, 1) (2_hello, 1) (2_hello, 1)。接着对打上随机数后的数据，执行reduceByKey等聚合操作，进行局部聚合，那么局部聚合结果，就会变成了(1_hello, 2) (2_hello, 2)。然后将各个key的前缀给去掉，就会变成(hello,2)(hello,2)，再次进行全局聚合操作，就可以得到最终结果了，比如(hello, 4)。 方案实现原理：将原本相同的key通过附加随机前缀的方式，变成多个不同的key，就可以让原本被一个task处理的数据分散到多个task上去做局部聚合，进而解决单个task处理数据量过多的问题。接着去除掉随机前缀，再次进行全局聚合，就可以得到最终的结果。具体原理见下图。 方案优点：对于聚合类的shuffle操作导致的数据倾斜，效果是非常不错的。通常都可以解决掉数据倾斜，或者至少是大幅度缓解数据倾斜，将Spark作业的性能提升数倍以上。 方案缺点：仅仅适用于聚合类的shuffle操作，适用范围相对较窄。如果是join类的shuffle操作，还得用其他的解决方案。 1234567891011121314151617181920212223242526272829303132333435363738394041424344// 第一步，给RDD中的每个key都打上一个随机前缀。JavaPairRDD&lt;String, Long&gt; randomPrefixRdd = rdd.mapToPair(new PairFunction&lt;Tuple2&lt;Long,Long&gt;, String, Long&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Tuple2&lt;String, Long&gt; call(Tuple2&lt;Long, Long&gt; tuple) throws Exception &#123; Random random = new Random(); int prefix = random.nextInt(10); return new Tuple2&lt;String, Long&gt;(prefix + "_" + tuple._1, tuple._2); &#125;&#125;);// 第二步，对打上随机前缀的key进行局部聚合。JavaPairRDD&lt;String, Long&gt; localAggrRdd = randomPrefixRdd.reduceByKey(new Function2&lt;Long, Long, Long&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Long call(Long v1, Long v2) throws Exception &#123; return v1 + v2; &#125;&#125;);// 第三步，去除RDD中每个key的随机前缀。JavaPairRDD&lt;Long, Long&gt; removedRandomPrefixRdd = localAggrRdd.mapToPair(new PairFunction&lt;Tuple2&lt;String,Long&gt;, Long, Long&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Tuple2&lt;Long, Long&gt; call(Tuple2&lt;String, Long&gt; tuple) throws Exception &#123; long originalKey = Long.valueOf(tuple._1.split("_")[1]); return new Tuple2&lt;Long, Long&gt;(originalKey, tuple._2); &#125;&#125;);// 第四步，对去除了随机前缀的RDD进行全局聚合。JavaPairRDD&lt;Long, Long&gt; globalAggrRdd = removedRandomPrefixRdd.reduceByKey(new Function2&lt;Long, Long, Long&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Long call(Long v1, Long v2) throws Exception &#123; return v1 + v2; &#125;&#125;); 5.5 方案五：将reduce join转为map join 方案适用场景： 如果两个RDD要进行join，其中一个RDD是比较小的。比如一个RDD是100万数据，一个RDD是1万数据。（一个RDD是1亿数据，一个RDD是100万数据）。 其中一个RDD必须是比较小的，broadcast出去那个小RDD的数据以后，就会在每个executor的block manager中都保存一份。要确保你的内存足够存放那个小RDD中的数据。这种方式下，根本不会发生shuffle操作，肯定也不会发生数据倾斜。从根本上杜绝了join操作可能导致的数据倾斜的问题。 方案实现思路：不使用join算子进行连接操作，而使用Broadcast变量与map类算子实现join操作，进而完全规避掉shuffle类的操作，彻底避免数据倾斜的发生和出现。将较小RDD中的数据直接通过collect算子拉取到Driver端的内存中来，然后对其创建一个Broadcast变量；接着对另外一个RDD执行map类算子，在算子函数内，从Broadcast变量中获取较小RDD的全量数据，与当前RDD的每一条数据按照连接key进行比对，如果连接key相同的话，那么就将两个RDD的数据用你需要的方式连接起来。 方案实现原理：普通的join是会走shuffle过程的，而一旦shuffle，就相当于会将相同key的数据拉取到一个shuffle read task中再进行join，此时就是reduce join。但是如果一个RDD是比较小的，则可以采用广播小RDD全量数据+map算子来实现与join同样的效果，也就是map join，此时就不会发生shuffle操作，也就不会发生数据倾斜。具体原理如下图所示。 方案优点：对join操作导致的数据倾斜，效果非常好，因为根本就不会发生shuffle，也就根本不会发生数据倾斜。 方案缺点：适用场景较少，因为这个方案只适用于一个大表和一个小表的情况。毕竟我们需要将小表进行广播，此时会比较消耗内存资源，driver和每个Executor内存中都会驻留一份小RDD的全量数据。如果我们广播出去的RDD数据比较大，比如10G以上，那么就可能发生内存溢出了。因此并不适合两个都是大表的情况。 123456789101112131415161718192021222324252627282930313233// 首先将数据量比较小的RDD的数据，collect到Driver中来。 List&lt;Tuple2&lt;Long, Row&gt;&gt; rdd1Data = rdd1.collect()// 然后使用Spark的广播功能，将小RDD的数据转换成广播变量，这样每个Executor就只有一份RDD的数据。 // 可以尽可能节省内存空间，并且减少网络传输性能开销。 final Broadcast&lt;List&lt;Tuple2&lt;Long, Row&gt;&gt;&gt; rdd1DataBroadcast = sc.broadcast(rdd1Data);// 对另外一个RDD执行map类操作，而不再是join类操作。 JavaPairRDD&lt;String, Tuple2&lt;String, Row&gt;&gt; joinedRdd = rdd2.mapToPair(new PairFunction&lt;Tuple2&lt;Long,String&gt;, String, Tuple2&lt;String, Row&gt;&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Tuple2&lt;String, Tuple2&lt;String, Row&gt;&gt; call(Tuple2&lt;Long, String&gt; tuple) throws Exception &#123; // 在算子函数中，通过广播变量，获取到本地Executor中的rdd1数据。 List&lt;Tuple2&lt;Long, Row&gt;&gt; rdd1Data = rdd1DataBroadcast.value(); // 可以将rdd1的数据转换为一个Map，便于后面进行join操作。 Map&lt;Long, Row&gt; rdd1DataMap = new HashMap&lt;Long, Row&gt;(); for(Tuple2&lt;Long, Row&gt; data : rdd1Data) &#123; rdd1DataMap.put(data._1, data._2); &#125; // 获取当前RDD数据的key以及value。 String key = tuple._1; String value = tuple._2; // 从rdd1数据Map中，根据key获取到可以join到的数据。 Row rdd1Value = rdd1DataMap.get(key); return new Tuple2&lt;String, String&gt;(key, new Tuple2&lt;String, Row&gt;(value, rdd1Value)); &#125;&#125;);// 这里得提示一下。 // 上面的做法，仅仅适用于rdd1中的key没有重复，全部是唯一的场景。 // 如果rdd1中有多个相同的key，那么就得用flatMap类的操作，在进行join的时候不能用map，而是得遍历rdd1所有数据进行join。 // rdd2中每条数据都可能会返回多条join后的数据。 5.6 方案六：采样倾斜key并分拆join操作 方案适用场景：两个RDD/Hive表进行join的时候，如果数据量都比较大，无法采用“解决方案五”，那么此时可以看一下两个RDD/Hive表中的key分布情况。如果出现数据倾斜，是因为其中某一个RDD/Hive表中的少数几个key的数据量过大，而另一个RDD/Hive表中的所有key都分布比较均匀，那么采用这个解决方案是比较合适的。 方案实现思路： 对包含少数几个数据量过大的key的那个RDD，通过sample算子采样出一份样本来，然后统计一下每个key的数量，计算出来数据量最大的是哪几个key。 然后将这几个key对应的数据从原来的RDD中拆分出来，形成一个单独的RDD，并给每个key都打上n以内的随机数作为前缀，而不会导致倾斜的大部分key形成另外一个RDD。 接着将需要join的另一个RDD，也过滤出来那几个倾斜key对应的数据并形成一个单独的RDD，将每条数据膨胀成n条数据，这n条数据都按顺序附加一个0~n的前缀，不会导致倾斜的大部分key也形成另外一个RDD。 再将附加了随机前缀的独立RDD与另一个膨胀n倍的独立RDD进行join，此时就可以将原先相同的key打散成n份，分散到多个task中去进行join了。 而另外两个普通的RDD就照常join即可。 最后将两次join的结果使用union算子合并起来即可，就是最终的join结果。 方案实现原理：对于join导致的数据倾斜，如果只是某几个key导致了倾斜，可以将少数几个key分拆成独立RDD，并附加随机前缀打散成n份去进行join，此时这几个key对应的数据就不会集中在少数几个task上，而是分散到多个task进行join了。具体原理见下图。 方案优点：对于join导致的数据倾斜，如果只是某几个key导致了倾斜，采用该方式可以用最有效的方式打散key进行join。而且只需要针对少数倾斜key对应的数据进行扩容n倍，不需要对全量数据进行扩容。避免了占用过多内存。 方案缺点：如果导致倾斜的key特别多的话，比如成千上万个key都导致数据倾斜，那么这种方式也不适合。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102// 首先从包含了少数几个导致数据倾斜key的rdd1中，采样10%的样本数据。 JavaPairRDD&lt;Long, String&gt; sampledRDD = rdd1.sample(false, 0.1);// 对样本数据RDD统计出每个key的出现次数，并按出现次数降序排序。 // 对降序排序后的数据，取出top 1或者top 100的数据，也就是key最多的前n个数据。 // 具体取出多少个数据量最多的key，由大家自己决定，我们这里就取1个作为示范。 JavaPairRDD&lt;Long, Long&gt; mappedSampledRDD = sampledRDD.mapToPair(new PairFunction&lt;Tuple2&lt;Long,String&gt;, Long, Long&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Tuple2&lt;Long, Long&gt; call(Tuple2&lt;Long, String&gt; tuple) throws Exception &#123; return new Tuple2&lt;Long, Long&gt;(tuple._1, 1L); &#125;&#125;);JavaPairRDD&lt;Long, Long&gt; countedSampledRDD = mappedSampledRDD.reduceByKey(new Function2&lt;Long, Long, Long&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Long call(Long v1, Long v2) throws Exception &#123; return v1 + v2; &#125;&#125;);JavaPairRDD&lt;Long, Long&gt; reversedSampledRDD = countedSampledRDD.mapToPair(new PairFunction&lt;Tuple2&lt;Long,Long&gt;, Long, Long&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Tuple2&lt;Long, Long&gt; call(Tuple2&lt;Long, Long&gt; tuple) throws Exception &#123; return new Tuple2&lt;Long, Long&gt;(tuple._2, tuple._1); &#125;&#125;);final Long skewedUserid = reversedSampledRDD.sortByKey(false).take(1).get(0)._2;// 从rdd1中分拆出导致数据倾斜的key，形成独立的RDD。 JavaPairRDD&lt;Long, String&gt; skewedRDD = rdd1.filter(new Function&lt;Tuple2&lt;Long,String&gt;, Boolean&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Boolean call(Tuple2&lt;Long, String&gt; tuple) throws Exception &#123; return tuple._1.equals(skewedUserid); &#125;&#125;);// 从rdd1中分拆出不导致数据倾斜的普通key，形成独立的RDD。 JavaPairRDD&lt;Long, String&gt; commonRDD = rdd1.filter(new Function&lt;Tuple2&lt;Long,String&gt;, Boolean&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Boolean call(Tuple2&lt;Long, String&gt; tuple) throws Exception &#123; return !tuple._1.equals(skewedUserid); &#125;&#125;);// rdd2，就是那个所有key的分布相对较为均匀的rdd。 // 这里将rdd2中，前面获取到的key对应的数据，过滤出来，分拆成单独的rdd，并对rdd中的数据使用flatMap算子都扩容100倍。 // 对扩容的每条数据，都打上0～100的前缀。 JavaPairRDD&lt;String, Row&gt; skewedRdd2 = rdd2.filter(new Function&lt;Tuple2&lt;Long,Row&gt;, Boolean&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Boolean call(Tuple2&lt;Long, Row&gt; tuple) throws Exception &#123; return tuple._1.equals(skewedUserid); &#125;&#125;).flatMapToPair(new PairFlatMapFunction&lt;Tuple2&lt;Long,Row&gt;, String, Row&gt;() &#123;private static final long serialVersionUID = 1L;@Overridepublic Iterable&lt;Tuple2&lt;String, Row&gt;&gt; call( Tuple2&lt;Long, Row&gt; tuple) throws Exception &#123;Random random = new Random();List&lt;Tuple2&lt;String, Row&gt;&gt; list = new ArrayList&lt;Tuple2&lt;String, Row&gt;&gt;();for(int i = 0; i &lt; 100; i++) &#123; list.add(new Tuple2&lt;String, Row&gt;(i + "_" + tuple._1, tuple._2));&#125;return list;&#125;&#125;);// 将rdd1中分拆出来的导致倾斜的key的独立rdd，每条数据都打上100以内的随机前缀。 // 然后将这个rdd1中分拆出来的独立rdd，与上面rdd2中分拆出来的独立rdd，进行join。 JavaPairRDD&lt;Long, Tuple2&lt;String, Row&gt;&gt; joinedRDD1 = skewedRDD.mapToPair(new PairFunction&lt;Tuple2&lt;Long,String&gt;, String, String&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Tuple2&lt;String, String&gt; call(Tuple2&lt;Long, String&gt; tuple) throws Exception &#123; Random random = new Random(); int prefix = random.nextInt(100); return new Tuple2&lt;String, String&gt;(prefix + "_" + tuple._1, tuple._2); &#125;&#125;).join(skewedUserid2infoRDD).mapToPair(new PairFunction&lt;Tuple2&lt;String,Tuple2&lt;String,Row&gt;&gt;, Long, Tuple2&lt;String, Row&gt;&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Tuple2&lt;Long, Tuple2&lt;String, Row&gt;&gt; call( Tuple2&lt;String, Tuple2&lt;String, Row&gt;&gt; tuple) throws Exception &#123; long key = Long.valueOf(tuple._1.split("_")[1]); return new Tuple2&lt;Long, Tuple2&lt;String, Row&gt;&gt;(key, tuple._2); &#125;&#125;);// 将rdd1中分拆出来的包含普通key的独立rdd，直接与rdd2进行join。 JavaPairRDD&lt;Long, Tuple2&lt;String, Row&gt;&gt; joinedRDD2 = commonRDD.join(rdd2);// 将倾斜key join后的结果与普通key join后的结果，uinon起来。 // 就是最终的join结果。 JavaPairRDD&lt;Long, Tuple2&lt;String, Row&gt;&gt; joinedRDD = joinedRDD1.union(joinedRDD2); 5.7 方案七：使用随机前缀和扩容RDD进行join 方案适用场景：如果在进行join操作时，RDD中有大量的key导致数据倾斜，那么进行分拆key也没什么意义，此时就只能使用最后一种方案来解决问题了。 方案实现思路： 该方案的实现思路基本和“解决方案六”类似，首先查看RDD/Hive表中的数据分布情况，找到那个造成数据倾斜的RDD/Hive表，比如有多个key都对应了超过1万条数据。 然后将该RDD的每条数据都打上一个n以内的随机前缀。 同时对另外一个正常的RDD进行扩容，将每条数据都扩容成n条数据，扩容出来的每条数据都依次打上一个0~n的前缀。 最后将两个处理后的RDD进行join即可。 方案实现原理：将原先一样的key通过附加随机前缀变成不一样的key，然后就可以将这些处理后的“不同key”分散到多个task中去处理，而不是让一个task处理大量的相同key。该方案与“解决方案六”的不同之处就在于，上一种方案是尽量只对少数倾斜key对应的数据进行特殊处理，由于处理过程需要扩容RDD，因此上一种方案扩容RDD后对内存的占用并不大；而这一种方案是针对有大量倾斜key的情况，没法将部分key拆分出来进行单独处理，因此只能对整个RDD进行数据扩容，对内存资源要求很高。 方案优点：对join类型的数据倾斜基本都可以处理，而且效果也相对比较显著，性能提升效果非常不错。 方案缺点：该方案更多的是缓解数据倾斜，而不是彻底避免数据倾斜。而且需要对整个RDD进行扩容，对内存资源要求很高。 123456789101112131415161718192021222324252627282930// 首先将其中一个key分布相对较为均匀的RDD膨胀100倍。JavaPairRDD&lt;String, Row&gt; expandedRDD = rdd1.flatMapToPair(new PairFlatMapFunction&lt;Tuple2&lt;Long,Row&gt;, String, Row&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Iterable&lt;Tuple2&lt;String, Row&gt;&gt; call(Tuple2&lt;Long, Row&gt; tuple) throws Exception &#123; List&lt;Tuple2&lt;String, Row&gt;&gt; list = new ArrayList&lt;Tuple2&lt;String, Row&gt;&gt;(); for(int i = 0; i &lt; 100; i++) &#123; list.add(new Tuple2&lt;String, Row&gt;(0 + "_" + tuple._1, tuple._2)); &#125; return list; &#125;&#125;);// 其次，将另一个有数据倾斜key的RDD，每条数据都打上100以内的随机前缀。JavaPairRDD&lt;String, String&gt; mappedRDD = rdd2.mapToPair(new PairFunction&lt;Tuple2&lt;Long,String&gt;, String, String&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Tuple2&lt;String, String&gt; call(Tuple2&lt;Long, String&gt; tuple) throws Exception &#123; Random random = new Random(); int prefix = random.nextInt(100); return new Tuple2&lt;String, String&gt;(prefix + "_" + tuple._1, tuple._2); &#125;&#125;);// 将两个处理后的RDD进行join即可。JavaPairRDD&lt;String, Tuple2&lt;String, Row&gt;&gt; joinedRDD = mappedRDD.join(expandedRDD);]]></content>
      <categories>
        <category>大数据学习</category>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zookeeper中的分布式一致性协议]]></title>
    <url>%2Fposts%2Fe3052d18.html</url>
    <content type="text"><![CDATA[1. zookeeper中的一致性协议-ZAB协议 在深入了解ZK之前，相信很多同学都会认为ZK就是Paxos算法的一个实现。但事实上，ZK并没有完全采用Paxos算法，而是使用了一种称为ZooKeeper Atomic Broadcast(ZAB,ZooKeeper原子消息广播协议)的协议作为其数据一致性的核心算法。 ZAB协议是为分布式协调服务ZooKeeper专门设计的一种支持崩渍恢复的原子广播协议。ZAB协议的开发设计人员在协议设计之初并没有要求其具有很好的扩展性，最初只是为雅虎公司内部那些高吞吐量、低延迟、健壮、简单的分布式系统场景设计的。在ZooKeeper的官方文档中也指出，ZAB协议并不像Paxos算法那样，是一种通用的分布式一致性算法，它是一种特别为ZooKeeper设计的崩溃可恢复的原子消息广播算法。 ZAB 协议的核心是定义了对于那些会改变ZooKeeper服务器数据状态的事务请求的处理方式，即： 所有事务请求必须由一个全局唯一的服务器来协调处理，这样的服务器被称为Leader服务器，而余下的其他服务器则成为Follower服务器。Leader服务器负责将一个客户端事务请求转换成一个事务Proposal(提议),并将该Proposal分发给集群中所有的Follower服务器。之后Leader 服务器需要等待所有Follower服务器的反馈，一旦超过半数的Follower 服务器进行了正确的反馈后，那么Leader就会再次向所有的Follower 服务器分发Commit消息，要求其将前一个Proposal进行提交。 1.1 ZAB协议的两种基本模式 ZAB协议包括两种基本的模式，分别是崩愤恢复和消息广播。 消息广播： ZAB 协议的消息广播过程使用的是一个原子广播协议，类似于一个二阶段提交过程。针对客户端的事务请求，Leader服务器会为其生成对应的事务Proposal，并将其发送给集群中其余所有的机器，然后再分别收集各自的选票，最后进行事务提交。 在ZAB协议的二阶段提交过程中，移除了中断逻辑，所有的Follower服务器要么正常反馈Leader提出的事务Proposal，要么就抛弃Leader服务器。同时，ZAB协议将二阶段提交中的中断逻辑移除意味着我们可以在过半 的Follower服务器已经反馈Ack之后就开始提交事务Proposal了，而不需要等待集群中所有的Follower服务器都反馈响应。 当然，在这种简化了的二阶段提交模型下，是无法处理Leader服务器崩溃退出而带来的数据不一致问题的，因此在ZAB协议中添加了另一个模式，即采用崩愤恢复模式来解决这个问题。另外，整个消息广播协议是基于具有FIFO特性的TCP协议来进行网络通信的，因此能够很容易地保证消息广播过程中消息接收与发送的顺序性。 在整个消息广播过程中，Leader服务器会为每个事务请求生成对应的Proposal来进行广播，并且在广播事务Proposal之前，Leader服务器会首先为这个事务Proposal 分配一个全局单调递增的唯一ID，我们称之为事务ID(即ZXID)。由于ZAB协议需要保证每一个消息严格的因果关系，因此必须将每一个事务Proposal按照其ZXID的先后顺序来进行排序与处理。 在消息广播过程中，Leader服务器会为每一个Follower服务器都各自分配一个单独的队列，然后将需要广播的事务Proposal依次放入这些队列中去，并且根据FIFO策略进行消息发送。每一个Follower服务器在接收到这个事务Proposal之后，都会首先将其以事务日志的形式写入到本地磁盘中去，并且在成功写入后反馈给Leader服务器一个Ack响应。当Leader服务器接收到超过半数Follower的Ack响应后，就会广播一个Commit消息给所有的Follower服务器以通知其进行事务提交，同时Leader自身也会完成对事务的提交，而每一个Follower服务器在接收到Commit消息后，也会成对事务提交。 崩溃恢复： 上面我们说过一旦Leader服务器出现崩愤，或者说由于网络原因导致Leader服务器失去了与过半Follower的联系，那么就会进入崩溃恢复模式。在ZAB协议中，为了保证程序的正确运行，整个恢复过程结束后需要选举出一个新的Leader服务器。 为了确保ZAB可以快速的选出一个Leader，同时让新Leader知道自己被选举为Leader和让集群中的机器也快速的感知到产生了新的Leader，ZAB需要确保一些特性： ZAB协议需要确保那些已经在Leader服务器上提交的事务最终被所有服务器都提交。 假设一个事务在Leader服务器上被提交了，并且已经得到过半Follower服务器的Ack反馈，但是在它将Commit消息发送给所有Follower机器之前，Leader服务器挂了。 ZAB协议需要确保丢弃那些只在Leader服务器上被提出的事务。 如果在崩愤恢复过程中出现一个需要被丢弃的提案，那么在崩溃恢复结束后需要跳过该事务Proposal。，假设初始的Leader服务器Server1在提出了一个事务Proposal3之后就崩愤退出了，从而导致集群中的其他服务器都没有收到这个事务Proposal。于是，当Server恢复过来再次加入到集群中的时候，ZAB协议需要确保丢弃Proposal3这个事务。 结合上面提到的这两个崩愤恢复过程中需要处理的特殊情况，就决定了ZAB 协议必须设计这样一个Leader选举算法： 能够确保提交已经被Leader提交的事务Proposal，同时丢弃已经被跳过的事务Proposal。针对这个要求，如果让Leader选举算越能够保证新选举出来的Leader服务器拥有集群中所有机器最高编号(即ZXID最大)的事务Proposal,那么就可以保证这个新选举出来的Leader一定具有所有已经提交的提案。更为重要的是，如果让具有最高编号事务Proposal的机器来成为Leader，就可以省去Leader服务器检查Proposal的提交和丢弃工作的这一步操作了。 1.2 数据同步 所有正常运行的服务器，要么成为Leader，要么成为Follower并和Leader保持同步。Leader服务器需要确保所有的Follower服务器能够接收到每一条事务Proposal，并且能够正确地将所有已经提交了的事务Proposal应用到内存数据库中去。具体的，Leader服务器会为每一个 Follower服务器都准备一个队列，并将那些没有被各Follower服务器同 步的事务以Proposal以消息的形式逐个发送给Follower服务器，并在每一个Proposal消息后面紧接着再发送一个Commit消息，以表示该事务已经被提交。等到Follower服务器将所有其尚未同步的事务Proposal都从Leader服务器上同步过来并成功应用到本地数据库中后，Leader服务器就会将该Follower服务器加入到真正的可用Follower列表中，并开始之后的其他流程。 1.3 ZAB算法描述 整个ZAB 协议主要包括消息广播和崩愤恢复两个过程，进一步可以细分为三个阶段，分别是发现(Discovery)、同步(Synchronization)和广播(Broadcast)阶段。组成ZAB协议的每一个分布式进程，会循环地执行这三个阶段，我们将这样一个循环称为一个主进程周期。 ZAB协议算法表述说明： 术语名 说明 Fp Follower f处理过的最后一个事务 Proposal F.zxid Follower f 处理过的历史事务Proposal 中最后一个事务Proposal 的事务标识ZXID hf 每一个Follower f通常都已经处理（接受）了不少事务Proposal ,并且会有一个针对已经处理过的事务的集合， 将其表示为h&lt;sub&gt;f&lt;/sub&gt;, 表示Follower f 已经处理过的事务序列 Ie 初始化历史记录，在某一个主进程周期epoch e 中， 当准Leader 完成阶段一之后，此时它的hf就被标记为Ie 下面我们就从发现、同步和广播这三个阶段展开来讲解ZAB 协议的内部原理。 阶段1：发现 阶段一主要就是Leader选举过程，用于在多个分布式进程中选举出主进程，准Leader L和Follower F的工作流程分别如下： 步骤F.1.1： Follower F将自己最后接受的事务Proposal的epoch值CEPOCH(F.p)发送给准Leader L。 步骤L.1.1 当接收到来自过半Follower的CEPOCH(F.p)消息后，准Leader L会生成NEWEPOCH(e’)悄息给这些过半的Follower。 关于这个epoch值e’，准Leader L 会从所有接收到的CEPOCH(F.p）消息中选取出最大的epoch值，然后对其进行加l操作，即为e’。 步骤F.1.2：当Follower接收到来自准Leader L 的NEWEPOCH(e’)消息后，如果其检测到当前的CEPOCH(F.p)值小于e’，那么就会将CEPOCH(F.p)赋值为e’,同时向这个准Leader L反馈Ack消息。在这个反馈消息(ACK-E(F.p,hf)中,包含了当前该Follower 的epoch CEPOCH(F.p),以及该Follower的历史事务Proposal集合:hr。 当Leader L接收到来自过半Follower的确认消息Ack之后，Leader L就会从这过半服务器中选取出一个Follower F,并使用其作为初始化事务集合Ie’。 阶段2：同步 在这一阶段中，Leader L和Follower F的工作流程分别如下。 步骤L.2.1: Leader L会将e’和Ie’以NEWLEADER(e’,Ie’）消息的形式发送给所有Quorum中的Follower。 步骤F.2.1:当Follower接收到来自Leader L的NEWLEADER(e’,Ie’)消息后，如果Follower发现CEPOCH(F.p)那么直接进入下一轮循环，因为此时Follower发现自己还在上一轮，或者更上轮，无法参与本轮的同步。如果CEPOCH(F.p) = e’，那么Follower就会执行事务应用操作。具体的，对于每一个事务Proposal:&lt;v,z&gt;∈Ie’,Follower都会接受&lt;e’,&lt;v,z&gt;&gt;。最后 Follower会反馈给Leader，表明自己已经接受并处理了所有Ie’中的事务Proposal。 步骤L.2.2 :当Leader接收到来自过半Follower针对NEWLEADER(e’，Ie’)的反馈消息后，就会向所有的Follower发送Commit消息。至此Leader完成阶段二。 步骤F.2.2: 当Follower收到来自Leader的Commit消息后，就会依次处理并提交所有在Ie’'中未处理的事务。至此Follower完成阶段二。 阶段3：广播 完成同步阶段之后，ZAB协议就可以正式开始接收客户端新的事务请求，井进行消息广播流程。 步骤L.3.1： Leader L接收到客户端新的事务请求后，会生成对应的事务Proposal并根据zxid的顺序向所有Follower发送提案&lt;e’,&lt;v,z&gt;&gt;,其中epoch(z) = e’。 步骤F.3.1： Follower根据消息接收的先后次序来处理这些来自Leader的事务Proposal,并将他们追加到hr中去，之后再反馈给Leader。 步骤L.3.1：当Leader接收到来自过半Follower针对事务Proposal&lt; e’,&lt;v,z&gt;&gt;的Ack消息后，就会发送Commit&lt;e’,&lt;v,z&gt;&gt;消息给所有的Follower，要求它们进行事务的提交。 步骤F.3.2: 当Follower F接收到来自Leader的Commit&lt;e’,&lt;v,z&gt;&gt;消息后，就会开始提交事务Proposal&lt;e’,&lt;v,z&gt;&gt;。需要注意的是，此时该Follower F必定已经提交了事务Proposal &lt;v’,z’&gt;，其中&lt;v’,z’&gt;∈hf,z’ ∈ z Z。 以上就是整个ZAB 协议的三个核心工作流程，以上各个状态字段说明如下： CEPOCH: Follower进程向准Leader发送自己处理过的最后一个事务Proposal的epoch值。 NEWEPOCH ：准Leader进程根据接收的各进程的epoch，来生成新一轮周期的epoch值。 ACK-E: Follower进程反馈准Leader进程发来的NEWEPOCH消息。 NEWLEADER：准Leader程确立自己的领导地位，并发送NEWLEADER消息给各进程。 ACK-LD : Follower进程反馈Leader进程发来的NEWLEADER消息。 COMMIT-LD：要求Follower进程提交相应的历史事务Proposal。 PROPOSE:Leader进程生成一个针对客户端事务请求的Proposal。 ACK: Follower进程反馈Leader进程发来的PROPOSAL消息。 COMMIT:Leader发送COMMIT消息，要求所有进程提交事务PROPOSE。 1.4 ZAB与Paxos算法的区别 ZAB 协议并不是Paxos算法的一个典型实现，在讲解ZAB和Paxos之间的区别之前，我们首先来看下两者的联系： 两者都存在一个类似于Leader进程的角色，由其负责协调多个Follower进程的运行。 Leader进程都会等待超过半数的Follower做出正确的反馈后，才会将一个提案进行提交。 在ZAB协议中，每个Proposal中都包含了一个epoch值，用来代表当前的Leader周期，在Paxos算法中，同样存在这样的一个标识，只是名字变成了Ballot。 在Paxos算法中，一个新选举产生的主进程会进行两个阶段的工作。第一阶段被称为读阶段，在这个阶段中，这个新的主进程会通过和所有其他进程进行通信的方式来收集上一个主进程提出的提案，井将它们提交。第二阶段被称为写阶段，在这个阶段，当前主进程开始提出它自己的提案。在Paxos算法设计的基础上，ZAB协议额外添加了一个同步阶段。在同步阶段之前，ZAB协议也存在一个和Paxos算法中的读阶段非常类似的过 程，称为发现(Discovery)阶段。在同步阶段中，新的Leader会确保存在过半的Follower已经提交了之前Leader周期中的所有事务Proposal。这一同步阶段的引入，能够有效地保证Leader在新的周期中提出事务Proposal之前，所有的进程都已经完成了对之前所有事务Proposal的提交。一旦完成同步阶段后，那么ZAB就会执行和Paxos算法类似的写阶段。 总的来讲，ZAB协议和Paxos算法的本质区别在于，两者的设计目标不太一样。ZAB协议主要用于构建一个高可用的分布式数据主备系统，例如ZooKeeper，而Paxos算法则是用于构建一个分布式的一致性状态机系统。]]></content>
      <categories>
        <category>分布式组件</category>
        <category>zookeeper</category>
      </categories>
      <tags>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Paxos算法原理]]></title>
    <url>%2Fposts%2F64ecab81.html</url>
    <content type="text"><![CDATA[1.从ACID到CAP 我们知道传统集中式系统中实现ACID是很简单的，在分布式环境中，涉及到不同的节点，节点内的ACID可以控制，那么节点间的ACID如何控制呢？构建一个可用性和一致性的分布系统成为难题，于是出现了CAP和BASE这样的理论。 CAP定理： CAP告诉我们一个分布式系统不可能同时满足一致性(Consistency)、可用性(Availability)、分区容错性(Partition tolerance)。最多只能同时满足其中两项。 放弃CAP定理 说明 放弃P 放弃分区容错性意味着你把所有的数据都放在同一个节点上，这可能会导致出错时系统不可用。同时也放弃了系统的可扩展性。 放弃A 放弃可用性带来的影响就更大了，出现故障系统直接停止提供服务。 放弃C 放弃一致性指的是需要系统保持强一致还是保持最终一致。 为解决分布式一致性问题，涌现出来一大批经典的一致性算法和协议，其中最著名的是二阶段提交协议，三阶段提交协议和Paxos算法。 2PC： two-Phase-Commit。通常二阶段提交协议也被认为是一种一致性协议，用来保证分布式系统的数据一致性。目前绝大部分关系型数据库都是采用二阶段提交来完成分布式事务处理的。 协议说明： 提交事务请求; 执行事务提交； 优缺点： 二阶段提交协议的优点：原理简单，实现方便。 二阶段提交协议的缺点：同步阻塞、单点问题、脑裂、太过保守。 3PC： 将二阶段提交协议的“提交事务请求”过程一分为二，形成了由CanCommit，PreCommit和doCommit三个阶段组成的事务处理协议，协议设计如下图： 阶段1，canCommit 事务查询； 各参与者向协调者反馈事务询问的响应。 阶段2,preCommit 假如协调者从所有的参与者获得的反馈都是Yes 响应，那么就会执行事务预提交。 假如任何一个参与者向协调者反馈了No 响应，或者在等待超时之后，协调者尚无能接收到所有参与者的反馈响应，那么就会中断事务。 阶段3，doCommit 执行提交； 进入这一阶段，假设协调者处于正常工作状态，井且有任意一个参与者向协调者反馈了No 响应，或者在等待超时之后，协调者尚无告接收到所有参与者的反馈响应，那么就会中断事务。 优缺点： 三阶段提交协议的优点：相较于二阶段提交协议，三阶段提交协议最大的优点就是降低了参与者的阻塞范围，并且能够在出现单点故障后继续达成一致。 三阶段提交协议的缺点：三阶段提交协议在去除阻塞的同时也引入了新的问题，那就是在参与者接收到preCommit消息后，如果网络出现分区，此时协调者所在的节点和参与者无能进行正常的网络通信，在这种情况下， 该参与者依然会进行事务的提交，这必然出现数据的不一致性。 2. Paxos算法 Paxos算法的核心是一个一致性算法，也就是论文The Part-time Parliament中提到的&quot;synid&quot;算法，我们将从对一致性问题的描述开始来讲解该算法需要解决的实际需求。 问题描述： 假设有一组可以提出提案的进程集合，那么对于一个一致性算越来说需要保证以下几点： 在这些被提出的提案中，只有一个会被选定 如果没有提案被提出，那么就不会有被选定的提案 当一个提案被迫定后，进程应该可以获取被选定的提案信息 对于一致性来说，安全性（ Safety ）的需求如下： 只有被提出的提案才能被选定（ Chosen ） 只能有一个值被选定。 如果某个进程认为某个提案被选定了，那么这个提案必须是真的被选定的那个 从整体上来说，Paxos算法的目标就是要保证最终有一个提案会被选定， 当提案被选定后，进程最终也能获取到被选定的提案。 要选定一个唯一提案的最简单方式莫过于只允许一个Accpetor存在，这样的话，Proposer只能发送提案给该Accpetor, Acceptor 会选择接收到的第一个提案作为被选定的提案。这种解决方式尽管实现起来非常简单，但是却很难让人满意，因为一旦这个Accpetor出现问题，那么整个系统就无能工作了。 因此，应该寻找一种更好的解决方式，例如可以使用多个Accpetor 来避免Accpetor 的单点问题。现在我们就来看看在存在多个Acceptor 的情况下如何进行提案的选取： Proposer向一个Acceptor集合发送提案，同样，集合中的每个Acceptor 都可能会批准(Accept）该提案，当有足够多的Acceptor批准这个提案的时候，我们就可以认为该提案被选定了。那么，什么是足够多呢？我们假定足够多的Acceptor是整个Acceptor集合的一个子集，井且让这个集合大得可以包含Acceptor集合中的大多数成员，因为任意两个包含大多数Acceptor的子集至少有一个公共成员。另外我们再规定，每一个Acceptor最多只能批准一个提案，那么就能保证只有一个提案被选定了。 在没有失败和消息丢失的情况下，如果我们希望即使在只有一个提案被提出的情况下，仍然可以选出一个提案，这就暗示了如下的需求： 条件1： 一个Acceptor 必须批准它收到的第一个提案。 上面这个需求就引出了另外一个问题：如果有多个提案被不同的Proposer 同时提出，这可能会导致虽然每个Acceptor都批准了它收到的第一个提案，但是没有一个提案是由多数人都批准的。 不同的Proposer 分别提出每个提案。 另外，即使只有两个提案被提出，如果每个提案都被差不多一半的 Acceptor 批准了，此时即使只有一个Acceptor出错，都有可能导致无法确定该选定哪个提案。 任意一个Acceptor 出现问题。 因此在需求1的基础上，再加上一个提案被选定需要由半数以上的Acceptor批准的需求暗示着一个Acceptor必须能够批准不止一个提案。 在这里，我们使用一个全局的编号（这种全局唯一编号的生成并不是Paxos算法需要关注的地方，就算法本身而言，其假设当前已经具备这样的外部组件能够生成一个全局唯一的编号）来唯一标识每一个被Acceptor 批准的提案，当一个具有某Value值的提案被半数以上的Acceptor批准后，我们就认为该Value被选定了，此时我们也认为该提案被选定了。需要注意的是，此处讲到的提案已经和Value不是同一个概念了，提案变成了一个由编号和Value组成的组合体，因此我们以“［编号，Value］＂来表示一个提案。 根据上面讲到的内容，我们虽然允许多个提案被选定，但同时必须要保证所有被选定的提案都具有相同的Value值这是一个关于提案Value的约定， 结合提案的编号，该约定可以定义如下： 条件2： 如果编号为Mo, Value值为V0的提案(即[Mo, Vo])被选定了，那么所 有比编号Mo更高的，且被选定的提案，其Value值必须也是Vo。 因为提案的编号是全序的，条件2就保证了只有一个Value值被选定这一关键安全性属性。同时，一个提案要被选定，其首先必须被至少一个Acceptor 批准，因此我们可以通过满足如下条件来满足条件2： 条件2.1： 如果编号为Mo，Value值为Vo的提案(即［Mo, Vo])被选定了，那么所 有比编号Mo更高的，且被Acceptor批准的提案，其Valu值必须也是Vo。 因此，我们在条件1的前提下来发起提案，但是因为通信是异步的，一个提案可能会在某个Acceptor还未收到任何提案时就被选定了。 在Acceptorl没有收到任何提案的情况下，其他4个Acceptor已经批准 了来自Proposer2的提案［Mo,V1],而此时,Proposerl产生了一个具有其他Value值的、编号更高的提案［M1,V2］，并发送给了Acceptor1。根据条件1就需要Acceptorl批准该提案，但是这与条件2矛盾，因此如果要同时满足条件1和1，需要对条件2进行如下强化： 条件2.2： 如果一个提案[Mo,Vo]被选定后，那么之后任何Proposer产生的编号是 高的提案，其Value值都为Vo。 因为一个提案必须在被Proposer提出后才能被Acceptor批准，因此2.2 包含了2.1，进而包含了2。于是，接下去的重点就是论证2.2成立即可： 假设某个提案[Mo,Vo]已经被选定了，证明任何编号Mn&gt; Mo的提案，其Value值都是Vo。 2.1 证明 我们可以将上述结论转化为如下： 假设编号在Mo到Mn-1之间的提案，其Value值都是Vo，证明编号为Mn的提案的Value值也为Vo。 因为编号为Mo的提案已经被选定了，这就意味着肯定存在一个由半数以上的Acceptor组成的集合c, c中的每个Acceptor都批准了该提案。再结合归纳假设，&quot;编号为Mo的提案被选定&quot;意味着： C中的每个Acceptor都批准了一个编号在Mo到Mn-范围内的提案，并且每个编号在Mo到Mn-1范围内的被Acceptor批准的提案，其Value值都为Vo。 因为任何包含半数以上Acceptor的集合S都至少包含C中的一个成员，因此我们可以认为如果保持了下面条件2.3的不变性，那么编号为Mn的提案的Value也为Vo。 条件2.3： 对于任意的Mn和Vn，如果提[Mn,Yn]被提出，那么肯定存在一个由半数以上的Acceptor组成的集合S ，满足以下两个条件中的任意一个。 S中不存在任何批准过编号小于Mn的提案的Acceptor。 选取S中所有Acceptor批准的编号小于Mn的提案，其中编号最大的那个提案其Value值是Vn。 至此，只需要通过保持条件2.3我们就能够满足条件2.2了。 实际上2.3规定了每个Proposer如何产生一个提案：对于产生的每个提案[Mn,Yn]，需要满足如下条件： 存在一个由超过半数的Acceptor 组成的集合S : 要么S中没有Acceptor批准过编号小干Mn的任何提案。 要么S中的所有Acceptor批准的所有编号小于Mn的提案中，编号最大的那个提案的Value值为Vn。 当每个Proposer都按照这个规则来产生提案时，就可以保证满足2.2了。我们继续来证明2.3。 首先假设提案[Mo,Vo]被选定了,设比该提案编号大的提案为[Mn,Vn]，我们需要证明的就是在2.3的前提下，对于所有的[Mn,Vn]，存在Vn=Vo。 当Mn= Mo+1时，如果有这样一个编号为Mn的提案，首先我们知道[Mo,Vo]已经被选定了，那么就一定存在－个Acceptor的子集S，且S中的Acceptor已经批准了小于M的提案，于是，Vn只能是多数集S中编号小于M但为最大编号的那个提案的值。而此时因为Mn= Mo+1，因此理论上编号小于Mn但为最大编号的那个提案肯定是[Mo,Vo]同时由于S和通过[Mo,Vo]的Acceptor集合都是多数集,也就是说二者肯定有交集—这样Proposer在确定Vn取值的时候，就一定会选择Vo。 值得注意的一点是，Paxos算法的证明过程使用的是第二数学归纳法，上面实际上就是数学归纳陆的第一步，验证了某个初始值成立。接下去，就需要假设编号在Mo + 1到Mn-l 区间内时成立，并在此基础上推导出当编号为Mn时也成立。 根据假设，编号在Mo+1到Mn-1区间内的所有提案的Value值为Vo,需要证明的是编号为Mn的提案的Value值也为Vo。根据2.3，首先同样一定存在一个Acceptor的子集S，且S中的Acceptor已经批准了小子Mn的提案，那么编号为Mn的提案的Value值只能是这个多数集S中编号小于Mn但为最大编号的那个提案的值。如果这个最大编号落在Mo+1到Mn-1区间内，那么Value值肯定是Vo，如果不落在Mo+1到Mn-1区间内，那么它的编号不可能比Mo再小了,肯定就是Mo,因为S也肯定会与批准[Mo,Vo]这个提案的Acceptor集合S有交集，而如果编号是Mo,那么它的Value值也是Vo,由此得证。 proposer生成提案 对于一个Proposer来说，获取那些已经被通过的提案远比预恻未来可能会被通过的提案来得简单。因此，Proposer 在产生一个编号为Mn的提案时，必须要知道当前某一个将要或已经被半数以上Acceptor批准的编号小于Mn但为最大编号的提案。并且，Proposer会要求所有的Acceptor都不 要再批准任何编号小于Mn的提案—这就引出了如下的提案生成算法。 Proposer选择一个新的提案编号Mn，然后向某个Acceptor集合的成员发送请求，要求该集合中的Acceptor做出如下回应： 向Proposer承诺，保证不再批准任何编号小于Mn 的提案。 如果Acceptor已经批准过任何提案，那么其就向Proposer反馈当前该Acceptor已经批准的编号小于Mn但为最大编号的那个提案的值。 我们将该请求称为编号为Mn的提案的Prepare请求。 如果Proposer收到了来自半数以上的Acceptor的晌应结果，那么它就可以产生编号为Mn，Value值为Vn的提案，这里的Vn是所有响应中编号最大的提案的Value值。当然还存在另一种情况，就是半数以上的Acceptor 都没有批准过任何提案，即响应中不包含任何的提案，那么此时Vn值就可以由Proposer任意选择。 Acceptor批准提案： 一个Acceptor可能会收到来自Proposer的两种请求，分别是Prepare 请求和Accept请求，对这两类请求做出响应的条件分别如下： Prepare请求：Acceptor可以在任何时候响应一个Prepare请求。 Accept请求：在不违背Accept现有承诺的前提下，可以任意响应Accept请求。 因此对Acceptor逻辑处理的约束条件大体如下： 一个Acceptor只要尚未响应过任何编号大于Mn的Prepare请求，那么它 就可以接受这个编号为Mn的提案。 2.2 算法优化 在上面的内容中，我们分别从Proposer和Acceptor对提案的生成和批准两方面来讲解了Paxos算法在提案选定过程中的算也细节，同时也在提案的编号全局唯一的前提下，获得了一个满足安全性需求的提案选定算法，接下来我们再对这个初步算法做一个小优化，尽可能地忽略Prepare请求： 假设一个Acceptor收到了一个编号为Mn的Prepare请求，但此时该Acceptor已经对编号大于Mn的Prepare请求做出了响应，因此它肯定不会再批准任何新的编号为Mn的提案，那么很显然，Acceptor就没有必要对这个Prepare请求做出响应，于是Acceptor可以选择忽略这样的Prepare 请求。同时，Acceptor也可以忽略掉那些它已经批准过的提案的Prepare 请求。 通过这个优化，每个Acceptor只需要记住它已经批准的提案的最大编号以及它已经做出Prepare请求响应的提案的最大编号，以便在出现故障或节点重启的情况下，也能保证条件2.3的不变性。而对干Proposer来说，只要它可以保证不会产生具有相同编号的提案，那么就可以丢弃任意的提案以及它所有的运行时状态信息。 根据我们的优化，结合Proposer和Acceptor对提案的处理逻辑，就可以得到如下类似于两阶段提交的算能执行过程： 阶段1： Proposer选择一个提案编号Mn，然后向Acceptor的某个超过半数的子集成员发送编号为Mn的Prepare请求。 如果一个Acceptor收到一个编号为Mn的Prepare请求，且编号Mn大于该Acceptor已经响应的所有Prepare请求的编号，那么它就会将它已经批准过的最大编号的提案作为响应反馈给Proposer，同时该Acceptor会承诺不会再批准任何编号小于Mn的提案。 举个例子来说，假定一个Acceptor已经响应过的所有Prepare请求对应的提案编号分别为l，2, …，5 和7，那么该Acceptor在接收到一个编号为8的Prepare请求后，就会将编号为7的提案作为响应反馈给Proposer。 阶段2： 如果Proposer收到来自半数以上的Acceptor对于其发出的编号为Mn 的Prepare请求的晌应，那么它就会发送一个针对［Mn,Vn］提案的Accept请求给Acceptor。注意，Vn的值就是收到的响应中编号最大的提案的值，如果响应中不包含任何提案，那么它就是任意值。 如果Acceptor收到这个针对[Mn,Vn]提案的Accept请求，只要该Acceptor尚未对编号大于Mn的Prepare请求做出响应，它就可以通过这个提案。 当然，在实际运行过程中，每一个Proposer都有可能会产生多个提案，但只要每个Proposer都遵循如上所述的算陆运行，就一定能够保证算怯执行的正确性。值得一提的是，每个Proposer都可以在任意时刻丢弃一个提案，哪怕针对该提案的请求和响应在提案被丢弃后会到达，但根据Paxos 算法的一系列规约，依然可以保证其在提案选定上的正确性。事实上，如果某个Proposer已经在试图生成编号更大的提案，那么丢弃一些旧的提案未尝不是一个好的选择。因此，如果一个Acceptor因为已经收到过更大编号的Prepare请求而忽略某个编号更小的Prepare或者Accept请求，那么它也应当通知其对应的Proposer，以便该Proposer也能够将该提案进行丢弃—这和上面“算法优化”部分中提到的提案丢弃是一致的。 Learner获取提案： Learner获取提案大致有以下几种方案： 方案一： Learner获取一个已经被选定的提案的前提是，该提案已经被半数以上的Acceptor批准。因此，最简单的做怯就是一旦Acceptor批准了一个提案，就将该提案发送给所有的Learner。 很显然，这种做法虽然可以让Learner尽快地获取被选定的提案，但是却需要让每个Acceptor与所有的Learner逐个进行一次通信，通信的次数至少为二者个数的乘积。 方案二： 另一种可行的方案是，我们可以让所有的Acceptor将它们对提案的批准情况，统一发送给一个特定的Learner(下文中我们将这样的Learner称为&quot;主Learner&quot;），在不考虑拜占庭将军问题的前提下，我们假定Learner之间可以通过消息通信来互相感知提案的选定情况。基于这样的前提，当主Learner被通知一个提案已经被选定时，它会负责通知其他的Learner。 在这种方案中，Acceptor首先会将得到批准的提案发送给圭Learner，再由其同步给其他Learner，因此较方案一而言，方案二虽然需要多一个步骤才能将提案通知到所有的Learner，但其通信次数却大大减少了，通常只是Acceptor和Learner的个数总和。但同时，该方案引入了一个新的不稳定因素：主Learner随时可能出现故障。 方案三： 在讲解方案二的时候，我们提到，方案二最大的问题在于主Learner存在单点问题，即主Learner随时可能出现故障。因此，对方案二进行改进，可以将主Learner的范围扩大，即Acceptor可以将批准的提案发送给一个特定的Learner集合，该集合中的每个Learner都可以在一个提案被选定后通知所有其他的Learner。这个Learner集合中的Learner个数越多，可靠性就越好，但同时网络通信的复杂度也就越高。 2.3 通过选举出主Proposer保证算法的活性 根据前面的内容讲解，我们已经基本上了解了Paxos算法的核心逻辑，下面我们再来看看Paxos算法在实际运作过程中的一些细节。假设存在这样一种极端情况，有两个Proposer依次提出了一系列编号递增的议案，但是最终都无法被选定，具体流程如下： Proposer P1提出了一个编号为M1的提案，并完成了上述阶段一的流程。但与此同时，另外一个Proposer P2提出了一个编号为M2(M2&gt;M1)的提案，同样也完成了阶段一的流程，于是Acceptor已经承诺不再批准编号小于M2 的提案了。因此，当P1进入阶段二的时候，其发出的Acceptor请求将被Acceptor忽略，于是P1再次进入阶段一并提出了一个编号为M3(M3 &gt; M2)的提案，而这又导致P2在第二阶段的Accept请求被忽略，以此类格，提案的选定过程将陷入死循环。 为了保证Paxos算法流程的可持续性，以避免陷入上述提到的“死循环”，就必须选择一个主Proposer，并规定只有主Proposer才能提出议案。这样一来，只要主Proposer和过半的Acceptor能够正常进行网络通信，那么但凡主Proposer提出一个编号更高的提案，该提案终将会被批准。当然，如果Proposer发现当前算法流程中已经有一个编号更大的提案被提出或正在接受批准，那么它会丢弃当前这个编号较小的提案，并最终能够选出一个编号足够大的提案。因此，如果系统中有足够多的组件（包括Proposer，Acceptor和其他网络通信组件）能够正常工作，那么通过选择一个主Proposer，整套Paxos算住流程就能够保持活性。]]></content>
      <categories>
        <category>分布式组件</category>
        <category>zookeeper</category>
      </categories>
      <tags>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark核心API开发]]></title>
    <url>%2Fposts%2Fe7493326.html</url>
    <content type="text"><![CDATA[开发人员在编写Spark应用的时候，需要提供一个包含main函数的驱动程序作为程序的入口，开发人员根据自己的需求，在main函数中调用Spark提供的数据操纵接口，利用集群对数据执行并行操作。 Spark为开发人员提供了两类抽象接口。 第一类抽象接口是弹性分布式数据集（Resilient Distributed Dataset，下文简称RDD），顾名思义，RDD是对数据集的抽象封装，开发人员可以通过RDD提供的开发接口来访问和操纵数据集合，而无需了解数据的存储介质（内存或磁盘）、文件系统（本地文件系统、HDFS或Tachyon）、存储节点（本地或远程节点）等诸多实现细节； 第二类抽象是共享变量（Shared Variables），通常情况下，一个应用程序在运行的时候会被划分成分布在不同执行器之上的多个任务，从而提高运算的速度，每个任务都会有一份独立的程序变量拷贝，彼此之间互不干扰，然而在某些情况下需要任务之间相互共享变量，Apache Spark提供了两类共享变量，它们分别是：广播变量（Broadcast Variable）和累加器（Accumulators）。后面将介绍RDD的基本概念和RDD提供的编程接口，并在后面详细解读接口的源码实现，从而加深对RDD的理解，此外会介绍两类共享变量的使用方法。 另外，除了单独编写一个应用程序的方式之外，Spark还提供了一个交互式Shell来使用。在Shell中，用户的每条语句都能在输入完毕后及时得到结果，而无需手动编译和运行程序。启用非常简单，在spark的安装目录找到bin目录下面的spark-shell命令即可。 在Shell中，系统根据命令提供的参数自动配置和生成了一个SparkContext对象sc，直接使用即可，无需再手动实例化SparkContext。除了结果会实时显示之外，其余操作与编写单独应用程序类似。读者可直接参考Spark官方提供的Spark ProgrammingGuide等文档，在此不做具体介绍。 1. Spark Context SparkContext是整个项目程序的入口，无论从本地读取文件（textfile方法）还是从HDFS读取文件或者通过集合并行化获得RDD，都先要创建SparkContext对象，然后使用SparkContext对RDD进行创建和后续的转换操作。本节主要介绍SparkContext类的作用和创建过程，然后通过一个简单的例子向读者介绍SparkContext的应用方法，从应用角度来理解其作用。 1.1 SparkContext的作用 SparkContext除了是Spark的主要入口，它也可以看作是对用户的接口，它代表与Spark集群的连接对象，由下图2可以看到，SparkContext主要存在于Driver Program中。可以使用SparkContext来创建集群中的RDD、累积量和广播量，在后台SparkContext还能发送任务给集群管理器。每一个JVM只能有运行一个程序，即对应只有一个SparkContext处于激活状态，因此在创建新的SparkContext前需要把旧的SparkContext停止。 下面有一个简单的单词统计例子来看一下SparkContext的使用方式： 12345678910object WordCount &#123; def main(args: Array[String]) &#123; val inputFile = "file:///usr/local/spark/test/word.txt" val conf = new SparkConf().setAppName("WordCount").setMaster("local") val sc = new SparkContext(conf) val textFile = sc.textFile(inputFile) val wordCount = textFile.flatMap(line =&gt; line.split(" ")).map(word =&gt; (word, 1)).reduceByKey((a, b) =&gt; a + b) wordCount.foreach(println) &#125;&#125; 这个例子中，首先创建配置文件conf，使用本地模式，将配置文件放入sparkContext上下文并实例化。从sparkContext上下文中读取文件数据并将数据转化RDD，然后统计词频。 2. RDD算子 下面再来说RDD的两个主要操作算子Transformation和Action的使用方法，由于Spark是基于延迟计算，Transforamation算子并不立即执行，这时只是保存计算状态，当Action算子出现才真正执行计算。 2.1 单值型Tranformation算子 单值型的算子就是输入为单个值形式，这里主要介绍map、flatMap、mapPartitions、union、cartesian、groupBy、filter、distinct、subtract、foreach、cache、persist、sample以及takeSample方法： 方法名 方法定义 map def map[U](f: (T) ⇒ U)(implicit arg0: ClassTag[U]): RDD[U] flatMap defmapPartitions[U](f: (Iterator[T])⇒ Iterator[U], preservesPartitioning: Boolean = false) mapPartition def mapPartitions[U](f: (Iterator[T])⇒ Iterator[U], preservesPartitioning: Boolean = false)(implicit arg0: ClassTag[U]): RDD[U] mapPartitionsWithIndex def mapPartitionsWithIndex[U](f: (Int, Iterator[T])⇒ Iterator[U], preservesPartitioning: Boolean = false)(implicit arg0: ClassTag[U]): RDD[U] foreach def foreach(f: (T) ⇒ Unit): Unit foreachPartition def foreachPartition(f: (Iterator[T])⇒ Unit): Unit glom def glom(): RDD[Array[T]] union def union(other: RDD[T]): RDD[T] cartesian def cartesian[U](other: RDD[U])(implicit arg0: ClassTag[U]): RDD[(T, U)] groupBy def groupBy[K](f: (T) ⇒ K, p: Partitioner)(implicit kt: ClassTag[K], ord: Ordering[K] = null): RDD[(K, Iterable[T])] filter def filter(f: (T) ⇒ Boolean): RDD[T] distinct def distinct(): RDD[T] subtract def subtract(other: RDD[T], p: Partitioner)(implicit ord: Ordering[T] = null): RDD[T] cache def cache(): RDD.this.type persist def persist(): RDD.this.type sample def sample(withReplacement: Boolean, fraction: Double, seed: Long = Utils.random.nextLong): RDD[T] takeSample def takeSample(withReplacement: Boolean, num: Int, seed: Long = Utils.random.nextLong): Array[T] 2.1.1 map 对原来每一个输入的RDD数据集进行函数转换，返回的结果为新的RDD，该方法对分区操作是一对一的。 示例： 1234567val a = sc.parallelize(List("bit", "linc", "xwc", "fjg", "wc","spark"), 3) //创建RDDval b = a.map(word =&gt; word.length) //计算每个单词的长度val c = a.zip(b) //拉链方法，把两列数据对应配对成键值对格式c.collect //把结果转换为数组 结果：scala&gt; c.collectres3: Array[(String, Int)] = Array((bit,3), (linc,4), (xwc,3), (fjg,3), (wc,2), (spark,5)) 2.1.2 flatMap flapMap方法与map方法类似，但是允许在一次map方法中输出多个对象，而不是map中的一个对象经过函数转换生成另一个对象。 示例： 12345scala&gt; val a = sc.parallelize(1 to 3, 5)a: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:24scala&gt; a.flatMap(num =&gt; 1 to num).collectres0: Array[Int] = Array(1, 1, 2, 1, 2, 3) 2.1.3 mapPartitions mapPartitions是map的另一个实现。map的输入函数是应用于RDD中每个元素，而mapPartitio的输入函数是作用于每个分区，也就是把每个分区中的内容作为整体来处理的。 示例： 123456789101112131415scala&gt; val a = sc.parallelize(1 to 9, 3)a: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:24scala&gt; def myfunc[T](iter: Iterator[T]) : Iterator[(T, T)] = &#123; | var res = List[(T, T)]() | var pre = iter.next | while (iter.hasNext) &#123; | val cur = iter.next | res .::= (pre, cur) | pre = cur | &#125; | res.iterator | &#125;myfunc: [T](iter: Iterator[T])Iterator[(T, T)]scala&gt; a.mapPartitions(myfunc).collectres0: Array[(Int, Int)] = Array((2,3), (1,2), (5,6), (4,5), (8,9), (7,8)) 解释一下这段程序：先得到一个分为3个分区的1-9的序列然，即每个分区的数据分别是(1,2,3),(4,5,6),(7,8,9)。下面定义了一个myfunc函数，遍历一个集合，集合里面是tuple对象，如果这个tuple的元素有next元素，name将pre和cur构造成一个tuple存入list。下面 mapPartitions函数将myfunc函数作为参数去作用于每一个分区，所以获得的结果为(1,2),(2,3),因为3没有next元素，所以每一个分区中只有两个tuple，下面的分区以此类推。 2.1.4 mapPartitionWithIndex mapPartitionWithIndex方法与mapPartitions方法功能类似，不同的是mapPartitionWithIndex还会对原始分区的索引进行追踪，这样能知道分区所对应的元素，方法的参数为一个函数，函数的输入为整型索引和迭代器。 示例： 12345678scala&gt; val a = sc.parallelize(1 to 9, 3)a: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:24scala&gt; def myfunc(index: Int, iter: Iterator[Int]) : Iterator[String] = &#123; | iter.toList.map(x =&gt; index + "," + x).iterator | &#125;myfunc: (index: Int, iter: Iterator[Int])Iterator[String]scala&gt; a.mapPartitionsWithIndex(myfunc).collect()res1: Array[String] = Array(0,1, 0,2, 0,3, 1,4, 1,5, 1,6, 2,7, 2,8, 2,9) 这段程序就是先得到一个3个分区的序列，然后调用mapPartitionWithIndex函数可以获取每个分区的下标，方便追踪当前处理的分区。myfunc函数将每个分区的下标和值用&quot;,&quot;拼装在一起，从输出结果我们可以看到分区下标是从0开始的。 2.1.5 foreach foreach方法主要是对输入的数据对象执行循环操作，该方法常用来输出RDD中的内容。 这个方法比较直观： 1234scala&gt; val c = sc.parallelize(List("aa", "bb", "cc", "dd", "ee", "ff", "mm", "kk", "zz", "rr"), 3)c: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[2] at parallelize at &lt;console&gt;:24scala&gt; c.foreach(x =&gt; println("word is : "+x)) 2.1.6 foreachPartition foreachPartition方法的作用是通过迭代器参数对RDD中每一个分区的数据对象应用函数。mapPartitions方法的作用于foreachPartition方法作用非常相似，区别就在于使用的参数是否有返回值。 12val b = sc.parallelize(List(1, 2, 3, 4, 5, 6, 7, 8, 9), 3)b.foreachPartition(x =&gt; println((a,b) =&gt; x.reduce(a + b) 2.1.7 glom 作用类似collect，但它不是直接将所有RDD直接转化为数组形式，glom方法的作用是将RDD中分区数据进行组装到数组类型RDD中，每一个返回的数组包含一个分区的元素，按分区转化为数组，最后有几个分区就返回几个数组类型的RDD。 示例： 1234scala&gt; val a = sc.parallelize(1 to 10, 3)a: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at parallelize at &lt;console&gt;:24scala&gt; a.glom.collectres2: Array[Array[Int]] = Array(Array(1, 2, 3), Array(4, 5, 6), Array(7, 8, 9, 10)) 2.1.8 union union方法（等价于“++”）是将两个RDD取并集，取并集过程中不会把相同元素去掉。union操作是输入分区与输出分区多对一模式。 示例： 1234567891011scala&gt; val a = sc.parallelize(1 to 4, 2)a: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[3] at parallelize at &lt;console&gt;:24scala&gt; val b = sc.parallelize(2 to 4, 1)b: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[4] at parallelize at &lt;console&gt;:24scala&gt; (a ++ b).collectres3: Array[Int] = Array(1, 2, 3, 4, 2, 3, 4)scala&gt; a.union(b).collectres4: Array[Int] = Array(1, 2, 3, 4, 2, 3, 4) 可见结果集没有执行distinct操作。 2.1.9 cartesian 计算两个RDD中每个对象的笛卡尔积（例如第一个RDD中的每一个对象与第二个RDD中的对象join连接），但使用该方法时要注意可能出现内存不够的情况。 示例： 12345678scala&gt; val x =sc.parallelize(List(1,2,3),1)x: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[7] at parallelize at &lt;console&gt;:24scala&gt; val y =sc.parallelize(List(4,5),1)y: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[8] at parallelize at &lt;console&gt;:24scala&gt; x.cartesian(y).collectres5: Array[(Int, Int)] = Array((1,4), (1,5), (2,4), (2,5), (3,4), (3,5)) 谨慎执行笛卡尔积操作。 2.1.10 groupBy groupBy方法有三个重载方法，功能是将元素通过map函数生成Key-Value格式，然后使用reduceByKey方法对Key-Value对进行聚合。 示例： 1234567891011121314scala&gt; val a = sc.parallelize(1 to 9, 3)a: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[10] at parallelize at &lt;console&gt;:24scala&gt; a.groupBy(x =&gt; &#123; if (x % 2 == 0) "even" else "odd" &#125;).collectres6: Array[(String, Iterable[Int])] = Array((even,CompactBuffer(2, 4, 6, 8)), (odd,CompactBuffer(1, 3, 5, 7, 9)))scala&gt; def myfunc(a: Int) : Int =&#123;a % 2&#125;myfunc: (a: Int)Intscala&gt; a.groupBy(myfunc).collectres7: Array[(Int, Iterable[Int])] = Array((0,CompactBuffer(2, 4, 6, 8)), (1,CompactBuffer(1, 3, 5, 7, 9)))scala&gt; a.groupBy(myfunc(_),1).collectres9: Array[(Int, Iterable[Int])] = Array((0,CompactBuffer(2, 4, 6, 8)), (1,CompactBuffer(1, 3, 5, 7, 9))) 第一个方法使用的是默认分区器，只需要传入自定义函数即可，第二个和第三个方法本质没有区别。 2.1.11 filter filter方法通过名称就能猜出来功能，其实就是对输入元素进行过滤，参数是一个返回值为boolean的函数，如果函数对输入元素运算结果为true，则通过该元素，否则将该元素过滤，不能进入结果集。 示例： 12345678scala&gt; val a = sc.parallelize(1 to 10, 3)a: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[25] at parallelize at &lt;console&gt;:24scala&gt; val b = a.filter(x =&gt; x % 2 == 0)b: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[26] at filter at &lt;console&gt;:27scala&gt; b.collectres13: Array[Int] = Array(2, 4, 6, 8, 10) 2.1.12 distinct 将RDD中重复的元素去掉，只留下唯一的RDD元素。 示例： 12345scala&gt; val x = sc.parallelize(List(1,2,3,3,5,5,7,7,9,10), 3)x: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[27] at parallelize at &lt;console&gt;:24scala&gt; x.distinct.collectres14: Array[Int] = Array(3, 9, 1, 7, 10, 5, 2) 2.1.12 subtract subtract的含义就是求集合A-B的差，即把集合A中包含集合B的元素都删除，结果是剩下的元素。 示例： 1234567891011scala&gt; val a = sc.parallelize(1 to 9, 3)a: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[31] at parallelize at &lt;console&gt;:24scala&gt; val b = sc.parallelize(1 to 3, 3)b: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[32] at parallelize at &lt;console&gt;:24scala&gt; val c = a.subtract(b)c: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[36] at subtract at &lt;console&gt;:27scala&gt; c.collectres15: Array[Int] = Array(6, 9, 4, 7, 5, 8) 2.1.13 persist,cache 顾名思义，是缓存数据，其作用是把RDD缓存到内存中，以方便下一次计算被再次调用。 有所不同的是：cache只是将数据保存在内存中，而persist方法的作用是把RDD根据不同的级别进行持久化，通过使用带参数方法能指定持久化级别，如果不带参数则为默认持久化级别，即只保存到内存，与cache等价。 persist方法通过StorageLevel函数来设置持久化级别，如果不设置持久化级别默认为持久化级别，即只保存到内准中，与cache一样。 示例： 123456scala&gt; val c = sc.parallelize(List("a", "b", "c", "d", "e", "f"),1)c: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[37] at parallelize at &lt;console&gt;:24scala&gt; c.cacheres16: c.type = ParallelCollectionRDD[37] at parallelize at &lt;console&gt;:24scala&gt; c.persist(StorageLevel.MEMORY_ONLY) 注意：c.persist(StorageLevel.MEMORY_ONLY) 在spark-shell中是无法执行的，StorageLevel是spark-core包中的对象。 Persist StorageLevel说明： 123456class StorageLevel private( private var _useDisk: Boolean, private var _useMemory: Boolean, private var _useOffHeap: Boolean, private var _deserialized: Boolean, private var _replication: Int = 1) 初始化StorageLevel可以传入5个参数，分别对应是否存入磁盘、是否存入内存、是否使用堆外内存、是否不进行序列化，副本数（默认为1）。 Storagelevel对应的枚举类型： 12345678910111213object StorageLevel &#123; val NONE = new StorageLevel(false, false, false, false) val DISK_ONLY = new StorageLevel(true, false, false, false) val DISK_ONLY_2 = new StorageLevel(true, false, false, false, 2) val MEMORY_ONLY = new StorageLevel(false, true, false, true) val MEMORY_ONLY_2 = new StorageLevel(false, true, false, true, 2) val MEMORY_ONLY_SER = new StorageLevel(false, true, false, false) val MEMORY_ONLY_SER_2 = new StorageLevel(false, true, false, false, 2) val MEMORY_AND_DISK = new StorageLevel(true, true, false, true) val MEMORY_AND_DISK_2 = new StorageLevel(true, true, false, true, 2) val MEMORY_AND_DISK_SER = new StorageLevel(true, true, false, false) val MEMORY_AND_DISK_SER_2 = new StorageLevel(true, true, false, false, 2) val OFF_HEAP = new StorageLevel(true, true, true, false, 1) 在持久化操作中需要注意的是：如果我们希望只是保存某一时刻的RDD信息，而不是在持久化到内存或者磁盘中仍然会变化的RDD，我们可以执行checkpoint()算子，可以把RDD持久化到HDFS，同时切断RDD之间的依赖。 示例： 12val c = sc.parallelize(List("a", "b", "c", "d", "e", "f"),1)c.checkpoint() checkpoint : 可以把RDD持久化到HDFS，同时切断RDD之间的依赖 对于切断RDD之间的依赖的说明： 当业务逻辑很复杂时，RDD之间频繁转换，RDD的血统很长，如果中间某个RDD的数据丢失，还需要重新从头计算，如果对中间某个RDD调用了checkpoint()方法，把这个RDD上传到HDFS，同时让后面的RDD不再依赖于这个RDD，而是依赖于HDFS上的数据，那么下次计算会方便很多。 checkpoint()执行原理： 当RDD的job执行完毕后，会从finalRDD从后往前回溯 当回溯到调用了checkpoint()方法的RDD后，会给这个RDD做一个标记 Spark框架自动启动一个新的job，计算这个RDD的数据，然后把数据持久化到HDFS上 优化：对某个RDD执行checkpoint()之前，对该RDD执行cache()，这样的话，新启动的job只需要把内存中的数据上传到HDFS中即可，不需要重新计算。 2.1.14 sample sample的作用是随机的对RDD中的元素采样，获得一个新的子集RDD，根据参数能指定是否又放回采样、子集占总数的百分比和随机种子。 示例： 12345678scala&gt; val a = sc.parallelize(1 to 1000, 2)a: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[38] at parallelize at &lt;console&gt;:24scala&gt; a.sample(false, 0.1, 0).collectres21: Array[Int] = Array(10, 39, 41, 53, 54, 58, 60, 80, 89, 98, 113, 114, 128, 134, 144, 161, 173, 176, 177, 198, 201, 211, 212, 213, 218, 224, 232, 235, 241, 242, 243, 246, 264, 287, 307, 319, 321, 325, 334, 339, 353, 354, 366, 367, 373, 392, 403, 407, 419, 426, 429, 434, 458, 465, 466, 478, 492, 495, 514, 522, 536, 547, 550, 557, 560, 567, 575, 595, 613, 618, 638, 639, 643, 658, 659, 660, 662, 672, 681, 682, 686, 693, 694, 696, 716, 720, 737, 763, 769, 774, 775, 791, 795, 802, 804, 812, 815, 817, 822, 877, 888, 889, 915, 942, 947, 948, 971, 972, 976, 985, 992)scala&gt; a.sample(true, 0.1, 0).collectres22: Array[Int] = Array(10, 23, 25, 35, 50, 68, 69, 79, 79, 85, 91, 91, 110, 122, 132, 133, 133, 153, 166, 171, 172, 175, 180, 198, 208, 211, 229, 233, 241, 271, 275, 287, 289, 307, 318, 322, 329, 337, 338, 339, 354, 358, 366, 369, 374, 377, 379, 388, 394, 407, 415, 420, 424, 425, 428, 436, 447, 452, 462, 492, 492, 494, 515, 526, 529, 550, 571, 587, 587, 597, 601, 603, 621, 626, 630, 638, 643, 644, 649, 650, 657, 659, 661, 670, 686, 689, 720, 722, 722, 728, 743, 748, 761, 763, 782, 787, 787, 796, 817, 820, 825, 833, 844, 846, 855, 872, 873, 881, 899, 904, 916, 935, 936, 951, 952, 967, 982, 990, 992, 993) 上述例子中sample方法第一个参数为true时使用放回抽样（泊松抽样），为false时使用不放回抽样（伯努利抽样），第二个参数fraction是百分比，第三个参数seed是种子，也就是随机取值的起源数字。从例子2中还看出当选择放回抽样时，取出的元素中会出现重复值。 2.2 键值对型Transformation算子 RDD的操作算子除了单值型还有键值对（Key-Value）型。这里开始介绍键值对型的算子，主要包括groupByKey、combineByKey、reduceByKey、sortByKey、cogroup和join，如下表所示： 方法名 方法定义 groupByKey def groupByKey(partitioner: Partitioner): RDD[(K, Iterable[V])] combineByKey def combineByKey[C](createCombiner: V =&gt; C, mergeValue: (C,V) =&gt; C, mergeCombiners: (C, C) =&gt; C) : RDD[(K, C)] reduceByKey def reduceByKey(partitioner: Partitioner, func: (V, V) =&gt; V): RDD[(K, V)] sortByKey def sortByKey(ascending: Boolean = true, numPartitions: Int = self.partitions.size): RDD[P] cogroup def cogroup[W](other: RDD[(K, W)], partitioner: Partitioner): RDD join def join[W](other: RDD[(K, W)], partitioner: Partitioner): RDD[(K, (V, W))] 2.2.1 groupByKey 类似groupBy方法，作用是把每一个相同Key值的的Value聚集起来形成一个序列，可以使用默认分区器和自定义分区器，但是这个方法开销比较大，如果想对同一Key进行Value的聚合或求平均，则推荐使用aggregateByKey或者reduceByKey。 示例： 12345678scala&gt; val a = sc.parallelize(List("mk", "zq", "xwc", "fjg", "dcp", "snn"), 2)a: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[41] at parallelize at &lt;console&gt;:24scala&gt; val b = a.keyBy(x =&gt; x.length)b: org.apache.spark.rdd.RDD[(Int, String)] = MapPartitionsRDD[42] at keyBy at &lt;console&gt;:27scala&gt; b.groupByKey.collectres23: Array[(Int, Iterable[String])] = Array((2,CompactBuffer(mk, zq)), (3,CompactBuffer(xwc, fjg, dcp, snn))) 2.2.2 combineByKey combineByKey方法能高效的将键值对形式的RDD按相同的Key把Value合并成序列形式，用户能自定义RDD的分区器和是否在map端进行聚合操作。 spark新版本中函数名更新为combineByKeyWithClassTag，为了兼容两个函数同时保留都可以使用。 如下为combineByKey的定义： 123456def combineByKey[C]( createCombiner: V =&gt; C, mergeValue: (C, V) =&gt; C, mergeCombiners: (C, C) =&gt; C): RDD[(K, C)] = self.withScope &#123; combineByKeyWithClassTag(createCombiner, mergeValue, mergeCombiners)(null) &#125; 解释下3个重要的函数参数： createCombiner: V =&gt; C ，这个函数把当前的值作为参数，此时我们可以对其做些附加操作(类型转换)并把它返回 (这一步类似于初始化操作) mergeValue: (C, V) =&gt; C，该函数把元素V合并到之前的元素C(createCombiner)上 (这个操作在每个分区内进行) mergeCombiners: (C, C) =&gt; C，该函数把2个元素C合并 (这个操作在不同分区间进行) 示例： 123456789101112131415scala&gt; val initialScores = Array(("Fred", 88.0), ("Fred", 95.0), ("Fred", 91.0), ("Wilma", 93.0), ("Wilma", 95.0), ("Wilma", 98.0))initialScores: Array[(String, Double)] = Array((Fred,88.0), (Fred,95.0), (Fred,91.0), (Wilma,93.0), (Wilma,95.0), (Wilma,98.0))scala&gt; val d1 = sc.parallelize(initialScores)d1: org.apache.spark.rdd.RDD[(String, Double)] = ParallelCollectionRDD[52] at parallelize at &lt;console&gt;:26scala&gt; type MVType = (Int, Double)defined type alias MVTypescala&gt; d1.combineByKey( | score =&gt; (1, score), | (c1: MVType, newScore) =&gt; (c1._1 + 1, c1._2 + newScore), | (c1: MVType, c2: MVType) =&gt; (c1._1 + c2._1, c1._2 + c2._2) | ).map &#123; case (name, (num, socre)) =&gt; (name, socre / num) &#125;.collectres27: Array[(String, Double)] = Array((Fred,91.33333333333333), (Wilma,95.33333333333333)) 上面是combineByKey来求解平均数的例子，解释一下参数： score =&gt; (1, score)，我们把分数作为参数,并返回了附加的元组类型。 以&quot;Fred&quot;为列，当前其分数为88.0 =&gt;(1,88.0) 1表示当前科目的计数器，此时只有一个科目； (c1: MVType, newScore) =&gt; (c1._1 + 1, c1._2 + newScore)，注意这里的c1就是createCombiner初始化得到的(1,88.0)。在一个分区内，我们又碰到了&quot;Fred&quot;的一个新的分数91.0。当然我们要把之前的科目分数和当前的分数加起来即c1._2 + newScore,然后把科目计算器加1即c1._1 + 1； (c1: MVType, c2: MVType) =&gt; (c1._1 + c2._1, c1._2 + c2._2)，注意&quot;Fred&quot;可能是个学霸,他选修的科目可能过多而分散在不同的分区中。所有的分区都进行mergeValue后,接下来就是对分区间进行合并了,分区间科目数和科目数相加分数和分数相加就得到了总分和总科目数。 ######2.2.3 reduceByKey 使用一个reduce函数来实现对相同Key的Value的聚集操作，在发送结果给reduce前会在map端的执行本地merge操作。该方法的底层实现就是调用combineByKey方法的一个重载方法。 示例： 1234567891011121314151617scala&gt; val list = sc.parallelize(List("dcp", "fjg", "snn", "wc", "zq"), 2)list: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[55] at parallelize at &lt;console&gt;:24scala&gt; val map = list.map(x =&gt; (x.length, x))map: org.apache.spark.rdd.RDD[(Int, String)] = MapPartitionsRDD[56] at map at &lt;console&gt;:27scala&gt; map.reduceByKey((a,b) =&gt; a + b).collectres28: Array[(Int, String)] = Array((2,wczq), (3,dcpfjgsnn))scala&gt; val a = sc.parallelize(List(3,12,124,32,5 ), 2)a: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[58] at parallelize at &lt;console&gt;:24scala&gt; val b = a.map(x =&gt; (x.toString.length, x))b: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[59] at map at &lt;console&gt;:27scala&gt; b.reduceByKey(_ + _).collectres29: Array[(Int, Int)] = Array((2,44), (1,8), (3,124)) 上面示例中先用map方法映射出键值对，然后调用reduceByKey方法对相同Key的Value值进行累加,因为第一个示例是字符串，所以结果为每个key中字符串的拼接，示例2中为数字，所以结果每个key中数字之和。 2.2.4 sortByKey 这个函数会根据Key值对键值对进行排序，如果Key是字母，则按字典顺序排序，如果Key是数字，则从小到大排序（或从大到小），该方法的第一个参数控制是否为升序排序，当为true时是升序，反之为降序。 示例： 1234567891011121314scala&gt; val a = sc.parallelize(List("dog", "cat", "owl", "gnu", "ant"), 2)a: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[61] at parallelize at &lt;console&gt;:24scala&gt; val b = sc.parallelize(1 to a.count.toInt, 2) #得到单词的字母个数b: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[62] at parallelize at &lt;console&gt;:26scala&gt; val c = a.zip(b)c: org.apache.spark.rdd.RDD[(String, Int)] = ZippedPartitionsRDD2[63] at zip at &lt;console&gt;:27scala&gt; c.sortByKey(true).collectres30: Array[(String, Int)] = Array((ant,5), (cat,2), (dog,1), (gnu,4), (owl,3))scala&gt; c.sortByKey(false).collectres31: Array[(String, Int)] = Array((owl,3), (gnu,4), (dog,1), (cat,2), (ant,5)) 2.2.5 cogroup cogroup是一个比较高效的函数，能根据Key值聚集最多3个键值对的RDD，把相同Key值对应的Value聚集起来。 示例： 1234567891011121314151617scala&gt; val a = sc.parallelize(List(1, 2, 2, 3, 1, 3), 1)a: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[78] at parallelize at &lt;console&gt;:24scala&gt; val b = a.map(x =&gt; (x, "b"))b: org.apache.spark.rdd.RDD[(Int, String)] = MapPartitionsRDD[79] at map at &lt;console&gt;:27scala&gt; val c = a.map(y =&gt; (y, "c"))c: org.apache.spark.rdd.RDD[(Int, String)] = MapPartitionsRDD[80] at map at &lt;console&gt;:27scala&gt; b.cogroup(c).collectres35: Array[(Int, (Iterable[String], Iterable[String]))] = Array((1,(CompactBuffer(b, b),CompactBuffer(c, c))), (3,(CompactBuffer(b, b),CompactBuffer(c, c))), (2,(CompactBuffer(b, b),CompactBuffer(c, c))))scala&gt; val d = a.map(m =&gt; (m, "x"))d: org.apache.spark.rdd.RDD[(Int, String)] = MapPartitionsRDD[83] at map at &lt;console&gt;:25scala&gt; b.cogroup(c, d).collectres36: Array[(Int, (Iterable[String], Iterable[String], Iterable[String]))] = Array((1,(CompactBuffer(b, b),CompactBuffer(c, c),CompactBuffer(x, x))), (3,(CompactBuffer(b, b),CompactBuffer(c, c),CompactBuffer(x, x))), (2,(CompactBuffer(b, b),CompactBuffer(c, c),CompactBuffer(x, x)))) 2.2.6 join 对键值对的RDD进行cogroup操作，然后对每个新的RDD下Key的值进行笛卡尔积操作，再对返回结果使用flatMapValues方法，最后返回结果。 示例： 1234567891011121314151617scala&gt; val a = sc.parallelize(List("fjg", "wc", "xwc","dcp"), 2)a: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[90] at parallelize at &lt;console&gt;:24scala&gt; val b = a.keyBy(_.length)//得到诸如（3，"fjg"），（2，"wc"）的键值对序列b: org.apache.spark.rdd.RDD[(Int, String)] = MapPartitionsRDD[91] at keyBy at &lt;console&gt;:25scala&gt; val c = sc.parallelize(List("fjg", "wc", "snn", "zq", "xwc","dcp"), 2)c: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[92] at parallelize at &lt;console&gt;:24scala&gt; val d = c.keyBy(_.length)d: org.apache.spark.rdd.RDD[(Int, String)] = MapPartitionsRDD[93] at keyBy at &lt;console&gt;:25scala&gt; b.cogroup(d).collectres1: Array[(Int, (Iterable[String], Iterable[String]))] = Array((2,(CompactBuffer(wc),CompactBuffer(wc, zq))), (3,(CompactBuffer(fjg, xwc, dcp),CompactBuffer(fjg, snn, xwc, dcp))))scala&gt; b.join(d).collectres38: Array[(Int, (String, String))] = Array((2,(wc,wc)), (2,(wc,zq)), (3,(fjg,fjg)), (3,(fjg,snn)), (3,(fjg,xwc)), (3,(fjg,dcp)), (3,(xwc,fjg)), (3,(xwc,snn)), (3,(xwc,xwc)), (3,(xwc,dcp)), (3,(dcp,fjg)), (3,(dcp,snn)), (3,(dcp,xwc)), (3,(dcp,dcp))) 上面的例子中，cogroup将两个RDD聚集起来，join的操作其实就是在cogroup的基础上做了笛卡尔积。 2.3 Action算子 当Spark的计算模型中出现Action算子时才会执行提交作业的runJob动作，这时会触发后续的DAGScheduler和TaskScheduler工作。这里主要讲解常用的Action算子，有collect、reduce、take、top、count、takeSample、saveAsTextFile、countByKey、aggregate，具体方法和定义如下表所示： Action算子 Action 算子作用 reduce(func) 令原RDD中的每个&#20540;依次经过函数func，func的类型为(T, T) =&gt; T，返回最终结果 collect() 将原RDD中的数据打包成数组并返回 count() 返回原RDD中数据的个数 first() 返回原RDD中的第一个数据项 take(n) 返回原RDD中前n个数据项，返回结果为数组 takeSample(withReplacement, num, [seed]) 对原RDD中的数据进行采样，返回num个数据项 saveAsTextFile(path) 将原RDD中的数据写入到文本文件当中 saveAsSequenceFile(path)(Java and Scala) 将原RDD中的数据写入到序列文件当中 savaAsObjectFile(path)(Java and Scala) 将原RDD中的数据序列化并写入到文件当中。可以通过SparkContext.objectFile()方法加载 countByKey() 原RDD数据的类型为(K, V)，返回hashMap(K, Int)，用于统计K出现的次数 foreach(func) 对于原RDD中的每个数据执行函数func，返回数组 2.3.1 collect 这个我们已经很熟悉了，上面使用很多次，collect方法的作用是把RDD中的元素以数组的方式返回。 示例： 12345scala&gt; val c = sc.parallelize(List("a", "b", "c", "d", "e", "f"), 2)c: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:24scala&gt; c.collectres0: Array[String] = Array(a, b, c, d, e, f) 2.3.2 reduce reduce方法使用一个带两个参数的函数把元素进行聚集，返回一个元素结果，注意该函数中的二元操作应该满足交换律和结合律，这样才能在并行系统中正确计算。 示例： 12345scala&gt; val a = sc.parallelize(1 to 10)a: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at parallelize at &lt;console&gt;:24scala&gt; a.reduce((a,b)=&gt; a + b)res1: Int = 55 2.3.3 take take方法会从RDD中取出前n[[1]](file:///C:/Users/admin/Desktop/%E5%86%99%E4%B9%A6/Spark%E4%B9%A6%E7%B1%8D%E7%AC%AC%E4%BA%8C%E6%AC%A1%E4%BF%AE%E6%94%B9%EF%BC%88%E6%9C%80%E7%BB%88%E7%A8%BF%EF%BC%892015-11-20.docx#_ftn1)个元素。方法是先扫描一个分区并后从分区中得到结果，然后评估得到的结果是否达到取出元素个数，如果没达到则继续从其他分区中扫描获取。 示例： 1234567891011scala&gt; val b = sc.parallelize(List("a", "b", "c", "d", "e"), 2)b: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[2] at parallelize at &lt;console&gt;:24scala&gt; b.take(2)res2: Array[String] = Array(a, b)scala&gt; val b = sc.parallelize(1 to 100, 5)b: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[3] at parallelize at &lt;console&gt;:24scala&gt; b.take(30)res3: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30) 2.3.4 top top方法会利用隐式排序转换方法（见实现源码中implicit修饰的方法）来获取最大的前n个元素。 示例： 12345scala&gt; val c = sc.parallelize(Array(1, 3, 2,4, 9, 2,11,5), 3)c: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[4] at parallelize at &lt;console&gt;:24scala&gt; c.top(3)res4: Array[Int] = Array(11, 9, 5) 2.3.5 count count方法计算并返回RDD中元素的个数。 示例： 12345scala&gt; val c = sc.parallelize(Array(1,3, 2,4, 9, 2,11,5), 2)c: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[6] at parallelize at &lt;console&gt;:24scala&gt; c.countres5: Long = 8 2.3.6 takeSample takeSample方法返回一个固定大小的数组形式的采样子集，此外还把返回的元素顺序随机打乱，方法的三个参数含义依次是否放回数据、返回取样的大小和随机数生成器的种子。 示例： 12345scala&gt; val x = sc.parallelize(1 to 100, 2)x: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[7] at parallelize at &lt;console&gt;:24scala&gt; x.takeSample(true, 30, 1)res6: Array[Int] = Array(4, 97, 99, 95, 46, 86, 16, 88, 55, 93, 77, 10, 69, 7, 48, 70, 22, 4, 17, 49, 25, 25, 100, 86, 2, 48, 70, 54, 41, 91) 2.3.7 saveAsTextFile 把RDD存储为文本文件，一次存一行。 示例： 1234scala&gt; val a = sc.parallelize(1 to 100, 3)a: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[9] at parallelize at &lt;console&gt;:24scala&gt; a.saveAsTextFile("1") 2.3.8 countByKey 类似count方法，不同的是countByKey方法会根据相同的Key计算其对应的Value个数，返回的是map类型的结果。 示例： 12345scala&gt; val a = sc.parallelize(List((1, "bit"), (2, "xwc"), (2, "fjg"), (3, "wc"),(3, "wc"),(3, "wc")), 2)a: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:24scala&gt; a.countByKeyres1: scala.collection.Map[Int,Long] = Map(2 -&gt; 2, 1 -&gt; 1, 3 -&gt; 3) 2.3.9 aggregate aggregate方法先将每个分区里面的元素进行聚合，然后用combine函数将每个分区的结果和初始值(zeroValue)进行combine操作。这个函数最终返回的类型不需要和RDD中元素类型一致。 aggregate有两个函数seqOp和combOp，这两个函数都是输入两个参数，输出一个参数，其中seqOp函数可以看成是reduce操作，combOp函数可以看成是第二个reduce操作（一般用于combine各分区结果到一个总体结果），由定义，combOp操作的输入和输出类型必须一致。 先看一下函数定义： 12345678910def aggregate[U: ClassTag](zeroValue: U)(seqOp: (U, T) =&gt; U, combOp: (U, U) =&gt; U): U = withScope &#123; // Clone the zero value since we will also be serializing it as part of tasks var jobResult = Utils.clone(zeroValue, sc.env.serializer.newInstance()) val cleanSeqOp = sc.clean(seqOp) val cleanCombOp = sc.clean(combOp) val aggregatePartition = (it: Iterator[T]) =&gt; it.aggregate(zeroValue)(cleanSeqOp, cleanCombOp) val mergeResult = (index: Int, taskResult: U) =&gt; jobResult = combOp(jobResult, taskResult) sc.runJob(this, aggregatePartition, mergeResult) jobResult &#125; seqOp函数是处理数据，combOp函数是拿到处理完成的数据做处理。aggregate相当于是传入了两个函数进去了。 示例： 12345678910111213141516171819202122232425262728293031// 分区0的reduce操作是max(0, 2,3) = 3// 分区1的reduce操作是max(0, 4,5) = 5// 分区2的reduce操作是max(0, 6,7) = 7// 最后的combine操作是0 + 3 + 5 + 7 = 15scala&gt; val z = sc.parallelize(List(2,3,4,5,6,7), 3)z: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[3] at parallelize at &lt;console&gt;:24scala&gt; z.aggregate(0)((a,b) =&gt; math.max(a, b), (c,d) =&gt; c + d )res2: Int = 15// 分区0的reduce操作是max(3, 2,3) = 3// 分区1的reduce操作是max(3, 4,5) = 5// 分区2的reduce操作是max(3, 6,7) = 7// 最后的combine操作是3 + 3 + 5 + 7 = 18scala&gt; z.aggregate(3)((a,b) =&gt; math.max(a, b), (c,d) =&gt; c + d )res4: Int = 18scala&gt; val z = sc.parallelize(List("a","b","c","d","e","f"),2)z: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[4] at parallelize at &lt;console&gt;:24scala&gt; z.aggregate("")(_ + _, _+_)res5: String = defabcscala&gt; val z = sc.parallelize(List("a","b","c","d","e","f"),2)z: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[5] at parallelize at &lt;console&gt;:24scala&gt; z.aggregate("x")(_ + _, _+_)res6: String = xxabcxdef 2.3.10 fold fold方法与aggregate方法原理类似，区别就是少了一个seqOp方法。fold方法是把每个分区的元素进行聚合，然后调用reduce（op）方法处理。 示例： 123456789101112131415161718// 分区0的reduce操作是0 + 1 + 2 + 3 = 6// 分区1的reduce操作是0 + 4 + 5 + 6 = 15// 分区2的reduce操作是0 + 7 + 8 + 9 = 24// 最后的combine操作是0 + 6 + 15 + 24 = 45scala&gt; val a = sc.parallelize(List(1,2,3,4,5,6,7,8,9), 3)a: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[6] at parallelize at &lt;console&gt;:24scala&gt; a.fold(0)(_ + _)res7: Int = 45// 分区0的reduce操作是1 + 1 + 2 + 3 = 7// 分区1的reduce操作是1 + 4 + 5 + 6 = 16// 分区2的reduce操作是1 + 7 + 8 + 9 = 25// 最后的combine操作是1 + 7 + 16 + 25 = 53scala&gt; val a = sc.parallelize(List(1,2,3,4,5,6,7,8,9), 3)a: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[7] at parallelize at &lt;console&gt;:24scala&gt; a.fold(1)(_ + _)res8: Int = 49 这个例子中的使用方式与aggregate方法非常相似，注意zeroValue参与所有分区计算。fold计算是保证每个分区能独立计算，它与aggregate最大的区别是aggregate对不同分区提交的最终结果定义了一个专门的comOp函数来处理，而fold方法是采用一个方法来处理aggregate的两个方法过程。 3. 共享变量 因为在tasks之间读写共享变量会很低效，spark提供两种类型的共享变量类型，即broadcast variables和accumulators。 3.1 广播变量 广播变量（Broadcast variables）允许用户将一个只读变量缓存到每一台机器之上，而不像传统变量一样，拷贝到每一个任务当中，同一台机器上的不同任务可以共享该变量值。如下面例子代码所示，对于变量v，只需要调用SparkContext.broadcast(v)即可得到变量v的广播变量broadcastVar，通过调用broadcastVar的value方法即可取得变量值。 示例： 1234scala&gt; val broadcastVar = sc.broadcast(Array(1, 2, 3))broadcastVar:spark.Broadcast[Array[Int]] = spark.Broadcast(b5c40191-a864-4c7d-b9bf-d87e1a4e787c)scala&gt; broadcastVar.valueres0: Array[Int] = Array(1, 2, 3) 3.2 累加器 累加器（Accumulators）是另外一种共享变量。累加器变量只能执行加法操作，但其支持并行操作，这意味着不同任务多次对累加器执行加法操作后，加法器最后的值等于所有累加的和。累加器的值只能被驱动程序访问，集群中的任务无法访问该值。 示例： 12345scala&gt; val accum = sc.accumulator(0, "My Accumulator")scala&gt; accum.value（） //(通过这种方法进行读取原始变量值)accum: spark.Accumulator[Int] = 0scala&gt; sc.parallelize(Array(1, 2, 3, 4)).foreach(x =&gt; accum += x)res2:Int = 10]]></content>
      <categories>
        <category>大数据学习</category>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RDD的设计与运行原理]]></title>
    <url>%2Fposts%2Fd422b66e.html</url>
    <content type="text"><![CDATA[Spark的核心是建立在统一的抽象RDD之上，使得Spark的各个组件可以无缝进行集成，在同一个应用程序中完成大数据计算任务 。 1. RDD设计背景 在实际应用中，存在许多迭代式算法（比如机器学习、图算法等）和交互式数据挖掘工具，这些应用场景的共同之处是，不同计算阶段之间会重用中间结果，即一个阶段的输出结果会作为下一个阶段的输入。但是，目前的MapReduce框架都是把中间结果写入到HDFS中，带来了大量的数据复制、磁盘IO和序列化开销。虽然，类似Pregel等图计算框架也是将结果保存在内存当中，但是，这些框架只能支持一些特定的计算模式，并没有提供一种通用的数据抽象。RDD就是为了满足这种需求而出现的，它提供了一个抽象的数据架构，我们不必担心底层数据的分布式特性，只需将具体的应用逻辑表达为一系列转换处理，不同RDD之间的转换操作形成依赖关系，可以实现管道化，从而避免了中间结果的存储，大大降低了数据复制、磁盘IO和序列化开销。 2.RDD概念 一个RDD就是一个分布式对象集合，本质上是一个只读的分区记录集合，每个RDD可以分成多个分区，每个分区就是一个数据集片段，并且一个RDD的不同分区可以被保存到集群中不同的节点上，从而可以在集群中的不同节点上进行并行计算。 RDD提供了一种高度受限的共享内存模型，即RDD是只读的记录分区的集合，不能直接修改，只能基于稳定的物理存储中的数据集来创建RDD，或者通过在其他RDD上执行确定的转换操作（如map、join和groupBy）而创建得到新的RDD。 RDD提供了一组丰富的操作以支持常见的数据运算，分为“行动”（Action）和“转换”（Transformation）两种类型，前者用于执行计算并指定输出的形式，后者指定RDD之间的相互依赖关系。两类操作的主要区别是： 转换操作（比如map、filter、groupBy、join等）接受RDD并返回RDD 行动操作（比如count、collect等）接受RDD但是返回非RDD（即输出一个值或结果）。 RDD提供的转换接口都非常简单，都是类似map、filter、groupBy、join等粗粒度的数据转换操作，而不是针对某个数据项的细粒度修改。因此，RDD比较适合对于数据集中元素执行相同操作的批处理式应用，而不适合用于需要异步、细粒度状态的应用，比如Web应用系统、增量式的网页爬虫等。正因为这样，这种粗粒度转换接口设计，会使人直觉上认为RDD的功能很受限、不够强大。但是，实际上RDD已经被实践证明可以很好地应用于许多并行计算应用中，可以具备很多现有计算框架（比如MapReduce、SQL、Pregel等）的表达能力，并且可以应用于这些框架处理不了的交互式数据挖掘应用。 RDD典型的执行过程如下： RDD读入外部数据源（或者内存中的集合）进行创建； RDD经过一系列的“转换”操作，每一次都会产生不同的RDD，供给下一个“转换”使用； 最后一个RDD经“行动”操作进行处理，并输出到外部数据源（或者变成Scala集合或标量）。 需要说明的是，RDD采用了惰性调用，即在RDD的执行过程中（如下图1所示），真正的计算发生在RDD的“行动”操作，对于“行动”之前的所有“转换”操作，Spark只是记录下“转换”操作应用的一些基础数据集以及RDD生成的轨迹，即相互之间的依赖关系，而不会触发真正的计算。 例如，在图2中，从输入中逻辑上生成A和C两个RDD，经过一系列“转换”操作，逻辑上生成了F（也是一个RDD），之所以说是逻辑上，是因为这时候计算并没有发生，Spark只是记录了RDD之间的生成和依赖关系。当F要进行输出时，也就是当F进行“行动”操作的时候，Spark才会根据RDD的依赖关系生成DAG，并从起点开始真正的计算。 上述这一系列处理称为一个“血缘关系（Lineage）”，即DAG拓扑排序的结果。采用惰性调用，通过血缘关系连接起来的一系列RDD操作就可以实现管道化（pipeline），避免了多次转换操作之间数据同步的等待，而且不用担心有过多的中间数据，因为这些具有血缘关系的操作都管道化了，一个操作得到的结果不需要保存为中间数据，而是直接管道式地流入到下一个操作进行处理。同时，这种通过血缘关系把一系列操作进行管道化连接的设计方式，也使得管道中每次操作的计算变得相对简单，保证了每个操作在处理逻辑上的单一性；相反，在MapReduce的设计中，为了尽可能地减少MapReduce过程，在单个MapReduce中会写入过多复杂的逻辑。 3.RDD之间的依赖关系 RDD中不同的操作会使得不同RDD中的分区会产生不同的依赖。RDD中的依赖关系分为窄依赖（Narrow Dependency）与宽依赖（Wide Dependency）： **宽依赖：**父RDD的分区被子RDD的多个分区使用 , 例如 groupByKey、reduceByKey、sortByKey等操作会产生宽依赖，会产生shuffle； **窄依赖：**父RDD的每个分区都只被子RDD的一个分区使用 例如map、filter、union等操作会产生窄依赖； 4.阶段的划分 Spark通过分析各个RDD的依赖关系生成了DAG，再通过分析各个RDD中的分区之间的依赖关系来决定如何划分阶段，具体划分方法是：在DAG中进行反向解析，遇到宽依赖就断开，遇到窄依赖就把当前的RDD加入到当前的阶段中；将窄依赖尽量划分在同一个阶段中，可以实现流水线计算 。例如，如图4所示，假设从HDFS中读入数据生成3个不同的RDD（即A、C和E），通过一系列转换操作后再将计算结果保存回HDFS。对DAG进行解析时，在依赖图中进行反向解析，由于从RDD A到RDD B的转换以及从RDD B和F到RDD G的转换，都属于宽依赖，因此，在宽依赖处断开后可以得到三个阶段，即阶段1、阶段2和阶段3。可以看出，在阶段2中，从map到union都是窄依赖，这两步操作可以形成一个流水线操作，比如，分区7通过map操作生成的分区9，可以不用等待分区8到分区9这个转换操作的计算结束，而是继续进行union操作，转换得到分区13，这样流水线执行大大提高了计算的效率。 由上述论述可知，把一个DAG图划分成多个“阶段”以后，每个阶段都代表了一组关联的、相互之间没有Shuffle依赖关系的任务组成的任务集合。每个任务集合会被提交给任务调度器（TaskScheduler）进行处理，由任务调度器将任务分发给Executor运行。 5.RDD运行过程 通过上述对RDD概念、依赖关系和阶段划分的介绍，结合之前介绍的Spark运行基本流程，这里再总结一下RDD在Spark架构中的运行过程： （1）创建RDD对象； （2）SparkContext负责计算RDD之间的依赖关系，构建DAG； （3）DAGScheduler负责把DAG图分解成多个阶段，每个阶段中包含了多个任务，每个任务会被任务调度器分发给各个工作节点（Worker Node）上的Executor去执行。 6.RDD转换操作 转换（Transformation）操作是由一个RDD转换到另一个新的RDD，例如，map操作在RDD中是一个转换操作，map转换会让RDD中的每一个数据都通过一个指定函数得到一个新的RDD。 RDD内部可以封装任意类型的数据，但某些操作只能应用在封装键值对类型数据的RDD之上，例如转换操作reduceByKey、groupByKey和countByKey等下表提供了所有转换操作： 表1：RDD提供的转换操作 Transformation 算子作用 map(func) 新RDD中的数据由原RDD中的每个数据通过函数func得到 filter(func) 新RDD种的数据由原RDD中每个能使函数func返回true&#20540;的数据组成 flatMap(func) 类&#20284;于map转换，但func的返回&#20540;是一个Seq对象，Seq中的元素个数可以是0或者多个 mapPartitions(func) 类&#20284;于map转换，但func的输入不是一个数据项，则是一个分区，若RDD内数据类型为T，则func必须是Iterator&lt;T&gt; =&gt; Iterator&lt;U&gt;类型 mapPartitionsWithIndex(func) 类&#20284;于mapPartitions转换，但func的数据还多了一个分区索引，即func类型是(Int, Iterator&lt;T&gt; =&gt; Iterator&lt;U&gt;) sample(withReplacement, fraction, seed) 对fraction中的数据进行采样，可以选择是否要进行替换，需要提供一个随机数种子 union(otherDataset) 新RDD中数据是原RDD与RDD otherDataset中数据的并集 Intersection(otherDataset) 新RDD中数据是原RDD与RDD otherDataset中数据的交集 distinct([numTasks]) 新RDD中数据是原RDD中数据去重的结果 groupByKey([numTasks]) 原RDD中数据类型为(K, V)对，新RDD中数据类型为(K, Iterator(V))对，即将相同K的所有V放到一个迭代器中 reduceByKey(func, [numTasks]) 原RDD和新RDD数据的类型都为(K, V)对，让原RDD相同K的所有V依次经过函数func，得到的最终&#20540;作为K的V aggregateByKey(zeroValue)(seqOp, combOp, [numTasks]) 原RDD数据的类型为(K, V)，新RDD数据的类型为(K, U)，类&#20284;于groupbyKey函数，但聚合函数由用户指定。键&#20540;对的&#20540;的类型可以与原RDD不同 sortByKey([ascending], [numTasks]) 原RDD和新RDD数据的类型为(K, V)键&#20540;对，新RDD的数据根据ascending的指定顺序或者逆序排序 join(otherDataset, [numTasks]) 原RDD数据的类型为(K, V)，otherDataset数据的类型为(K, W)，对于相同的K，返回所有的(K, (V, W)) cogroup(otherDataset, [numTasks]) 原RDD数据的类型为(K, V)，otherDataset数据的类型为(K, W)，对于相同的K，返回所有的(K, Iterator&lt;V&gt;, Iterator&lt;W&gt;) catesian(otherDataset) 原RDD数据的类型为为T，otherDataset数据的类型为U，返回所有的(T, U) pipe(command, [envValue]) 令原RDD中的每个数据以管道的方式依次通过命令command，返回得到的标准输出 coalesce(numPartitions) 减少原RDD中分区的数目至指定&#20540;numPartitions repartition(numPartitions) 修改原RDD中分区的数目至指定&#20540;numPartitions 7.RDD动作操作 相对于转换，动作（Action）操作用于向驱动（Driver）程序返回值或者将值写入到文件当中。例如reduce动作会使用同一个指定函数让RDD中的所有数据做一次聚合，把运算的结果返回。 表2 RDD动作操作 Action 算子作用 reduce(func) 令原RDD中的每个&#20540;依次经过函数func，func的类型为(T, T) =&gt; T，返回最终结果 collect() 将原RDD中的数据打包成数组并返回 count() 返回原RDD中数据的个数 first() 返回原RDD中的第一个数据项 take(n) 返回原RDD中前n个数据项，返回结果为数组 takeSample(withReplacement, num, [seed]) 对原RDD中的数据进行采样，返回num个数据项 saveAsTextFile(path) 将原RDD中的数据写入到文本文件当中 saveAsSequenceFile(path)(Java and Scala) 将原RDD中的数据写入到序列文件当中 savaAsObjectFile(path)(Java and Scala) 将原RDD中的数据序列化并写入到文件当中。可以通过SparkContext.objectFile()方法加载 countByKey() 原RDD数据的类型为(K, V)，返回hashMap(K, Int)，用于统计K出现的次数 foreach(func) 对于原RDD中的每个数据执行函数func，返回数组 8.惰性计算 需要注意的是，一个RDD执行转换操作之后，数据的计算是延迟的，新生成的RDD会记录转换的相关信息，包括父RDD的编号、用户指定函数等等，但并不会立即执行计算操作，真正的计算操作过程得等到遇到一个动作操作（Action）才会执行，此外，除非用户指定持久化操作，否则转换过程中产生的中间数据在计算完毕后会被丢弃，即数据是非持久化。即使对同一个RDD执行相同的转换操作，数据同样会被重新计算。 Spark采取惰性计算机制有其道理所在。例如可以实现通过map方法创建的一个新数据集，然后使用reduce方法，最终只返回 reduce 的结果给driver，而不是整个大的新数据集。 9.RDD持久化 惰性计算的缺陷也是明显的：中间数据默认不会保存，每次动作操作都会对数据重复计算，某些计算量比较大的操作可能会影响到系统的运算效率，因此Spark允许将转换过程中手动将某些会被频繁使用的RDD执行持久化操作，持久化后的数据可以被存储在内存、磁盘或者Tachyon当中，这将使得后续的动作(Actions)变得更加迅速（通常快10倍）。 通过调用RDD提供的cache或persist函数即可实现数据的持久化，persist函数需要指定存储级别（StorageLevel），cache等价于采用默认存储级别的persist函数，Spark提供的存储级别及其含义如表3所示： 表3 RDD的存储级别 存储级别 含义 MEMORY_ONLY 把RDD以非序列化状态存储在内存中，如果内存空间不够，则有些分区数据会在需要的时候进行计算得到 MEMORY_AND_DISK 把RDD以非序列化存储在内存中，如果内存空间不够，则存储在硬盘中 &nbsp; MEMORY_ONLY_SER 把RDD以Java对象序列化储存在内存中，序列化后占用空间更小，尤其当使用快速序列化库（如Kyro[1]）时效果更好。缺点是读数据要反序列化，会消耗CPU计算资源 MEMORY_AND_DISK_SER 类&#20284;MEMORY_ONLY_SER，区别是当内存不够的时候会把RDD持久化到磁盘中，而不是在需要它们的时候实时计算 DISK_ONLY 只把RDD存储到磁盘中 MEMORY_ONLY_2, 类&#20284;MEMORY_ONLY，不同的是会复制一个副本到另一个集群节点 MEMORY_AND_DISK_2, etc. 类&#20284;MEMORY_AND_DISK，不同的是会复制一个副本到另一个集群节点 &nbsp; OFF_HEAP 把RDD以序列化形式存储在Tachyon中，与MEMORY_ONLY_SER不同的是，使用OFF-HEAP模式会减少垃圾回收的开销，此外还能让执行器共享内存，这种模式更适应于多并发和对内存要求高的环境 10.RDD检查点 因为DAG中血统（lineage）如果太长，当重计算的时候开销会很大，故使用检查点机制，将计算过程持久化到磁盘，这样如果出现计算故障的时候就可以在检查点开始重计算，而不需要从头开始。RDD的检查点（Checkpoint）机制类似持久化机制中的persist(StorageLevel.DISK_ONLY)，数据会被存储在磁盘当中，两者最大的区别在于：持久化机制所存储的数据，在驱动程序运行结束之后会被自动清除；检查点机制则会将数据永久存储在磁盘当中，如果不手动删除，数据会一直存在。换句话说，检查点机制存储的数据能够被下一次运行的应用程序所使用。 检查点的使用与持久化类似，调用RDD的checkpoint方法即可。]]></content>
      <categories>
        <category>大数据学习</category>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark集群环境搭建]]></title>
    <url>%2Fposts%2F9d4156ed.html</url>
    <content type="text"><![CDATA[1. 试验环境 jdk：1.8 Scala：2.12.6 hadoop：2.7.3 Spark：spark-2.3.0-bin-hadoop2.7.tgz 2.系统规划 3个节点，一个master，2个slave。 Master节点服务角色：namenode、SecondNameNodeResourceManager Slave节点服务角色：datanode、NodeManager 3.安装Spark前必备： Spark依赖的服务比较多。确保在安装Spark之前已经安装了如下： jdk(注意版本) Scala Hadoop 4.安装 默认已经完成上面程序的安装，直接安装Spark。 首先将下载的Spark包上传至服务器，并且解压缩。解压缩完毕，配置环境变量： #vi /etc/profile export SPARK_HOME=/usr/local/spark export PATH=$SPARK_HOME/bin:$PATH 使环境变量生效: source /etc/profile 配置完环境变量之后开始配置文件,进入conf文件夹下： cp slaves.template slaves cp spark-env.sh.template spark-env.sh 然后准备配置这两个文件： vim conf/spark-env.sh 在最后加入： export JAVA_HOME=/usr/java/jdk export SCALA_HOME=/usr/local/scala export HADOOP_HOME=/usr/local/hadoop export HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop export SPARK_MASTER_IP=hadoopmaster export SPARK_WORKER_MEMORY=1g export SPARK_WORKER_CORES=2 export SPARK_WORKER_INSTANCES=1 变量说明 JAVA_HOME：Java安装目录 SCALA_HOME：Scala安装目录 HADOOP_HOME：hadoop安装目录 HADOOP_CONF_DIR：hadoop集群的配置文件的目录 SPARK_MASTER_IP：spark集群的Master节点的ip地址 SPARK_WORKER_MEMORY：每个worker节点能够最大分配给exectors的内存大小 SPARK_WORKER_CORES：每个worker节点所占有的CPU核数目 SPARK_WORKER_INSTANCES：每台机器上开启的worker节点的数目 修改slaves文件： vim slaves 删除原来的location，加入两台slaver的机器名： hadoopslaver1 hadoopslaver2 以上是在master上配置的，需要将如上配置同步到两台slaver上，使用scp命令： scp -r spark hadoop@hadoopslaver1:/usr/local scp -r spark hadoop@hadoopslaver2:/usr/local 同步文件夹需要加&quot;-r&quot;。 以上我们就做好了spark的安装配置，下面就可以启动： Spark需要用到hadoop的HDFS存储,所以正式使用的时候需要启动hadoop的HDFS： start-dfs.sh 启动Spark： sh start-all.sh 成功打开Spark集群之后可以进入Spark的WebUI界面，可以通过: http://192.168.131.128:8080/ 打开Spark-shell 使用命令： sh spark-shell Spark-shell可以查看当前正在执行的任务,可以通过如下端口来访问： http://192.168.131.128:4040/jobs/]]></content>
      <categories>
        <category>大数据学习</category>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hadoop学习(九)----hadoop HA环境搭建]]></title>
    <url>%2Fposts%2Fa5e8b95c.html</url>
    <content type="text"><![CDATA[我们先回忆一下hadoop在完全分布式环境中运行的基本知识： HDFS集群有两类节点以管理者-工作者模式运行：一个namenode和多个datanode。namenode管理文件系统的命名空间，维护着文件系统树以及整棵树内所有的文件和目录，这些信息以两个文件形式永久的存储在本地磁盘上：命令空间镜像(fsimage)和编辑日志文件(edits)。 namenode也记录着每个文件中的各个块所在的数据节点信息，但它并不永久的保存块的位置信息，因为这些信息在系统启动时由数据节点重建。 客户端代表用户通过与namenode和datanode交互来访问整个文件系统。客户端提供一个提供文件系统接口来访问数据，因此用户在编程时无需知道namenode和datanode也可以实现相应的操作功能。 datanode时文件系统的工作节点。他们被客户端或者namenode调度去触发存储或检索数据块，并定期向namenode发送他们所存储的块的列表信息。 1.hadoop HA和Federation 如果没有namenode，hdfs文件系统将无法使用。如果运行namenode服务的机器被destroy，后果就是文件系统上的所有文件将会成为废数据：因为你根本就不知道如何去重建datanode。基于此对namenode实现容错非常重要，hadoop为此提供了两种机制。 1.1 hadoop2.0之前的单点故障解决方案 secondary namenode（以下简称SNN）。SNN并不是HA，它本身所起的作用是阶段性的合并edits和fsimage以缩短集群启动的时间。因为SNN是在每次checkpoint的时候触发合并，如果namenode宕机SNN肯定会丢失一段时间的数据，这段时间取决于checkpoint的周期。所以这种方式无法保证数据完整性。 NFS方案(网络文件系统)。设置多个data目录，让namenode在持久化元数据的时候同时同时写入多个目录。这种方案在院里上是可行的。 1.2 hadoop2.0单点故障解决方案 Hadoop官方提供了一种quorum journal manager来实现高可用，在高可用配置下，edit log不再存放在名称节点，而是存放在一个共享存储的地方，这个共享存储由若干Journal Node组成，一般是3个节点(JN小集群)， 每个JN专门用于存放来自NN的编辑日志，编辑日志由活跃状态的名称节点写入。 要有2个NN节点，二者之中只能有一个处于活跃状态（active），另一个是待命状态（standby），只有active节点才能对外提供读写HDFS服务，也只有active态的NN才能向JN写入编辑日志；standby的名称节点只负责从JN小集群中的JN节点拷贝数据到本地存放。另外，各个datanode也要同时向两个namenode节点报告状态(心跳信息、块信息)。 一主一从的2个NameNode节点同时和3个JN构成的组保持通信，活跃的NameNode节点负责往JN集群写入编辑日志，待命的NN节点负责观察JN组中的编辑日志,并且把日志拉取到待命节点（接管Secondary NameNode的工作）。再加上两节点各自的fsimage镜像文件，这样一来就能确保两个NN的元数据保持同步。一旦active不可用，standby继续对外提供服。架构分为手动模式和自动模式，其中手动模式是指由管理员通过命令进行主备切换，这通常在服务升级时有用，自动模式可降低运维成本，但存在潜在危险。这两种模式下的架构如下。 【手动模式】 【自动模式】 图片来源于：HDFS-1623 作者：Sanjay Radia, Suresh Srinivas Yahoo! Inc 以上两种模式都是基于共享存储系统，active master将信息写入sharedStorage，standby读取信息保持与mastr同步。常用的共享文件系统有zookeeper(被YARN HA采用)，NFS(被HDFS HA采用)，HDFS(被MapReduce HA采用)。 自动方式中采用了ZKFailoverController，是基于zk实现的切换控制器，主要由两个核心组件构成：ActiveStandByElector和HealthMonitor，中，ActiveStandbyElector负责与zookeeper集群交互，通过尝试获取全局锁，以判断所管理的master进入active还是standby状态；HealthMonitor负责监控各个活动master的状态，以根据它们状态进行状态切换。 datanode需要同时向两个namenode汇报，这是让standby随时保持集群最新状态的必要步骤。 隔离机制(Fencing),防止脑裂。脑裂是指在主备切换时，由于切换不彻底或其他原因，导致客户端和Slave误以为出现两个active master，最终使得整个集群处于混乱状态。 隔离机制一般包括三个方面： 共享存储fencing：确保只有一个Master往共享存储中写数据。 客户端fencing：确保只有一个Master可以响应客户端的请求。 Slave fencing：确保只有一个Master可以向Slave下发命令。 Hadoop公共库中对外提供了两种fencing实现，分别是sshfence和shellfence（缺省实现），其中sshfence是指通过ssh登陆目标Master节点上，使用命令fuser将进程杀死（通过tcp端口号定位进程pid，该方法比jps命令更准确），shellfence是指执行一个用户事先定义的shell命令（脚本）完成隔离。 1.3 hadoop2.0 里的Federation实现 图片来源： HDFS-1052 设计文档 根据文档大致理解他的设计： 多个namenode共用一个集群里的datanode的资源，每个datanode都可以单独对外提供服务； 每个namenode都会定义一个存储池，有单独的id，每个datanode都会为所有的存储池提供服服务； datanode会按照存储池id向其对应的namenode汇报信息，同时datanode也会向所有namenode汇报本地存储资源可用情况； 如果需要在客户端方便的访问若干个namende上的资源，可以使用客户端挂载表，把不同的目录映射到不同的namenode上去，但前提是namenode必须存在相应的目录。 2. 搭建我们的 HA hadoop集群 前面我们已经搭建过权完全分布的hadoop集群。HA的搭建引入了JournalNode节点，所以需要修改之前的配置文件，我们先把需要修改的部分贴出来，这次我们的集群规划是： zookeeper集群： 192.168.238.128 hadoopmaster 192.168.238.129 hadoopslaver1 192.168.238.130 hadoopslaver2 hadoop集群： 192.168.238.128 hadoopmaster namenodeMaster节点 ResourceManager节点 Journalnode节点 192.168.238.129 hadoopslaver1 namenodeStandBy节点 ResourceManager节点 Journalnode节点 192.168.238.130 hadoopslaver2 datanode节点 nodemanager1 192.168.238.131 hadoopslaver3 datanode节点 nodemanager1 下面看需要修改的配置文件： core-site.xml &lt;configuration&gt; &lt;!-- 指定hdfs的nameservice为ns1 --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://ns1&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定HDFS数据存放路径，默认存放在linux的/tmp目录中 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/usr/local/hadoop/tmp&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定zookeeper的地址 --&gt; &lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;hadoopmaster:2181,hadoopslaver1:2181,hadoopslaver2:2181&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 之前配置的value是：hdfs://机器名:9000,现在改为名称服务，会对应到hdfs-site.xml中的配置。 hdfs-site.xml，配置这个nameservice中有几个namenode： &lt;configuration&gt; &lt;!--指定hdfs的nameservice为ns1，需要和core-site.xml中的保持一致 --&gt; &lt;property&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;ns1&lt;/value&gt; &lt;/property&gt; ​ &lt;!-- ns1下面有两个NameNode，分别是nn1，nn2 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.namenodes.ns1&lt;/name&gt; &lt;value&gt;nn1,nn2&lt;/value&gt; &lt;/property&gt; ​ &lt;!-- nn1的RPC通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.ns1.nn1&lt;/name&gt; &lt;value&gt;hadoopmaster:9000&lt;/value&gt; &lt;/property&gt; ​ &lt;!-- nn1的http通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.ns1.nn1&lt;/name&gt; &lt;value&gt;hadoopmaster:50070&lt;/value&gt; &lt;/property&gt; ​ &lt;!-- nn2的RPC通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.ns1.nn2&lt;/name&gt; &lt;value&gt;hadoopslaver1:9000&lt;/value&gt; &lt;/property&gt; ​ &lt;!-- nn2的http通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.ns1.nn2&lt;/name&gt; &lt;value&gt;hadoopslaver1:50070&lt;/value&gt; &lt;/property&gt; ​ &lt;!-- 指定NameNode的日志在JournalNode上的存放位置 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt; &lt;value&gt;qjournal://hadoopmaster:8485;hadoopslaver1:8485;/ns1&lt;/value&gt; &lt;/property&gt; ​ &lt;!-- 指定JournalNode在本地磁盘存放数据的位置 --&gt; &lt;property&gt; &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt; &lt;value&gt;usr/local/hadoop/journal&lt;/value&gt; &lt;/property&gt; ​ &lt;!-- 开启NameNode失败自动切换 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; ​ &lt;!-- 配置失败自动切换实现方式 --&gt; &lt;property&gt; &lt;name&gt;dfs.client.failover.proxy.provider.ns1&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt; &lt;/property&gt; ​ &lt;!-- 配置隔离机制方法，多个机制用换行分割，即每个机制暂用一行--&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt; &lt;value&gt; sshfence shell(/bin/true) &lt;/value&gt; &lt;/property&gt; &lt;!-- 使用sshfence隔离机制时需要ssh免登陆 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt; &lt;value&gt;/root/.ssh/id_rsa&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置sshfence隔离机制超时时间 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.connect-timeout&lt;/name&gt; &lt;value&gt;30000&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; mapred-site.xml: &lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; yarn-site.xml: &lt;configuration&gt; &lt;!-- 开启RM高可靠 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定RM的cluster id --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt; &lt;value&gt;yrc&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定RM的名字 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt; &lt;value&gt;rm1,rm2&lt;/value&gt; &lt;/property&gt; &lt;!-- 分别指定RM的地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt; &lt;value&gt;hadoopmaster&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt; &lt;value&gt;hadoopslaver1&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定zk集群地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt; &lt;value&gt;hadoopmaster:2181,hadoopslaver1:2181,hadoopslaver2:2181&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 将配置好的HA分发到其余机器上。 下面我们来启动HA集群： 启动zookeeper集群： 在每一台机器上输入： zkServer.sh start 启动journalnode: 在hadoopmaster和hadooslaver1上启动journalnode节点： hadoop-daemon.sh start journalnode 格式化HDFS和zookeeper： 格式化HDFS: hdfs namenode -format 格式化zookeeper： hdfs zkfc -formatZK 以上操作完毕我们就可以启动hadoop 集群了： start-all.sh 在hadoopslaver1上启动resourceManager作为Yarn的备用主节点： yarn-daemon.sh start resourcemanager]]></content>
      <categories>
        <category>大数据学习</category>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hadoop学习(八)----mapReduce的工作机制]]></title>
    <url>%2Fposts%2Fde556527.html</url>
    <content type="text"><![CDATA[前面我们分析了MapReduce的运行过程，对mapReduce的执行流程有了一个大致的了解。那么这一节我们开始看MapReduce的工作机制，顺带实现一下MapReduce编程。 MapReduce的工作机制 前面我们是从数据流的角度解释了数据从输入到HDFS到mapReduce是如何流向的，那么具体到一个MapReduce作业到底是怎么样的一个运作过程呢。又得祭出这一张经典的图： 上图大体介绍了MapReduce的执行过程。我们就上面的过程简要介绍一下。 客户端提交作业到JobTracker，即提交我们实现好了的Mapper、Reducer类，那么job类是如何提交到JobTracker的呢，大致过程如下： 上图中，首先由JobClient的SubJobInternal方法提交作业（步骤1），并且在waitForCompletion方法中调用monitorAndPrintJob方法轮询作业进度，作业进度信息会输出到控制台。 在JobClient的submitInternal方法中，通过调用JobTracker的getNewJobId方法向JobTracker请求一个作业id（步骤2），然后检查作业输出目录和输入分片是否符合要求，将运行作业所需要的资源包括jar包复制到HDFS下指定的目录（步骤3），通过调用JobTracker的submitJob方法通知JobTracker作业准备执行（&lt;font color=#7FFF00 size=4步骤4）。 当JobTracker收到其对submitJob方法的调用之后，会将调用交给作业调度器进行调度并初始化，创建一个表示正在运行作业的对象（步骤5）； 为了给TaskTracker分配任务，必须先从HDFS系统中获得已经计算好的输入分片信息（步骤6）； 然后为每个分片创建一个Map任务，而创建的Reduce任务的个数由mapred-site.xml文件的mapred.reduce.tasks配置决定，默认是l，可通过setNumReduceTasks方法针对每个作业设置。此时,Map任务和Reduce任务的任务ID 将被指定。 JobTracker和TaskTracker之间采用push通信模型，即JobTracker不会主动与TaskTracker通信，而是被动等待TaskTasker通过心跳告知JobTracker是否存活以及资源状态等。如果TaskTracker通过心跳信息告知JobTracker已经准备好运行新任务，那么JobTracker将会通过心跳返回值为TaskTracker分配一个任务（步骤7）。 TaskTracker在接收到启动任务的命令之后，会把作业相关的jar包复制到TaskTracker所在节点的本地目录（步骤8）,由mapred-site.xml文件的mapred.local.dir配置。 然后TaskTracker会新建一个TaskRunner实例来运行任务，TaskRuner启动一个JVM（步骤9）运行每个任务（步骤10），即每一个任务都是一个单独的容器在执行，任务之间是不会有影响的。 当JobTracker收到最后一个任务已经完成的通知后，会把作业状态设置为成功。JobClient查询状态时会将作业完成的消息打印在控制台，最后JobTracker会清空作业的工作状态并通知TaskTracker也清空作业的工作状态。 MapReduce关键类介绍 1 Writable类 序列化对于hadoop是必备的操作，从文件中读取数据然后通过序列化转换为&lt;key,value&gt;存储到HDFS中。这一切离不开Writable接口以及其实现类的操作。 上面源码中看到Writable只是一个接口，hadoop的序列化操作无非就是上一篇中提到的7种基本数据类型，我们可以随便点开一个： IntWritable实现的是WritableComparable接口，接着看WritableComparable接口： 可以看到WritableComparable接口继承了Writable接口，单独作为基本数据类型的实现接口。 接口中有两个方法： write(DataOutput out) 将对象写入DataOutput二进制流 readFields(DataInput in) 从DataInput二进制流中反序列化 实际开发过程中也许hadoop提供的Writable不一定能满足我们的需要，那么我们可以自己编写自己的Writable类。从上面的分析可知编写自己的Writable类需要实现WritableComparable接口而不是Writable接口。比如我们想写一个数据类型存储文件的名字和类型作为一对字符串： import org.apache.hadoop.io.Text; import org.apache.hadoop.io.WritableComparable; import java.io.DataInput; import java.io.DataOutput; import java.io.IOException; ​ public class FilePair implements WritableComparable { private Text fileName; private Text fileType; public FilePair(String fileName,String fileType){ set(fileName,fileType); } public FilePair(Text fileName,Text fileType){ this.fileName = fileName; this.fileType = fileType; } private void set(String fileName,String fileType){ this.fileName =+- new Text(fileName); this.fileType = new Text(fileType); } public Text getFileName() { return fileName; } public void setFileName(Text fileName) { this.fileName = fileName; } public Text getFileType() { return fileType; } public void setFileType(Text fileType) { this.fileType = fileType; } @Override public int compareTo(FilePair o) { return 0; } @Override public void write(DataOutput dataOutput) throws IOException { fileName.write(dataOutput); fileType.write(dataOutput); } @Override public void readFields(DataInput dataInput) throws IOException { fileName.readFields(dataInput); fileType.readFields(dataInput); } } 大致的实现如上代码所示，当然如果你有需要还可以去实现compareTo方法和equals方法。 2 Mapper类和Reducer类 在MapReduce任务中Mapper类的作用是将任务处理成键值对的形式，即做数据处理然后交给reducer去做计算和归并。 Mapper类中有 setup、map、cleanup、和run四个方法，setup一般用来进行map的预处理工作；map是主方法，将数据包装我们需要的形式；cleanup则是执行收尾的工作比如关闭文件或者是执行map后的键值对分发；run方法则是提供了依次setup–map–cleanup的执行模板。 Hadoop 自带了一些Mapper类的实现，如InverseMapper类和TokenCounterMapper 类。InverseMapper类的作用是调换键值对的顺序再原样输出TokenCounterMapper 类的作用和WordCount中的Mapper类的作用是一样的，单词计数，读者可以根据需要选择。如果需要自己编写Mapper 类时，用户需要做的只是继承Mapper 类并实现其中的map 方法即可。 reducer类承担的任务是实现数据处理逻辑，用法和Mapper类相似就不说。 3 shuffle 前面在写wordCount程序的时候我们只用到了Mapper和Reducer两个过程。Map是映射负责数据的过滤和分发，Reducer是处理，负责数据的计算和合并。简单的逻辑在Map阶段我们就可以完成，但是如果有复杂的逻辑Map阶段是无法支撑的，还记得我们在前面说MapReduce原理的时候画过一个MapReduce过程图： map到reduce之间其实还有一个shuffle阶段。根据进程的阶段我们将shuffle过程分为两个阶段：map shuffle和reduce shuffle。 map阶段将数据初始化处理为键值对，以内存缓冲的方式存到内存并在内存中进行分区，排序后，缓冲区的使用量达到一定比例以后就会开启一个后台线程开始把缓冲区数据写入磁盘，这个写的过程叫做spill。spill的buffer比例默认为0.8，可以通过mapreduce.map.sort.spill.percent配置。在后台线程写入的同时，map继续将输出写入这个环形缓冲，如果缓冲池写满了，map会阻塞直到spill过程完成，而不会覆盖缓冲池中的已有的数据。 上面说到了将数据进行分区，这个分区的作用是什么呢：即决定将数据交给哪个reduce处理。hadoop处理分区的方法为Partitioner，他是一个抽象类： 类里面的方法：getPartition()返回的是分区号。之前在wordCount中我们并没有指定Partitioner，这能说明wordCount就没有做shuffle吗？其实并不是，因为Partitioner提供了两个Partitioner的子类：HashPartitioner和TotalOrderPartitioner，Hadoop默认会使用HashPartitioner类。 我们能看到HashPartitioner中的getPartition()第三个参数：numReduceTasks，该参数为Reduce Task的数量允许用户自定义。 前面我们也提到在MapReduce任务中一共发生了三次排序，上面从环形缓冲区写到磁盘之间做了一次，第二次发生在什么时候呢？ 我们知道如果map任务输出的数据量很大可能会进行好几次spill，相应产生的输出文件也很多会分布在不同的磁盘上，为了给后面的reduce省下工作量，所以在该阶段做了一次merge。因为spill产生的文件分为out文件和index文件，out文件即map数据，index文件即文件位置索引信息。那么每次做合并操作的时候只需要去扫描这些index文件即可以找到out文件，然后一个partition一个partition的进行合并和排序并生成一个最终的out和index文件。 以上就是map端的suffle，下面说reduce端的suffle。 reduce任务通过http请求去询问各个Map里面是否有他要的数据，如果map正好是该reduce对应的数据reduce则将改数据拖到内存中等待处理，reduce要去询问每一个map然后把数据拖到内存中，当内存中的Map数据占用空间达到一定程度时开始启动内存中的merge，把内存中的merge输出到磁盘中保存。 如果内存中放不下map数据的话则直接把map数据写到磁盘上，有一个map就会创建一个文件，当文件的数量达到阈值时开始启动磁盘文件的merge，把这些文件合并输出到一个文件。 还有第三种情况是有些map的数据量小时在内存中存放，而有些map太大是存放在磁盘里，那么最后reduce任务会对两个部分的数据来一个总的合并。 当然在合并的过程中也是在做sort，一般reduce任务都是在拖取map数据的时候就开始做sort，这就是mapreduce的第三次排序。]]></content>
      <categories>
        <category>大数据学习</category>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hadoop学习(七)----mapReduce原理以及操作过程]]></title>
    <url>%2Fposts%2Fb8ffaf86.html</url>
    <content type="text"><![CDATA[前面我们使用HDFS进行了相关的操作，也了解了HDFS的原理和机制，有了分布式文件系统我们如何去处理文件呢，这就的提到hadoop的第二个组成部分-MapReduce。 MapReduce充分借鉴了分而治之的思想，将一个数据的处理过程分为Map（映射）和Reduce（处理）两步。那么用户只需要将数据以需要的格式交给reduce函数处理就能轻松实现分布式的计算，很多的工作都由mapReduce框架为我们封装好，大大简化了操作流程。 1 MapReduce的编程思想 MapReduce的设计思路来源于LISP和其他的函数式编程语言中的映射和化简操作。数据操作的最小单位是一个键值对。用户在使用MapReduce编程模型的时候第一步就是要将数据转化为键值对的形式，map函数会以键值对作为输入，经过map函数的处理，产生新的键值对作为中间结果，然后MapReduce计算框架会自动将这些中间结果数据做聚合处理，然后将键相同的数据分发给reduce函数处理。reduce函数以键值对的形式对处理结果进行输出。要用表达式的形式表达的话，大致如下： {key1,value1}-----&gt;{key2,List&lt;value2&gt;}-----&gt;{key3,value3} 2 MapReduce的运行环境 与HDFS相同的是，MapReduce计算框也是主从架构。支撑MapReduce计算框架的是JobTracker和TaskTracker两类后台进程。 2.1 JobTracker Job Tracker 在集群中扮演了主的角色，它主要负责任务调度和集群资源监控这两个功能，但并不参与具体的计算。一个Hadoop 集群只有一个JobTracker ，存在单点故障的可能，所以必须运行在相对可靠的节点上，一JobTracker 出错，将导致集群所有正在运行的任务全部失败。 与HDFS 的NameNode 和DataNode 相似， TaskTracker 也会通过周期性的心跳向JobTracker汇报当前的健康状况和状态，心跳信息里面包括了自身计算资源的信息、被占用的计算资源的信息和正在运行中的任务的状态信息。JobTracker 则会根据各个TaskTracker 周期性发送过来的心跳信息综合考虑TaskTracker 的资源剩余量、作业优先级、作业提交时间等因素，为TaskTracker分配合适的任务。 2.2 TaskTracker TaskTracker 在集群中扮模了从的角色，它主要负责汇报心跳和执行JobTracker 的命令这两个功能。一个集群可以有多个TaskTracker ，但一个节点只会有一个TaskTracker ， 并且TaskTracker和DataNode 运行在同一个节点之中，这样， 一个节点既是计算节点又是存储节点。TaskTracker会周期性地将各种信息汇报给JobTracker ，而JobTracker 收到心跳信息， 会根据心跳信息和当前作业运行情况为该TaskTracker 下达命令， 主要包括启动任务、提交任务、杀死任务、杀死作业和重新初始化5 种命令。 2.3 客户端 用户通过客户端提交编写的MapReduce程序给JobTracker。 3 MapReduce 作业和任务 Map Reduce 作业（ job ）是用户提交的最小单位， 而Map/Reduce 任务（ task ） 是MapReduce 计算的最小单位。当用户向Hadoop 提交一个MapReduce 作业时， JobTracker 的作业分解模块会将其分拆为 任务交由各个TaskTracker 执行， 在MapReduce 计算框架中，任务分为两种-Map 任务和Reduce 任务。 4 MapReduce 的计算资源划分 一个MapReduce 作业的计算工作都由TaskTracker 完成。用户向Hadoop 提交作业，Job Tracker 会将该作业拆分为多个任务， 并根据心跳信息交由空闲的TaskTracker 启动。一个Task Tracker 能够启动的任务数量是由TaskTracker 配置的任务槽（ slot ） 决定。槽是Hadoop 的计算资源的表示模型， Hadoop 将各个节点上的多维度资源（ CPU 、内存等）抽象成一维度的槽，这样就将多维度资源分配问题转换成一维度的槽分配的问题。在实际情况中，Map 任务和Reduce任务需要的计算资源不尽相同， Hadoop 又将槽分成Map 槽和Reduce 槽， 并且Map 任务只能使用Map槽， Reduce 任务只能使用Reduce槽。 Hadoop 的资源管理采用了静态资源设置方案，即每个节点配置好Map 槽和Reduce 槽的数量（配置项为mapred-site.xml 的mapred.task:tracker.map.tasks.maximum 和mapred.taskTracker.reduce.tasks.maximum),一旦Hadoop启动后将无法动态更改。 5 MapReduce 的局限性 从MapReduce 的特点可以看出MapReduce 的优点非常明显，但是MapReduce 也有其局限性，井不是处理海量数据的普适方法。它的局限性主要体现在以下几点： MapReduce 的执行速度慢。一个普通的MapReduce作业一般在分钟级别完成，复杂的作业或者数据量更大的情况下，也可能花费一小时或者更多，好在离线计算对于时间远没有OLTP那么敏感。所以MapReduce 现在不是，以后也不会是关系型数据库的终结者。MapReduce的慢主要是由于磁盘1/0 , MapReduce 作业通常都是数据密集型作业，大量的中间结果需要写到磁盘上并通过网络进行传输，这耗去了大量的时间。 MapReduce过于底层。与SQL相比，MapReduce显得过于底层。对于普通的查询，一般人是不会希望写一个map 函数和reduce函数的。对于习惯于关系型数据库的用户，或者数据分析师来说，编写map 函数和reduce 函数无疑是一件头疼的事情。好在Hive的出现，大大改善了这种状况。 不是所有算法都能用MapReduce 实现。这意味着，不是所有算法都能实现并行。例如机器学习的模型训练， 这些算法需要状态共享或者参数间有依赖，且需要集中维护和更新。 6 来一个Hello world 本次hello world为word count程序，通过这个小程序来了解一下MapReduce的用法。 Mapper类： import java.io.IOException; import java.util.StringTokenizer; import org.apache.hadoop.io.IntWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Mapper; public class TokenizerMapper extends Mapper&lt;Object, Text, Text, IntWritable&gt; { //声时一个IntWritable变量，作计数用，每出现一个key，给其一个value=1的值 IntWritable one = new IntWritable(1); Text word = new Text(); public void map(Object key, Text value, Context context) throws IOException,InterruptedException { //Hadoop读入的value是以行为单位的，其key为该行所对应的行号，因为我们要计算每个单词的数目， //默认以空格作为间隔，故用StringTokenizer辅助做字符串的拆分，也可以用string.split(&quot;&quot;)来作。 StringTokenizer itr = new StringTokenizer(value.toString()); //遍历每一个单词 while(itr.hasMoreTokens()) { word.set(itr.nextToken()); context.write(word, one); } } } 以上就是Map打散过程。 上面我们看到有IntWritable这个数据类型，这里的Text相当于jdk中的String IntWritable相当于jdk的int类型，hadoop有8个基本数据类型，它们均实现了WritableComparable接口： 类 描述 BooleanWritable 标准布尔型数值 ByteWritable 单字节数值 DoubleWritable 双字节数 FloatWritable 浮点数 IntWritable 整型数 LongWritable 长整型数 Text 使用UTF8格式存储的文本 NullWritable 当《key,value》中的key或value为空时使用 下面接着写reduce类： import java.io.IOException; import org.apache.hadoop.io.IntWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Reducer; public class MyReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; { IntWritable result = new IntWritable(); public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException,InterruptedException { int sum = 0; //因为map已经将文本处理为键值对的形式，所以在这里只用取出map中保存的键值 for(IntWritable val:values) { sum += val.get(); } result.set(sum); context.write(key,result); } } 下面是主函数： import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.IntWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Job; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; import org.apache.hadoop.util.GenericOptionsParser; public class WordCount { public static void main(String[] args) throws Exception { Configuration conf = new Configuration(); String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs(); //判断一下命令行输入路径/输出路径是否齐全，即是否为两个参数 if(otherArgs.length != 2) { System.err.println(&quot;Usage: wordcount &lt;in&gt; &lt;out&gt;&quot;); System.exit(2); } //此程序的执行，在hadoop看来是一个Job，故进行初始化job操作 Job job = new Job(conf, &quot;wordcount&quot;); //可以认为成，此程序要执行WordCount.class这个字节码文件 job.setJarByClass(WordCount.class); //在这个job中，我用TokenizerMapper这个类的map函数 job.setMapperClass(TokenizerMapper.class); //在这个job中，我用MyReducer这个类的reduce函数 job.setCombinerClass(MyReducer.class); job.setReducerClass(MyReducer.class); //在reduce的输出时，key的输出类型为Text job.setOutputKeyClass(Text.class); //在reduce的输出时,value的输出类型为IntWritable job.setOutputValueClass(IntWritable.class); //初始化要计算word的文件的路径 FileInputFormat.addInputPath(job, new Path(otherArgs[0])); //初始化要计算word的文件的之后的结果的输出路径 FileOutputFormat.setOutputPath(job, new Path(otherArgs[1])); //提交job到hadoop上去执行了，意思是指如果这个job真正的执行完了则主函数退出了，若没有真正的执行完就退出了。 System.exit(job.waitForCompletion(true)?0:1); } } 工程目录结构如下： 然后就是打jar包，我用的是idea，打包的过程就不表，自己百度一下。我打的jar包名为：hadoop.jar。打完之后将jar包上传至服务器。 在你的hadoop服务开启的状态下，输入如下命令： #hadoop jar hadoop.jar cn.edu.hust.demo1.WordCount /user/input /user/output 中间的是主程序的路径，后面第一个路径是你的单词文档的路径，我将单词文档上传到/user/input下，/user/output的路径为输出路径。这个路径在执行上面这个命令之前不能存在，不然会报错。 执行完等一会之后，如果出现上面的输出表示运行成功。 下面是输出路径的内容： 我上传的txt 单词文件内容为： hello world big hello world xiaoming the world is beautiful xiaoming is my friend 运行完程序之后的单词统计结果为： 由此我们的单词统计就完成了。 下面我们借着词频统计来说一下MapReduce的运行过程。 7 MapReduce的运行过程深入了解 从前面的WordCount可以看出， 一个MapReduce作业经过了input,map,combine,reduce,output 五个阶段,其中combine阶段并不一定发生， map输出的中间结果被分发到reducer的过程被称为shuffle （数据混洗）。 7.1 从输入到输出的状态 在shuffle阶段还会发生copy（复制）和sort（排序)。 在MapReduce 的过程中，一个作业被分成Map和Reduce计算两个阶段，它们分别由两个或者多个Map任务和Reduce任务组成，这个在前面已经说过了。Reduce任务默认会在Map任务数量完成5%后才开始启动。 Map任务的执行过程可概括为： 首先通过用户指定的Inputformat类（如WordCount中的FilelnputFormat 类）中的getSplits方法和next方法将输入文件切片并解析成键值对作为map函数的输入。 然后map函数经过处理之后输出并将中间结果交给指定的Partitioner处理，确保中间结果分发到指定的Reduce任务处理，此时如果用户指定了Combiner，将执行combine操作。 最后map函数将中间结果保存到本地。 Reduce 任务的执行过程可概括为： 首先需要将已经完成的Map任务的中问结果复制到Reduce任务所在的节点，待数据复制完成后，再以key进行排序，通过排序，将所有key相同的数据交给reduce函数处理，处理完成后，结果直接输出到HDFS上。 7.2 input过程 如果使用HDFS上的文件作为MapReduce的输入（由于用户的数据大部分数据是以文件的 形式存储在HDFS上，所以这是最常见的情况）MapReduce计算框架首先会用 org.apache.hadoop.mapreduce.InputFormat类的子类FilelnputFormat类将作为输入的HDFS 上的文件切分形成输入分片（InputSplit），每个InputSplit将作为一个Map任务的输入，再将InputSplit解析为键值对。InputSplit的大小和数量对于MapReduce作业的性能有非常大的影响，因此有必要深入了解InputSplit 。 在List getSplits(JobContext job)方法中会对JobTracker进行拆分，确定需要多少个taskTracker。 该方法中调用了computeSplitSize(blockSize, minSize, maxSize)方法。用来计算拆分多少个taskTracker。我们看到三个参数：blockSize, minSize, maxSize， 其中minSize由mapred-site.xml文件中的配置项mapred.min.split.size决定，默认为l; maxSize由mapred-site.xml文件中的配置项mapred.max.split.size决定，默认为9223372036854775807; 而blockSize也是由hdfs-site.xml文件中的配置项dfs.block.size决定，默认为67108864字节(64MB)。 一般来说， dfs.block.size的大小是确定不变的，所以得到目标InputSplit大小，只需改变mapred.min.split.size和mapred.max.split.size的大小即可。 对于Map任务来说，处理的单位为一个InputSplit。而InputSplit是一个逻辑概念， InputSplit所包含的数据是仍然是存储在HDFS 的块里面，它们之间的关系如下图所示： InputSplit 可以不和块对齐，根据前面的公式也可以看出，一个InputSplit的大小可以大于一个块的大小亦可以小于一个块的大小。Hadoop在进行任务调度的时候，会优先考虑本节点的数据，如果本节点没有可处理的数据或者是还需要其他节点的数据， Map 任务所在的节点会从其他节点将数据通过网络传输给自己。当InputSplit的容量大于块的容量，Map任务就必须从其他节点读取一部分数据，这样就不能实现完全数据本地性，所以当使用FileIputFormat实现InputFormat时，应尽量使InputSplit的大小和块的大小相同以提高Map任务计算的数据本地性。 7.3 Map及中间结果的输出 InputSplit将解析好的键值对交给用户编写的map函数处理，处理后的中间结果会写到本地磁盘上，在刷写磁盘的过程中，还做了partition（分区）和sort（排序）的操作。map 函数产生输出时，并不是简单地刷写磁盘。为了保证I/0 效率，采取了先写到内存的环形缓冲区，并作一次预排序。 每个Map任务都有一个环形内存缓冲区，用于存储map函数的输出。默认情况下，缓冲 区的大小是100 MB ，该值可以通过mapred-site.xml文件的io.sort.mb的配置项配置。一旦缓冲区内容达到阀值（由mapred-site.xml文件的io.sort.spill.percent 的值决定，默认为0.80或80%），一个后台线程便会将缓冲区的内容溢写（spill）到磁盘中。在写磁盘的过程中，map函数的输出继续被写到缓冲区，但如果在此期间缓冲区被填满,map会阻塞直到写磁盘过程完成。写磁盘会以轮询的方式写到mapred.local.dir(mapred-site.xml文件的配置项）配置的作业特定目录下。 在写磁盘之前，线程会根据数据最终要传送到的Reducer把缓冲区的数据划分成（默认是按照键）相应的分区。在每个分区中，后台线程技键进行内排序，此时如果有一个Combiner,它会在排序后的输出上运行。 如果己经指定Combiner且溢出写次数至少为3时，Combiner就会在输出文件写到磁盘之前运行。如前文所述，Combiner可以多次运行，并不影响输出结果。运行Combiner 的意义在于使map 输出的中间结果更紧凑，使得写到本地磁盘和传给Reducer的数据更少。 为了提高磁盘I/O 性能，可以考虑压缩map的输出，这样会让写磁盘的速度更快，节约磁盘空间，从而使传送给Reducer的数据量减少。默认情况下，map的输出是不压缩的，但只要将mapred-site.xml文件的配置项mapred.compress.map.output设为true 即可开启压缩功能。使用的压缩库由mapred-site.xml文件的配置项mapred.map.output.compression.codec指定。 map输出的中间结果存储的格式为IFile,IFile是一种支持行压缩的存储格式。Reducer通过HTTP方式得到输出文件的分区。将map输出的中间结果（map 输出）发送 到Reducer的工作线程的数量由mapred-site.xml文件的tasktracker.http.threads 配置项决定，此配置针对每个节点，即每个TaskTracker,而不是每个Map任务，默认是40，可以根据作业大小，集群规模以及节点的计算能力而增大。 7.4 shuffle shuffle,也叫数据混洗。在某些语境中，代表map函数产生输出到reduce的消化输入的整个过程，上面我们也有一个流程图中看到shuffle是在combine和reduce之间的过程，前面map已经处理完了数据那为什么还要shuffle呢，这就是shuffle的功能：完成数据和合并和排序。 前面我们说了，Map任务输出的结果位于运行Map任务的TaskTracker所在的节点的本地磁盘上。TaskTracker需要为这些分区文件（map输出）运行Reduce任务。但是 Reduce任务可能需要多个Map 任务的输出作为其特殊的分区文件。每个Map 任务的完成时间可能不同，当只要有一个任务完成，Reduce 任务就开始复制其输出。这就是shuffle中的copy阶段。Reduce任务有少量复制线程，可以井行取得Map 任务的输出，默认值是5个线程，该值可以通过设置mapred-site.xml的mapred.reduce.parallel.copies配置项来改变。 如果map输出相当小，则会被复制到Reducer所在的TaskTracker的内存的缓冲区中，缓冲区的大小由mapred-site.xml文件中的mapred.job.shuffie.input.buffer.percent配置项指定。否则， map输出将会被复制到磁盘。一旦内存缓冲区达到阀值大小（由mapred-site.xml 文件的 mapred.job.shuffle.merge.percent 配置项决定）或缓冲区的文件数达到阀值大小（由 mapred-site.xml文件的mapred.inmem.merge.threshold配置项决定）,则合并后溢写到磁盘中。 随着溢写到磁盘的文件增多，后台线程会将它们合并为更大的、有序的文件，这会为后面的合并节省时间为了合并,压缩的中间结果都将在内存中解压缩。 复制完所有的map输出，shuffle进入sort阶段。这个阶段将合并map的输出文件，并维持其顺序排序，其实做的是归并排序。排序的过程是循环进行，如果有50个map的输出文件，而合并因子（由mapred-site.xml文件的io.sort.factor配置项决定，默认为10）为10,合并操作将进行5次，每次将10个文件合,并成一个文件,最后会有5个文件，这5个文件由于不满足合并条件（文件数小于合并因子），则不会进行合井，将会直接把这5 个文件交给reduce函数处理。至此，shuffle 阶段完成。 从shuffle的过程可以看出,Map任务处理的是一个InputSplit,而Reduce任务处理的是所有Map任务同一个分区的中间结果。 reduce阶段操作的实质就是对经过shuffle处理后的文件调用reduce函数处理。由于经过了shuffle的处理，文件都是按键分区且有序，对相同分区的文件调用一次reduce函数处理。与map的中间结果不同的是,reduce的输出一般为HDFS。 7.5 sort 上面我们讲到shuffle的过程在合并Map的时候顺带做了排序。其实排序贯穿Map和Reduce的所有任务，在MapReduce任务中一共发生了三次排序： 当map函数产生输出时，会首先写入内存的环形缓冲区，当达到设定的阀值，在刷写磁盘之前，后台线程会将缓冲区的数据划分为相应的分区。在每个分区中，后台线程按照键的值进行内排序。 在Map任务完成之前，磁盘上存在多个己经分好区并排好序的、大小和缓冲区一样的溢写文件，这时溢写文件将被合并成一个己分区且己排序的输出文件。由于溢写文件己经经过第一次排序，所以合并文件时只需再做一次排序就可使输出文件整体有序。 在shuffle阶段，需要将多个Map任务的输出文件合并，由于经过第二次排序，所以合并文件时只需再做一次排序就可使输出文件整体有序。 在这3次排序中第一次是在内存缓冲区做的内排序，使用的算法是快速排序，第二次排序和第三次排序都是在文件合并阶段发生的，使用的是归并排序。 至此mapReduce的工作过程我们就说完啦，多看几次好好消化消化吧！]]></content>
      <categories>
        <category>大数据学习</category>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zookeeeper开源客户端Curator]]></title>
    <url>%2Fposts%2Fc934629c.html</url>
    <content type="text"><![CDATA[开源zk客户端-Curator 创建会话： RetryPolicy retryPolicy = new ExponentialBackoffRetry(1000,3); CuratorFramework client = CuratorFrameworkFactory.newClient(&quot;192.168.131.128:2181&quot;,retryPolicy); 使用CuratorFrameworkFactory工厂类的静态方法newClient来创建会话。 在重试策略上， Curator通过一个接口RetryPolicy来让用户实现自定义的重试策略。在RetryPolicy接口中只定义了一个方法： boolean allowRetry(int var1, long var2, RetrySleeper var4); 三个参数分别为： baseSleepTimeMs：初始sleep时间 maxRetries：最大重试次数 maxSleepMs：最大sleep时间 ExponentialBackoffRetry的重试策略设计如下： 给定一个初始sleep时间baseSleepTimeMs，在这个基础上结合重试次数，通过以下公式计算出当前需要sleep的时间： 当前sleep时间 = baseSleepTimeMs * Math.max(1,random.nextInt(1 &lt;&lt; (retryCount = 1))) 可以看出，随着重试次数的增加，计算出的sleep时间会越来越大。如果该sleep时间在maxSleepMs的范围之内，那么就使用该sleep时间，否则使用maxSleepMs。另外，maxRetries参数控制了最大重试次数，以避免无限制的重试。 另外，newClient方法并没有完成创建客户端的工作，你需要主动调用CuratorFramework的start()方法来完成创建客户端。 一个完整的创建客户端的例子： import org.apache.curator.RetryPolicy; import org.apache.curator.framework.CuratorFramework; import org.apache.curator.framework.CuratorFrameworkFactory; import org.apache.curator.framework.api.CreateBuilder; import org.apache.curator.retry.ExponentialBackoffRetry; import org.apache.zookeeper.CreateMode; public class Demo1 { public static void main(String[] args) { RetryPolicy retryPolicy = new ExponentialBackoffRetry(1000,3); CuratorFramework client = CuratorFrameworkFactory.newClient(&quot;192.168.131.128:2181&quot;,retryPolicy); client.start(); CreateBuilder builder = client.create(); try { builder.withMode(CreateMode.EPHEMERAL).forPath(&quot;/test&quot;,&quot;ceshi&quot;.getBytes()); } catch (Exception e) { e.printStackTrace(); } } } 创建一个节点，初始内容为空： builder.forPath(&quot;/test&quot;,“ceshi”.getBytes()); 注意，如果没有设置节点属性，那么Curator默认创建的是持久节点，内容默认是空。 创建一个临时节点，初始内容为空： builder.withMode(CreateMode.EPHEMERAL).forPath(&quot;/test&quot;); 创建一个临时节点，并自动逆归创建父节点： builder.creatingParentsIfNeeded().withMode(CreateMode.EPHEMERAL).forPath(&quot;/test&quot;,&quot;ceshi&quot;.getBytes()); 这个接口非常有用，在使用ZooKeeper 的过程中，开发人员经常会碰到 NoNodeException异常，其中一个可能的原因就是试图对一个不存在的父节点创建子节点。因此，开发人员不得不在每次创建节点之前，都判断－下该父节点是否存在。在使用Curator之后，通过调用creatingParentsIfNeeded接口，Curator就能够自动地递归创建所有需要的父节点。 同时要注意的一点是，由于在ZooKeeper中规定了所有非叶子节点必须为持久节点，调用上面这个API之后，只有path参数对应的数据节点是临时节点，其父节点均为持久节点。 删除节点： 同样创建和删除操作都是由CuratorFramework接口发出来的。 client.delete().forPath(&quot;/test/test1&quot;); 使用上面的方法只能删除叶子节点。 删除一个节点，并递归删除其所有子节点： client.delete().deletingChildrenIfNeeded().forPath(&quot;/test/test1&quot;); 删除一个节点，强制指定版本进行删除： client.delete().withVersion(1).forPath(&quot;/test/test1&quot;); 删除一个节点，强制保证删除： client.delete().guaranteed().forPath(&quot;/test/test1&quot;); 一个完整的创建节点删除节点的例子： import org.apache.curator.RetryPolicy; import org.apache.curator.framework.CuratorFramework; import org.apache.curator.framework.CuratorFrameworkFactory; import org.apache.curator.framework.api.CreateBuilder; import org.apache.curator.retry.ExponentialBackoffRetry; import org.apache.zookeeper.CreateMode; public class Demo1 { public static void main(String[] args) { RetryPolicy retryPolicy = new ExponentialBackoffRetry(1000,3); CuratorFramework client = CuratorFrameworkFactory.newClient(&quot;192.168.131.128:2181&quot;,retryPolicy); client.start(); CreateBuilder builder = client.create(); try { builder.creatingParentsIfNeeded().withMode(CreateMode.PERSISTENT).forPath(&quot;/test/test1&quot;,&quot;ceshi&quot;.getBytes()); client.delete().guaranteed().forPath(&quot;/test/test1&quot;); } catch (Exception e) { e.printStackTrace(); } } } 异步接口： Curator中引入了BackgroundCallback接口，用来处理异步接口调用。CreateBuilder提供了一个inBackground()方法可供使用，此接口就是Curator提供的异步调用入口。对应的异步处理接口为BackgroundCallback。此接口指提供了一个processResult的方法，用来处理回调结果。其中processResult的参数event中的getType()包含了各种事件类型，getResultCode()包含了各种响应码。 import org.apache.curator.RetryPolicy; import org.apache.curator.framework.CuratorFramework; import org.apache.curator.framework.CuratorFrameworkFactory; import org.apache.curator.framework.api.BackgroundCallback; import org.apache.curator.framework.api.CreateBuilder; import org.apache.curator.framework.api.CuratorEvent; import org.apache.curator.retry.ExponentialBackoffRetry; import org.apache.zookeeper.CreateMode; import java.util.concurrent.Executors; public class Demo2 { public static void main(String[] args) { RetryPolicy retryPolicy = new ExponentialBackoffRetry(1000,3); CuratorFramework client = CuratorFrameworkFactory.newClient(&quot;192.168.131.128:2181&quot;,retryPolicy); client.start(); CreateBuilder builder = client.create(); try { builder.withMode(CreateMode.PERSISTENT).inBackground(new BackgroundCallback() { public void processResult(CuratorFramework client, CuratorEvent event) throws Exception { System.out.println(&quot;当前线程：&quot; + Thread.currentThread().getName() + &quot;,code:&quot; + event.getResultCode() + &quot;,type:&quot; + event.getType()); } }, Executors.newFixedThreadPool(10)).forPath(&quot;/test&quot;); ​ builder.withMode(CreateMode.EPHEMERAL).inBackground(new BackgroundCallback() { public void processResult(CuratorFramework client, CuratorEvent event) throws Exception { System.out.println(“当前线程：” + Thread.currentThread().getName() + “,code:” + event.getResultCode() + “,type:” + event.getType()); } }).forPath(&quot;/test1&quot;); ​ } catch (Exception e) { e.printStackTrace(); } } } 注意:如果自己指定了线程池,那么相应的操作就会在线程池中执行,如果没有指定,那么就会使用Zookeeper的EventThread线程对事件进行串行处理。 事件监听： ZooKeeper原生支持通过注册Watcher来进行事件监听，但是其使用并不是特别方便，需要开发人员自己反复注册Watcher，比较繁琐。Curator 引入了Cache来实现对ZooKeeper服务端事件的监听。Cache是Curator 中对事件监听的包装，其对事件的监听其实可以近似看作是一个本地缓存视图和远程ZooKeeper视图的对比过程。同时Curator能够自动为开发人员处理反复注册监听，从而大大简化了原生API开发的繁琐过程。Cache分为两类监听类型：节点监听和子节点监听。 NodeCache： NodeCache不仅可以用于监听数据节点的内容变更，也能监听指定节点是否存在。如果原本节点不存在，那么Cache 就会在节点被创建后触发NodeCachelistener。但是，如果该数据节点被删除，那么Curator就无法触发NodeCachelistener了。 import org.apache.curator.RetryPolicy; import org.apache.curator.framework.CuratorFramework; import org.apache.curator.framework.CuratorFrameworkFactory; import org.apache.curator.framework.api.BackgroundCallback; import org.apache.curator.framework.api.CreateBuilder; import org.apache.curator.framework.api.CuratorEvent; import org.apache.curator.framework.recipes.cache.NodeCache; import org.apache.curator.framework.recipes.cache.NodeCacheListener; import org.apache.curator.retry.ExponentialBackoffRetry; import org.apache.zookeeper.CreateMode; import java.util.concurrent.Executors; public class Demo2 { public static void main(String[] args) { RetryPolicy retryPolicy = new ExponentialBackoffRetry(1000,3); CuratorFramework client = CuratorFrameworkFactory.newClient(&quot;192.168.131.128:2181&quot;,retryPolicy); client.start(); CreateBuilder builder = client.create(); try { final NodeCache cache = new NodeCache(client,&quot;/test/test1&quot;,false); cache.start(); cache.getListenable().addListener(new NodeCacheListener() { @Override public void nodeChanged() throws Exception { System.out.println(&quot;cache: &quot;+cache.getCurrentData().getData()); } }); builder.creatingParentsIfNeeded().withMode(CreateMode.PERSISTENT).forPath(&quot;/test/test1&quot;,&quot;ceshi&quot;.getBytes()); client.setData().forPath(&quot;/test/test1&quot;,&quot;haha&quot;.getBytes()); client.delete().deletingChildrenIfNeeded().forPath(&quot;/test/test1&quot;); } catch (Exception e) { e.printStackTrace(); } } } PathChildrenCache: PathChildrenCache用于监听指定ZooKeeper数据节点的子节点变化情况。 当指定节点的子节点发生变化时，就会回调该方法。PathChildrenCacheEvent类中定义了所有的事件类型，主要包括新增子节点(CHILD_ADDED)、子节点数据变更(CHILD_UPDATED)和子节点删除(CHILD_RE问OVED)三类。 import org.apache.curator.RetryPolicy; import org.apache.curator.framework.CuratorFramework; import org.apache.curator.framework.CuratorFrameworkFactory; import org.apache.curator.framework.api.BackgroundCallback; import org.apache.curator.framework.api.CreateBuilder; import org.apache.curator.framework.api.CuratorEvent; import org.apache.curator.framework.recipes.cache.*; import org.apache.curator.retry.ExponentialBackoffRetry; import org.apache.zookeeper.CreateMode; import java.util.concurrent.Executors; public class Demo2 { public static void main(String[] args) { RetryPolicy retryPolicy = new ExponentialBackoffRetry(1000,3); CuratorFramework client = CuratorFrameworkFactory.newClient(&quot;192.168.131.128:2181&quot;,retryPolicy); client.start(); CreateBuilder builder = client.create(); try { final PathChildrenCache cache = new PathChildrenCache(client,&quot;/test&quot;,true); cache.start(PathChildrenCache.StartMode.POST_INITIALIZED_EVENT); cache.getListenable().addListener(new PathChildrenCacheListener() { @Override public void childEvent(CuratorFramework curatorFramework, PathChildrenCacheEvent pathChildrenCacheEvent) throws Exception { switch (pathChildrenCacheEvent.getType()){ case CHILD_ADDED: System.out.println(&quot;CHILD_ADDED: &quot;+pathChildrenCacheEvent.getData().getPath()); break; case CHILD_UPDATED: System.out.println(&quot;CHILD_UPDATED: &quot;+pathChildrenCacheEvent.getData().getPath()); break; case CONNECTION_SUSPENDED: System.out.println(&quot;CONNECTION_SUSPENDED: &quot;+pathChildrenCacheEvent.getData().getPath()); break; case INITIALIZED: System.out.println(&quot;INITIALIZED: &quot;+pathChildrenCacheEvent.getData().getPath()); break; case CONNECTION_RECONNECTED: System.out.println(&quot;CONNECTION_RECONNECTED: &quot;+pathChildrenCacheEvent.getData().getPath()); break; case CHILD_REMOVED: System.out.println(&quot;CHILD_REMOVED: &quot;+pathChildrenCacheEvent.getData().getPath()); break; case CONNECTION_LOST: System.out.println(&quot;CONNECTION_LOST: &quot;+pathChildrenCacheEvent.getData().getPath()); break; } } }); builder.withMode(CreateMode.PERSISTENT).forPath(&quot;/test&quot;,&quot;ceshi&quot;.getBytes()); builder.withMode(CreateMode.PERSISTENT).forPath(&quot;/test/test1&quot;,&quot;ceshi&quot;.getBytes()); client.setData().forPath(&quot;/test/test1&quot;,&quot;haha&quot;.getBytes()); client.delete().deletingChildrenIfNeeded().forPath(&quot;/test/test1&quot;); } catch (Exception e) { e.printStackTrace(); } } } 4. Curator应用 4.1 master选举 有这样的场景：在分布式系统中，我们需要从集群中选举出一台机器作为master来分发任务。借助zk我们可以很方便的实现master选举功能。大体思路如下： 选择一个根节点，例如/master，多台机器同时向该节点创建一个子节点 力naster/lock ,利用ZooKeeper的特性，最终只有一台机器能够创建成功，成功的那台机器就作为Master。 Curator也是基于这个思路，但是它将节点创建、事件监听和自动选举过程进行了封装，开发人员只需要调用简单的API 即可实现Master选举。 import org.apache.curator.RetryPolicy; import org.apache.curator.framework.CuratorFramework; import org.apache.curator.framework.CuratorFrameworkFactory; import org.apache.curator.framework.api.BackgroundCallback; import org.apache.curator.framework.api.CreateBuilder; import org.apache.curator.framework.api.CuratorEvent; import org.apache.curator.framework.recipes.cache.*; import org.apache.curator.framework.recipes.leader.LeaderSelector; import org.apache.curator.framework.recipes.leader.LeaderSelectorListener; import org.apache.curator.framework.state.ConnectionState; import org.apache.curator.retry.ExponentialBackoffRetry; import org.apache.zookeeper.CreateMode; import java.util.concurrent.ExecutorService; import java.util.concurrent.Executors; public class Demo2 { public static void main(String[] args) { RetryPolicy retryPolicy = new ExponentialBackoffRetry(1000,3); CuratorFramework client = CuratorFrameworkFactory.newClient(&quot;192.168.131.128:2181&quot;,retryPolicy); client.start(); ExecutorService excutor = Executors.newFixedThreadPool(20); for(int i = 0;i&lt;4;i++){ excutor.submit(new LeaderSelect(client)); } } } class LeaderSelect implements Runnable{ private CuratorFramework client; public LeaderSelect(CuratorFramework client) { this.client = client; } @Override public void run() { createLeader(); } private void createLeader (){ LeaderSelector selector = new LeaderSelector(client, &quot;/test&quot;, new LeaderSelectorListener() { @Override public void takeLeadership(CuratorFramework curatorFramework) throws Exception { System.out.println(Thread.currentThread().getName()+&quot; 成为leader&quot;); } @Override public void stateChanged(CuratorFramework curatorFramework, ConnectionState connectionState) { } }); selector.autoRequeue(); selector.start(); } } 输出结果： Curator-LeaderSelector-2 成为leader Curator-LeaderSelector-3 成为leader Curator-LeaderSelector-1 成为leader Curator-LeaderSelector-0 成为leader Curator-LeaderSelector-2 成为leader Curator-LeaderSelector-3 成为leader Curator-LeaderSelector-1 成为leader Curator-LeaderSelector-0 成为leader Curator-LeaderSelector-2 成为leader Curator-LeaderSelector-3 成为leader Curator-LeaderSelector-1 成为leader Curator-LeaderSelector-0 成为leader Curator-LeaderSelector-2 成为leader ... 4.2 分布式锁 锁的问题经常会遇到，在分布式环境中更甚。zk实现分布式锁的逻辑是：各个节点同时在某个根节点&quot;Lock&quot;下创建临时顺序子节点： /Lock/instance1_00001 /Lock/instance2_00002 /Lock/instance3_00003 ... 然后对比谁的序号最小即谁获得锁。 那么Curator也是同理做了封装：InterProcessMutex类提供了分布式锁支持。 import org.apache.curator.RetryPolicy; import org.apache.curator.framework.CuratorFramework; import org.apache.curator.framework.CuratorFrameworkFactory; import org.apache.curator.framework.recipes.locks.InterProcessMutex; import org.apache.curator.retry.ExponentialBackoffRetry; import java.text.SimpleDateFormat; import java.util.Date; public class Demo5 { public static void main(String[] args) { RetryPolicy retryPolicy = new ExponentialBackoffRetry(1000,3); CuratorFramework client = CuratorFrameworkFactory.newClient(&quot;192.168.131.128:2181&quot;,retryPolicy); client.start(); final InterProcessMutex lock = new InterProcessMutex(client,&quot;/test/test1&quot;); for(int i = 0;i&lt;30;i++){ new Thread(new Runnable() { @Override public void run() { try { lock.acquire(); } catch (Exception e) { e.printStackTrace(); } SimpleDateFormat format = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss|SSS&quot;); String date = format.format(new Date()); System.out.println(&quot;date is : &quot;+date); try { lock.release(); } catch (Exception e) { e.printStackTrace(); } } }).start(); } } } 结果： date is : 2018-05-05 21:40:55|129 date is : 2018-05-05 21:40:55|175 date is : 2018-05-05 21:40:55|193 date is : 2018-05-05 21:40:55|249 date is : 2018-05-05 21:40:55|266 date is : 2018-05-05 21:40:55|291 date is : 2018-05-05 21:40:55|301 date is : 2018-05-05 21:40:55|314 date is : 2018-05-05 21:40:55|335 date is : 2018-05-05 21:40:55|357 date is : 2018-05-05 21:40:55|377 date is : 2018-05-05 21:40:55|383 date is : 2018-05-05 21:40:55|393 date is : 2018-05-05 21:40:55|404 date is : 2018-05-05 21:40:55|411 date is : 2018-05-05 21:40:55|422 date is : 2018-05-05 21:40:55|426 date is : 2018-05-05 21:40:55|431 date is : 2018-05-05 21:40:55|439 date is : 2018-05-05 21:40:55|446 date is : 2018-05-05 21:40:55|456 date is : 2018-05-05 21:40:55|465 date is : 2018-05-05 21:40:55|472 date is : 2018-05-05 21:40:55|480 date is : 2018-05-05 21:40:55|488 date is : 2018-05-05 21:40:55|492 date is : 2018-05-05 21:40:55|502 date is : 2018-05-05 21:40:55|519 date is : 2018-05-05 21:40:55|535 date is : 2018-05-05 21:40:55|541 Process finished with exit code 0 4.3 分布式计数器 如果有需求是在分布式环境中统计系统访问人数，那么这个时候分布式计数器可以发挥作用。基于zk的分布式计数器实现思路也很简单： 指定一个zk数据节点作为计数器，多个应用实例在分布式锁的控制下，通 过更新该数据节点的内容来实现计数功能。 Curator同样将这一系列逻辑封装在了DistributedAtomic开头的类中，从其类名我们可以看出这是一个可以在分布式环境中使用的原子整型。具体使用与java中的Atomic类一样： RetryPolicy policy = new RetryNTimes(3,1000); DistributedAtomicLo ng atomicLong = new DistributedAtomicLong(client,&quot;/test&quot;,policy); try { atomicLong.increment(); } catch (Exception e) { e.printStackTrace(); }]]></content>
      <categories>
        <category>分布式组件</category>
        <category>zookeeper</category>
      </categories>
      <tags>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zookeeeper应用实践]]></title>
    <url>%2Fposts%2F4bbefda4.html</url>
    <content type="text"><![CDATA[zk的应用还是非常广泛的。 1. 分布式锁 单机环境下的锁还是很容易去实现的，但是在分布式环境下一切都变得不是那么简单。zk实现分布式锁的原理还简单，因为在分布式环境中的zk节点的变化会被每一台机器watch，有任何变化都会被通知，所以我们可以利用这个机制： 我们创建一个根节点Lock，在获取锁的时候在该节点下创建临时顺序节点，释放锁的时候删除该临时节点。客户端调用createNode方法在Lock下创建临时顺序节点。 . 调用getChildren(&quot; Lock&quot;)来获取Lock下的所有节点，注意此时不用设置任何的Watcher。 客户端获取到所有子节点的path之后发现如果自己之前所创建的子节点序号最小，那么就认为该客户端获取到了锁。如果发现自己所创的节点并非Lock中最小的说明自己还没获得到锁。 此时客户端需要找到比自己小的马哥节点，然后对其调用exist()方法，同时对其注册时间监听器。是山猫关注这个节点是否被删除（前面我们说了释放所就会删除节点）。 当监听到了删除事件，再次判断自己所创建的节点是否是Lock中节点顺序最小的节点，如果是则获取到了锁，如果不是则继续重复以上步骤直到获取到比自己小的节点的删除事件并注册监听。 大致流程就是上面所说，我们可以画一个流程图来表示： 下面写一个测试例子运行一下我们的逻辑： DistributedLock： import java.io.IOException; import java.util.ArrayList; import java.util.Collections; import java.util.List; import java.util.concurrent.CountDownLatch; import java.util.concurrent.TimeUnit; import java.util.concurrent.locks.Condition; import java.util.concurrent.locks.Lock; import org.apache.zookeeper.CreateMode; import org.apache.zookeeper.KeeperException; import org.apache.zookeeper.WatchedEvent; import org.apache.zookeeper.Watcher; import org.apache.zookeeper.ZooDefs; import org.apache.zookeeper.ZooKeeper; import org.apache.zookeeper.data.Stat; ​ public class DistributedLock implements Lock, Watcher{ private ZooKeeper zk; private String root = “/locks”;//根 private String lockName;//竞争资源的标志 private String waitNode;//等待前一个锁 private String myZnode;//当前锁 private CountDownLatch latch;//计数器 private int sessionTimeout = 30000; private List exception = new ArrayList(); /** * 创建分布式锁,使用前请确认config配置的zookeeper服务可用 * @param config 127.0.0.1:2181 * @param lockName 竞争资源标志,lockName中不能包含单词lock */ public DistributedLock(String config, String lockName){ this.lockName = lockName; // 创建一个与服务器的连接 try { zk = new ZooKeeper(config, sessionTimeout, this); Stat stat = zk.exists(root, false); if(stat == null){ // 创建根节点 zk.create(root, new byte[0], ZooDefs.Ids.OPEN_ACL_UNSAFE,CreateMode.PERSISTENT); } } catch (IOException e) { exception.add(e); } catch (KeeperException e) { exception.add(e); } catch (InterruptedException e) { exception.add(e); } } /** * zookeeper节点的监视器 */ public void process(WatchedEvent event) { if(this.latch != null) { this.latch.countDown(); } } public void lock() { if(exception.size() &gt; 0){ throw new LockException(exception.get(0)); } try { if(this.tryLock()){ System.out.println(&quot;Thread &quot; + Thread.currentThread().getId() + &quot; &quot; +myZnode + &quot; get lock true&quot;); return; } else{ waitForLock(waitNode, sessionTimeout);//等待锁 } } catch (KeeperException e) { throw new LockException(e); } catch (InterruptedException e) { throw new LockException(e); } } public boolean tryLock() { try { String splitStr = &quot;_lock_&quot;; if(lockName.contains(splitStr)) throw new LockException(&quot;lockName can not contains \\u000B&quot;); //创建临时子节点 myZnode = zk.create(root + &quot;/&quot; + lockName + splitStr, new byte[0], ZooDefs.Ids.OPEN_ACL_UNSAFE,CreateMode.EPHEMERAL_SEQUENTIAL); System.out.println(myZnode + &quot; is created &quot;); //取出所有子节点 List&lt;String&gt; subNodes = zk.getChildren(root, false); //取出所有lockName的锁 List&lt;String&gt; lockObjNodes = new ArrayList&lt;String&gt;(); for (String node : subNodes) { String _node = node.split(splitStr)[0]; if(_node.equals(lockName)){ lockObjNodes.add(node); } } Collections.sort(lockObjNodes); System.out.println(myZnode + &quot;==&quot; + lockObjNodes.get(0)); if(myZnode.equals(root+&quot;/&quot;+lockObjNodes.get(0))){ //如果是最小的节点,则表示取得锁 return true; } //如果不是最小的节点，找到比自己小1的节点 String subMyZnode = myZnode.substring(myZnode.lastIndexOf(&quot;/&quot;) + 1); waitNode = lockObjNodes.get(Collections.binarySearch(lockObjNodes, subMyZnode) - 1); } catch (KeeperException e) { throw new LockException(e); } catch (InterruptedException e) { throw new LockException(e); } return false; } public boolean tryLock(long time, TimeUnit unit) { try { if(this.tryLock()){ return true; } return waitForLock(waitNode,time); } catch (Exception e) { e.printStackTrace(); } return false; } private boolean waitForLock(String lower, long waitTime) throws InterruptedException, KeeperException { Stat stat = zk.exists(root + &quot;/&quot; + lower,true); //判断比自己小一个数的节点是否存在,如果不存在则无需等待锁,同时注册监听 if(stat != null){ System.out.println(&quot;Thread &quot; + Thread.currentThread().getId() + &quot; waiting for &quot; + root + &quot;/&quot; + lower); this.latch = new CountDownLatch(1); this.latch.await(waitTime, TimeUnit.MILLISECONDS); this.latch = null; } return true; } public void unlock() { try { System.out.println(&quot;unlock &quot; + myZnode); zk.delete(myZnode,-1); myZnode = null; zk.close(); } catch (InterruptedException e) { e.printStackTrace(); } catch (KeeperException e) { e.printStackTrace(); } } public void lockInterruptibly() throws InterruptedException { this.lock(); } public Condition newCondition() { return null; } public class LockException extends RuntimeException { private static final long serialVersionUID = 1L; public LockException(String e){ super(e); } public LockException(Exception e){ super(e); } } } 这里面的实现逻辑和上面叙述的一样。然后我们创建一个多线程环境的测试： import java.util.ArrayList; import java.util.Collections; import java.util.List; import java.util.concurrent.CopyOnWriteArrayList; import java.util.concurrent.CountDownLatch; import java.util.concurrent.atomic.AtomicInteger; ​ ​ public class ConcurrentTest { private CountDownLatch startSignal = new CountDownLatch(1);//开始阀门 private CountDownLatch doneSignal = null;//结束阀门 private CopyOnWriteArrayList list = new CopyOnWriteArrayList(); private AtomicInteger err = new AtomicInteger();//原子递增 private ConcurrentTask[] task = null; public ConcurrentTest(ConcurrentTask... task){ this.task = task; if(task == null){ System.out.println(&quot;task can not null&quot;); System.exit(1); } doneSignal = new CountDownLatch(task.length); start(); } /** * @param args * @throws ClassNotFoundException */ private void start(){ //创建线程，并将所有线程等待在阀门处 createThread(); //打开阀门 startSignal.countDown();//递减锁存器的计数，如果计数到达零，则释放所有等待的线程 try { doneSignal.await();//等待所有线程都执行完毕 } catch (InterruptedException e) { e.printStackTrace(); } //计算执行时间 getExeTime(); } /** * 初始化所有线程，并在阀门处等待 */ private void createThread() { long len = doneSignal.getCount(); for (int i = 0; i &lt; len; i++) { final int j = i; new Thread(new Runnable(){ public void run() { try { startSignal.await();//使当前线程在锁存器倒计数至零之前一直等待 long start = System.currentTimeMillis(); task[j].run(); long end = (System.currentTimeMillis() - start); list.add(end); } catch (Exception e) { err.getAndIncrement();//相当于err++ } doneSignal.countDown(); } }).start(); } } /** * 计算平均响应时间 */ private void getExeTime() { int size = list.size(); List&lt;Long&gt; _list = new ArrayList&lt;Long&gt;(size); _list.addAll(list); Collections.sort(_list); long min = _list.get(0); long max = _list.get(size-1); long sum = 0L; for (Long t : _list) { sum += t; } long avg = sum/size; System.out.println(&quot;min: &quot; + min); System.out.println(&quot;max: &quot; + max); System.out.println(&quot;avg: &quot; + avg); System.out.println(&quot;err: &quot; + err.get()); } public interface ConcurrentTask { void run(); } } 接下来写一个测试方法： public class ZkTest { public static void main(String[] args) { Runnable task1 = new Runnable(){ public void run() { DistributedLock lock = null; try { lock = new DistributedLock(&quot;192.168.131.128:2181&quot;,&quot;test1&quot;); //lock = new DistributedLock(&quot;127.0.0.1:2182&quot;,&quot;test2&quot;); lock.lock(); Thread.sleep(3000); System.out.println(&quot;===Thread &quot; + Thread.currentThread().getId() + &quot; running&quot;); } catch (Exception e) { e.printStackTrace(); } finally { if(lock != null) lock.unlock(); } } }; new Thread(task1).start(); try { Thread.sleep(1000); } catch (InterruptedException e1) { e1.printStackTrace(); } ConcurrentTest.ConcurrentTask[] tasks = new ConcurrentTest.ConcurrentTask[10]; for(int i=0;i&lt;tasks.length;i++){ ConcurrentTest.ConcurrentTask task3 = new ConcurrentTest.ConcurrentTask(){ public void run() { DistributedLock lock = null; try { lock = new DistributedLock(&quot;192.168.131.130:2181&quot;,&quot;test2&quot;); lock.lock(); System.out.println(&quot;Thread &quot; + Thread.currentThread().getId() + &quot; running&quot;); } catch (Exception e) { e.printStackTrace(); } finally { lock.unlock(); } } }; tasks[i] = task3; } new ConcurrentTest(tasks); } } 运行结果如下： /locks/test1_lock_0000000001 is created /locks/test1_lock_0000000001==test1_lock_0000000001 Thread 12 /locks/test1_lock_0000000001 get lock true /locks/test2_lock_0000000002 is created /locks/test2_lock_0000000003 is created /locks/test2_lock_0000000004 is created /locks/test2_lock_0000000005 is created /locks/test2_lock_0000000002==test2_lock_0000000002 Thread 18 /locks/test2_lock_0000000002 get lock true Thread 18 running unlock /locks/test2_lock_0000000002 /locks/test2_lock_0000000003==test2_lock_0000000002 /locks/test2_lock_0000000004==test2_lock_0000000002 /locks/test2_lock_0000000005==test2_lock_0000000002 /locks/test2_lock_0000000006 is created Thread 24 waiting for /locks/test2_lock_0000000003 Thread 21 running unlock /locks/test2_lock_0000000003 Thread 22 waiting for /locks/test2_lock_0000000004 /locks/test2_lock_0000000007 is created /locks/test2_lock_0000000008 is created /locks/test2_lock_0000000009 is created /locks/test2_lock_0000000006==test2_lock_0000000003 /locks/test2_lock_0000000010 is created /locks/test2_lock_0000000011 is created /locks/test2_lock_0000000008==test2_lock_0000000004 Thread 24 running unlock /locks/test2_lock_0000000004 /locks/test2_lock_0000000007==test2_lock_0000000004 /locks/test2_lock_0000000009==test2_lock_0000000004 Thread 20 waiting for /locks/test2_lock_0000000005 /locks/test2_lock_0000000011==test2_lock_0000000004 /locks/test2_lock_0000000010==test2_lock_0000000004 Thread 15 waiting for /locks/test2_lock_0000000007 Thread 23 waiting for /locks/test2_lock_0000000006 Thread 22 running unlock /locks/test2_lock_0000000005 Thread 19 waiting for /locks/test2_lock_0000000008 Thread 17 waiting for /locks/test2_lock_0000000010 Thread 16 waiting for /locks/test2_lock_0000000009 Thread 20 running unlock /locks/test2_lock_0000000006 Thread 23 running unlock /locks/test2_lock_0000000007 Thread 15 running unlock /locks/test2_lock_0000000008 Thread 19 running unlock /locks/test2_lock_0000000009 Thread 16 running unlock /locks/test2_lock_0000000010 Thread 17 running unlock /locks/test2_lock_0000000011 min: 93 max: 156 avg: 124 err: 0 ===Thread 12 running unlock /locks/test1_lock_0000000001 Process finished with exit code 0 通过测试例子中： 第一次的测试只创建了一个节点test1_01,很显然是可以成功获取到锁的。 下面的测试中创建了10组线程，可以发现他们之间是有锁竞争的。到最后每个线程严格按照节点创建顺序分别都获取到了锁。]]></content>
      <categories>
        <category>分布式组件</category>
        <category>zookeeper</category>
      </categories>
      <tags>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zookeeper的命令行操作]]></title>
    <url>%2Fposts%2F590af71f.html</url>
    <content type="text"><![CDATA[1、 ZooKeeper服务命令 在准备好相应的配置之后，可以直接通过zkServer.sh 这个脚本进行服务的相关操作 1. 启动ZK服务: sh bin/zkServer.sh start 2. 查看ZK服务状态: sh bin/zkServer.sh status 3. 停止ZK服务: sh bin/zkServer.sh stop 4. 重启ZK服务: sh bin/zkServer.sh restart 2、 zk客户端命令 启动客户端： ./zkCli.sh -server localhost:2181 或者 zkCli.sh 连接到本机的zookeeper服务； ZooKeeper命令行工具类似于Linux的shell环境，不过功能肯定不及shell啦，但是使用它我们可以简单的对ZooKeeper进行访问，数据创建，数据修改等操作. 使用 zkCli.sh -server 127.0.0.1:2181 连接到 ZooKeeper 服务，连接成功后，系统会输出 ZooKeeper 的相关环境以及配置信息。 命令行工具的一些简单操作如下： 1. 显示根目录下、文件： ls / 使用 ls 命令来查看当前 ZooKeeper 中所包含的内容 2. 创建文件，并设置初始内容： create /zk &quot;test&quot; 创建一个新的 znode节点“ zk ”以及与它关联的字符串 3. 获取文件内容： get /zk 确认 znode 是否包含我们所创建的字符串 4. 修改文件内容： set /zk &quot;zkbak&quot; 对 zk 所关联的字符串进行设置 5. 删除文件： delete /zk 将刚才创建的 znode 删除 6. 递归删除一个目录： rms /zk 6. 退出客户端： quit 7. 帮助命令： help 我们来进入zk的客户端操作一下： 初始化只有一个节点 [zk: localhost:2181(CONNECTED) 0] ls / [zookeeper] 创建一个节点node，值为node1 [zk: localhost:2181(CONNECTED) 1] create /node node1 Created /node [zk: localhost:2181(CONNECTED) 2] ls / [node, zookeeper] 查看新建的节点，还显示了创建时间，修改时间，version,长度，children个数等 [zk: localhost:2181(CONNECTED) 3] get /node node1 cZxid = 0x2 ctime = Mon Mar 26 14:50:55 CST 2018 mZxid = 0x2 mtime = Mon Mar 26 14:50:55 CST 2018 pZxid = 0x2 cversion = 0 dataVersion = 0 aclVersion = 0 ephemeralOwner = 0x0 dataLength = 5 numChildren = 0 修改新建节点的值，我们看到dataVersion，修改时间，数据长度的值变了 [zk: localhost:2181(CONNECTED) 4] set /node reset_node_value cZxid = 0x2 ctime = Mon Mar 26 14:50:55 CST 2018 mZxid = 0x3 mtime = Mon Mar 26 14:51:56 CST 2018 pZxid = 0x2 cversion = 0 dataVersion = 1 aclVersion = 0 ephemeralOwner = 0x0 dataLength = 16 numChildren = 0 节点的值变为修改后的值 [zk: localhost:2181(CONNECTED) 5] get /node reset_node_value cZxid = 0x2 ctime = Mon Mar 26 14:50:55 CST 2018 mZxid = 0x3 mtime = Mon Mar 26 14:51:56 CST 2018 pZxid = 0x2 cversion = 0 dataVersion = 1 aclVersion = 0 ephemeralOwner = 0x0 dataLength = 16 numChildren = 0 在node节点下创建子节点 [zk: localhost:2181(CONNECTED) 6] create /node/sub_node sub_node_value Created /node/sub_node [zk: localhost:2181(CONNECTED) 7] ls /node [sub_node] 查看父节点，numChildren的值变为1 [zk: localhost:2181(CONNECTED) 8] get /node reset_node_value cZxid = 0x2 ctime = Mon Mar 26 14:50:55 CST 2018 mZxid = 0x3 mtime = Mon Mar 26 14:51:56 CST 2018 pZxid = 0x4 cversion = 1 dataVersion = 1 aclVersion = 0 ephemeralOwner = 0x0 dataLength = 16 numChildren = 1 查看子节点 [zk: localhost:2181(CONNECTED) 9] get /node/sub_node sub_node_value cZxid = 0x4 ctime = Mon Mar 26 14:53:09 CST 2018 mZxid = 0x4 mtime = Mon Mar 26 14:53:09 CST 2018 pZxid = 0x4 cversion = 0 dataVersion = 0 aclVersion = 0 ephemeralOwner = 0x0 dataLength = 14 numChildren = 0 删除节点，发现如果父节点有子节点的话是删不了的 [zk: localhost:2181(CONNECTED) 10] delete /node Node not empty: /node 需要先删除子节点 [zk: localhost:2181(CONNECTED) 11] delete /node/sub_node [zk: localhost:2181(CONNECTED) 12] delete /node [zk: localhost:2181(CONNECTED) 13] ls / [zookeeper] [zk: localhost:2181(CONNECTED) 14] 3、 ZooKeeper 常用四字命令 ZooKeeper 支持某些特定的四字命令字母与其的交互。它们大多是查询命令，用来获取 ZooKeeper 服务的当前状态及相关信息。用户在客户端可以通过 telnet 或 nc 向 ZooKeeper 提交相应的命令 1. 可以通过命令：echo stat|nc 127.0.0.1 2181 来查看哪个节点被选择作为follower或者leader 2. 使用echo ruok|nc 127.0.0.1 2181 测试是否启动了该Server，若回复imok表示已经启动。 3. echo dump| nc 127.0.0.1 2181 ,列出未经处理的会话和临时节点。 4. echo kill | nc 127.0.0.1 2181 ,关掉server 5. echo conf | nc 127.0.0.1 2181 ,输出相关服务配置的详细信息。 6. echo cons | nc 127.0.0.1 2181 ,列出所有连接到服务器的客户端的完全的连接 / 会话的详细信息。 7. echo envi |nc 127.0.0.1 2181 ,输出关于服务环境的详细信息（区别于 conf 命令）。 8. echo reqs | nc 127.0.0.1 2181 ,列出未经处理的请求。 9. echo wchs | nc 127.0.0.1 2181 ,列出服务器 watch 的详细信息。 10. echo wchc | nc 127.0.0.1 2181 ,通过 session 列出服务器 watch 的详细信息，它的输出是一个与 watch 相关的会话的列表。 11. echo wchp | nc 127.0.0.1 2181 ,通过路径列出服务器 watch 的详细信息。它输出一个与 session 相关的路径。 4、Zookeeper java API使用 创建一个maven工程，加入zookeeper的jar包： &lt;dependency&gt; &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt; &lt;artifactId&gt;zookeeper&lt;/artifactId&gt; &lt;version&gt;3.4.6&lt;/version&gt; &lt;/dependency&gt; 测试代码如下： import org.apache.zookeeper.*; import org.apache.zookeeper.data.Stat; import java.io.IOException; import java.util.List; public class Test1 { private ZooKeeper zk = null; /** * 创建ZK连接 * @param connectString * @param sessionTimeout */ public void createConnection( String connectString, int sessionTimeout ) { this.releaseConnection(); try { zk = new ZooKeeper(connectString, sessionTimeout, new Watcher() { // 监控所有被触发的事件 public void process(WatchedEvent event) { // TODO Auto-generated method stub System.out.println(&quot;已经触发了&quot; + event.getType() + &quot;事件！&quot;); } }); } catch ( IOException e ) { System.out.println( &quot;连接创建失败，发生 IOException&quot; ); e.printStackTrace(); } } /** * 关闭ZK连接 */ public void releaseConnection() { if ( zk != null ) { try { this.zk.close(); } catch ( InterruptedException e ) { // ignore e.printStackTrace(); } } } /** * 创建节点 * @param path 节点path * @param data 初始数据内容 * @return */ public boolean createPath( String path, String data ) { try { String node = zk.create( path, data.getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT );//临时、永久...节点 System.out.println( &quot;节点创建成功, Path: &quot; + node + &quot;, content: &quot; + data ); } catch ( KeeperException e ) { System.out.println( &quot;节点创建失败，发生KeeperException&quot; ); e.printStackTrace(); } catch ( InterruptedException e ) { System.out.println( &quot;节点创建失败，发生 InterruptedException&quot; ); e.printStackTrace(); } return true; } /** * 读取指定节点数据内容 * @param path 节点path * @return */ public String readData( String path ) { try { String res = new String( zk.getData( path, false, null ) ); System.out.println( &quot;获取数据成功：&quot; + res ); return res; } catch ( KeeperException e ) { System.out.println( &quot;读取数据失败，发生KeeperException，path: &quot; + path ); e.printStackTrace(); return &quot;&quot;; } catch ( InterruptedException e ) { System.out.println( &quot;读取数据失败，发生 InterruptedException，path: &quot; + path ); e.printStackTrace(); return &quot;&quot;; } } /** * 更新指定节点数据内容 * @param path 节点path * @param data 数据内容 * @return */ public boolean writeData( String path, String data ) { try { Stat stat = zk.setData( path, data.getBytes(), -1 );//-1表示匹配所有版本 System.out.println( &quot;更新数据成功，path：&quot; + path + &quot;, stat: &quot; + stat); } catch ( KeeperException e ) { System.out.println( &quot;更新数据失败，发生KeeperException，path: &quot; + path ); e.printStackTrace(); } catch ( InterruptedException e ) { System.out.println( &quot;更新数据失败，发生 InterruptedException，path: &quot; + path ); e.printStackTrace(); } return false; } /** * 删除指定节点 * @param path 节点path */ public void deleteNode( String path ) { try { zk.delete( path, -1 ); System.out.println( &quot;删除节点成功，path：&quot; + path ); } catch ( KeeperException e ) { System.out.println( &quot;删除节点失败，发生KeeperException，path: &quot; + path ); e.printStackTrace(); } catch ( InterruptedException e ) { System.out.println( &quot;删除节点失败，发生 InterruptedException，path: &quot; + path ); e.printStackTrace(); } } public List&lt;String&gt; getChildrens(String path ) { try { return zk.getChildren(path, true); } catch ( KeeperException e ) { System.out.println( &quot;删除节点失败，发生KeeperException，path: &quot; + path ); e.printStackTrace(); return null; } catch ( InterruptedException e ) { System.out.println( &quot;删除节点失败，发生 InterruptedException，path: &quot; + path ); e.printStackTrace(); return null; } } /** * @param args * @throws Exception */ public static void main(String[] args) throws Exception { Test1 test = new Test1(); test.createConnection(&quot;192.168.27.130:2181&quot;, 3000); test.createPath(&quot;/node&quot;, &quot;node2&quot;); test.readData(&quot;/node&quot;); test.createPath(&quot;/node/node1&quot;, &quot;node1&quot;); test.readData(&quot;/node/node1&quot;); List&lt;String&gt; childrens = test.getChildrens(&quot;/node&quot;); for (String str :childrens) { System.out.println(str); } test.releaseConnection(); } } ​ 执行完main方法之后我们去zk的控制台去看一下是否创建创建成功： [zk: localhost:2181(CONNECTED) 14] ls / [node, zookeeper] [zk: localhost:2181(CONNECTED) 15] ls /node [node1] 可以看到已经成功创建了。]]></content>
      <categories>
        <category>分布式组件</category>
        <category>zookeeper</category>
      </categories>
      <tags>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hadoop学习(六)----HDFS的shell操作]]></title>
    <url>%2Fposts%2Fc689ad69.html</url>
    <content type="text"><![CDATA[# HDFS的shell操作 标签（空格分隔）： HDFS HDFS所有命令： [uploaduser@rickiyang ~]$ hadoop fs Usage: hadoop fs [generic options] [-appendToFile &lt;localsrc&gt; ... &lt;dst&gt;] [-cat [-ignoreCrc] &lt;src&gt; ...] [-checksum &lt;src&gt; ...] [-chgrp [-R] GROUP PATH...] [-chmod [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; PATH...] [-chown [-R] [OWNER][:[GROUP]] PATH...] [-copyFromLocal [-f] [-p] [-l] &lt;localsrc&gt; ... &lt;dst&gt;] [-copyToLocal [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;] [-count [-q] [-h] &lt;path&gt; ...] [-cp [-f] [-p | -p[topax]] &lt;src&gt; ... &lt;dst&gt;] [-createSnapshot &lt;snapshotDir&gt; [&lt;snapshotName&gt;]] [-deleteSnapshot &lt;snapshotDir&gt; &lt;snapshotName&gt;] [-df [-h] [&lt;path&gt; ...]] [-du [-s] [-h] &lt;path&gt; ...] [-expunge] [-find &lt;path&gt; ... &lt;expression&gt; ...] [-get [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;] [-getfacl [-R] &lt;path&gt;] [-getfattr [-R] {-n name | -d} [-e en] &lt;path&gt;] [-getmerge [-nl] &lt;src&gt; &lt;localdst&gt;] [-help [cmd ...]] [-ls [-d] [-h] [-R] [&lt;path&gt; ...]] [-mkdir [-p] &lt;path&gt; ...] [-moveFromLocal &lt;localsrc&gt; ... &lt;dst&gt;] [-moveToLocal &lt;src&gt; &lt;localdst&gt;] [-mv &lt;src&gt; ... &lt;dst&gt;] [-put [-f] [-p] [-l] &lt;localsrc&gt; ... &lt;dst&gt;] [-renameSnapshot &lt;snapshotDir&gt; &lt;oldName&gt; &lt;newName&gt;] [-rm [-f] [-r|-R] [-skipTrash] &lt;src&gt; ...] [-rmdir [--ignore-fail-on-non-empty] &lt;dir&gt; ...] [-setfacl [-R] [{-b|-k} {-m|-x &lt;acl_spec&gt;} &lt;path&gt;]|[--set &lt;acl_spec&gt; &lt;path&gt;]] [-setfattr {-n name [-v value] | -x name} &lt;path&gt;] [-setrep [-R] [-w] &lt;rep&gt; &lt;path&gt; ...] [-stat [format] &lt;path&gt; ...] [-tail [-f] &lt;file&gt;] [-test -[defsz] &lt;path&gt;] [-text [-ignoreCrc] &lt;src&gt; ...] [-touchz &lt;path&gt; ...] [-truncate [-w] &lt;length&gt; &lt;path&gt; ...] [-usage [cmd ...]] Generic options supported are -conf &lt;configuration file&gt; specify an application configuration file -D &lt;property=value&gt; use value for given property -fs &lt;local|namenode:port&gt; specify a namenode -jt &lt;local|resourcemanager:port&gt; specify a ResourceManager -files &lt;comma separated list of files&gt; specify comma separated files to be copied to the map reduce cluster -libjars &lt;comma separated list of jars&gt; specify comma separated jar files to include in the classpath. -archives &lt;comma separated list of archives&gt; specify comma separated archives to be unarchived on the compute machines. The general command line syntax is bin/hadoop command [genericOptions] [commandOptions] 1&gt; –ls显示当前目录结构（从根路径开始） [uploaduser@rickiyang ~]$ hadoop fs -ls / 2&gt; -R递归显示目录结构 [uploaduser@rickiyang ~]$ hadoop fs -ls -R / 3&gt; 创建带内容的文本 echo &quot;hello world&quot; &gt; test1.txt 4&gt; 上传文件 上传并重命名 [uploaduser@rickiyang tmp]$ hadoop fs -put t1.txt /tmp/test1.txt 上传文件夹 [uploaduser@rickiyang tmp]$ hadoop fs -put newPkg /tmp/pkg 一次上传多个文件 [uploaduser@rickiyang tmp]$ hadoop fs -put /tmp/t1.txt /tmp/test1.txt 覆盖上传,如果根目录同一级有同名的文件则会覆盖，-f 表示强制覆盖 [uploaduser@rickiyang tmp]$ hadoop fs -put -f /tmp/test1.txt ​ 5&gt; 追加内容到文件末尾 [uploaduser@rickiyang tmp]$ hadoop fs -appendToFile /tmp/test2.txt /user/uploadser/test1.txt 6&gt; 查看HDFS某个文件的内容 [uploaduser@rickiyang tmp]$ hadoop fs -cat /user/uploaduser/test1.txt 7&gt; 下载文件/目录 从HDFS到本地文件系统 [uploaduser@rickiyang ~]$ hadoop fs -get /user/uploaduser/test2.txt /tmp/1.txt 与get等价的命令是copyToLocal [uploaduser@rickiyang ~]$ hadoop fs -copyToLocal /user/uploaduser/test2.txt /tmp/1.txt 其余的命令跟正常的linux命令大同小异，在实践中多多尝试。]]></content>
      <categories>
        <category>大数据学习</category>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hadoop学习(三)----hadoop2.x完全分布式环境搭建]]></title>
    <url>%2Fposts%2F82777c64.html</url>
    <content type="text"><![CDATA[今天我们来完成hadoop2.x的完全分布式环境搭建，话说学习本来是一件很快乐的事情，可是一到了搭环境就怎么都让人快乐不起来啊，搭环境的时间比学习的时间还多。都是泪。话不多说，走起。 1 准备工作 开工之前先说一下我们的机器环境： 一共是4台主机，主机1,2,3都是从master克隆过来的： 主机名 IP 用户 hadoopmaster 192.168.131.128 hadoop hadoopslaver1 192.168.131.130 hadoop hadoopslaver2 192.168.131.131 hadoop hadoopslaver3 192.168.131.132 hadoop 集群组成： Namenode：2台 Datanode： 3台 Journalnode：3台 Zookeeper： 3台 ZKFC：2台 ResourceManager：2台 NodeManager：3台 ​ Namenode Datanode Journalnode Zookeeper ZKFC ResourceManager NodeManager hadoopmaster 1 1 1 1 hadoopslaver1 1 1 1 1 1 1 hadoopslaver2 1 1 1 1 1 hadoopslaver3 1 1 1 首先我们保证做好了如下工作： 关闭防火墙 安装jdk 配置好了环境变量 本次安装的hadoop版本为2.7.3。 1.1 关闭防火墙，修改主机名、配置host文件、设置时间同步 1.1.1 防火墙状态 查看防火墙是否开启 # service iptables status 如果开启则使用如下命令关闭 # service iptables stop 1.1.2 修改主机名： 修改master主机的 /etc/sysconfig/network文件，将HOSTNAME改为master,其余三台主机分别改为对应的主机名。 # vim /etc/sysconfig/network ​ NETWORKING=yes HOSTNAME=hadoopmaster 其余三台分别改为相应的主机名，现在四台主机的主机名分别是： 192.168.131.128 hadoopmaster 192.168.131.130 hadoopslaver1 192.168.131.131 hadoopslaver2 192.168.131.132 hadoopslaver3 1.1.3 修改host文件 # vim /etc/hosts 添加如下配置： 192.168.131.128 hadoopmaster 192.168.131.130 hadoopslaver1 192.168.131.131 hadoopslaver2 192.168.131.132 hadoopslaver3 1.1.4 删除持久化文件/etc/udev/rules.d/70-persistent-net.rules ,重启虚拟主机，使配置生效。 # rm /etc/udev/rules.d/70-persistent-net.rules TIPS：该文件记录的是当前主机的MAC地址与IP地址的映射关系，因为我们后面三台主机都是通过克隆来的，所以四台主机的MAC地址是一样的。后面我们四台主机通信的时候你却会发现有问题，我们需要删除这个持久化文件，然后重启一下机器，系统会检测该文件，发现不存在会重新生成该文件。 1.1.5 4台主机做时间同步 # yum install ntp 使用ntp工具来同步时间，这里使用的时间服务器的ip是上海交通大学学的NTP服务器地址，其他的地址自行百度： # ntpdate 202.120.2.101 需要注意的是，如果执行上面的命令出现如下提示： ntpdate[2747]: no server suitable for synchronization found 首先检查你的防火墙是否关闭，没有的话先关闭，如果关闭了还是不行可以试一下如下方法： 使用rdate命令来更新服务器时间， 查看时间服务器的时间: # rdate time-b.nist.gov 如果命令不生效请安装rdate， yum install -y rdate 设置时间和时间服务器同步: # rdate -s time-b.nist.gov TIPS：以上所有操作需要在4台机器上同步执行！！！ 2 配置ssh免密 如果设置主机master到主机slaver1的免密钥登录，可按如下方式进行： 在A主机执行 1、# ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa -t 密钥类型（rsa 或 dsa） -P 旧密码 ''代表为空 -f 密钥文件生成后，保存的位置 2、# ssh-copy-id 192.168.131.130 IP是主机slaver1的IP地址，将主机master生成的密钥，拷贝到远程主机对应的账户下 执行过程中，会提示让输入主机slaver1的登录密码 我们可以使用上面的命令设置4台机器互相免密。 3 安装jdk，hadoop，zookeeper，配置环境变量 3.1 配置环境变量 这里我用的jdk是1.8，hadoop是2.7，zookeeper是3.4.11.配置环境变量： # vim /etc/profile 环境信息如下： JAVA_HOME=/usr/java/jdk JRE_HOME=/usr/java/jdk/jre PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin CLASSPATH=.:$JAVA_HOME/lib/jt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/lib export JAVA_HOME JRE_HOME PATH CLASSPATH export HADOOP_HOME=/usr/local/hadoop export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin export ZOOKEEPER_HOME=/usr/local/zookeeper export PATH=$PATH:$ZOOKEEPER_HOME/bin 配置完成以后执行： # source /etc/profile 使配置生效。 3.2 安装zookeeper 注意： 环境配置中后面有关zookeeper这两行只用在master、slaver1和slaver2中配置，即只在这三台机器中安装zookeeper。 zookeeper解压到/usr/local/zookeeper目录，我们进入目录/usr/local/zookeeper/conf修改文件模板文件： mv zoo_sample.cfg zoo.cfg 1.首先修改文件中的dataDir，路径是zookeeper的快照存储路径,不要使用默认的路径，重启的话会清空/tmp目录导致数据丢失： dataDir=/usr/local/zookeeper/tmp ✔此处注意：tmp目录是我新建的。 2.在文件的最后加入zookeeper节点的配置： server.1=hadoopmaster:2888:3888 server.2=hadoopslaver1:2888:3888 server.3=hadoopslaver2:2888:3888 然后我们需要注意的是：上面指定的配置节点server1.2.3是需要我们手动指定，需要在快照目录新建一个文件：myid。 # vim myid 然后内容为该服务器对应的节点id，本次试验：hadoopmaster对应的是1，hadoopslaver1对应的是2，hadoopslaver2对应的是3. 上面我们在master中配置完之后需要分别在另外两台也要配置，我们直接复制过去得了： scp -r zookeeper hadoopslaver1:/usr/local/zookeeper scp -r zookeeper hadoopslaver2:/usr/local/zookeeper 然后记得改一下myid，hadoopslaver1的改为2，hadoopslaver2的改为3。 好了，配置完成，接下来到了激动人心的启动时刻，启动脚本在bin目录下的zkServer.sh文件: zkServer.sh start TIPS：✔✔注意：三台机器都需要启动啊！！！ 查看zookeeper的状态： zkServer.sh status 如果出现如下提示表明启动成功： 关闭命令： zkServer.sh stop 3.3 安装hadoop hadoop集群的配置文件主要涉及到以下这几个： hadoop-env.sh core-site.xml hdfs-site.xml slaves mapred-site.xml yarn-site.xml 下面以此对上面的文件进行配置： hadoop-env.sh文件，设置jdk的路径： export JAVA_HOME=/usr/java/jdk core-site.xml文件，在configuration里面添加如下配置： &lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;!--此处名称需要和hdfs-site.xml中的dfs.nameservice保持一致--&gt; &lt;value&gt;hdfs://rickiyang&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!--配置路径--&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/usr/local/hadoop/tmp&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;hadoopmaster:2181,hadoopslaver1:2181,hadoopslaver2:2181&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; hdfs-site.xml文件： &lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;!--此处名称需与core-site.xml中fs.defaultFS的值保持一致--&gt; &lt;value&gt;rickiyang&lt;/value&gt; &lt;/property&gt; &lt;!--配置namenode的组成 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.namenodes.rickiyang&lt;/name&gt; &lt;value&gt;nn1,nn2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!--配置namenode1的rpc端口 --&gt; &lt;name&gt;dfs.namenode.rpc-address.rickiyang.nn1&lt;/name&gt; &lt;value&gt;hadoopmaster:8020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!--配置namenode2的rpc端口 --&gt; &lt;name&gt;dfs.namenode.rpc-address.rickiyang.nn2&lt;/name&gt; &lt;value&gt;hadoopslaver1:8020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!--配置namenode1的http端口 --&gt; &lt;name&gt;dfs.namenode.http-address.rickiyang.nn1&lt;/name&gt; &lt;value&gt;hadoopmaster:50070&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!--配置namenode2的http端口 --&gt; &lt;name&gt;dfs.namenode.http-address.rickiyang.nn2&lt;/name&gt; &lt;value&gt;hadoopslaver1:50070&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!--配置journalnode集群 --&gt; &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt; &lt;value&gt;qjournal://hadoopmaster:8485;hadoopslaver1:8485;hadoopslaver2:8485/rickiyang&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!--配置后台类。作用：HDFS Client用此类联系处于active状态的namenode --&gt; &lt;name&gt;dfs.client.failover.proxy.provider.rickiyang&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!--用于主备节点切换时实现隔离机制的 --&gt; &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt; &lt;value&gt;sshfence&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!--配置密钥的路径 需与前面免密钥设置的路径保持一致--&gt; &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt; &lt;value&gt;/hadoop/.ssh/id_rsa&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!--journalnode的持久化文件保存路径 --&gt; &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt; &lt;value&gt;/usr/local/hadoop/journal/data&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!--设置故障自动切换功能可用 --&gt; &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 复制配置文件的时候把注释删除，注释是给你看的，不用放进去。 slaves文件： 配置datanode的主机名称： hadoopslaver1 hadoopslaver2 hadoopslaver3 mapred-site.xml文件，配置yarn： 使用自带的模板文件 # mv mapred-site.xml.template mapred-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; yarn-site.xml文件： &lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt; &lt;value&gt;myyarncluster&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt; &lt;value&gt;rm1,rm2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt; &lt;value&gt;hadoopslaver2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt; &lt;value&gt;hadoopslaver3&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt; &lt;value&gt;hadoopmaster:2181,hadoopslaver1:2181,hadoopslaver2:2181&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 以上6个文件配置完毕，同步至其他3台主机： 进入路径：/usr/local/hadoop/etc/hadoop/下： scp * hadoopslaver1:`pwd` scp * hadoopslaver2:`pwd` scp * hadoopslaver3:`pwd` 启动journalnode集群。（之所以将journalnode集群放在namenode之前启动，是因为namenode启动时，会往journalnode写入edits日志文件） 在三台journalnode主机分别启动： hadoop-daemon.sh start journalnode NN的格式化 与 同步 在其中一台NN上，执行文件系统的格式化操作。注意：此处的格式化并非真正的格式化，而是一些准备和清除操作（比如清空fsimage和edits文件的存储目录下已有的文件） hdfs namenode -format 此时，这一台NN已经完成了格式化操作，接下来，需要将格式化之后的文件系统同步到另一台NN。同步时，第二台NN会从第一台NN读取数据，故需要先将第一台NN启动起来。 hadoop-daemon.sh start namenode 第一台NN启动后，在第二台NN上执行以下命令（此时NN尚未启动），即可完成同步操作： hdfs namenode -bootstrapStandby zookeeper集群的格式化 在上面已经启动了zookeeper集群，故此处在其中一台NN上执行以下命令，完成zookeeper的格式化： 集群的一些高可用信息读到zookeeper中 hdfs zkfc -formatZK 至此，Hadoop完全分布式集群的配置工作都已经完成！ 接下来，启动集群，在其中一台主机上执行以下命令（最好是在NN上执行，因为NN-&gt;DN设置了免密钥）： start-all.sh 然后分别在RS主机上启动resourcemanager： yarn-daemon.sh start resourcemanager 在start-all.sh的时候会有如下信息输出： 在上面截图的信息中我们可以看到，集群的启动顺序是： NN --> DN --> JN -->ZKFC --> NM jps 查看进程 hadoop集群的启动顺序： 1、启动zk集群 zkServer.sh start | status 2、start-all.sh 3、手动启动两台RS yarn-daemon.sh start resourcemanager 停止顺序： 1、stop-all.sh 2、手动停止两台rs yarn-daemon.sh stop resourcemanager 3、停止zk集群 启动完成，浏览器访问： 至此，我们的高可用环境就搭建完毕了。大家动手尝试吧！ ​]]></content>
      <categories>
        <category>大数据学习</category>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zookeeeper环境搭建]]></title>
    <url>%2Fposts%2Fbfffb12b.html</url>
    <content type="text"><![CDATA[zk一般是有2n+1个节点组成的集群。在Zookeeper服务有两个角色，一个是leader，负责写服务和数据同步；剩下的是follower，提供读服务。(为什么是2n+1个节点请看paxos算法) leader实效后会在follower中重新选举新的leader。（paxos算法） 每个follower都和leader有链接，接受leader的数据更新操作。（zab算法） 客户端可以连接到每个server，每个server的数据完全相同。 Server记录事务日志和快照到持久存储。 1. zk的特点如下： 最终一致性：为客户端展示同一个视图，这是zookeeper里面一个非常重要的功能 可靠性：如果消息被到一台服务器接受，那么它将被所有的服务器接受。 实时性：Zookeeper不能保证两个客户端能同时得到刚更新的数据，如果需要最新数据，应该在读数据之前调用sync()接口。 独立性 ：各个Client之间互不干预 原子性 ：更新只能成功或者失败，没有中间状态。 顺序性 ：所有Server，同一消息发布顺序一致。 2. zk中的角色： 领导者(Leader)：领导者负责进行投票的发起和决议，更新系统状态，处理写请求 跟随者(Follwer)：Follower用于接收客户端的读写请求并向客户端返回结果，在选主过程中参与投票 观察者（Observer）：观察者可以接收客户端的读写请求，并将写请求转发给Leader，但Observer节点不参与投票过程，只同步leader状态，Observer的目的是为了，扩展系统，提高读取速度。在3.3.0版本之后，引入Observer角色的原因： Zookeeper需保证高可用和强一致性； 为了支持更多的客户端，需要增加更多Server； Server增多，投票阶段延迟增大，影响性能； 权衡伸缩性和高吞吐率，引入Observer ； Observer不参与投票； Observers接受客户端的连接，并将写请求转发给leader节点； 加入更多Observer节点，提高伸缩性，同时不影响吞吐率。 客户端(Client)： 执行读写请求的发起方 3. 安装： 环境：3台虚拟机：已设置ssh免密登录： 192.168.131.128 192.168.131.130 192.168.131.131 下载zookeeper并解压到/usr/local/zookeeper目录。 添加到环境变量： ZOOKEEPER_HOME=/usr/local/zookeeper export PATH=$PATH:$ZOOKEEPER_HOME/bin:$ZOOKEEPER_HOME/conf 使配置生效 source /etc/profile 修改配置文件： 在zk的conf/目录下，将zoo_sample.cfg重命名为zoo.cfg，然后： 修改内容： # The number of milliseconds of each tick tickTime=2000 # The number of ticks that the initial # synchronization phase can take initLimit=10 # The number of ticks that can pass between # sending a request and getting an acknowledgement syncLimit=5 # the directory where the snapshot is stored. dataDir=/usr/local/zookeeper/data # the port at which the clients will connect clientPort=2183 #the location of the log file dataLogDir=/usr/local/zookeeper/log server.0=hadoopmaster:2288:3388 server.1=hadoopslaver1:2288:3388 server.2=hadoopslaver2:2288:3388 上面我们设置了数据库快照位置-data目录，该目录需要手动创建。 创建myid： 在dataDir(/usr/local/zk/data)目录创建myid文件 hadoopmaster的内容为：0 hadoopslaver1的内容为：1 hadoopslaver2的内容为：2 注意：以上操作三台机器都需要进行。 然后我们分别在三台机器启动zk，在bin目录下执行如下命令： zkServer.sh start； 各个机器启动情况如下： 当然哪一台机器是leader这都是随机的，一般第一台启动的会是。 zk的配置： Zookeeper的功能特性是通过Zookeeper配置文件来进行控制管理的(zoo.cfg).这样的设计其实有其自身的原因，通过前面对Zookeeper的配置可以看出，在对Zookeeper集群进行配置的时候，它的配置文档是完全相同的。集群伪分布模式中，有少部分是不同的。这样的配置方式使得在部署Zookeeper服务的时候非常方便。如果服务器使用不同的配置文件，必须确保不同配置文件中的服务器列表相匹配。 我们来看一下上面配置文件中的基本参数： client：监听客户端连接的端口。 tickTime：基本事件单元，这个时间是作为Zookeeper服务器之间或客户端与服务器之间维持心跳的时间间隔，每隔tickTime时间就会发送一个心跳；最小 的session过期时间为2倍tickTime。 dataDir：存储内存中数据库快照的位置，如果不设置参数，更新食物的日志将被存储到默认位置。 高级配置： dataLogdDir：这个操作让管理机器把事务日志写入“dataLogDir”所指定的目录中，而不是“dataDir”所指定的目录。这将允许使用一个专用的日志设备，帮助我们避免日志和快照的竞争。 maxClientCnxns：这个操作将限制连接到Zookeeper的客户端数量，并限制并发连接的数量，通过IP来区分不同的客户端。此配置选项可以阻止某些类别的Dos攻击。将他设置为零或忽略不进行设置将会取消对并发连接的限制。 minSessionTimeout和maxSessionTimeout ：即最小的会话超时和最大的会话超时时间。在默认情况下，minSession=2tickTime；maxSession=20tickTime。 集群配置： initLimit：此配置表示，允许follower(相对于Leaderer言的“客户端”)连接并同步到Leader的初始化连接时间，以tickTime为单位。当初始化连接时间超过该值，则表示连接失败。 syncLimit：此配置项表示Leader与Follower之间发送消息时，请求和应答时间长度。如果follower在设置时间内不能与leader通信，那么此follower将会被丢弃。 server.A=B：C：D A：其中 A 是一个数字，表示这个是服务器的编号； B：是这个服务器的 ip 地址； C：Leader选举的端口； D：Zookeeper服务器之间的通信端口。 myid和zoo.cfg：除了修改 zoo.cfg 配置文件，集群模式下还要配置一个文件 myid，这个文件在 dataDir 目录下，这个文件里面就有一个数据就是 A 的值，Zookeeper 启动时会读取这个文件，拿到里面的数据与 zoo.cfg 里面的配置信息比较从而判断到底是那个 server。]]></content>
      <categories>
        <category>分布式组件</category>
        <category>zookeeper</category>
      </categories>
      <tags>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hadoop学习(五)----HDFS的java操作]]></title>
    <url>%2Fposts%2F6f52a716.html</url>
    <content type="text"><![CDATA[前面我们基本学习了HDFS的原理，hadoop环境的搭建，下面开始正式的实践，语言以java为主。这一节来看一下HDFS的java操作。 1 环境准备 上一篇说了windows下搭建hadoop环境，开始之前先启动hadoop。我本地的编译器是idea。搭建maven工程： pom.xml文件： &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;2.7.3&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt; &lt;version&gt;2.7.3&lt;/version&gt; &lt;/dependency&gt; ​ org.apache.hadoop hadoop-mapreduce-client-core 2.7.3 &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-mapreduce-client-jobclient&lt;/artifactId&gt; &lt;version&gt;2.7.3&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.17&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 新建测试类：FileOperator.java import org.apache.commons.compress.utils.IOUtils; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.*; import org.apache.hadoop.fs.permission.FsPermission; import org.apache.hadoop.hdfs.DistributedFileSystem; import org.junit.Before; import org.junit.Test; import org.slf4j.Logger; import org.slf4j.LoggerFactory; import java.io.File; import java.io.FileInputStream; import java.io.FileOutputStream; import java.io.InputStream; import java.net.URI; import java.text.SimpleDateFormat; import java.util.Date; /** * Created by Administrator on 2017/12/3. */ public class FileOperator { private static final Logger logger = LoggerFactory.getLogger(FileOperator.class); public static DistributedFileSystem dfs=null; public static String nameNodeUri=&quot;hdfs://localhost:9000&quot;; @Before public void initFileSystem() throws Exception{ logger.info(&quot;initial hadoop env----&quot;); dfs=new DistributedFileSystem(); dfs.initialize(new URI(nameNodeUri), new Configuration()); logger.info(&quot;connection is successful&quot;); Path workingDirectory = dfs.getWorkingDirectory(); System.out.println(&quot;current workspace is ：&quot;+workingDirectory); } /** * 创建文件夹 * @throws Exception */ @Test public void testMkDir() throws Exception{ boolean res = dfs.mkdirs(new Path(&quot;/test/aaa/bbb&quot;)); System.out.println(&quot;目录创建结果：&quot;+(res?&quot;创建成功&quot;:&quot;创建失败&quot;)); } /** * 删除目录/文件 * @throws Exception */ @Test public void testDeleteDir() throws Exception{ dfs.delete(new Path(&quot;/test/aaa/bbb&quot;), false); } /** * 获取指定目录下所有文件(忽略目录) * @throws Exception * @throws IllegalArgumentException */ @Test public void testFileList() throws Exception{ RemoteIterator&lt;LocatedFileStatus&gt; listFiles = dfs.listFiles(new Path(&quot;/&quot;), true); SimpleDateFormat sdf=new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss.SSS&quot;); while (listFiles.hasNext()) { LocatedFileStatus fileStatus = (LocatedFileStatus) listFiles.next(); //权限 FsPermission permission = fileStatus.getPermission(); //拥有者 String owner = fileStatus.getOwner(); //组 String group = fileStatus.getGroup(); //文件大小byte long len = fileStatus.getLen(); long modificationTime = fileStatus.getModificationTime(); Path path = fileStatus.getPath(); System.out.println(&quot;-------------------------------&quot;); System.out.println(&quot;permission:&quot;+permission); System.out.println(&quot;owner:&quot;+owner); System.out.println(&quot;group:&quot;+group); System.out.println(&quot;len:&quot;+len); System.out.println(&quot;modificationTime:&quot;+sdf.format(new Date(modificationTime))); System.out.println(&quot;path:&quot;+path); } } /** * 【完整】文件上传 * 注意：文件上传在Window开发环境下，使用apache-common提供的&lt;code&gt;org.apache.commons.io.IOUtils.copy&lt;/code&gt;可能存在问题 */ @Test public void testUploadFullFile() throws Exception{ FSDataOutputStream out = dfs.create(new Path(&quot;/test/aaa/testFile.txt&quot;), true); InputStream in = new FileInputStream(&quot;F:\\test\\cluster\\input\\testFile.txt&quot;); IOUtils.copy(in, out); System.out.println(&quot;上传完毕&quot;); } ​ /** * 【分段|部分】文件上传 * 注意：文件上传在Window开发环境下，使用apache-common提供的org.apache.commons.io.IOUtils.copy可能存在问题 / @Test public void testUploadFile2() throws Exception{ FSDataOutputStream out = dfs.create(new Path(&quot;/test/aaa/testFile1.txt&quot;), true); InputStream in = new FileInputStream(“F:\test\cluster\input\testFile.txt”); org.apache.commons.io.IOUtils.copyLarge(in, out, 6, 12); System.out.println(“上传完毕”); } /* * 【完整】下载文件 * 注意：windows开发平台下，使用如下API */ @Test public void testDownloadFile() throws Exception{ //使用Java API进行I/O,设置useRawLocalFileSystem=true dfs.copyToLocalFile(false,new Path(&quot;/test/aaa/testFile.txt&quot;), new Path(“E:/”),true); System.out.println(“下载完成”); } /** * 【部分】下载文件 */ @Test public void testDownloadFile2() throws Exception{ //使用Java API进行I/O,设置useRawLocalFileSystem=true FSDataInputStream src = dfs.open(new Path(&quot;/test/aaa/testFile.txt&quot;)); FileOutputStream des = new FileOutputStream(new File(&quot;E:/&quot;,&quot;download_testFile.txt&quot;)); src.seek(6); org.apache.commons.io.IOUtils.copy(src, des); System.out.println(&quot;下载完成&quot;); } } 代码如上，如要使用请调整相关目录。]]></content>
      <categories>
        <category>大数据学习</category>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zookeeper基本知识入门]]></title>
    <url>%2Fposts%2F5f43c4fb.html</url>
    <content type="text"><![CDATA[之前我们在搭建hadoop分布式环境的时候用到过Zookeeper注册hadoop服务。那么到底Zookeeper在分布式环境中发挥了什么作用呢，这次我们就来讨论这个问题。 在分布式系统中通常都会有多台机器构成一个集群来对外提供服务，对外来说有几台机器在提供服务它并不关心，那么对内而言组成集群的机器如何互相协调保持一致这是个问题。Zookeeper将提供这样的一个功能–分布式协调技术。 1. Zookerrper概述 ZooKeeper是一种为分布式应用所设计的高可用、高性能且一致的开源协调服务，它提供了一项基本服务：分布式锁服务。由于ZooKeeper的开源特性，后来我们的开发者在分布式锁的基础上，摸索了出了其他的使用方法：配置维护、组服务、分布式消息队列、分布式通知/协调等。 ZooKeeper性能上的特点决定了它能够用在大型的、分布式的系统当中。从可靠性方面来说，它并不会因为一个节点的错误而崩溃。除此之外，它严格的序列访问控制意味着复杂的控制原语可以应用在客户端上。ZooKeeper在一致性、可用性、容错性的保证，也是ZooKeeper的成功之处，它获得的一切成功都与它采用的协议——Zab协议是密不可分的。 ZooKeeper在实现这些服务时，首先它设计一种新的数据结构——Znode，然后在该数据结构的基础上定义了一些原语，也就是一些关于该数据结构的一些操作。有了这些数据结构和原语还不够，因为我们的ZooKeeper是工作在一个分布式的环境下，我们的服务是通过消息以网络的形式发送给我们的分布式应用程序，所以还需要一个通知机制——Watcher机制。那么总结一下，ZooKeeper所提供的服务主要是通过：数据结构+原语+watcher机制，三个部分来实现的。那么我就从这三个方面，给大家介绍一下ZooKeeper。 2. Zookeeper的数据模型Znode Znode用来存储节点信息，它的数据模型和我们平常的文件管理系统目录树非常相似。不同之处在于： 1）路径引用 Znode通过路径引用来管理子节点。路径必须是绝对的，所以路径都是以斜杠字符来开头。其次，与我们的文件系统一样，路径必须的是惟一的。 2）Znode数据结构 Znode不单单是维护着节点信息，同时自身也保存着一些关联信息。这些信息可以分为3个部分： stat：状态信息，描述Znode的版本以及权限信息； data; 与该Znode关联的数据信息； children：该Znode下的子节点信息； 其中stat中的信息与我们平常的操作息息相关，stat中又包含如下字段： czxid： 引起这个znode创建的zxid mzxid： znode最后更新的zxid ctime： znode被创建的毫秒数(从1970年开始) mtime： znode最后修改的毫秒数(从1970年开始) version： znode数据变化号 cversion： znode子节点变化号 aversion： znode访问控制列表的变化号 ephemeralOwner： 如果是临时节点这个是znode拥有者的session id。如果不是临时节点则是0 dataLength： znode的数据长度 numChildren： znode子节点数量 后面我们再分析这些字段的意义。 data部分并不是存放一些非常大的数据。Zookeeper设计并不是作为常规的数据仓库或者数据库的，他作为分布式协调任务调度器，通常是会保存一些必要的配置文件，状态，以及路由信息，这些信息的存储通常都不会太大，Zookeeeper的服务端和客户端都被设计为严格检查每个Znode的数据大小最大为1M。 3）数据访问 Zookeeper中每个节点存储的数据被要求为原子操作。每一个节点都有自己的ACL（访问控制列表），这个列表规定了用户的权限。 4）节点类型 zk中的节点分为两种类型：临时节点和永久节点。节点的类型在创建的时候就被确定并且不能更改。 临时节点：节点的生命周期依赖于创建他们的会话。一旦会话(session)结束，临时节点就会被自动删除。一般临时节点虽然是某一个客户端发起会话创建的，但是他们对所有的客户端都是可见的。另外**，zk中规定临时节点不可以拥有子节点。** 永久节点：该节点的生命周期不依赖于会话，只有客户端发起删除命令才会被删除。 5）观察 客户端可以在节点上设置watch，即监视器。当节点发生改变的时候出发watch所对应的操作，zk将会向客户端发送一条且仅发送一条通知。因为watch只能被出发一次，这样就减少了网络的流量消耗。 2.1 zk中的时间 zk中记录时间并不是一个简单的时间戳，包含如下属性： Zxid zk中节点状态改变的每一个操作都会被记录一个Zxid格式的时间戳，这个时间戳全局有序。即每个对节点的改变都会记录一个全局唯一的时间戳，如果Zxid1的值小于Zxid2，那么Zxid1所发生的时间必然在Zxid2之前。事实上zk的每一个节点都维护者三个Zxid：分别为：cZxid，mZxid，pZxid： cZxid： 是节点的创建时间所对应的Zxid格式时间戳。 mZxid： 是节点的修改时间所对应的Zxid格式时间戳。 pZxid: 最新修改的Zxid，是不是与mZxid重复了。 Zxid是一个64位的数字，高32位是epoch用来标识leader关系是否改变，每次一个leader被选出来，他都会有一个新的epoch。低32位是个递增计数器。 Version 版本号 对节点的每一个操作都会使这个节点的版本号递增。每一个节点分别维护着三个版本号： version：节点数据版本号 cversion：子节点版本号 aversion：节点所拥有的ACL版本号 3. zk中的基本操作 create : 创建Znode（如果是zi节点，父节点必须存在） delete : 删除Znode（Znode没有子节点） exists : 测试Znode是否存在，存在则获取他的元数据信息 getACL/setACL : 为Znode获取/设置ACL信息 getChildren ：获取Znode所有子节点列表 getData/setData ：获取/设置Znode的相关数据 sync ： 是客户端的Znode师徒与zk同步 操作 说明 create 创建Znode（如果是zi节点，父节点必须存在） delete 删除Znode（Znode没有子节点） exists 测试Znode是否存在，存在则获取他的元数据信息 getACL/setACL 为Znode获取/设置ACL信息 getChildren 获取Znode所有子节点列表 getData/setData 获取/设置Znode的相关数据 sync 是客户端的Znode师徒与zk同步 4. watch触发器 ZooKeeper可以为所有的读操作设置watch，这些读操作包括：exists()、getChildren()及getData()。watch事件是一次性的触发器。 watch类型可以分为两类： 数据watch(data watches)：getData和exists负责设置数据watch 孩子watch(child watches)：getChildren负责设置孩子watch 5. zk的应用场景 1）分布式锁 共享锁在同一个进程中是很容易实现，但是在跨进程或者是不同的server中实现起来却不是那么容易。zk实现这个功能却是很容易。 在实现中，获得锁的 Server 创建一个 EPHEMERAL_SEQUENTIAL 目录节点，然后调用 getChildren方法获取当前的目录节点列表中最小的目录节点是不是就是自己创建的目录节点，如果正是自己创建的，那么它就获得了这个锁，如果不是那么它就调用exists(String path, boolean watch) 方法并监控 Zookeeper 上目录节点列表的变化，一直到自己创建的节点是列表中最小编号的目录节点，从而获得锁，释放锁很简单，只要删除前面它自己所创建的目录节点就行了。 具体步骤如下： 加锁： ZooKeeper 将按照如下方式实现加锁的操作： ZooKeeper调用create（）方法来创建一个路径格式为“locknode/lock- ”的节点，此节点类型为sequence （连续）和 ephemeral （临时）。也就是说，创建的节点为临时节点，并且所有的节点连续编号，即“lock-i ”的格式。 在创建的锁节点上调用getChildren（）方法，来获取锁目录下的最小编号节点，并且不设置 watch 。 步骤 2 中获取的节点恰好是步骤1中客户端创建的节点，那么此客户端获得此种类型的锁，然后退出操作。 客户端在锁目录上调用exists（）方法，并且设置 watch 来监视锁目录下比自己小一个的连续临时节点的状态。 如果监视节点状态发生变化，则跳转到第2步，继续进行后续的操作，直到退出锁竞争。 2）配置管理(数据发布与订阅) 在分布式系统里，我们会把一个服务应用分别部署到n台服务器上，这些服务器的配置文件是相同的，如果配置文件的配置选项发生变化，那么我们就得一个个去改这些配置文件，如果我们需要改的服务器比较少，这些操作还不是太麻烦，如果我们分布式的服务器特别多，那么更改配置选项就是一件麻烦而且危险的事情。这时我们可以将配置信息保存在 Zookeeper 的某个目录节点中，然后将所有需要修改的应用机器监控配置信息的状态，一旦配置信息发生变化，每台应用机器就会收到 Zookeeper 的通知，然后从 Zookeeper 获取新的配置信息应用到系统中。 3）集群管理 Zookeeper 能够很容易的实现集群管理的功能，如有多台 Server 组成一个服务集群，那么必须要一个“总管”知道当前集群中每台机器的服务状态，一旦有机器不能提供服务，集群中其它集群必须知道，从而做出调整重新分配服务策略。同样当增加集群的服务能力时，就会增加一台或多台 Server，同样也必须让“总管”知道。 Zookeeper 不仅能够帮你维护当前的集群中机器的服务状态，而且能够帮你选出一个“总管”，让这个总管来管理集群，这就是 Zookeeper 的另一个功能 Leader Election。 它们的实现方式都是在 Zookeeper 上创建一个 EPHEMERAL 类型的目录节点，然后每个 Server 在它们创建目录节点的父目录节点上调用 getChildren(String path, boolean watch) 方法并设置 watch 为 true，由于是 EPHEMERAL 目录节点，当创建它的 Server 死去，这个目录节点也随之被删除，所以 Children 将会变化，这时 getChildren上的 Watch 将会被调用，所以其它 Server 就知道已经有某台 Server 死去了。新增 Server 也是同样的原理。 Zookeeper 如何实现 Leader Election，也就是选出一个 Master Server。和前面的一样每台 Server 创建一个 EPHEMERAL 目录节点，不同的是它还是一个 SEQUENTIAL 目录节点，所以它是个 EPHEMERAL_SEQUENTIAL 目录节点。之所以它是 EPHEMERAL_SEQUENTIAL 目录节点，是因为我们可以给每台 Server 编号，我们可以选择当前是最小编号的 Server 为 Master，假如这个最小编号的 Server 死去，由于是 EPHEMERAL 节点，死去的 Server 对应的节点也被删除，所以当前的节点列表中又出现一个最小编号的节点，我们就选择这个节点为当前 Master。这样就实现了动态选择 Master，避免了传统意义上单 Master 容易出现单点故障的问题。 4）队列管理 Zookeeper 可以处理两种类型的队列： 当一个队列的成员都聚齐时，这个队列才可用，否则一直等待所有成员到达，这种是同步队列。 队列按照 FIFO 方式进行入队和出队操作，例如实现生产者和消费者模型。 A、同步队列 用 Zookeeper 实现的实现思路如下： 创建一个父目录 /synchronizing，每个成员都监控标志（Set Watch）位目录 /synchronizing/start 是否存在，然后每个成员都加入这个队列，加入队列的方式就是创建 /synchronizing/member_i 的临时目录节点，然后每个成员获取 / synchronizing 目录的所有目录节点，也就是 member_i。判断 i 的值是否已经是成员的个数，如果小于成员个数等待 /synchronizing/start 的出现，如果已经相等就创建 /synchronizing/start。 B、FIFO队列： 实现的思路也非常简单，就是在特定的目录下创建 SEQUENTIAL 类型的子目录 /queue_i，这样就能保证所有成员加入队列时都是有编号的，出队列时通过 getChildren( ) 方法可以返回当前所有的队列中的元素，然后消费其中最小的一个，这样就能保证 FIFO。]]></content>
      <categories>
        <category>分布式组件</category>
        <category>zookeeper</category>
      </categories>
      <tags>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hadoop学习(四)----windows环境下安装hadoop]]></title>
    <url>%2Fposts%2Fa713f481.html</url>
    <content type="text"><![CDATA[因为我们不能在线上环境进行调试hadoop，这样就只能在本地先调试好了再放到线上去啦。我本地是windows环境，今天先记下windows下搭建hadoop2.7的步骤。 1 本地环境 windows7 64位 hadoop-2.7.3.tar.gz hadoop在apache下载，看别的教程都说是需要搭建Cygwin，但是后来在国外论坛上看到已经有大神给出了windows下搭建hadoop环境的依赖库，所以我们省下了甚多麻烦哈。依赖环境点击这里下载。将解压的bin目录和etc目录替换掉你的本地hadoop里面的bin和etc即可，etc目录最好还是你自己配置。 替换完成之后我们别忘了检查你的环境变量，我本地hadoop文件夹在E盘根目录，配置到环境变量： 新建 HADOOP_HOME 变量： HADOOP_HOME E:\hadoop 在path里面加上： %HADOOP_HOME%\bin; 然后还有把刚才替换的hadoop/bin里面的 hadoop.dll库文件复制一份放到c:/windows/System32里面。 然后配置etc/hadoop里面的4个文件： core-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; hdfs-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/hadoop/data/dfs/namenode&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/hadoop/data/dfs/datanode&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; mapred-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; yarn-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 以上四个文件都是按照最小配置来写的如果你还有别的需求请参考官网的配置。 接下来我们需要格式化namenode，进入hadoop/bin目录执行： hadoop namenode -format 执行完毕再进入sbin目录执行启动命令： start-all.cmd 接着会启动四个窗口，分别启动本地的namenode，datanode，你可以使用jps命令查看是否启动完成： 如上便是成功启动啦，接着我们可以启动浏览器查看datanode控制台和mapreduce控制台： http://localhost:50070 http://localhost:8088/cluster 这样我们的本地开发环境就搭建好了。 停止服务使用如下命令： sbin目录下执行： stop-all.cmd]]></content>
      <categories>
        <category>大数据学习</category>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hadoop学习(二)----HDFS简介及原理]]></title>
    <url>%2Fposts%2F84e37d27.html</url>
    <content type="text"><![CDATA[前面简单介绍了hadoop生态圈，大致了解hadoop是什么、能做什么。带着这些目的我们深入的去学习他。今天一起看一下hadoop的基石----文件存储。因为hadoop是运行与集群之上，处于分布式环境之中，所以他的文件存储也不同与普通的本地存储，而是分布式存储系统，HDFS（The Hadoop Distributed File System）。 因为数据量越来越大，一台机器管理的磁盘数量是有限的，所有的数据由很多台机器管理。那么对于这么多台机器管理的数据如何进行协调处理呢？这个时候分布式文件管理系统就出现了。分布式文件系统是一种允许文件通过网络在多台主机上分享的 文件的系统，可让多机器上的多用户分享文件和存储空间。现有的分布式文件系统种类很多：CEPH,glusterfs,moosefs,mogilefs ,fastDFS(国人在mogileds基础上改写的)，Lustre等等。我们今天要学的HDFS只是分布式文件管理系统的一种，触类旁通，学会了一种其余的都好解除。HDFS的使用场景：适用于一次写入、多次查询的情况，不支持并发写情况，小文件不合适。因为小文件也占用一个块，小文件越多（1000个1k文件）块越 多，NameNode压力越大。 我们借用官方的HDFS结构图： （解释一下图中的一些词： Rack 是指机柜的意思，一个block的三个副本通常会保存到两个或者两个以上的机柜中（当然是机柜中的服务器），这样做的目的是做防灾容错，因为发生一个机柜掉电或者一个机柜的交换机挂了的概率还是蛮高的。 数据块(block):HDFS(Hadoop Distributed File System)默认的最基本的存储单位是64M的数据块。HDFS中，如果一个文件小于一个数据块的大小，并不占用整个数据块存储空间。从上图中看，一个文件被分成一个或多个Block，存储在一组Datanode上） HDFS主要由三个部分组成：namenode,datanode,secondaryNamenode. 这三者的关系简单理解就是： namenode是主人，颐指气使； datanode是丫鬟们，为主子服务； secondarynamenode就是那个天天跟着主人后面的太监为主子发号命令的； 下面我们分别介绍三者。 1 namenode NameNode的作用是 管理文件目录结构，接受用户的操作请求,是管理数据节点的。名字节点维护两套数据： 一套 是文件 目录与数据块之间的关系 另一套 是 数据块与节点之间的关系 。 前一套 数据是 静态的 ，是存放在磁盘上的， 通过fsimage和edits文件来维护 ； 后一套 数据是 动态的 ，不持久放到到磁盘的，每当集群启动的时候，会自动建立这些信息，所以一般都放在内存中。 内存的数据主要是一些元数据信息，元数据信息就像是一个索引信息，通过索引可以轻松的找到需求的数据的位置包括副本位置；元数据的存在主要是为了便于读取hdfs中的数据。 硬盘中的数据比较多，最新格式化的namenode会生成以下文件目录结构： VERSION 记载了一些namenode的基础信息，其中有一个namespaceID，这是这个namenode的唯一标示。edits是hdfs的日志文件，这里记录着namenode上的一些读写操作,这是namenode储存的第一个重要的信息，它记录了近期的所有操作记录以及操作状态和操作内容.fsimage是namenode的存在内存中的元数据在硬盘上的镜像文件，但镜像文件并不是与matadata（元数据）同步的，在达到一定条件fsimage会执行更新操作来保持和内存中的元数据信息保持一致，而执行这个镜像同步操作的凭据就是edits。stime，镜像生成或者修改时间. namenode主要涉及到的就是读和写操作，其中都比较简单（相对），写则会有一个过程，我们先看读： 读文件过程： 1.客户端（Client）用FileSystem的open（）函数打开文件。 2.DistributedFileSystem用RPC调用元数据节点，得到文件的数据块信息。 对于每一个数据块，元数据节点返回保存数据块的数据节点的地址。 3.DistributedFileSystem返回FSDataInputStream给客户端，用来读取数据。 4.客户端调用stream的read()函数开始读取数据。 5.DFSInputStream连接保存此文件第一个数据块的最近的数据节点。 6.Data从数据节点读到客户端(client) 7.当此数据块读取完毕时，DFSInputStream关闭和此数据节点的连接，然后连接此文件下一个数据块的最近的数据节点。 8.当客户端读取完毕数据的时候，调用FSDataInputStream的close函数。 在读取数据的过程中，如果客户端在与数据节点通信出现错误，则尝试连接包含此数据块的下一个数据节点。 失败的数据节点将被记录，以后不再连接。 看来看写文件： 1.客户端调用create()来创建文件 2.DistributedFileSystem用RPC调用元数据节点，在文件系统的命名空间中创建一个新的文件。 3.元数据节点首先确定文件原来不存在，并且客户端有创建文件的权限，然后创建新文件。 4.DistributedFileSystem返回DFSOutputStream，客户端用于写数据。 5.客户端开始写入数据，DFSOutputStream将数据分成块，写入data queue。 6.Data queue由Data Streamer读取，并通知元数据节点分配数据节点，用来存储数据块(每块默认复制3块)。分配的数据节点放在一个pipeline里。 7.Data Streamer将数据块写入pipeline中的第一个数据节点。第一个数据节点将数据块发送给第二个数据节点。第二个数据节点将数据发送给第三个数据节点。 8.DFSOutputStream为发出去的数据块保存了ack queue，等待pipeline中的数据节点告知数据已经写入成功。 如果数据节点在写入的过程中失败： 1.关闭pipeline，将ack queue中的数据块放入data queue的开始。 2.当前的数据块在已经写入的数据节点中被元数据节点赋予新的标示，则错误节点重启后能够察觉其数据块是过时的，会被删除。 3.失败的数据节点从pipeline中移除，另外的数据块则写入pipeline中的另外两个数据节点。 4.元数据节点则被通知此数据块是复制块数不足，将来会再创建第三份备份。 5.当客户端结束写入数据，则调用stream的close函数。此操作将所有的数据块写入pipeline中的数据节点，并等待ack queue返回成功。最后通知元数据节点写入完毕。 2 DataNode Datanode 将 HDFS 数据以文件的形式存储在本地的文件系统中，它并不知道有 关 HDFS 文件的信息。它把每个 HDFS 数据块存储在本地文件系统的一个单独的文件 中。 Datanode 并不在同一个目录创建所有的文件，实际上，它用试探的方法来确定 每个目录的最佳文件数目，并且在适当的时候创建子目录。在同一个目录中创建所 有的本地文件并不是最优的选择，这是因为本地文件系统可能无法高效地在单个目 录中支持大量的文件。 当一个 Datanode 启动时，它会扫描本地文件系统，产生一个这些本地文件对应 的所有 HDFS 数据块的列表，然后作为报告发送到 Namenode ，这个报告就是块状态 报告。 3 Secondary NameNode SecondaryNameNode不是说NameNode挂了的备用节点 。他的主要功能只是定期合并日志, 防止日志文件变得过大 。合并过后的镜像文件在NameNode上也会保存一份。 secondaryname作为一个附庸，其实它也一直在工作，他的工作就是解决matadata和fsimage之间的不和谐（不一致），这里为神马会使用到secondarynamenode来帮助namenode来管理namenode，原因是因为namenode不断的写，会产生大量的日志，若namenode重启，那么加载这些日志文件就会消耗大量的时间，而采用secondarynamenode处理过edits和faimage后，edits的大小始终保持一个比较小的水平，那么naemnode就算重启也可以快速启动而且保持前面的状态。 我们来看一下secondaryNameNode是如何工作的： namenode 响应 Secondary namenode 请求，将 edit log 推送给 Secondary namenode ， 开始重新写一个新的 edit log Secondary namenode 收到来自 namenode 的 fsimage 文件和 edit log Secondary namenode 将 fsimage 加载到内存，应用 edit log ， 并生成一 个新的 fsimage 文件 Secondary namenode 将新的 fsimage 推送给 Namenode Namenode 用新的 fsimage 取代旧的 fsimage ， 在 fstime 文件中记下检查 点发生的时 关于hdfs的机制我们先简单介绍这么多，用多少先学多少等到后面接触的时候我们继续深入学习。 ​]]></content>
      <categories>
        <category>大数据学习</category>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive基础知识]]></title>
    <url>%2Fposts%2F88686410.html</url>
    <content type="text"><![CDATA[hive基础知识 Hive是建立在 Hadoop上的数据仓库基础构架。它提供了一系列的工具，可以用来进行数据提取转化加载（ETL）， 这是一种可以存储、查询和分析存储在Hadoop中的大规模数据的机制。Hive定义了简单的类 SQL查询语言，称为HQL， 它允许熟悉SQL的用户查询数据。同时，这个语言也允许熟悉MapReduce开发者的开发自定义的mapper和reducer来处理 内建的mapper和reducer无法完成的复杂的分析工作。 2.为什么会产生hive 方便非java编程者（熟悉SQL语言）对hdfs的数据做mapreduce操作。 3.hive能做什么 数据仓库： 不与用户交互；存放历史数据；反范式设计，专门引入冗余数据，保证数据完整。数据仓库面向分析，里面存放的数据用来做分析和挖掘。 Hive是数据仓库，Hive将SQL转化为MapReduce可以识别的操作，承担解释器、编译器、优化器等角色，Hive运行时，元数据（表的结构、属性）存储在关系型数据库里面。因为元数据信息需要高效的读取。 4.初识hive 4.1 hive数据类型 hive支持的数据类型包括： 基本类型：tinyint, smallint, int,bigint, boolean, float, double, string,varchar,char 复杂类型：struct，map，array,data,timestamp 4.2 hive数据模型 hive支持四中数据模型： external table 外部表 table 默认为内部表 partion 分区表 bucket 桶表 内部表： 为指定表为别的形式的表默认都为内部表，hive会为其建立一个相应的目录保存。删除表时，元数据和数据都会被删除。 12create table test (name string , age string) location '/input/table_data';load data inpath '/input/data' into table test ; load命令会将/input/data下的数据加载到/input/table_data目录下，当删除test表的时候test表数据和/input/table_data下的数据都被删除。当然/input/data下也没有数据。如果创建内部表的时候没有指定location，就会在hdfs/hive，hdfs的默认目录下新建一个表目录。 注：本质上load data会转移数据。 外部表: 12create external table etest (name string , age string);load data inpath '/input/edata' into table etest; 当把/input/edata下的数据转移到hdfs的默认目录后执行删除操作，默认目录中已经载入的数据不会被删除，但是/input/edata下的数据已经没有了，相当于做了一次剪切操作。 分区表： hive的表在hdfs上是对应一个文件目录保存的，当使用hive进行查询的时候会对该目录下的文件进行全表扫描，这样是很浪费性能的，这样就引入了partion(分区)和bucket(桶)的概念。 分区表是指创建表时，指定partition的分区空间: partition by(字段名 字段类型) 分区表又分动态分区和静态分区，静态分区要求在建表的时候指定分区字段： 12345678create table dyn_part_test_spark( name string, score string)partitioned by ( grade string, class string)ROW FORMAT DELIMITED FIELDS TERMINATED BY ','; 在装载完数据之后，分区的列grade和class所对应的值会生成若干个子目录，假如有如下插入语句： 1insert overwrite table dyn_part_test_spark partition(grade='grade1',class='class1') 那么查询的时候数据就位于grade1目录下的class1目录里面。 但是静态分区会有一个问题，那就是在分区之前对应分区的值是需要被确定的，即partition后面的值需要被确定。当值有很多个的时候，就需要重复执行很多类似于上面insert语句这样的插入语句。即在插入的时候需要指定你要插入那个分区，不然插入失败。 为了解决这个问题，所以提出了动态分区。 123456CREATE TABLE order_created_dynamic_partition ( orderNumber STRING, event_time STRING)PARTITIONED BY (event_month string);insert overwrite table dyn_part_test_spark partition(grade,class); 注意到insert语句后面没有指定具体的值，执行成功以后hdfs就会生成相应值对应的文件夹。这就是动态分区的好处。 要让hive表自动生成分区需要配置开启动态分区，配置如下： SET hive.exec.dynamic.partition=true; //是否开启动态分区，默认为false SET hive.exec.dynamic.partition.mode=nonstrict; //动态分区的模式，默认为strict SET hive.exec.max.dynamic.partitions.pernode = 1000;//在每个执行MR的节点上最大可创建分区个数，默认值100 SET hive.exec.max.dynamic.partitions=1000;//在所有执行MR的节点上，最大可创建多少分区，默认1000 桶表： 对于每一个表或者是分区可以进一步组织为桶。在分桶时，对指定字段进行hash运算得到hash值，用hash值除以桶个数取余得到的值作为分桶依据。余数相同的数据会分到同一个桶。做hash运算时hash函数的选择取决于分桶字段的数据类型。分桶之后的查询效率比分区之后的效率要高。 1234create table teacher(id INT, name string, tno string,age INT)partitioned by (work_date string)clustered by (id) sorted by (name) into 2 bucketsrow format delimited fields terminated by ',' stored as textfile; hive中的table可以拆分为partition,table和partition可以进一步的通过&quot;clustered by&quot;拆分为桶，桶中的数据可以通过&quot;sorted by&quot;进行排序。比如上面的sql语句中通过id进行分桶，每个桶中的数据通过id进行排序。 插入数据： 1234567//创建中间表并插入数据create table tmp_teacher(id INT, name string, tno string,age INT);load data inpath '/data/input/' into table tmp_teacher;//将临时表的数据插入到桶表中insert into table teacher select * from tmp_teacher;//修改桶表中bucket的数量 alter table teacher clustered by(name) sorted by(age) into 10 buckets; 4.3 hive基本操作 创建库： 1create database userdb location '/local/path/' comment 'is coment message'; 显示数据库的路径： 1describe database userdb; 删除数据库： 1drop database if exists userdb; 默认情况下，hive是不允许删除含有表的数据库，首先删除表，之后在命令行使用‘CASCADE’关键词，同样可以使用‘RESTRICT’： 1drop database if exists userdb cascade; 建表： 123create table t_order (order_sn string, user_id string,amount int,create_time timestamp)row format delimited fields terminated by '\t'location '/external/hive/t_order'; 建表指定导入数据的存储格式为每一列中间使用空格分开。 导入数据: 1load data local inpath '/root/order.txt' into table t_order; order.txt的内容如下： 3016080910 30086 10 2016-08-19 11:50:50 3016080911 30086 11 2016-08-19 11:51:22 3016080912 30000 9 3016080913 30000 2016-08-19 11:52:12 3016080914 30010 5rmb 2016-08-19 11:53:59 我们看到数据中第三第四列是有数据缺失的，第五列有数据格式错误，但是我们执行数据插入之后查询发现： hive&gt; select * from t_order; 3016080910 30086 10 2016-08-19 11:50:50 NULL NULL NULL 3016080911 30086 11 2016-08-19 11:51:22 NULL NULL NULL 3016080912 30000 9 NULL NULL NULL 3016080913 30000 2016-08-19 11:52:12 NULL NULL NULL 3016080914 30010 NULL 2016-08-19 11:53:59 NULL NULL NULL Time taken: 2.144 seconds, Fetched: 5 row(s) 数据缺失和格式错误的列会自动改为为NULL值。 如果数据文件原本就在hdfs上，当我们加载hdfs上的数据到创建的（内部）表的时候，直接将文件移动到该hdfs文件夹下。 1load data inpath '/order.txt' into table t_order; 加载后，文件被移动到了对应表的hdfs文件夹下,同样可以直接查询到新增加的数据。 external类型的表,表对应的是文件夹，对于文件的位置不做任何限制，放到hdfs任何位置都可以。 创建临时表： 创建表时通过SQL语句得到表结构和数据,用于创建一些临时表存储中间结果,这样的表在hdfs中有相应的目录结构和文件。 123create table t_order_tmpasselect order_sn,user_id from t_order; 这样会将从t_order中查询出来的两列作为t_order_tmp表的列并创建t_order_tmp表。 复制表结构（只能复制表结构，无法复制表的内容） 1create table t_order_like like t_order; insert into 是追加数据,overwrite是覆盖写所有表。 1insert overwrite table t_order_like select * from t_order; 创建分区表 PARTITION(分区)添加一个新字段作为分区字段，在hdfs中表现为在t_order_part文件夹下创建以分区命名的文件夹，只能在创建表的时候就指定好（partitioned关键字必须在row format 之前） 123create table t_order_part (order_sn string, user_id string,amount int,create_time timestamp)partitioned by (month string)row format delimited fields terminated by '\t'; 数据集如下： 2016080910 10086 10 2016-08-19 11:50:50 20160820 2016080911 10086 11 2016-08-19 11:51:22 20160820 2016080912 10000 9 2016-08-19 11:51:42 20160820 2016080913 10000 20 2016-08-19 11:52:12 20160820 2016080914 10010 100 2016-08-19 11:53:59 20160820 2016080910 10086 10 2016-08-19 11:50:50 20160821 2016080911 10086 11 2016-08-19 11:51:22 20160821 2016080912 10000 9 2016-08-19 11:51:42 20160821 2016080913 10000 20 2016-08-19 11:52:12 20160821 2016080914 10010 100 2016-08-19 11:53:59 20160821 加载数据集： 1load data local inpath '/usr/local/order.txt' into table t_order_part partition (month='20160820'); 因为建表的时候没有指定动态分区，静态分区需要在加载数据集的时候手动指定数据插入那个分区。 hive函数 操作符： 关系操作符，如=、!=、&gt;、&lt;、is null、is not null、like、rlike 数学操作符，如+、-、*、/、%、&amp;、|、^、~ 逻辑操作符，如and、or、not、&amp;&amp;、|、! 复杂类型操作符，如array[iter]、map[key]、struct.sub_item 函数： 数学函数，如rand()、ln()、sqrt()、abs()、sin() 字符串函数，如concat_ws()、length()、lower()、ltrim()、reverse() 日期函数，如year()、unix_timestamp() 聚合函数，如count([distinct])、sum()、avg()、max() 条件函数，if(condition, value_true, value_false)、case when a then b when c then d else e end、case a when b then c when d then e else f end 类型转换函数，如binary()、cast() 复杂类型函数，如size()、sort_array() 相关命令行： SHOW FUNCTIONS – 列出目前hive中所有函数 DESCRIBE FUNCTION function_name – 显示函数简单描述 DESCRIBE FUNCTION EXTENDED function_name – 获取函数详细描述]]></content>
      <categories>
        <category>大数据学习</category>
        <category>hive</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hadoop学习(一)----概念和整体架构]]></title>
    <url>%2Fposts%2Fd538d95c.html</url>
    <content type="text"><![CDATA[程序员就得不停地学习啊，故步自封不能满足公司的业务发展啊！所以我们要有搞事情的精神。都说现在是大数据的时代，可以我们这些码农还在java的业务世界里面转悠呢。好不容易碰到一个可能会用到大数据技术的场景时可惜你又没这个技术把这一票接下来！书到用时方恨少。所以我们也要与时代同进步，搞搞大数据。分布式的基础架构现在有hadoop和spark。虽然spark目前比较火，但是国内大厂因为最开始学习大数据的先锋们给公司搭架构都是使用hadoop，以至于spark火起来都不能撼动hadoop的地位（大家不会吃饱了没事干把底层重新返工一遍！），所以导致spark在国内应用并不是很广。再有就是hadoop目前的版本已经到了正式版2.8，测试版3.0。但是问题在于hadoop1.x和hadoop2.x的底层架构不一样！稍后我会讲到。面对这些问题我们要做的当然是学新的兼顾旧的（不学新的就是倒退）。所以下面我们会一直使用hadoop2.x的版本。 1.hadoop是什么 是什么呢？就是一个棕黄色玩具大象的名字。这是真的！hadoop的作者Doug Cutting说的，这是他儿子的玩具的名字。（是不是太随意了，想想国人取名字的场景。。。）我们回到正轨，hadoop是世界上最大的富豪Apache捐助的分布式系统基础架构。该框架由java语言设计实现，用以实现在大量计算机组成的集群中对海量数据进行分布式计算。Hadoop得以在大数据处理应用中广泛应用得益于其自身在数据提取、变形和加载(ETL)方面上的天然优势。Hadoop的分布式架构，将大数据处理引擎尽可能的靠近存储。 不知道大家有没有听说Nutch这个框架。如果有人使用java做爬虫就应该知道！该框架的作者就是Doug Cutting。hadoop也起源于Nutch并且借鉴了Google于2003年发表的GFS和MapReduce相关论文。有兴趣的可以翻出去看一下。 1.1 hadoop简介 我们先看一下hadoop1.x的生态系统： 我们对上面各个模块做一些简要说明： Ambari：基于web的Hadoop集群安装，部署，管理，监控工具。 HDFS：分布式文件系统，提供数据容错和对数据的高吞吐率访问。 MapReduce：分布式，并行编程模型。将任务分为map和reduce两个阶段，从而实现每个阶段对数据的并行处理。 ZooKeeper：高性能的分布式应用程序协调服务，是google的chubby的一个开源实现。 HBase：基于HDFS的面向列存储的分布式数据库，用于快速读写大量数据。 Hive：以类SQL语言提供实时大规模数据实时查询的数据仓库。 Pig：提供高级数据流语言和计算框架来代替mapreduce任务的编写。 Mahout：可扩展的基于mapreduce的机器学习和数据挖掘库。 Flume：高可用，高可靠，分布式的海量日志采集、聚合和传输的系统。提供对数据进行简单处理，并写到各种数据接收方的能力。 Sqoop：用于在关系数据库，数据仓库和Hadoop文件系统之间转移数据的工具。 我们再来看一下hadoop2.x系统架构： 2.核心模块说明 2.1 hadoop1.x----hdfs模块 HDFS作为分布式文件系统的一种，设计之初就是针对适合一次写入，多次查询的情况。不支持并发写，不适用于小文件存储，低时延的数据访问。HDFS包含3个部分： NameNode节点 SecondaryNameNode节点 DataNode节点 NameNode负责存储数据文件的元数据； SecondaryNameNode作为NameNode的冷备份，负责合并NameNode上的fsimage和edits文件，再发送给NameNode； DataNode负责存储实际的数据块。 NameNode负责管理文件系统目录结构，接受客户端的文件操作请求。NameNode维护两套数据： 一套是文件目录与数据块之间的对应关系 一套是数据块与存储节点之间的对应关系。 前一套数据是静态的，存放在磁盘上，通过fsimage和edits文件来维护；后一套是动态的，在集群重启时会在内存自动建立这些信息。其中fsimage存储的是某一时段NameNode内存元数据信息（配置时通过hdfs-default.xml中的dfs.name.dir选项设置）；edits记录操作日志文件（配置时通过hdfs-default.xml的dfs.name.edits.dir选项设置）；fstime保存最近一次checkpoint的时间。 DataNode负责按Block存储数据文件。每一个数据文件都会按照Block大小进行划分，每个Block都会进行多副本备份（一般为三份）。通常多个副本会按照一定的策略放在不同的DataNode节点上，配置时通过hdfs-default.xml的dfs.data.dir选项设置。 总体架构如下： HDFS1.x中存在的问题： NameNode单点故障，难以应用于在线场景 NameNode压力过大，且内存受限，影响系统可扩展性 2.2 hadoop1.x----MapReduce模块 MapReduce 框架的核心步骤主要包括两部分：Map 和Reduce。当你向MapReduce 框架提交一个计算作业时，它会首先把计算作业拆分成若干个Map 任务，然后分配到不同的节点上去执行，每一个Map 任务处理输入数据中的一部分，当Map 任务完成后，它会生成一些中间文件，这些中间文件将会作为Reduce 任务的输入数据。Reduce任务会对主要对若干个map任务的输出进行汇总。整个数据流图如下： MapReduce作业涉及两个重要的实体： JobTracker：初始化作业，分配作业，与TaskTracker通信，调度整个作业的执行。 TaskTracker：保持与JobTracker的通信，在分配的数据片段上执行Map或Reduce任务。 我们简单看一下mapreduce作业的运行过程： 上图中整体流程如下： 在集群中的任意一个节点提交MapReduce程序； JobClient收到作业后，JobClient向JobTracker请求获取一个Job ID； 将运行作业所需要的资源文件复制到HDFS上（包括MapReduce程序打包的JAR文件、配置文件和客户端计算所得的输入划分信息），这些文件都存放在JobTracker专门为该作业创建的文件夹中，文件夹名为该作业的Job ID； 获得作业ID后，提交作业； JobTracker接收到作业后，将其放在一个作业队列里，等待作业调度器对其进行调度，当作业调度器根据自己的调度算法调度到该作业时，会根据输入划分信息为每个划分创建一个map任务，并将map任务分配给TaskTracker执行； 对于map和reduce任务，TaskTracker根据主机核的数量和内存的大小有固定数量的map槽和reduce槽。这里需要强调的是：map任务不是随随便便地分配给某个TaskTracker的，这里有个概念叫：数据本地化（Data-Locality），意思是：将map任务分配给含有该map处理的数据块的TaskTracker上，同时将程序JAR包复制到该TaskTracker上来运行，这叫“运算移动，数据不移动”； TaskTracker每隔一段时间会给JobTracker发送一个心跳，告诉JobTracker它依然在运行，同时心跳中还携带着很多的信息，比如当前map任务完成的进度等信息。当JobTracker收到作业的最后一个任务完成信息时，便把该作业设置成“成功”。当JobClient查询状态时，它将得知任务已完成，便显示一条消息给用户； 运行的TaskTracker从HDFS中获取运行所需要的资源，这些资源包括MapReduce程序打包的JAR文件、配置文件和客户端计算所得的输入划分等信息； TaskTracker获取资源后启动新的JVM虚拟机，运行每一个任务。 MapReduce1.x存在的问题： JobTracker访问压力大，影响系统可扩展性 难以支持出MapReduce之外的其他计算框架，如Spark，Storm 2.3 hadoop2.x----HSFS模块 HDFS2针对HDFS1中问题的主要改进是提供了NameNode HA和NameNode Federation。 NameNode HA：主备NameNode解决单点故障，主NameNode对外提供服务，备NameNode同步主NameNode元数据以待切换，所有DataNode同时向两个NameNode汇报数据块信息 NameNode Federation：把元数据的存储和管理分散到多个节点上，每个NameNode管理文件系统命名空间的一部分，实现NameNode横向扩展，把单个NameNode的负载分散到多个节点上。 2.4 hadoop2.x—Yarn模块 Hadoop2.x为了改进1.x中MapReduce的缺点，促进框架的长远发展，从0.23.0版本开始，对MapReduce框架进行完全重构，并将新框架命名为Yarn（Yet Another Resource Negotiator），主要思想是将资源管理和任务调度监控划分到不同的组件中，整体架构如下图，包含三个主要组件ResourceManager，ApplicationMaster，NodeManager。 ResourceManager：基于应用程序对资源（内存，CPU，磁盘，网络等）的需求进行集群资源的仲裁，RM 会追踪集群中有多少可用的活动节点和资源，协调用户提交的哪些应用程序应该在何时获取这些资源，每一个应用程序需要不同类型的资源因此就需要不同的容器。包含ResourceScheduler（根据容量、队列等限制条件，将系统中的资源分配给各个正在运行的应用程序）和Applications Manager（负责管理整个系统中所有应用程序，包括应用程序提交、与调度器协商资源以启动ApplicationMaster、监控 ApplicationMaster运行状态并在失败时重新启动它等）l两个组件。 ApplicationMaster：向RM和NodeManager要求启动占用一定资源的Container，跟踪协调应用程序（MR作业，DAG作业）中所有任务的执行。 NodeManager：每个节点的框架代理，启动执行应用程序的容器，监控应用程序的资源使用情况，并向调度器汇报。 yarn工作流程： 用户向YARN中提交应用程序，其中包括ApplicationMaster程序、启动ApplicationMaster的命令、用户程序等。 ResourceManager为该应用程序分配第一个Container，并与对应的Node-Manager通信，要求它在这个Container中启动应用程序的ApplicationMaster。 ApplicationMaster首先向ResourceManager注册，这样用户可以直接通过ResourceManager查看应用程序的运行状态，然后它将为各个任务申请资源，并监控它的运行状态，直到运行结束，即重复步骤4~7。 ApplicationMaster采用轮询的方式通过RPC协议向ResourceManager申请和领取资源。 一旦ApplicationMaster申请到资源后，便与对应的NodeManager通信，要求它启动任务。 NodeManager为任务设置好运行环境（包括环境变量、JAR包、二进制程序等）后，将任务启动命令写到一个脚本中，并通过运行该脚本启动任务。 各个任务通过某个RPC协议向ApplicationMaster汇报自己的状态和进度，以让ApplicationMaster随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务。 在应用程序运行过程中，用户可随时通过RPC向ApplicationMaster查询应用程序的当前运行状态。 应用程序运行完成后，ApplicationMaster向ResourceManager注销并关闭自己。 hadoop2.x—pig模块 Pig提供类SQL语言（Pig Latin）通过MapReduce来处理大规模半结构化数据。而Pig Latin是更高级的过程语言，通过将MapReduce中的设计模式抽象为操作，如Filter，GroupBy，Join，OrderBy，由这些操作组成有向无环图（DAG）。例如如下程序： visits = load ‘/data/visits’ as (user, url, time); gVisits = group visits by url; visitCounts = foreach gVisits generate url, count(visits); urlInfo = load ‘/data/urlInfo’ as (url, category, pRank); visitCounts = join visitCounts by url, urlInfo by url; gCategories = group visitCounts by category; topUrls = foreach gCategories generate top(visitCounts,10); store topUrls into ‘/data/topUrls’; 描述了数据处理的整个过程。 而Pig Latin又是通过编译为MapReduce，在Hadoop集群上执行的。上述程序被编译成MapReduce时，会产生如下图所示的Map和Reduce： Pig解决了MapReduce存在的大量手写代码，语义隐藏，提供操作种类少的问题。类似的项目还有Cascading，JAQL等。 2.5 hadoop2.x—Tez模块 Apache Tez，Tez是HortonWorks的Stinger Initiative的的一部分。作为执行引擎，Tez也提供了有向无环图（DAG），DAG由顶点（Vertex）和边（Edge）组成，Edge是对数据的移动的抽象，提供了One-To-One，BroadCast，和Scatter-Gather三种类型，只有Scatter-Gather才需要进行Shuffle。 以如下SQL为例： SELECT a.state, COUNT(*), AVERAGE(c.price) FROM a JOIN b ON (a.id = b.id) JOIN c ON (a.itemId = c.itemId) GROUP BY a.state 图中蓝色方块表示Map，绿色方块表示Reduce，云状表示写屏障（write barrier，一种内核机制，可以理解为持久的写），Tez的优化主要体现在： 去除了连续两个作业之间的写屏障 去除了每个工作流中多余的Map阶段（Stage） 通过提供DAG语义和操作，提供了整体的逻辑，通过减少不必要的操作，Tez提升了数据处理的执行性能。 上面简单说了一下hadoop的框架和重要模块。其实说了这么久hadoop，你到底知不知道hadoop是干嘛的？弄清这个问题很重要哈。学以致用，用到了才算。文上我们也说了是在大数据的场景中诞生hadoop。那么什么是大数据呢？大数据的实质特性：针对增量中海量的结构化，非结构化，半结构数据，在这种情况下，如何快速反复计算挖掘出高效益的市场数据？ 带着这个问题渗透到业务中去分析，就知道hadoop需要应用到什么业务场景了！！！如果关系型数据库都能应付的工作还需要hadoop吗？ 我们举一些例子： 银行的信用卡业务，当你正在刷卡完一笔消费的那一瞬间，假如在你当天消费基础上再消费满某个额度，你就可以免费获得某种令你非常满意的利益等等，你可能就会心动再去消费，这样就可能提高银行信用卡业务，那么这个消费额度是如何从海量的业务数据中以秒级的速度计算出该客户的消费记录，并及时反馈这个营销信息到客户手中呢？这时候关系型数据库计算出这个额度或许就需要几分钟甚至更多时间，就需要hadoop了，这就是所谓的“秒级营销”. 针对真正的海量数据，一般不主张多表关联。 在淘宝，当你浏览某个商品的时候，它会及时提示出你感兴趣的同类商品的产品信息和实时销售情况，这或许也需要用到hadoop. .谷歌搜索引擎分析的时候应该也会用到。 hadoop 主要用于大数据的并行计算： 数据密集型并行计算：数据量极大，但是计算相对简单的并行处理 如：大规模Web信息搜索 计算密集型并行计算：数据量相对不是很大，但是计算较为复杂的并行计算 如：3-D建模与渲染，气象预报，科学计算 数据密集与计算密集混合型的并行计算 如：3－D电影的渲染 hadoop主要应用于数据量大的离线场景。特征为： 1、数据量大。一般真正线上用Hadoop的，集群规模都在上百台到几千台的机器。这种情况下，T级别的数据也是很小的。Coursera上一门课了有句话觉得很不错：Don’t use hadoop, your data isn’t that big 2、离线。（大数据不等于高并发）Mapreduce框架下，很难处理实时计算，作业都以日志分析这样的线下作业为主。另外，集群中一般都会有大量作业等待被调度，保证资源充分利用。 3、数据块大。由于HDFS设计的特点，Hadoop适合处理文件块大的文件。大量的小文件使用Hadoop来处理效率会很低。 举个例子，百度每天都会有用户对侧边栏广告进行点击。这些点击都会被记入日志。然后在离线场景下，将大量的日志使用Hadoop进行处理，分析用户习惯等信息。 总的来说Hadoop适合应用于大数据存储和大数据分析的应用，适合于服务器几千台到几万台的集群运行，支持PB级的存储容量。 Hadoop典型应用有：搜索、日志处理、推荐系统、数据分析、视频图像分析、数据保存等。 所以各位hadoop的应用场景还是有限的，不是任一个场景都可以上hadoop哈。适得其反就不好了。 Hadoop诞生至今已经十一年，经历了两次重大的版本更新，版图不断扩张，期间也经历了来自其他开源黑马的冲击。Spark在早期发展阶段通过全面兼容Hadoop从而发展成为成熟的生态系统，但Hadoop实际上已经拥抱了Spark技术，积极地适应着不同计算范式的软件平台。此外，Hadoop的商业化也滋养了成千上万的企业，比如Cloudera，Hortonworks，MapR。毫无疑问，Hadoop在这十一年已经是大数据的代名词，大数据的王者，大数据时代软件开发者的必备技能。我们作为技术人员在有这么多前辈无私贡献自己的知识的基础上，应该在有限的时间里多了解这些开源技术，追上时代的步伐！]]></content>
      <categories>
        <category>大数据学习</category>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala文件读写（五）]]></title>
    <url>%2Fposts%2F596588bb.html</url>
    <content type="text"><![CDATA[1.读取文件 在Scala中有两种基本方式可以打开并读取文件： 使用一个仅一行的代码，这种方式有个副作用-文件不会被自动关闭，对于那些生命周期很短的程序比如shell脚本可以使用。 使用稍长些的代码，确保文件流会被关闭。 使用简洁语法： var fileName = &quot;c:/1.txt&quot; //每次读取一行 for(line &lt;- Source.fromFile(fileName).getLines()){ println(line) } //将所有的行转为一个列表 val txt = Source.fromFile(fileName).getLines().toList println(txt) //将所有的内容当做一个字符串读取 val txt1 = Source.fromFile(fileName).getLines().mkString println(txt1) 结果： 白日依山尽 尽 黄河 List(白日依山尽 尽, 黄河) 白日依山尽 尽黄河 Process finished with exit code 0 文件流会伴随JVM的生命周期。 确保文件流会被关闭的步骤： var fileName = &quot;c:/1.txt&quot; val bufferSource = Source.fromFile(fileName) for(line &lt;- bufferSource.getLines()){ println(line) } bufferSource.close() 需要手动的调用文件流的close方法。 2.写入文件 Scala不提供写入文件的能力，但是你可以使用Java的字符流来写文件，比如：BufferedWriter,PrintWriter,或者FileWriter。 var fileName = &quot;c:/1.txt&quot; val pw = new PrintWriter(new File(fileName)) pw.write(&quot;我是PrintWriter&quot;) pw.close() val fileWriter = new FileWriter(fileName) fileWriter.write(&quot;我是FileWriter&quot;) fileWriter.close() val bufferedWriter = new BufferedWriter(new FileWriter(fileName)) bufferedWriter.write(&quot;我是BufferWriter&quot;) bufferedWriter.close() val outputStreamWriter = new OutputStreamWriter(new FileOutputStream(fileName)) outputStreamWriter.write(&quot;我是outputStream&quot;) outputStreamWriter.close() 3. 序列化 Scala的序列号方式和Java一样，继承Serilizable接口。但是在实现上有一些区别： @SerialVersionUID(100L) class User extends Serializable{ } 需要将@SerialVersionUID(100L)标注添加到类上。因为Serializable是一个特质，所以可以将他混入任何类中，即使这个类已经继承了别的类： @SerialVersionUID(100L) class User extends OldUser with Serializable{ }]]></content>
      <categories>
        <category>scala</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>scala学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala集合（四）]]></title>
    <url>%2Fposts%2Fc3007ea1.html</url>
    <content type="text"><![CDATA[1. 集合 集合主要有三种： Sequence Map Set sequence是一种线性元素的集合，可能会是索引或者线性的（链表）。map是包含键值对的集合，就像Java的Map,set是包含无重复元素的集合。 除了这三个主要的集合类之外，还有其他有用的集合类型，如Stack, Queue和Range。还有其他一些用起来像集合的类，如元组、枚举、Option/Some/None以及Try/Success/Failure类。 Scala通用的序列集合： .tg {border-collapse:collapse;border-spacing:0;} .tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;} .tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;} .tg .tg-us36{border-color:inherit;vertical-align:top} 不可变(Immutable) 可变 索引(Indexed) Vector ArrayBuffer 线性链表(Linked lists) List ListBuffer 主要不可变序列集合类： .tg {border-collapse:collapse;border-spacing:0;} .tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;} .tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;} .tg .tg-us36{border-color:inherit;vertical-align:top} .tg .tg-yw4l{vertical-align:top} 索引序列 线性序列 描述 List √ 一个单链表。适于拆分头和剩余链表的递归算法 Queue √ 先进先出数据结构 Range √ 整数值范围 Stack √ 后进先出 Stream √ 与链表相似，但是延迟并且持久。适用于大型或无限序列，与Haskell的链表类似 String √ 可以被当作一个不可变的，索引的字符序列 Vector √ “定位”不可变，可索引的序列。Scaladoc这样描述它， “处理split和join非常有效率的一组嵌套数组实现” 主要的可变序列集合： .tg {border-collapse:collapse;border-spacing:0;} .tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;} .tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;} .tg .tg-us36{border-color:inherit;vertical-align:top} .tg .tg-yw4l{vertical-align:top} 索引序列 线性序列 描述 Array √ 依靠于Java的数组，其中元素是可变的，但不能改变大小 ArrayBuffer √ 一个可变的序列集合的“定位”类。成本是常数 ArrayStack √ 后进先出数据结构。在性能比较重要时Stack更好 DoubleLinkedList √ 像一个单链表，但有一个prev方法。文档说“额外的连接让删除元素变得非常快。” LinkedList √ 一个可变的单链表 ListBuffer √ 像ArrayBuffer，但依靠链表。文档说， 如果想把buffer转成list，用ListBuffer而不是ArrayBuffer。前插后插开销都是常数。其他的大部分操作都是线性的 MutableList √ 一个可变的，单链表，后插开销是常数 Queue √ 先进先出数据结构 Stack √ 后进先出数据结构。（文档建议ArrayStack的效率稍微好些。） StringBuilder √ 像在循环里构建字符串 常用到的map： .tg {border-collapse:collapse;border-spacing:0;} .tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;} .tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;} .tg .tg-us36{border-color:inherit;vertical-align:top} .tg .tg-yw4l{vertical-align:top} 不可变 可变 描述 HashMap √ √ 不可变版本用“hash trie” （晗希线索）实现，可变版本用“哈希表”实现 LinkedHashMap √ “用哈希表实现可变map”，元素按插入顺序返回 ListMap √ √ 用链表数据结构实现的map。元素按插入的相反顺序返回，因为每次插入的元素都放在head Map √ √ 基础的map，有可变的和不可变的实现 SortedMap √ 接序存键的一个基本特质。（当前用5。rtedMap创建一个变量返回TreeMap。） TreeMap √ 不可变的，排序的map，由红黑树实现 WeakHashMap √ 一个java.util.WeakHashMap的包装，弱引用的hashmap 选择Set： .tg {border-collapse:collapse;border-spacing:0;} .tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;} .tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;} .tg .tg-us36{border-color:inherit;vertical-align:top} .tg .tg-yw4l{vertical-align:top} 不可变 可变 描述 BitSet √ √ 非负整数表示为比特放入64位字节的可变尺寸数组。当有一组整数时来节省内存空间 HashSet √ √ 不可变版本用“hash trie” （哈希线索），可变版本用“哈希表” ( hashtable) LinkedHashSet √ 一个由hashtable实现的可变Set，按照插入顺序返回元素 ListSet √ √ 用链表实现的set TreeSet √ √ 不可变版本用树实现。可变版本基于不可变的AVL树作为数据结构的SortedSet Set √ √ 一般的基础特质 SortedSet √ √ 一个基础特质 表现的像集合的类型： Scala提供了很多其他的集合类型，还有一些表现的像集合的类型，尽管它们不是集合： .tg {border-collapse:collapse;border-spacing:0;} .tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;} .tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;} .tg .tg-us36{border-color:inherit;vertical-align:top} .tg .tg-yw4l{vertical-align:top} 描述 Enumeration 一个包含常数值的有限集合（比如，一周的天数或一年的周数） Iterator 迭代器不是一个集合，它可以访问集合中的元素．它也是集合，然而，定义许多方法便会发现一个普通的集合类，包括foreach, map, flatMap等。也可以在需要时把迭代器转换为一个集合 Option 包含一个或者零个元素的集合，Some和None继承自Option。 Tuple 支持异构的集合元素。没有一个“元组”类，元组由Tuple1 到Tuple22组成， 支持从1至22个元素 2. List 创建List： val list = List(1,2,3) val list1 = List[Number](1,2,3,4) val list2 = List.range(1,10) val list3 = List.range(1,20,2) val list4 = List.fill[String](3)(&quot;list&quot;) List[Char] list5 = &quot;list&quot;.toList 创建可变列表： var buffer = new ListBuffer[String]() buffer.+=(&quot;you&quot;,&quot;have&quot;,&quot;a&quot;,&quot;child&quot;) buffer += &quot;。&quot; println(buffer.toList) 输出： List(you, have, a, child, 。) Process finished with exit code 0 List基本方法的使用： 1.合并两个list： val l3 = list ::: list1 val l7 = list ++ list1 val l8 = List.concat(list,list1) println(l3,l7,l8) 结果： (List(2, 3, 4, 5), List(2, 3, 4, 5), List(2, 3, 4, 5)) 2.+:和:+ 的用法区别-连接两个列表： val list = List(2,3) val list1 = List(4,5) val l2 = list +: list1 val l3 = list :+ list1 println(l2,l3) 输出： (List(List(2, 3), 4, 5), List(2, 3, List(4, 5))) 由结果可以看出+:和:+是根据&quot;:“的方法来解析数据的，”:&quot;朝向那一边，这一边的数据就会作为基数据，另一方的数据会被添加到这个List中。 3.使用Stream----惰性计算元素周期 List通过::构造， Stream可以用#::方法构建，表达式的结尾用Stream.empty代替Nil: val stream = 1#::2#::3#::Stream.empty println(stream) val iterator = stream.iterator while (iterator.hasNext){ println(iterator.next()) } 输出： Stream(1, ?) 1 2 3 Process finished with exit code 0 可以看到打印stream的时候只打出了1，后面的是?。这是因为流的结尾还没有执行,？表示惰性集合的结尾尚未被执行的表示方式。 2.数组 2.1 创建数组： val array = Array(1,2,3) val array1 = Array(&quot;I&quot;,&quot;am&quot;,&quot;a&quot;,&quot;boy&quot;) val array2 = Array[String](&quot;I&quot;,&quot;am&quot;,&quot;a&quot;,&quot;boy&quot;) val array3 = new Array[Number](3) array3(0) = 1 array3(1) = 2 array3(2) = 3 println(array.toList,array1.toList,array2.toList,array3.toList) 输出： (List(1, 2, 3), List(I, am, a, boy), List(I, am, a, boy), List(1, 2, 3)) 其他创建数组的方式： val a1 = Array.range(1,10) val a2 = Array.range(1,20,2) val a3 = Array.apply(2,3,4,5,6) val a4 = Array.fill[String](3)(&quot;oo&quot;) val a5 = Array.tabulate(5)(n=&gt;n+5) println(a1.toList,a2.toList,a3.toList,a4.toList,a5.toList) 输出： (List(1, 2, 3, 4, 5, 6, 7, 8, 9), List(1, 3, 5, 7, 9, 11, 13, 15, 17, 19), List(2, 3, 4, 5, 6), List(oo, oo, oo), List(5, 6, 7, 8, 9)) 创建可变数组： Array既是可变的又是不可变的，可变的是他的内容，不可变的是他的大小。要想创建一个大小可变的数组，可以使用ArrayBuffer。 val a1 = ArrayBuffer[Number](1) a1 += 2 a1 += 3 a1 += 4 println(a1) val a2 = List[Number](5,6) a1 ++= a2 println(a1) 输出： ArrayBuffer(1, 2, 3, 4) ArrayBuffer(1, 2, 3, 4, 5, 6) Process finished with exit code 0 删除操作： val a1 = ArrayBuffer[Number](1,2,3,4,5) a1 -= 1 println(a1) a1 -= (1,2) println(a1) a1 --= List(1,2,3) println(a1) val a2 = ArrayBuffer[Number](2,2,4,3,4,5) #删除第一个位置的数据 a2.remove(1) println(a2) #从第三个位置开始，删除两个数据 a2.remove(3,2) println(a2) 输出： ArrayBuffer(2, 3, 4, 5) ArrayBuffer(3, 4, 5) ArrayBuffer(4, 5) ArrayBuffer(2, 4, 3, 4, 5) ArrayBuffer(2, 4, 3) Process finished with exit code 0 3. Map 创建Map： var a1 = Map(&quot;name&quot;-&gt;&quot;xiaoming&quot;,&quot;age&quot;-&gt;12) val a2 = Map((&quot;name&quot;,&quot;xiaoming&quot;),(&quot;age&quot;,12)) a1 += (&quot;address&quot;-&gt;&quot;xxxx&quot;) println(a1,a2) 输出： (Map(name -&gt; xiaoming, age -&gt; 12, address -&gt; xxxx), Map(name -&gt; xiaoming, age -&gt; 12)) Process finished with exit code 0 修改Map： 默认Map是immutable类型的，所以如果要修改Map，需要手动引入Mutable的Map。 var a1 = mutable.Map(&quot;name&quot;-&gt;&quot;xiaoming&quot;,&quot;age&quot;-&gt;12) a1(&quot;name&quot;) = &quot;xiaohong&quot; println(a1) a1 ++= List(&quot;address&quot;-&gt;&quot;xxx&quot;,&quot;sex&quot;-&gt;1) println(a1) a1 -= &quot;name&quot; println(a1) a1 --= List(&quot;age&quot;,&quot;sex&quot;) println(a1) a1.put(&quot;idCard&quot;,&quot;111&quot;) a1.remove(&quot;address&quot;) println(a1) 输出： Map(age -&gt; 12, name -&gt; xiaohong) Map(address -&gt; xxx, age -&gt; 12, name -&gt; xiaohong, sex -&gt; 1) Map(address -&gt; xxx, age -&gt; 12, sex -&gt; 1) Map(address -&gt; xxx) Map(idCard -&gt; 111) 遍历映射： var a1 = mutable.Map(&quot;name&quot;-&gt;&quot;xiaoming&quot;,&quot;age&quot;-&gt;12,&quot;address&quot;-&gt;&quot;xxx&quot;,&quot;sex&quot;-&gt;1) for((k,v) &lt;- a1){ println(k,v) } a1.foreach(x=&gt;{ println(x._1,x._2) }) 常用方法： var a1 = mutable.Map(&quot;name&quot;-&gt;&quot;xiaoming&quot;,&quot;age&quot;-&gt;12,&quot;address&quot;-&gt;&quot;xxx&quot;,&quot;sex&quot;-&gt;1) //判断某个key是否存在 if(a1.contains(&quot;name&quot;)){ println(a1(&quot;name&quot;)) } //过滤元素,a1直接就被改变为过滤后的值 a1 = a1.retain((k,v)=&gt;k.length == 3) println(a1) //改变元素的值 a1.transform((k,v)=&gt;v + &quot;1&quot;) println(a1) a1 = mutable.Map(&quot;name&quot;-&gt;&quot;xiaoming&quot;,&quot;age&quot;-&gt;12,&quot;address&quot;-&gt;&quot;xxx&quot;,&quot;sex&quot;-&gt;1) //保留指定的key,一定要将返回结果赋值给新的变量，返回类型为MapLike val a2 =a1.filterKeys(_.length &gt; 3) println(a2) //也可以使用Set集合传递你需要的key val a3 = a1.filterKeys(Set(&quot;name&quot;,&quot;age&quot;)) println(a3) //使用filter方法可以任意指定过滤key还是value a1 = a1.filter(k=&gt;k._1.length &gt; 3) println(a1) //take方法可以提取map中的前n个元素，注意如果用Map因为是无序的，每次都不保证取出元素的顺序 a1 = mutable.LinkedHashMap(&quot;name&quot;-&gt;&quot;xiaoming&quot;,&quot;age&quot;-&gt;12,&quot;address&quot;-&gt;&quot;xxx&quot;,&quot;sex&quot;-&gt;1) a1 = a1.take(2) println(a1) 输出： xiaoming Map(age -&gt; 12, sex -&gt; 1) Map(age -&gt; 121, sex -&gt; 11) Map(address -&gt; xxx, name -&gt; xiaoming) Map(age -&gt; 12, name -&gt; xiaoming) Map(address -&gt; xxx, name -&gt; xiaoming) Map(name -&gt; xiaoming, age -&gt; 12) 4.Set 集合的基本操作： val a1 = mutable.Set(1,2,3,4) a1 += 5 //集合中的元素是无序的并且是唯一的 a1 += 1 println(a1) a1 += (6,7) println(a1) a1 ++= List(8,9) println(a1) a1.add(10) println(a1) a1 -= 1 println(a1) a1 -= (2,3) println(a1) a1 --=Array(4,5) println(a1) //过滤元素 a1.retain(_ &gt; 9) println(a1) 输出： Set(1, 5, 2, 3, 4) Set(1, 5, 2, 6, 3, 7, 4) Set(9, 1, 5, 2, 6, 3, 7, 4, 8) Set(9, 1, 5, 2, 6, 3, 10, 7, 4, 8) Set(9, 5, 2, 6, 3, 10, 7, 4, 8) Set(9, 5, 6, 10, 7, 4, 8) Set(9, 6, 10, 7, 8) Set(10) 创建有序集合： var a1 = SortedSet(1,2,3,4) println(a1) a1 = a1.filter(v=&gt;v&gt;3) println(a1) var a2 = mutable.LinkedHashSet(1,2,3,4) a2 = a2.filter(v=&gt;v&gt;3) println(a2) 输出： TreeSet(1, 2, 3, 4) TreeSet(4) Set(4) 从输出类型上可以看到SortedSet被改变之后输出为TreeSet对象，这是因为SortedSet只有不可变的版本，如果想用可变的集合，可以使用TreeSet。 5.队列和堆栈 var a1 = mutable.Queue(1,2,3) a1.enqueue(1) var a2 = a1.dequeueAll(x=&gt;x&gt;0) println(a2) println(a1) 输出： ArrayBuffer(1, 2, 3, 1) Queue() var a1 = mutable.Stack(1,2,3,4) a1.pop() println(a1) a1.push(1) println(a1) a1.pop() println(a1) 输出： Stack(2, 3, 4) Stack(1, 2, 3, 4) Stack(2, 3, 4) Process finished with exit code 0 6.强大的Range功能 var a1 = (1 to 10).toList println(a1) var a2 =(1 until(10)).toList println(a2) var a3 = (1 to 10 by(2)).toList println(a3) var a4 = ('a' to 'm').toList println(a4) 输出： List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10) List(1, 2, 3, 4, 5, 6, 7, 8, 9) List(1, 3, 5, 7, 9) List(a, b, c, d, e, f, g, h, i, j, k, l, m) Process finished with exit code 0 上面这些功能在for循环中经常用到。]]></content>
      <categories>
        <category>scala</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>scala学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala函数式编程（三）]]></title>
    <url>%2Fposts%2F64535299.html</url>
    <content type="text"><![CDATA[Scala既是一门面向对象(OOP)语言，又是一门函数式编程(FP)语言。作为一门支持函数式编程的语言，Scala鼓励面向表达式编程(EOP)模型。简单来说，EOP中每个语句都有返回值。这一模式很明显，因为if/else语句就可以返回值： val result = if(a &gt; b) a else b 1. 使用函数字面量(匿名函数) val a = List.range(1,10) val b = a.filter((i:Int) =&gt; i % 2 == 0) println(b) 结果： List(2, 4, 6, 8) Process finished with exit code 0 在上面的例子中代码： (i:Int) =&gt; i % 2 == 0 就是匿名函数。 2. 将函数转变为变量 val double = (i:Int) =&gt; {i * 2} println(double(3)) 结果： 6 这个时候的变量double是一个实例。在这种情况下他是函数实例你可以像函数一样去调用他。 除了调用，还可以将其传入到任何具有相同参数签名的方法或者函数中： val double = (i:Int) =&gt; {i * 2} val list = List.range(1,5) val l =list.map(double) println(l) 结果： List(2, 4, 6, 8) Process finished with exit code 0 Scala的Unit类： Unit类和java的Void类似，使用于表明函数无返回值的场景。比如我们定义main函数： def main(args: Array[String]): Unit = { } 3. 使用闭包 先看一个例子： package scope{ class Foo{ def exec(f:String =&gt; Unit,name:String): Unit ={ f(name) } } } object claEx extends App{ var hello = &quot;hello&quot; def sayHello(name:String){println(s&quot;$hello,$name&quot;)} val foo = new scope.Foo foo.exec(sayHello,&quot;hah&quot;) } 结果： hello,hah Process finished with exit code 0 关于闭包的定义有很多，有如下的说法，“闭包是满足下面三个条件的一段代码块”： 代码块可以当作值来传递。 同时可以被任何拥有该值的对象按需执行。 可以引用上下文中已经创建的变量（如它的封闭是相对于变量访问，在数学上称之为“关闭”）。 再举一个简单的例子： 1234567891011121314object Demo&#123; def main(args: Array[String]): Unit = &#123; val newAge = 18 val isMoreThanAge = (age:Int) =&gt; age &gt;= newAge println(isMoreThanAge(3)) println(isMoreThanAge(40)) printResult(isMoreThanAge,33) &#125; def printResult(f:Int =&gt; Boolean,x:Int): Unit =&#123; println(f(x)) &#125; &#125; 结果： false true true Process finished with exit code 0 4.部分应用函数 先看定义： val sum = (a:Int,b:Int,c:Int) =&gt; a+b+c val f = sum(3,6,_:Int) 在函数调用的时候不提供第三个参数，这个时候函数f就变为部分应用函数。 val sum = (a:Int,b:Int,c:Int) =&gt; a+b+c val f = sum(3,6,_:Int) println(f.toString()) 结果： Demo$$$Lambda$2/1329552164@1b40d5f0 结果表明f是一个Function。因为上面第三个参数为Int类型的空缺值，所以f函数还可以传入一个参数： println(f(3)) 结果： 12]]></content>
      <categories>
        <category>scala</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>scala学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala类和对象（二）]]></title>
    <url>%2Fposts%2Fa0c17a47.html</url>
    <content type="text"><![CDATA[1. 类和属性 1.1 如何控制构造函数字段的可见性 在Scala中： 如果一个字段被声明为var, Scala会为该字段生成getter和setter方法。 如果字段是val, Scala只生成getter方法。 如果一个字段没有var或者val的修饰符， Scala比较保守，不会生成getter和setter方法。 另外，var和val字段可以被private关键字修饰，这样可以防止生成getter和setter方法。 我们看一下测试代码： 12345678910object Demo&#123; def main(args: Array[String]): Unit = &#123; val p = new Person("haah") println(p.name) &#125; class Person(var name:String)&#123; &#125;&#125; 结果： haah 我们再把name设置为val: 还没有到运行阶段，编译器自动捕获了异常。改为var就可以了。 如果设置为非val和非var的字段： 直接找不到name这个对象，当构造函数参数既没有声明为val或var时，字段的可见性很受限制，井且Scala不会为此生成访问修改方法。 给val或者var加上private，这个关键字会阻止getter和setter方法的生成，所以这种宇段只能被类的成员变量使用。 2. 构造函数 如同在java中可以定义多个构造函数，Scala中也可以，不同的是除了主构造函数以外其余的构造函数都被称为辅助构造函数。所有的辅助构造函数必须以this为名，另外每个构造函数必须调用之前已经定义好了的构造函数： //主构造函数 class MakePizza(var size:Int,var pizzaType:String){ //辅构造函数 def this(pizzaAuthor:String){ this(size,&quot;3&quot;) } //辅构造函数 def this(){ this(&quot;libai&quot;) } } 定义私有主构造函数： class Order private{ } 私有构造函数是无法实例化的。我们知道在java中实现单例模式就是把构造函数私有化。提供一个getInstance方法来初始化类。 class MakePizza private (var size:Int,var pizzaType:String){ object getInstance extends MakePizza(size,pizzaType){ val makePizza = new MakePizza(size,pizzaType) } } 设置构造函数参数默认值 class Socket(var timeout:Int = 10000) 那么在调用构造函数的时候就可以不指定默认值： val s = new Socket 用case类生成模板代码：、 定义case类可以生成模板类，包括如下方法：apply，unapply，toString，equals，hashCode。 case class Person(name:String,relation:String) 将类定义为case类会生成许多模板代码，好处在于： 会生成一个apply方怯，这样就可以不用new关键字创建新的实例。 由于case类的构造函数参数默认是val ，那么构造函数参数会自动生成访问方站。如果是var也会有修改方怯。 会生成一个默认的toString方怯。 会生成一个unapply方法，在模式匹配时很好用。 会生成equals和hashCode方法。 还有一个copy方站。 定义case类，就不用再new来创建一个实例： val wmily = Person(&quot;xiaoming&quot;,&quot;niece&quot;) case类的构造参数默认是val，所以会自动生成get方法，不会生成set。当把case类的构造方法参数设置为var的时候就会有get和set。 case类主要是为了创建“不可变的记录”，这样容易在模式匹配中使用。正因为如此，casel的构造函数参数默认值是val，如果你改为var那就违背了case的本意。 3. 方法 在java中声明一个方法如下： public String doSomething(int x){ } 在scala中则是这样： sef doSomeThing(x:Int):String = { } 控制方法的作用域： Scala中的方法缺省是public。 在java中protected修饰的方法对同一包中的类都可见。但是在Scala中仅对该类的子类可见。 下表中给出了Scala中各种级别的访问控制： 修饰符 描述 private[this] 对当前实例可见 private 对当前类的所有实例可见 protected 对当前类以及其子类的实例可见 private[model] 对model包下的所有类可见 private[coolapp] 对cooapp包下的所有类可见 private[acme] 对acme包下的所有类可见 无修饰符 公开方法 调用父类中的方法： Scala中调用父类，用super代表父类，然后是方法名： class App extends Fruit{ override def onCreate(color:String){ super.onCreate(color) } } 可以继承多个类： **与java中不同的是Scala的类对象可以同时继承多个类。**如果继承的多个类中有相同的方法可以用如下方式选择要使用哪个类中的方法： super[类名].方法名 举个例子： 12345678910111213141516171819202122232425262728object Demo&#123; def main(args: Array[String]): Unit = &#123; var a = new ShowColor a.printApple("red") a.printBanana("yellow") &#125;&#125;trait Fruit&#123; def getColor(color: String): Unit =&#123; printf("the color is %s \n",color) &#125;&#125;trait Apple extends Fruit&#123; override def getColor(color: String): Unit = super.getColor(color)&#125;trait Banana extends Fruit&#123; override def getColor(color: String): Unit = super.getColor(color)&#125;class ShowColor extends Fruit with Apple with Banana&#123; def printSuper(color: String) = super.getColor(color: String) def printApple(color: String) = super[Apple].getColor(color: String) def printBanana(color: String) = super[Banana].getColor(color: String)&#125; 结果： the color is red the color is yellow Process finished with exit code 0 方法参数默认值： 如同python一样，Scala可以给方法的参数一个默认值： def makeCnnection(timeout:Int = 3000,[protocol:String=&quot;http&quot;){ println(&quot;timeout = %d,protocol=%s&quot;.format(timeout,protocol)) } 定义一个返回多个值(tuples)的方法 如果我们希望一个方法能够返回多个值，又不想把这些值字段用一个对象包装，那么可以使用tuples从方法中返回多个值。 tuples是Scala中的数据类型，表示元组的意思。元组是使用()表示的数据结构。后面我们介绍集合列表的时候会详细说。 加入有如下方法返回了包含3个字段的元组： def getStackInfo = { return (&quot;RPC&quot;,&quot;sufface&quot;,34) } 可以使用一个元组来接收： val result = getsStackInfo println(b._1,b._2,b._3) tuple中的值可以通过其位置来访问。 可变参数的方法 在java中可以传入String 类型的可变参数：String … str,在Scala中也提供可变参数的方法：在参数类型后面加一个 “*”，这个参数就变成了可变参数。 def printAll(str:String*){ } 注意：同java一样，当一个方法包含可变参数的时候，那么这个可变参数必须是在所有参数中最后一个位置，否则会报错。 *使用_来匹配一个序列 对于一个序列：Array，List，Seq，Vector，都可以使用_*来匹配里面的每一个对象，从而可以使他可以当做变参传递给一个方法： def main(args: Array[String]): Unit = { val fruit = List(&quot;apple&quot;,&quot;banana&quot;,&quot;pair&quot;) printAll(fruit: _*) } def printAll(str:String*): Unit ={ str.foreach(println) } 解析fruit的每一个对象然后作为参数传递给printAll()方法。 方法的异常声明 使用@throws注解声明可能抛出的异常。这个和java中的使用方式有区别。 @throws(classOf[Exception]) def paly: Unit ={ //code.... } 4. 对象 对象的强制转换： 使用asInstanceOf将一个实例转换为期望的类型。 val recongnizer = cm.lookup(&quot;recongnizer&quot;).asInstanceOf[Recognizer] 上面的Scala代码等于下面的java代码： Recognizer recognizer = (Recognizer)cm.lookup(&quot;Recognizer&quot;); java中的对象.class的Scala等价类 在java中当你有一个方法里面要求将不同的对象作为参数传进来然后使用不同对象的不同方法，这个时候你可以传入一个参数：class clz。然后利用反射获取不同对象的方法。 在Scala中使用classOf方法来代替java中的.class。 val info = new DataLine.Info(classOf[TargetDataLine],null) 等价于java中的： info = new DataLine.info(TargetDataLine.class,null) 用object 启动一个应用： 在Scala中启动一个应用有两种方法： 定义继承App特质的object； 定义一个object，并实现main方法 object Demo{ def main(args: Array[String]): Unit = { println(“hello”) } def printAll(str:String*): Unit ={ str.foreach(println) } } 或者： object Hello extends App{ println(&quot;hello&quot;) } 不用new关键字创建对象实例： 有两种办法： 为类对象创建伴生类，并在伴生类内定义一个apply方法； 将类定义为case类。 用apply方法创建一个伴生类： class Person{ var name:String = &quot;&quot; } object Person{ def apply(name:String): Person = { var p = new Person p.name = name p } } 调用方式： val carry = Person(&quot;carry&quot;) 在同一个文件中定义Person类和Person对象，在对象中定义apply方法接受期望的参数，这个方法本质上是类的构造函数。 将类声明为case类： case class Person(var name:String) val p = Person(&quot;carry&quot;) case类起的作用在于他在伴生类中生成了一个apply方法，上文我们已经说过将一个类定义为case类会自动生成很多方法。 5.包管理 Scala的包管理跟Java类似，但更灵活。除了在类文件的开头用package语句外，还可以用花括号，与C＋＋和C＃的命名空间很像。 Scala会隐式的导入两个包： java.lang._ scala._ Scala里的&quot;_“字符类似于java里的”*&quot;。 5.1 花括号风格的包记号法： package com.rickiyang.zoo{ class Foo{ //todo } class food{ //todo } } 这种方式允许在一个文件中放多个包。也可以用“花括号”方式定义嵌套的包。 5.2 在导入时重命名类名： import java.util.{ArrayList =&gt; JavaList} 然后再代码中就可以使用别名了。 当你为你的类取了别名后那么原来的名字就不可以使用了。否则会出错。 6. 特质—&gt;java中的接口 上文中我们使用过trait修饰类，trait就是Scala中的特质，相当于java中的接口。正如Java类能够实现多个接口一样， Scala类可以继承多个特质。所以trait的功能要比java接口的功能要强大的多。 做为普通的接口来使用： 方法如果不需要任何参数，在def后面指定方法名即可： trait BasePlay{ def playBasketBall def playFootBall } 需要参数只需要将其罗列出来： trait BasePlay{ def playBasketBall(count:Int) def playFootBall(count:Int) } 当一个类需要继承特质时，要使用extends和with关键字。只继承一个特质时，使用extends: class BasketBall extends BasketBall{ //todo } 继承一个类和一个或多个特质时，对类使用extends ，对特质使用with。 当一个类继承多个特质时，使用extends继承第一个特质，其余的使用with 。 除非实现特质的类是一个抽象类，否则它必须实现特质所有的抽象方怯。 如果一个类继承了一个特质但是没有实现它的抽象方法，这个类必须被声明为抽象类。 特质也可以继承另一个特质。 像抽象类一样使用特质： 定义为trait的类中既可以有抽象方法也可以有已经实现了的方法。 12345678910trait BasePlay&#123; def playBasketBall &#123;println("haha")&#125; def playFootBall&#125;class Football extends BasePlay &#123; override def playFootBall: Unit = &#123; println("hahahh football") &#125;&#125; 对于trait中已经实现了的方法，他的继承类可以重写也可以不重写。但是抽象方法一定要重写，否则继承类必须定义为抽象类(abstract)。]]></content>
      <categories>
        <category>scala</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>scala学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala基础语法学习（一）]]></title>
    <url>%2Fposts%2Fe8e22028.html</url>
    <content type="text"><![CDATA[1. val和var的区别 val定义的是一个常量，无法改变其内容 scala&gt; val s = 0 s: Int = 0 scala&gt; s = 2 &lt;console&gt;:12: error: reassignment to val s = 2 ^ 如果要声明其值可变的变量，可以使用var scala&gt; var s = 0 s: Int = 0 scala&gt; s = 5 s: Int = 5 在scala中建议使用val，除非你真的想改变他的内容。 2. 基本类型 Byte Char Short Int Long Float Double Boolean 3.字符串 3.1 测试字符串是否相等 123456789101112object Demo&#123; def main(args: Array[String]): Unit = &#123; val s = "hello world"; println(s) val s1 = "Hello"; val s2 = "Hello"; val s3 = "H"+"ello"; println(s1.equals(s2)) println(s1.eq(s3)) println(s1 == s2) &#125;&#125; 结果： hello world true true true Process finished with exit code 0 在Scala中，可以用＝＝方法测试对象的相等性。这与Java中用equals方法比较两个对象不同。 3.2 多行字符串 在Scala中可以使用三个引号创建多行字符串： 12345678910object Demo&#123; def main(args: Array[String]): Unit = &#123; val s = """ohh haha nonono """; println(s) &#125;&#125; 输出结果： ohh haha nonono ​ Process finished with exit code 0 结果中可以看到每从第二行起每一行都是以空格开头的，要解决这个问题可以这样做：在多行字符串结尾添加stripMargin方法，并且在第一行后面的所有行以管道符&quot;|&quot;开头： 12345678910object Demo&#123; def main(args: Array[String]): Unit = &#123; val s = """ohh |haha |nonono """.stripMargin; println(s) &#125;&#125; 结果： ohh haha nonono ​ Process finished with exit code 0 如果你不想用&quot;|&quot;,你也可以用任意字符： 12345val s = """ohh /haha /nonono """.stripMargin("/"); 也会有同样的输出效果。 3.3 字符串中的变量替换 Scala支持在字符串中代换变量。 如果某个字符串需要插入变量，需要在该字符串的前面加上字母&quot;s&quot;标志，然后再字符串中放入变量，每个变量都应该以&quot;$&quot;开头。 12345678object Demo&#123; def main(args: Array[String]): Unit = &#123; val name = "xiaoming" val age = 12 var s = s"$name is a man and he is $age years old" println(s) &#125;&#125; 结果： xiaoming is a man and he is 12 years old Process finished with exit code 0 在字符串前面添加&quot;s&quot;，其实是在创建一个处理字符串字面量。 在字符串字面量中使用表达式 除了把变量放到字符串中外，还可以用花括号把表达式包起来再放到字符串中。根据官方文档，&quot;${}&quot;内可嵌入任何表达式。 12val age = 12println(s"are you kidding me,I always thought you were only $&#123;age + 1&#125; years old") 字符串插值f 如果有一个需求是不改变原变量的前提下给该值添加两位小数，该如何实现呢？ 12val weight = 111println(f"ohh dear,I never realized that you have $weight%.2f kilograms") 结果： ohh dear,I never realized that you have 111.00 kilograms 这个简单的需求繁盛了“f字符串插值”。一个可以通过printf个时候内部字符串的方法。 用这种方法只需要如下两步即可： 1.在字符串前面加上字母f。 2.在变量之后使用printf的格式化操作符。 raw插入符 raw插入符“不会对字符串中的字符进行转义”。 scala&gt; s&quot;hello\nyou&quot; res0: String = hello you scala&gt; raw&quot;hello\nyou&quot; res1: String = hello\nyou printf格式化常用字符： 格式化符号 描述 %c 字符 %d 十进制数字 %e 指数浮点数 %f 浮点数 %i 整数 %o 八进制 %s 字符串 %u 无符号十进制 %x 十六进制 %% 打印一个百分号 \% 打印一个百分号 3.4 遍历字符串中的字符 Scala提供了很多方式遍历字符串以及对字符串进行操作，常用的有map，foreach，for，filter。 scala&gt; val s = &quot;you are a kid&quot;.map(c =&gt; c.toUpper) s: String = YOU ARE A KID scala&gt; val s = &quot;you are a kid&quot;.map(_.toUpper) s: String = YOU ARE A KID scala&gt; val s = &quot;you are a kid&quot;.filter(_ != &quot;\n&quot;).map(v =&gt; v) ^ s: String = you are a kid val name = &quot;542352354&quot; for(c &lt;- name){ println(c) } name.foreach(println) 4. 数值 Scala内建数值类型包括： Char Byte Short Int Long Float Double 4.1 从字符串到数字 使用String的to*方法。 scala&gt; &quot;200&quot;.toInt res0: Int = 200 如果需要转换的字符串不确定是几进制的，可以通过参数的形式来判断： val a = Integer.parseInt(&quot;01101&quot;,2) println(a) 结果： 13 Process finished with exit code 0 4.2 ++和– Scala中没有++ 和 --这样的操作。所以要是想使用的话只能使用变形的方法：数值类型的可以使用 += 或者是 -= 来操作： var a = 4 a += 3 println(a) a -= 5 println(a) 结果： 7 2 4.3 处理大数 Scala提供了BigInt和BigDecimal类来创建一个大数： var a = BigInt(987654321) var b = BigDecimal(987654321.543433) var c = a + a var d = a * a var e = b + b var f = b * b println(a,b,c,d,e,f) 结果为： (987654321,987654321.543433,1975308642,975461057789971041,1975308643.086866,975461058863418942.543305425489) 我们看到和java中不一样的是：BigDecimal对象支持所有普通数值类型的操作符。 4.4 生成随机数 Scala中也提供了Random类，用于随机数生成。 var a = Random.nextDouble() 4.5 创建一个数值区间、列表或者数组 使用Int的to方法创建数值区间： for(i &lt;- 0 to 9){ println(i) } 结果： 0 1 2 3 4 5 6 7 8 9 Process finished with exit code 0 也可以使用by方法设置步长： for(i &lt;- 0 to 9 by(2)){ println(i) } 结果： 0 2 4 6 8 Process finished with exit code 0 不得不说，跟java比是简洁太多了。 5. 控制语句 Scala中的if/else语句和java中的一样使用。 5.1 for循环 Scala中的for循环也很简单，使用&quot;&lt;-&quot;符号来获取循环指针。 var a = Array(&quot;liming&quot;,&quot;hanmeimei&quot;,&quot;xiaohong&quot;) for(user &lt;- a){ println(user) } 结果： liming hanmeimei xiaohong Process finished with exit code 0 for循环计数器： for(i &lt;- 0 to 5){ println(i) } for(i &lt;- until a.length){ println(i) } 封装index和值： var a = Array(&quot;liming&quot;,&quot;hanmeimei&quot;,&quot;xiaohong&quot;) for((e,count) &lt;- a.zipWithIndex){ println(e,count) } 结果： (liming,0) (hanmeimei,1) (xiaohong,2) Process finished with exit code 0 遍历Map： val map = Map(&quot;name&quot;-&gt;&quot;xiaoming&quot;,&quot;age&quot;-&gt;12,&quot;sex&quot;-&gt;1) for((k,v) &lt;- map){ println(k,v) } 结果： (name,xiaoming) (age,12) (sex,1) 在for循环中使用多个计数器： Scala允许在一个for循环中使用多个计数器，这个有点高端： var i = 1 var j = 1 var k = 1 for(i &lt;- 1 to 3;j &lt;- 1 to 4;k &lt;- 1 to 5){ println(i,j,k) } 结果： (1,1,1) (1,1,2) (1,1,3) (1,1,4) (1,1,5) (1,2,1) ... (3,4,2) (3,4,3) (3,4,4) (3,4,5) 当我们创建二维数组或者三维数组，就再也不用像java中那样需要三个for循环了。 for循环中嵌入if： for(i &lt;- 1 to 10 if i % 2 == 0){ println(i) } 过滤循环条件。 实现break和continue： Scala中没有break和continue关键字。但是提供了另外的方式来实现该功能,scala.util.control.Breaks类提供了类似的方法： import util.control.Breaks._ object Demo{ def main(args: Array[String]): Unit = { breakable{ for( i &lt;- 1 to 10){ println(i) if(i == 5) break } } } } 结果： 1 2 3 4 5 Process finished with exit code 0 注意：break和breakable不是关键字而是util.control.Breaks类中的方法。 再来看一下continue是如何执行的： import util.control.Breaks._ object Demo{ def main(args: Array[String]): Unit = { for( i &lt;- 1 to 10){ breakable{ if(i == 5){ break }else{ println(i) } } } } } 结果： 1 2 3 4 6 7 8 9 10 Process finished with exit code 0 其实就是换了一种方式使用breakable哈。 Scala中的switch/case： var i = 0 i match { case 1 =&gt; println(1) case 2 =&gt; println(2) } 在Scala中使用match函数来表示switch。 在Scala中一个case语句可以匹配多个条件，你可以这样用： var i = 0 i match { case 1 | 3 | 5 =&gt; println(1) case 2 | 4 | 6=&gt; println(2) } 使用&quot;|&quot;分割。对于字符串和其他类型使用的语法是一样的。]]></content>
      <categories>
        <category>scala</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>scala学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka消息的处理机制（5）]]></title>
    <url>%2Fposts%2F9b912c1b.html</url>
    <content type="text"><![CDATA[这一篇我们不在是探讨kafka的使用，前面几篇基本讲解了工作中的使用方式，基本api的使用还需要更深入的去钻研，多使用才会有提高。今天主要是探讨一下kafka的消息复制以及消息处理机制。 1. broker的注册 Kafka使用Zookeeper来维护集群成员的信息。每个broker都有一个唯一标识符，这个标识符可以在配置文件里指定，也可以自动生成。在kafka启动的时候，他通过创建临节点把自己的id注册到zk，kafka组件订阅zk的/broker/ids路径(broker在zk上的注册路径)，当有broker加入或者退出集群的时候，这些组件就可以获得通知。 如果当前id所在的broker已经注册然后启动另一个有相同id的broker，启动会出错，新的broker会试着进行注册，但是不会成功。因为zk中已经有一个相同名字的id注册过了。 如果broker出现停机或者网络长时间无响应，broker会从zk断开链接，zk中注册的临时节点会删除，下次broker启动需要重新注册。 如果是关闭broker那么他对应的节点也会消失，但是他的id也许会存在于其他的数据结构中。比如主题对应的副本，在完全关闭一个broker之后如果使用相同的id启动另一个全新的broker，他会立即加入集群，并且会拥有之前broker所有的主题和分区(前提是没有发生重排序，没有第二个新的broker加入)。 kafka的哪些组件需要注册到zookeeper？ （1）Broker注册到zk 每个broker启动时，都会注册到zk中，把自身的broker.id通知给zk。待zk创建此节点后，kafka会把这个broker的主机名和端口号记录到此节点。 （2）Topic注册到zk 当broker启动时，会到对应topic节点下注册自己的broker.id到对应分区的isr列表中；当broker退出时，zk会自动更新其对应的topic分区的ISR列表，并决定是否需要做消费者的rebalance （3）Consumer注册到zk 一旦有新的消费者组注册到zk，zk会创建专用的节点来保存相关信息。如果zk发现消费者增加或减少，会自动触发消费者的负载均衡。 （注意，producer不注册到zk） 2. kafka集群leader选举 在kafka集群中，第一个启动的broker会在zk中创建一个临时节点/controller让自己成为控制器。其他broker启动时也会试着创建这个节点当然他们会失败，因为已经有人创建过了。那么这些节点会在控制器节点上创建zk watch对象，这样他们就可以收到这个节点变更的通知。任何时刻都确保集群中只有一个leader的存在。 如果控制器被关闭或者与zk断开连接，zk上的KB是节点马上就会消失。那么其他订阅了leader节点的broker也会收到通知随后他们会尝试让自己成为新的leader，重复第一步的操作。 如果leader完好但是别的broker离开了集群，那么leader会去确定离开的broker的分区并确认新的分区领导者(即分区副本列表里的下一个副本)。然后向所有包含该副本的follower或者observer发送请求。随后新的分区首领开始处理请求。 3. kafka副本 Kafka每个topic的partition有N个副本，其中N是topic的复制因子。Kafka通过多副本机制实现故障自动转移，当Kafka集群中一个Broker失效情况下仍然保证服务可用。在Kafka中发生复制时确保partition的预写式日志有序地写到其他节点上。N个replicas中。其中一个replica为leader，其他都为follower，leader处理partition的所有读写请求，与此同时，follower会被动定期地去复制leader上的数据。 Kafka必须提供数据复制算法保证,如果leader发生故障或挂掉，一个新leader被选举并接收客户端的消息成功写入。Kafka确保从同步副本列表中选举一个副本为leader,或者换句话说,follower追赶leader数据。leader负责维护和跟踪ISR中所有follower滞后状态。当生产者发送一条消息到Broker,leader写入消息并复制到所有follower。消息提交之后才被成功复制到所有的同步副本。消息复制延迟受最慢的follower限制,重要的是快速检测慢副本,如果follower”落后”太多或者失效,leader将会把它从replicas从ISR移除。 3.1 kafka创建副本的2种模式——同步复制和异步复制 Kafka动态维护了一个同步状态的副本的集合（a set of In-Sync Replicas），简称ISR，在这个集合中的节点都是和leader保持高度一致的，任何一条消息只有被这个集合中的每个节点读取并追加到日志中，才会向外部通知说“这个消息已经被提交”。 只有当消息被所有的副本加入到日志中时，才算是“committed”，只有committed的消息才会发送给consumer，这样就不用担心一旦leader down掉了消息会丢失。消息从leader复制到follower,我们可以通过决定Producer是否等待消息被提交的通知(ack)来区分同步复制和异步复制。 同步复制流程： producer联系zk识别leader； 向leader发送消息； leadr收到消息写入到本地log； follower从leader pull消息； follower向本地写入log； follower向leader发送ack消息； leader收到所有follower的ack消息； leader向producer回传ack。 异步复制流程： 和同步复制的区别在于，leader写入本地log之后，直接向client回传ack消息，不需要等待所有follower复制完成。 既然卡夫卡支持副本模式，那么其中一个Broker里的挂掉，一个新的leader就能通过ISR机制推选出来，继续处理读写请求。 kafka判断一个broker节点是否存活，依据2个条件： 节点必须可以维护和ZooKeeper的连接，Zookeeper通过心跳机制检查每个节点的连接; 如果节点是个follower,他必须能及时的同步leader的写操作，延时不能太久。Leader会追踪所有“同步中”的节点，一旦一个down掉了，或是卡住了，或是延时太久，leader就会把它移除。 3.2 一些名词： Leader副本：每个分区都有多个副本，针对每个分区，都有一个唯一的一个Leader副本，负责该分区的读写请求处理。 Follower副本：从Leader副本拉取数据，作为Leader副本的热备。 AR：（Assigned Replica）副本集合（Leader+Follower的总和） ISR：（In-Sync Replica）同步副本集合，与leader副本消息镜像“相差”不多的副本集合，又称为“核心副本集”，与kafka 发送端的ACK的几种语义有关，后面会详聊（注意这个集合是动态的，是会剔除和新增的）。 HW：HW俗称高水位，HighWatermark的缩写，取一个partition对应的ISR中最小的LEO作为HW，consumer最多只能消费到HW所在的位置。另外每个replica都有HW,leader和follower各自负责更新自己的HW的状态。对于leader新写入的消息，consumer不能立刻消费，leader会等待该消息被所有ISR中的replicas同步后更新HW，此时消息才能被consumer消费。这样就保证了如果leader所在的broker失效，该消息仍然可以从新选举的leader中获取。对于来自内部broKer的读取请求，没有HW的限制。6. LEO：(Log End Offset)每个分区都会有的一个标记，标示当前分区的最后一条消息（针对Leader就是Leader上的最后一条消息，针对某个Follower，就是当前该Follower的最后一条消息） 3.3 图解AR,ISR,HW,LEO： 这里我们假设每个副本有三个分区，副本被剔除和加入ISR的临界条件为落后leader 三条消息，kafka判断是否符合ISR的条件有两个: Follower落后leader多少条消息，落后超过配置值后将踢出ISR Follwer多久没从leader同步消息，超过配置时间没拉取数据将从ISR踢出（kafka0.9后删除了该判断，a为唯一判断标准）。 下面我们用图来表达下上面的概念的关系： 1.时刻t1该分区的情况如下，此时ISR与AR一致（Leader，follower1，follower2），follower2 和 leader的消息一致，LEO都为4，follower1的LEO为2，因此leader的HW为2。 2.时刻t2 follower full gc： 3.时刻t3，leader接受producer发送来的2条消息5、6，此时发现Follower1已经落后了自己4条消息，将follower1踢出ISR集合： 4.时刻4，follower1从leader拉取到5这条消息，更新HW： 5.时刻5，follower1 full gc完成后，发现自己已经落后了很多消息，开始从leader追消息，待消息不落后leader太多时，申请加入ISR中。 经过上面的图解分析后，我们来看下几个需要注意的点: ISR是AR的一个子集，并且是不断伸缩的，变化的条件为“是否落后太多的消息” HW之前的消息代表被集群“commit”的消息，只有commit的消息才对client端（consumer以及request.required.acks为-1时的producer），在前面我们说过，这样能够使kafka在语义上支持不丢消息。我们从producer和consumer两个维度来分析： 在这之前，我们先说下request.required.acks的取值范围（1，0、-1） 1:leader成功就返回 0:无需等待leader响应 -1:ISR都成功才返回 从producer的角度：当producer将request.required.acks设置为-1时候，保证了消息已经在多个副本中存在了，此时即便leader挂了，这个消息还是存在的（leader选举会从ISR中选举出新的leader），那么假如ISR迟迟同步不成功怎么办呢？ 从consumer的角度：如果没有HW，consumer拉取到最新的消息后，而此时leader宕机，很有可能新的leader中并没有此消息。 当然不能保证消息永远不会丢，极端的情况下，如ISR中只有leader的时候（当然可以配置集群可用的最小核心副本集个数，但会极大的损失可用性），或者所有副本都宕机了（这个。。。没办法。），消息还是会丢的。]]></content>
      <categories>
        <category>kafka</category>
      </categories>
      <tags>
        <tag>学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka同步异步消费和消息的偏移量（4）]]></title>
    <url>%2Fposts%2Fb11b90c4.html</url>
    <content type="text"><![CDATA[1. 消费者位置(consumer position) 因为kafka服务端不保存消息的状态，所以消费端需要自己去做很多事情。我们每次调用poll()方法他总是返回已经保存在生产者队列中还未被消费者消费的消息。消息在每一个分区中都是顺序的，那么必然可以通过一个偏移量去确定每一条消息的位置。 偏移量在消费消息的过程中处于重要的作用。如果是自动提交消息，那么poll()方法会去在每次获取消息的时候自动提交获取最后一条消息的偏移量，告诉服务器我们已经消费到这个位置，下次从下一个位置开始消费。 我们把更新分区当前位置的操作叫做提交。消费者是如何提交偏移量的呢？kafka最新的api是这样做的：创建一个叫做_consumer_offset的特殊主题用来保存消息的偏移量。消费者每次消费都会往这个主题发送消息，消息包含每个分区的偏移量。 如果消费者一直处于运行的状态那么这个偏移量没有什么用。不过如果这个消费者崩溃或者有新的消费者加入群组触发再均衡策略，那么再均衡之后该分区的消费者如果不是之前的那一位，那么新的小伙伴怎么知道之前的伙计消费到哪里呢。所以提交他自己的offset就发挥作用了。 Consumer读取partition中的数据是通过调用发起一个fetch请求来执行的。而从KafkaConsumer来看，它有一个poll方法。但是这个poll方法只是可能会发起fetch请求。原因是：Consumer每次发起fetch请求时，读取到的数据是有限制的，通过配置项max.partition.fetch.bytes来限制的。而在执行poll方法时，会根据配置项个max.poll.records来限制一次最多pool多少个record。 那么就可能出现这样的情况： 在满足max.partition.fetch.bytes限制的情况下，假如fetch到了100个record，放到本地缓存后，由于max.poll.records限制每次只能poll出15个record。那么KafkaConsumer就需要执行7次才能将这一次通过网络发起的fetch请求所fetch到的这100个record消费完毕。其中前6次是每次pool中15个record，最后一次是poll出10个record。 在consumer中，还有另外一个配置项：max.poll.interval.ms ，它表示最大的poll数据间隔，如果超过这个间隔没有发起pool请求，但heartbeat仍旧在发，就认为该consumer处于 livelock状态。就会将该consumer退出consumer group。所以为了不使Consumer 自己被退出，Consumer 应该不停的发起poll(timeout)操作。而这个动作 KafkaConsumer Client是不会帮我们做的，这就需要自己在程序中不停的调用poll方法了。 当一个consumer因某种原因退出Group时，进行重新分配partition后，同一group中的另一个consumer在读取该partition时，怎么能够知道上一个consumer该从哪个offset的message读取呢？也是是如何保证同一个group内的consumer不重复消费消息呢？上面说了一次走网络的fetch请求会拉取到一定量的数据，但是这些数据还没有被消息完毕，Consumer就挂掉了，下一次进行数据fetch时，是否会从上次读到的数据开始读取，而导致Consumer消费的数据丢失吗？ 为了做到这一点，当使用完poll从本地缓存拉取到数据之后，需要client调用commitSync方法（或者commitAsync方法）去commit 下一次该去读取 哪一个offset的message。 而这个commit方法会通过走网络的commit请求将offset在coordinator中保留，这样就能够保证下一次读取（不论是进行rebalance）时，既不会重复消费消息，也不会遗漏消息。 对于offset的commit，Kafka Consumer Java Client支持两种模式：由KafkaConsumer自动提交，或者是用户通过调用commitSync、commitAsync方法的方式完成offset的提交。 2. 位移管理(offset management) 2.1 自动提交 Kafka默认是定期帮你自动提交位移的(enable.auto.commit = true)，使用这种简单的方式之前你需要知道可能会带来什么后果。 假设我们仍然使用默认的5s提交时间间隔，在最近一次提交之后的3s发生了再均衡，再均衡之后，消费者从最后一次提交的偏移量位置开始读取消息。这个时候偏移量已经落后 了3s，所以在这3s内到达的消息会被重复处理。可以通过修改提交时间间隔来更频繁地提交偏移量，减小可能出现重复悄息的时间窗，不过这种情况是无也完全避免的。 在使用自动提交时，每次调用轮询方告都会把上一次调用返回的偏移量提交上去，它并不知道具体哪些消息已经被处理了，所以在再次调用之前最好确保所有当前调用返回的消息都已经处理完毕（在调用close()方法之前也会进行自动提交）。一般情况下不会有什么问题，不过在处理异常或提前退出轮询时要格外小心。 2.2 手动提交 在多partition多consumer的场景下自动提交总会发生一些不可控的情况。所以消费者API也为我们提供了另外一种提交偏移量的方式。开发者可以在程序中自己决定何时提交，而不是基于时间间隔。 在使用手动提交之前我们需要先将： properties.put(&quot;enable.auto.commit&quot;, &quot;false&quot;); 然后使用： consumer.commitSync(); 来提交。 commitSync()方法会提交由poll()方法返回的最新偏移量，提交成功后马上返回，否则跑出异常。 我们处理消息的逻辑可以变成这样： 123456789101112while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; System.out.printf("offset = %d, key = %s, value = %s%n", record.offset(), record.key(), record.value()); try &#123; consumer.commitSync(); &#125; catch (Exception e) &#123; System.out.println("commit failed"); &#125; &#125;&#125; 每处理一次消息我们提交一次offset。 异步手动提交 上面我们使用commitSync()的方式提交数据，每次提交都需要等待broker返回确认结果。这样没提交一次等待一次会限制我们的吞吐量。 如果采用降低提交频率来保证吞吐量，那么则有增加消息重复消费的风险。所以kafka消费者提供了异步提交的API。我们只管发送提交请求无需等待broker返回。 1234567while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; System.out.printf("offset = %d, key = %s, value = %s%n", record.offset(), record.key(), record.value()); &#125; consumer.commitAsync();&#125; commitAsync()方法提交最后一个偏移量。在成功提交或碰到无怯恢复的错误之前，commitAsync()会一直重试，但是commitAsync()不会，这也是commitAsync()不好的一个地方。它之所以不进行重试，是因为在它收到服务器响应的时候， 可能有一个更大的偏移量已经提交成功。假设我们发出一个请求用于提交偏移量2000,这个时候发生了短暂的通信问题，服务器收不到请求，自然也不会作出任何响应。与此同时，我们处理了另外一批消息，并成功提交了偏移量3000。如果commitAsync()重新尝试提交偏移量2000 ，它有可能在偏移量3000之后提交成功。这个时候如果发生再均衡，就会出现重复消息。 当然使用手动提交最大的好处就是如果发生了错误我们可以记录下来。commitAsync()也支持回调方法，提交offset发生错误我们可以记下当前的偏移量。 1234567891011121314while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; System.out.printf("offset = %d, key = %s, value = %s%n", record.offset(), record.key(), record.value()); &#125; consumer.commitAsync(new OffsetCommitCallback() &#123; @Override public void onComplete(Map&lt;TopicPartition, OffsetAndMetadata&gt; map, Exception e) &#123; if(e != null)&#123; System.out.println("commit failed"+map); &#125; &#125; &#125;);&#125; 同步和异步组合提交 一般情况下，针对偶尔出现的提交失败，不进行重试不会有太大问题，因为如果提交失败是因为临时问题导致的，那么后续的提交总会有成功的。但如果这是发生在关闭消费者或再均衡前的最后一次提交，就要确保能够提交成功。因此，在消费者关闭前一般会组合使用commitAsync()和commitSync()。 1234567891011121314151617try &#123; while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; System.out.printf("offset = %d, key = %s, value = %s%n", record.offset(), record.key(), record.value()); &#125; consumer.commitAsync(); &#125;&#125; catch (Exception e) &#123; System.out.println("commit failed");&#125; finally &#123; try &#123; consumer.commitSync(); &#125; finally &#123; consumer.close(); &#125;&#125; 如果一切正常我们使用commitAsync()来提交。如果直接关闭消费者，就没有所谓的下一次提交了。使用commitSync()会一直重试，直到提交成功。 2.3 提交特定偏移量 上面我们手动提交使用的commitAsync()和commitSync()都是提交每一次消费最后一条消息的偏移量，那么如果我们一次拉取了很多消息但是没有消费完，想提交我们消费完成的位置该怎么处理呢？kafka也有相应的对策。 12345678910111213Map&lt;TopicPartition,OffsetAndMetadata&gt; currentOffset = new HashMap&lt;&gt;();while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; System.out.printf("offset = %d, key = %s, value = %s%n", record.offset(), record.key(), record.value()); currentOffset.put(new TopicPartition(record.topic(),record.partition()),new OffsetAndMetadata(record.offset(),"metadata")); try &#123; System.out.println("模拟消息处理失败的情况"); &#125; catch (Exception e) &#123; consumer.commitAsync(currentOffset,null); &#125; &#125;&#125; 这里调用的是commitAsync()，调用commitSync()也是可以的。代码中模拟我们在处理消息的过程中可能会出错的情况，每次读消息都把当前的offset存入map中，如果出错就提交当前已经消费到的偏移量。 2.4 再均衡监听器 前面我们说过当发生consumer退出或者新增，partition新增的时候会触发再均衡。那么发生再均衡的时候如果某个consumer正在消费的任务没有消费完该如何提交当前消费到的offset呢？kafka提供了再均衡监听器，在发生再均衡之前监听到，当前consumer可以在失去分区所有权之前处理offset关闭句柄等。 消费者API中有一个()方法： 1subscribe(Collection&lt;TopicPartition&gt; var1, ConsumerRebalanceListener var2); ConsumerRebalanceListener对象就是监听器的接口对象，我们需要实现自己的监听器继承该接口。接口里面有两个方法需要实现： 123void onPartitionsRevoked(Collection&lt;TopicPartition&gt; var1);void onPartitionsAssigned(Collection&lt;TopicPartition&gt; var1); 第一个方法会在再均衡开始之前和消费者停止读取消息之后被调用。如果在这里提交偏移量，下一个接管分区的消费者就知道该从哪里开始读取了。 第二个会在重新分配分区之后和消费者开始读取消息之前被调用。、 我们来模拟一下再均衡的场景： 1234567891011121314151617181920212223242526272829303132final Consumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props);consumer.subscribe(Arrays.asList("page_visits"));final Map&lt;TopicPartition,OffsetAndMetadata&gt; currentOffset = new HashMap&lt;&gt;();class HandleRebance implements ConsumerRebalanceListener&#123; @Override public void onPartitionsRevoked(Collection&lt;TopicPartition&gt; collection) &#123; System.out.println("partition is rebanlance"); consumer.commitAsync(currentOffset,null); &#125; @Override public void onPartitionsAssigned(Collection&lt;TopicPartition&gt; collection) &#123; &#125;&#125;consumer.subscribe(topic,new HandleRebance());while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; System.out.printf("offset = %d, key = %s, value = %s%n", record.offset(), record.key(), record.value()); currentOffset.put(new TopicPartition(record.topic(),record.partition()),new OffsetAndMetadata(record.offset(),"metadata")); try &#123; System.out.println("模拟消息处理失败的情况"); &#125; catch (Exception e) &#123; consumer.commitAsync(currentOffset,null); &#125; &#125;&#125; 首先实现了ConsumerRebalanceListener接口，实现方法里面如果监听到发生再均衡我们提交当前处理过的偏移量。 2.5 从特定偏移量处开始处理 前面都是consumer.poll()之后读取该批次的消息，kafka还提供了从分区的开始或者末尾读消息的功能： 12seekToEnd(Collection&lt;TopicPartition&gt; partitions)seekToBeginning(Collection&lt;TopicPartition&gt; partitions) 另外kafka还提供了从指定偏移量处读取消息，可以通过seek()方法来处理： 1seek(TopicPartition partition, long offset) 提交当前分区和当前消费位置信息。 2.6 独立消费者–不属于群组的消费者 到目前为止我们讨论的都是消费者群组，分区被自动分配给群组的消费者，群组的消费者有变动会触发再均衡。那么是不是可以回归到别的消息队列的方式：不需要群组消费者也可以自己订阅主题？ kafka也提供了这样的案例，因为kafka的主题有分区的概念，那么如果没有群组就意味着你的自己订阅到特定的一个分区才能消费内容。如果是这样的话，**就不需要订阅主题，而是为自己分配分区。**一个消费者可以订阅主题（井加入消费者群组），或者为自己分配分区，但不能同时做这两件事情。 下面的例子演示如何为自己分配分区并读取消息的： 1234567891011121314151617181920212223final Consumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props);List&lt;PartitionInfo&gt; partitionInfoList = consumer.partitionsFor("page_visits");List&lt;TopicPartition&gt; topicPartitionList = new ArrayList&lt;&gt;();if(partitionInfoList != null)&#123; for(PartitionInfo partitionInfo : partitionInfoList)&#123; topicPartitionList.add(new TopicPartition(partitionInfo.topic(),partitionInfo.partition())); consumer.assign(topicPartitionList); &#125;&#125;final Map&lt;TopicPartition,OffsetAndMetadata&gt; currentOffset = new HashMap&lt;&gt;();while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; System.out.printf("offset = %d, key = %s, value = %s%n", record.offset(), record.key(), record.value()); currentOffset.put(new TopicPartition(record.topic(),record.partition()),new OffsetAndMetadata(record.offset(),"metadata")); try &#123; System.out.println("模拟消息处理失败的情况"); &#125; catch (Exception e) &#123; consumer.commitAsync(currentOffset,null); &#125; &#125;&#125; consumer.partitionsFor(“主题”)方法允许我们获取某个主题的分区信息。 知道想消费的分区后使用assign()手动为该消费者分配分区。 除了不会发生再均衡，也不需要手动查找分区，其他的看起来一切正常。不过要记住，如果主题增加了新的分区，消费者并不会收到通知。所以，要么周期性地调用consumer.partitionsFor()方法来检查是否有新分区加入，要么在添加新分区后重启应用程序。]]></content>
      <categories>
        <category>kafka</category>
      </categories>
      <tags>
        <tag>学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka客户端和服务端开发（3）]]></title>
    <url>%2Fposts%2F34bc78bf.html</url>
    <content type="text"><![CDATA[前面我们已经搭建了kafka的单机和集群环境，分别写了简单的实例代码，对于代码里面使用到的参数并没有做解释。下面我们来详细说一下各个参数的作用。 1. 创建kafka生产者 kafka生产者有3个必选的属性： bootstrap.servers 该属性指定broker的地址清单，地址的格式为host:port。清单里不需要包含所有的broker 地址，生产者会从给定的broker里查找到其他broker 的信息。不过建议至少要提供两个broker的信息，一且其中一个若机，生产者仍然能够连接到集群上。 key.serializer broker 希望接收到的消息的键和值都是字节数组。生产者接口允许使用参数化类型，因此可以把Java对象作为键和值发送给broker。这样的代码具有良好的可读性，不过生产者需要知道如何把这些Java 对象转换成字节数组。key.serializer必须被设置为一个实现了org.apache.kafka.common.serialization.Serializer接口的类，生产者会使用这个类把键对象序列化成字节数组。 要注意， key.serializer是必须设置的，就算你打算只发送值内容。 value.serializer 与key.serializer一样,value.serializer指定的类会将值序列化。如果键和值都是字符串，可以使用与key.serializer一样的序列化器。否则就得使用不同的序列化器。 下面的代码中展示了如何创建一个生产者，这里只指定了必要的字段。 12345Properties props = new Properties();props.put("bootstrap.servers", "192.168.131.128:9092,192.168.131.130:9092,192.168.131.131:9092");props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");Producer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props); 我们的键值都定义为String对象，所以都是用的是内置的StringSerializer。 实例化生产者对象之后我们就可以发送消息了，发消息主要有一下3种方式： 发送并忘记(fire-and-forget) 我们把消息发送给服务器，但井不关心它是否正常到达。大多数情况下，消息会正常到达，因为Kafka 是高可用的，而且生产者会自动尝试重发。不过，使用这种方式有时候也会丢失一些消息。 同步发送 我们使用send()方怯发送消息， 它会返回一个Future对象，调用get()方法进行等待，就可以知道消息是否发送成功。 异步发送 我们调用send()方法并且指定一个回调函数，服务器在返回响应时调用该函数。 上面我们已经创建了一个生产者实例，接下来就可以发送消息了： 123456ProducerRecord&lt;String, String&gt; data = new ProducerRecord&lt;String, String&gt;("page_visits","test_key","test_msg");try&#123; producer.send(data);&#125;catch&#123; e.printStackTrace();&#125; 上面我们构造了一个ProducerRecord对象用来封装topic，消息的key-value。然后用生产者实例的send方法将消息体发送出去。 1.1 同步发送消息 最简单的同步发送消息如下： 123456ProducerRecord&lt;String, String&gt; data = new ProducerRecord&lt;String, String&gt;("page_visits","test_key","test_msg");try&#123; producer.send(data).get();&#125;catch&#123; e.printStackTrace();&#125; 即producer.send(data)返回的是一个Future对象，然后调用Future的get()方法僧带Kafka的响应。如果服务器返回错误，get()方法会抛异常。没有发生错误的话会得到一个RecordMetadata对象。可以用它来获取消息的偏移量。 1.2 异步发送消息 producer发送消息给broker，如果发送成功，会把目标主题，分区信息以及消息偏移量发送回来。但是发送端大多数时候并不关心这些，只在乎是否发送成功。有的时候对于发送是否成功的状态也不是那么的着急需要。在消息量比较大的时候，如果每一条消息都需要同步去确认发送状态很显然是会发生超时的，这个时候异步回调机制很好的帮我们解决了这个问题。 1234567891011121314ProducerRecord&lt;String, String&gt; data = new ProducerRecord&lt;&gt;("page_visits","test_key","test_msg");try&#123; producer.send(data,new Callback() &#123; public void onCompletion(RecordMetadata metadata, Exception e) &#123; if(e != null) &#123; e.printStackTrace(); &#125; else &#123; System.out.println("The offset of the record we just sent is: " + metadata.offset()); &#125; &#125; &#125;);&#125;catch&#123; e.printStackTrace();&#125; 实现回调需要实现org.apache.kafka.clients.producer.Callback接口，这个接口只有一个onCompletion方法。 上面我们简单实现了一个生产者，提供了一个基本实现，producer还有很多其余的配置可以灵活使用，下面我们来看一下其余的参数： acks ：acks参数指定了必须要有多少个分区副本收到消息，生产者才会认为消息写入是成功的。这个参数对消息丢失的可能性有重要影响。该参数有如下选项： acks=0 生产者在成功写入悄息之前不会等待任何来自服务器的响应。 acks=1 只要集群的首领节点收到消息， 生产者就会收到一个来自服务器的成功响应。 acks=all 只有当所有参与复制的节点全部收到消息时， 生产者才会收到一个来自服务器的成功响应。这种模式是最安全的，它可以保证不止一个服务器收到消息，就算有服务器发生崩溃，整个集群仍然可以运行,不过，它的延迟比acks=l 时更高，因为我们要等待不只一个服务器节点接收消息。 buffer.memory: 该参数用来设置生产者内存缓冲区的大小，生产者用它缓冲要发送到服务器的消息。如果应用程序发送消息的速度超过发送到服务器的速度，会导致生产者空间不足。 compression.type : 默认情况下，消息发送时不会被压缩。该参数可以设置为snappy，gzip或lz4，它指定了消息被发送给broker 之前使用哪一种压缩算陆进行压缩。 retries ： 生产者从服务器收到的错误有可能是临时性的错误（比如分区找不到首领）。在这种情况下，retries参数的值决定了生产者可以重发消息的次数，如果达到这个次数，生产者会放弃重试并返回错民。默认情况下，生产者会在每次重试之间等待lOOms，不过可以通过retry.backoff.ms参数来改变这个时间间隔。 batch.size : 当有多个消息需要被发送到同一个分区时，生产者会把它们放在同一个批次里。该参数指定了一个批次可以使用的内存大小，按照字节数计算（而不是消息个数）。当批次被填满，批次里的所有消息会被发送出去。不过生产者井不一定都会等到批次被填满才发送，半满的批次，甚至只包含一个消息的批次也有可能被发送。 linger.ms : 该参数指定了生产者在发送批次之前等待更多消息加入批次的时间。KafkaProducer会在批次填满或linger.ms达到上限时把批次发送出去。默认情况下,只要有可用的线程，生产者就会把消息发送出去，就算批次里只有一个消息。把linger.ms设置成比0大的数，让生产者在发送批次之前等待一会儿，使更多的消息加入到这个批次。 partitioner.class :实现Partitioner接口的分区类 max.block.ms : 这个配置控制着KafkaProducer.send() 和 KafkaProducer.partitionsFor()会阻塞多久。这个方法会因为缓冲区满了或是元数据不可用而被阻塞。在用户提供的 serializers 或 partitioner 里的阻塞将不会被计算在这个超时里。 client.id : 发送请求时传递给服务器的一个标识字符串。这样做的目的是能过追踪请求的来源除了除了IP/端口。它是通过允许一个逻辑应用名称被包括在一个服务器端的请求日志来实现的。 1.3分区和实现自定义分区策略 在前面的基本知识学习里面我们已经讲过kafka分区的问题，一个topic的消息会对应着一个或者多个partition。如果使用默认的分区器，那么记录将会被随机的发送到主题可用的各个分区上，分区器使用轮询算法均衡的分布到各个分区上。 kafka给用户提供了灵活的分区策略。如果你不想使用默认的均衡分布策略，而是有自己的逻辑需求，那么你可以自己控制。比如在一个topic下的消息分为两种：一种是用户信息增量更新的消息，一种是任职信息增量更新的消息。那么你可以使用自定义分区功能将两种消息分别散列在同一个topic的各个partition上。 实现自定义分区策略需要继承Partitioner接口。 1234567891011121314151617181920212223242526272829303132333435363738394041import java.util.List;import java.util.Map;import org.apache.kafka.clients.producer.Partitioner;import org.apache.kafka.common.Cluster;import org.apache.kafka.common.PartitionInfo;public class SimplePartitioner2 implements Partitioner &#123; private static String USER_INFO_KEY = "simple_user_info"; @Override public void configure(Map&lt;String, ?&gt; configs) &#123; // TODO Auto-generated method stub &#125; @Override public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) &#123; int partition = 0; List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic); int numPartitions = partitions.size(); if(numPartitions != 2)&#123; return partition; &#125; String stringKey = (String) key; if(stringKey.contains(USER_INFO_KEY))&#123; partition =0; &#125; return partition; &#125; @Override public void close() &#123; // TODO Auto-generated method stub &#125;&#125; 在上面的例子中，我们通过实现Partitioner的partition方法，对消息的key进行硬编码，确定key中是否包含某个标志来区分消息的类型进而确定该消息应该进入哪个分区。 2. 创建kafka消费者 2.1 消费者中的群组 熟悉JMS消息协议的应该知道，消息的消费应该有两种方式：一种是点对对的消息，一方发送，一方消费。一种是发布-订阅模式，一方发送，所有订阅方都可以消费。kafka也是遵循JMS消息规范的，所以这两种消息模式都是可以支持。但是支持的方式可能和我们想象的有些不一样！ kafka里面的消费者对应着一个消费者群组的概念。与别的消息中间件产品不同的是，每个消费者必须是属于某个消费者群组。一个群组里的消费者订阅的是同一个主题。我们通过kafka的&quot;group.id&quot;参数来配置消费者所属的群组。 如果你以为kafka的服务端收到一条推送的消息之后，订阅的群组里面所有消费者可以同时消费到这一条消息那你就错了。kafka对于同属一个群组的消费者的定义是： 同一个分区的消息只能被某个群组中的某个消费者同时消费。 如果一个群组中的多个消费者订阅了同一个分区，那么只能有一个消费者可以同时获得这一条消息。 只有一个消费者的群组是可以同时监听到某个主题下的多个分区的。 同一个群组下的一个消费者可以指定监听多个分区，如果消费者数量和分区数量相等也可以指定1V1的监听模式。但是，如果消费者数量大于分区数量，那么必然会有消费者无法获得消息。 由上我们得知：群组的概念并不是消费-订阅模式！ 我们需要严格保证同一个群组下的消费者数量必须小于等于所订阅的topic的分区数量。 如果存在消费者数量多于分区数量的情况，我们可以将消费者置于不同的分组中，然后再订阅该主题。kafka支持多个分组同时订阅。 关于具体的消费者的消费模式我们后面详细讨论，先来实现一个简单的消费者。 12345678Properties props = new Properties();props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "192.168.131.128:9092,192.168.131.130:9092,192.168.131.131:9092");props.put(ConsumerConfig.GROUP_ID_CONFIG ,"test") ;props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);Consumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props); 消费者基本的配置和生产者差不多，我们看到第二个设置，指定了分组id。 然后需要指定要订阅哪一个topic，当然一个分组是可以订阅多个topic的，所以参数为List： 1consumer.subscribe(Arrays.asList("page_visits")); 参数也可以是正则匹配。 消息轮询是消费者API的核心，通过一个简单的轮询向服务器请求数据： 123456while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; System.out.printf("offset = %d, key = %s, value = %s%n", record.offset(), record.key(), record.value()); &#125;&#125; 消费者必须不停地向kafka轮询，否则会被认为死亡。它的分区会被交给组里的其他消费者。poll()方法里面的参数是一个超时时间，用于设置该次请求的超时时间，如果参数被设置为0 ，请求会立即返回，否则请求会在指定的毫秒数内返回。 2.2 消费者基本配置项 消费者也有一些基本的配置项： fetch.min.bytes:该属性指定了消费者从服务器获取记录的最小字节数。 fetch.max.wait.ms:该属性指定了消费者从服务器获取记录的最大等待时间。该参数和fetch.min.bytes参数会互相克制。如果fetch.min.bytes没有得到满足，则会继续等待消息填充，但是如果到了fetch.max.wait.ms设置的时间，那么会直接返回。同理相反也是一样的。 max.partition.fetch.bytes:该属性指定了服务器从每个分区里返回给消费者的最大字节数。它的默认值是lMB。 session.timeout.ms:该属性指定了消费者在被认为死亡之前可以与服务器断开连接的时间，默认是3s。如果消费者没有session.timeout.ms定的时间内发送心跳给群组协调器，就被认为已经死亡，协调器就会触发再均衡，把它的分区分配给群组里的其他消费者。该属性与heartbeat.interval.ms紧密相关heartbeat.interval.ms指定了poll()向服务器发送心跳的频率， session.timeout.ms则指定了消费者可以多久不用给服务器发送心跳。 auto.offset.reset:该属性指定了消费者在读取一个没有偏移量的分区或者偏移量无效的情况下（因消费者长时间失效，包含偏移量的记录已经过时井被删除）该作何处理。即指定该从何位置开始读取记录。 enable.auto.commit:该属性指定了消费者是否自动提交偏移量，默认值是true。为了尽量避免出现重复数据和数据丢失，可以把它为false，由自己控制何时提交偏移量。如果把它设为true ，还可以通过配置auto.commit.interval.ms属性来控制提交的频率。 partition.assignment.strategy:分区会被分配给群组里的消费者。PartitionAssignor根据给定的消费者和主题，决定哪些分区应该被分配给哪个消费者。Kafka有两个默认的分配策略: range 该策略会把主题的若干个连续的分区分配给消费者。假设消费者Cl和消费者C2 同时订阅了主题Tl和主题口，井且每个主题有3 个分区。那么消费者Cl有可能分配到这两个主题的分区0和分区i，而消费者C2分配到这两个主题的分区2。因为每个主题拥有奇数个分区，而分配是在主题内独立完成的，第一个消费者最后分配到比第二个消费者更多的分区。只要使用了Range策略，而且分区数量无法被消费者数量整除，就会出现这种情况。 RoundRobin 该策略把主题的所有分区逐个分配给消费者。如果使用RoundRobin策略来给消费者Cl和消费者C2分配分区，那么消费者Cl将分到主题Tl的分区0和分区2以及主题T2的分区1 ，消费者C2 将分配到主题Tl 的分区l 以及主题口的分区0 和分区2 。一般来说，如果所有消费者都订阅相同的主题（这种情况很常见）, RoundRobin策略会给所有消费者分配相同数量的分区（或最多就差一个分区）。 可以通过设置partition.assignment.strategy来选择分区策略. client.id:该属性可以是任意字符串，broker用它来标识从客户端发送过来的消息，通常被用在日志、度量指标和配额里。 主要的配置项如上，如果工作中还有别的需要可以去看官网的api查看。 客户端其实要说的东西还是挺多，下一节学习我们就来看关于自动提交、手动提交和偏移量的关系，又够喝一壶的。]]></content>
      <categories>
        <category>kafka</category>
      </categories>
      <tags>
        <tag>学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka集群模式安装（2）]]></title>
    <url>%2Fposts%2Fa74a883e.html</url>
    <content type="text"><![CDATA[我们来安装kafka的集群模式，三台机器： 192.168.131.128 192.168.131.130 192.168.131.131 kafka集群需要依赖zookeeper，所以需要先安装好zk。 下载kafka安装包： kafka_2.11-1.1.0.tgz 解压到 /usr/local/下。 进入到kafka的config目录下： 我们看到有zk的配置文件，这是kafka自带的zk，如果你没有安装zk，可以使用kafka集成的zk，配置方式和单独安装是一样的。 我们默认已经安装zk，所以修改server.properties文件，大致的配置项有这些： broker.id=0 #每个实例不一样 listeners=PLAINTEXT://192.168.131.128:9092 #改为所在主机的ip advertised.host.name=192.168.131.128 #改为改为所在主机的ip num.network.threads=3 num.io.threads=8 socket.send.buffer.bytes=102400 socket.receive.buffer.bytes=102400 socket.request.max.bytes=104857600 log.dirs=/usr/local/kafka/log #需手动创建，kafka并不会根据配置文件自动创建 num.partitions=1 num.recovery.threads.per.data.dir=1 log.retention.hours=168 log.segment.bytes=1073741824 log.retention.check.interval.ms=300000 zookeeper.connect=192.168.131.128:2181,192.168.131.130:2181,192.168.131.131:2181 #修改为zookeeper所在主机ip:port zookeeper.connection.timeout.ms=6000 delete.topic.enable=true auto.create.topics.enable=false 需要修改的地方已经标注出来了。 然后我们需要将kafka同步到另外两台机器上： scp -r kafka hadoop@hadoopslaver1:/usr/local scp -r kafka hadoop@hadoopslaver2:/usr/local 下面我们准备启动，首先确保zk是启动的，如果没有安装可以使用kafka的zk： bin/zookeeper-server-start.sh config/zookeeper.properties &amp; 然后我们启动kafka： bin/kafka-server-start.sh -daemon config/server.properties &amp; 三台机器上都要执行启动操作，如果偶没有报错就是启动成功了。 接下来我们可以做一些测试。 消费端： 123456789101112131415161718192021222324252627282930313233import java.util.Arrays;import java.util.Properties;import org.apache.kafka.clients.consumer.Consumer;import org.apache.kafka.clients.consumer.ConsumerConfig;import org.apache.kafka.clients.consumer.ConsumerRecord;import org.apache.kafka.clients.consumer.ConsumerRecords;import org.apache.kafka.clients.consumer.KafkaConsumer;import org.apache.kafka.common.serialization.StringDeserializer;public class Consumer &#123; public static void main(String[] args) &#123; Properties props = new Properties(); props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "192.168.131.128:9092,192.168.131.130:9092,192.168.131.131:9092"); props.put(ConsumerConfig.GROUP_ID_CONFIG ,"test") ; props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, "true"); props.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, "1000"); props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class); props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class); Consumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props); consumer.subscribe(Arrays.asList("page_visits")); while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; System.out.printf("offset = %d, key = %s, value = %s%n", record.offset(), record.key(), record.value()); &#125; &#125; &#125;&#125; 生产者： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import java.util.Date;import java.util.Properties;import java.util.Random;import org.apache.kafka.clients.producer.Callback;import org.apache.kafka.clients.producer.KafkaProducer;import org.apache.kafka.clients.producer.Producer;import org.apache.kafka.clients.producer.ProducerRecord;import org.apache.kafka.clients.producer.RecordMetadata;public class Producer &#123; public static void main(String[] args) &#123; long events = 1; Random rnd = new Random(); Properties props = new Properties(); props.put("bootstrap.servers", "192.168.131.128:9092,192.168.131.130:9092,192.168.131.131:9092"); props.put("acks", "all"); props.put("retries", 0); props.put("batch.size", 16384); props.put("linger.ms", 1); props.put("buffer.memory", 33554432); props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer"); props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer"); //配置partitionner选择策略，可选配置 props.put("partitioner.class", "com.rickiyang.service.Partitioner"); Producer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props); for (long nEvents = 0; nEvents &lt; events; nEvents++) &#123; long runtime = new Date().getTime(); String ip = "192.168.2." + rnd.nextInt(255); String msg = runtime + ",www.example.com," + ip; ProducerRecord&lt;String, String&gt; data = new ProducerRecord&lt;String, String&gt;("page_visits", ip, msg); producer.send(data, new Callback() &#123; public void onCompletion(RecordMetadata metadata, Exception e) &#123; if(e != null) &#123; e.printStackTrace(); &#125; else &#123; System.out.println("The offset of the record we just sent is: " + metadata.offset()); &#125; &#125; &#125;); &#125; producer.close(); &#125;&#125; 自定义分区策略： 1234567891011121314151617181920212223242526272829303132333435363738import java.util.List;import java.util.Map;import org.apache.kafka.clients.producer.Partitioner;import org.apache.kafka.common.Cluster;import org.apache.kafka.common.PartitionInfo;public class Partitioner implements Partitioner &#123; @Override public void configure(Map&lt;String, ?&gt; configs) &#123; // TODO Auto-generated method stub &#125; @Override public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) &#123; int partition = 0; List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic); int numPartitions = partitions.size(); String stringKey = (String) key; int offset = stringKey.lastIndexOf('.'); if (offset &gt; 0) &#123; partition = Integer.parseInt( stringKey.substring(offset+1)) % numPartitions; &#125; return partition; &#125; @Override public void close() &#123; // TODO Auto-generated method stub &#125;&#125; 我们运行一下： Producer： Consumer 客户端可以接受到服务端的消息的。]]></content>
      <categories>
        <category>kafka</category>
      </categories>
      <tags>
        <tag>学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka基本知识入门（1）]]></title>
    <url>%2Fposts%2F2ea97000.html</url>
    <content type="text"><![CDATA[1、 基础知识 有关RabbitMQ,RocketMQ,kafka的区别这个网上很多，了解一下区别性能，分清什么场景使用。分布式环境下的消息中间件Kafka做的比较不错，在分布式环境下使用频繁，我也不免其俗钻研一下Kafka的使用。 任何消息队列都遵循AMQP协议，AMQP协议(Advanced Message Queuing Protocol,高级消息队列协议） AMQP是一个标准开放的应用层的消息中间件（Message Oriented Middleware）协议。AMQP定义了通过网络发送的字节流的数据格式。因此兼容性非常好，任何实现AMQP协议的程序都可以和与AMQP协议兼容的其他程序交互，可以很容易做到跨语言，跨平台。 Kafka是一个分布式的、可分区的、可复制的消息系统。它提供了普通消息系统的功能，但具有自己独特的设计。 我们先看一些基本的概念： 消费者：（Consumer）：从消息队列中请求消息的客户端应用程序 生产者：（Producer） ：向broker发布消息的应用程序 AMQP服务端（broker）：用来接收生产者发送的消息并将这些消息路由给服务器中的队列，便于fafka将生产者发送的消息，动态的添加到磁盘并给每一条消息一个偏移量，所以对于kafka一个broker就是一个应用程序的实例 主题（Topic）：一个主题类似新闻中的体育、娱乐、教育等分类概念，在实际工程中通常一个业务一个主题。 分区（Partition）：一个Topic中的消息数据按照多个分区组织，分区是kafka消息队列组织的最小单位，一个分区可以看作是一个FIFO（ First Input First Output的缩写，先入先出队列）的队列。 kafka将消息以topic为单位进行归纳，每个broker其实就是一个应用服务器，一个broker中会有很多的topic，每个topic其实就是不同的服务需要消息的消息的聚集地。因为每个topic其实会很大，所以就出现了partition个概念，将每个topic的消息分区存储。 kafka中的消费者有一个分组的概念，每个consumer属于一个consumer group;反过来说,每个group中可以有多个consumer.发送到Topic的消息,只会被订阅此Topic的每个group中的一个consumer消费（而不是该group下的所有consumer，一定要注意这点） 如果所有的consumer都具有相同的group,这种情况和queue模式很像;消息将会在consumers之间负载均衡. 如果所有的consumer都具有不同的group,那这就是&quot;发布-订阅&quot;;消息将会广播给所有的消费者. 在kafka中,一个partition中的消息只会被group中的一个consumer消费;每个group中consumer消息消费互相独立;我们可以认为一个group是一个&quot;订阅&quot;者,一个Topic中的每个partions,只会被一个&quot;订阅者&quot;中的一个consumer消费,不过一个consumer可以消费多个partitions中的消息. 分布式环境中，Kafka默认使用zookeeper作为注册中心，kafka集群几乎不维护任何consumer和producer的信息状态，这些信息都由zookeeper保存，所以consumer和producer非常的轻量级，随时注册和离开都不会对Kafka造成震荡。 producer和consumer通过zookeeper去发现topic，并且通过zookeeper来协调生产和消费的过程。 producer、consumer和broker均采用TCP连接，通信基于NIO实现。Producer和consumer能自动检测broker的增加和减少。 上面图中没有说明partition的组成，partition物理上由多个segment组成，每一个segment 数据文件都有一个索引文件对应。每个partition都由一系列有序的、不可变的消息组成，这些消息被连续的追加到partition中。partition中的每个消息都有一个连续的序列号叫做offset,用于partition唯一标识一条消息. 相比传统的消息系统，Kafka可以很好的保证有序性。 传统的队列在服务器上保存有序的消息，如果多个consumers同时从这个服务器消费消息，服务器就会以消息存储的顺序向consumer分发消息。虽然服务器按顺序发布消息，但是消息是被异步的分发到各consumer上，所以当消息到达时可能已经失去了原来的顺序，这意味着并发消费将导致顺序错乱。为了避免故障，这样的消息系统通常使用“专用consumer”的概念，其实就是只允许一个消费者消费消息，当然这就意味着失去了并发性。 在这方面Kafka做的更好，通过分区的概念，Kafka可以在多个consumer组并发的情况下提供较好的有序性和负载均衡。将每个分区分只分发给一个consumer组，这样一个分区就只被这个组的一个consumer消费，就可以顺序的消费这个分区的消息。因为有多个分区，依然可以在多个consumer组之间进行负载均衡。注意consumer组的数量不能多于分区的数量，也就是有多少分区就允许多少并发消费。 Kafka只能保证一个分区之内消息的有序性，在不同的分区之间是不可以的，这已经可以满足大部分应用的需求。如果需要topic中所有消息的有序性，那就只能让这个topic只有一个分区，当然也就只有一个consumer组消费它。 1.1、 message 被分配到 partition 的过程 每一条消息被发送到broker时，会根据paritition规则（有两种基本的策略，一是采用Key Hash算法，一是采用Round Robin算法）选择被存储到哪一个partition。如果partition规则设置的合理，所有消息可以均匀分布到不同的partition里，这样就实现了水平扩展。（如果一个topic对应一个文件，那这个文件所在的机器I/O将会成为这个topic的性能瓶颈，而partition解决了这个问题）。 在发送一条消息时，可以指定这条消息的key，producer根据这个key和partition机制来判断将这条消息发送到哪个parition。paritition机制可以通过指定producer的paritition.class这一参数来指定，该class必须实现kafka.producer.Partitioner接口。 1.2、 segment文件存储结构 segment file由2大部分组成，分别为index file和data file，这两个文件一一对应，成对出现，后缀&quot;.index&quot;和“.log”分别表示为segment索引文件、数据文件。 segment文件命名规则：partion全局的第一个segment从0开始，后续每个segment文件名为上一个segment文件最后一条消息的offset值。数值最大为64位long大小，19位数字字符长度，没有数字用0填充。 文件类似于下面这种形式： 0000000000000000001.index 0000000000000000001.log 0000000000000036581.index 0000000000000036581.log 0000000000000061905.index 0000000000000061905.log index和data-file的对应关系如下： index file 存储索引文件，文件中的元数据指向对应数据文件中message的物理偏移地址。 2、 Kafka单机环境搭建 下载kafka，解压缩 配置环境变量： export KAFKA_HOME=/usr/local/kafka export PATH=$PATH:$KAFKA_HOME/bin 重启生效 source /etc/profile Kafka用到了zeekeeper，所以需要先启动zookeeper，没有安装的需要先安装zk，安装好了以后我们可以启动，我们先来实现单机版的kafka，先启动一个单单例的zk服务，可以在命令的结尾加个&amp;符号，这样就可以启动后离开控制台。 # bin/zookeeper-server-start.sh config/zookeeper.properties &amp; 再启动kafka： # bin/kafka-server-start.sh config/server.properties 创建topic： # bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test 创建producer，可以在控制台手动输入消息： # bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test this is a message ctrl+c 可以退出发送。 创建consumer： # bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic test --from-beginning this is a message 会收到刚才的发送的消息 我们的一个简单的单机环境就搭建好了。]]></content>
      <categories>
        <category>kafka</category>
      </categories>
      <tags>
        <tag>学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive安装]]></title>
    <url>%2Fposts%2Fc40dabb2.html</url>
    <content type="text"><![CDATA[hive安装 下载hive，我下载的版本是：apache-hive-2.3.2-bin.tar.gz。解压文件： #tar -zxvf apache-hive-2.3.2-bin.tar.gz 设置环境变量： #vi /etc/profile export HIVE_HOME=/usr/local/hive export PATH=PATH:PATH:PATH:HIVE_HOME/bin #source /etc/profile Hive中metastore（元数据存储）的存储方式有三种： a)内嵌Derby方式 b)Local方式 c)Remote方式 本次选择使用local方式，存储到当前机器的mysql中。没有安装mysql的需要先安装mysql。 另外，因为hive启动需要从元数据存储器中查找元数据并加载，所以需要将mysql的jar包放在hive安装包下的lib目录下。&lt;mysql-connector-java-5.1.38.jar&gt;下载放入即可。 进入hive下的conf目录：将hive-default.xml.template 复制一份存为： hive-site.xml；将hive-env.sh.template修改存为：hive-env.sh。 修改hive-site.xml中的内容： 12345678910111213141516171819202122232425262728293031&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/user/hive_remote/warehouse&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.metastore.local&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://localhost/hive_remote?createDatabaseIfNotExist=true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;hive&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;password&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; mysql的用户名和密码写自己的。另外还有两处需要修改： 123456789101112&lt;property&gt;&lt;name&gt;hive.exec.scratdir&lt;/name&gt;&lt;value&gt;/usr/hive/tmp&lt;/value&gt;&lt;/property&gt;这个是设定临时文件目录--------------------------------------//这个在笔者的文件中没有可以自己添加&lt;property&gt;&lt;name&gt;hive.querylog.location&lt;/name&gt;&lt;value&gt;/usr/hive/log&lt;/value&gt;&lt;/property&gt;这个是用于存放hive相关日志的目录 如果不改启动的时候会报错的！ 修改hive-env.sh，添加hadoop的路径： HADOOP_HOME=/usr/local/hadoop 上面我们在hive-site.xml中配置了hive的元数据存放库，那么在mysql中也需要创建相应的库，我使用的用户名是hive如果你想使用root或者别的也是可以的： mysql&gt; create database hive_remote; mysql&gt; GRANT ALL PRIVILEGES ON *.* TO 'hive'@'%' IDENTIFIED BY 'hive'; 元数据库创建完毕，需要hive元数据库初始化，执行： schematool -dbType mysql -initSchema 没有出错就是初始化完毕，有出错查看相关错误解决。接下来我们就可以启动hive试一下了，进入bin目录，输入：hive即可： [hadoop@hadoopmaster bin]$ hive which: no hbase in (/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/usr/java/jdk/bin:/usr/java/jdk/jre/bin:/usr/local/hadoop/bin:/usr/local/hadoop/sbin:/usr/local/zookeeper/bin:/usr/local/hive/bin:/home/hadoop/bin) SLF4J: Class path contains multiple SLF4J bindings. SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory] Logging initialized using configuration in jar:file:/usr/local/hive/lib/hive-common-2.3.2.jar!/hive-log4j2.properties Async: true Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases. hive&gt; show tables; OK Time taken: 5.91 seconds hive&gt; show databases; OK default Time taken: 0.068 seconds, Fetched: 1 row(s) 如上便是成功了。]]></content>
      <categories>
        <category>大数据学习</category>
        <category>hive</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git学习（2）]]></title>
    <url>%2Fposts%2Feed666b7.html</url>
    <content type="text"><![CDATA[git常用指令一览表 GIT指令 说明 git add . 将全部文件的内容加到Git索引以便执行commit. 这个指令不会检查文件夹中是否有文件被删除。 要注意的是，只有执行“ git add”时的文件内容会被加入Git 索引。如果后来又修改了文件， 新的文件内容不会在Git索引中。我们必须重新执行“ git add ”指令，才会更新Git索引。 git add 文件名 文件名 ... 将指定的文件的内容加到Git 索引，以便执行commit 操作。要注意的是， 只有执行“ git add”时，文件内容才会被加入Git 索引。如果后来又修改了文件， 新的文件内容不会在Git 索引中。我们必须重新执行“ git add ”指令，才会更新Git 索引。 git add -A 除了把全部文件的内容加到Git 索引以外，也会检查文件夹中是否有文件被删除。这些 被删除的文件会标记在Git 索引中，当执行commit 指令的时候，被标记删除的文件也会 从新的commit 节点中被删除。我们可以从Git 丈档库的历史版本中找回被删除的文件。 git add --update 或是 git add -u 对比当前文件夹中的文件内容和Git 文档库中的文件内容，把有修改的部分和删除的文 或是件加到Git 索引，以便执行commit 。这个指令不会增加新的文件到Git 索引，只会更新 或是删除文件。 git blame 文件名 或者 git blame -L 起始行，结束行文件名 或者 git blame -L 起始行，文件名 或者 git blame -L 结束行，文件名 显示文件的每一行是由谁修改。可以搭配“－L”选项，指定要从哪一行开始到哪一行 或是结束。如果没有指定起始行，表示是从文件的第一行开始。如果没有指定结束行， 表示要到文件的最后一行。 git branch 自己取的分支名称[commit 节点标识符或是标签] 按照参数的多少，会有不同的功能： 1. 如果最后指定了commit 节点标识符或是标签，就会从该节点“长”出分支；如果没 有指定commit 节点，就会从最新的commit“长”出分支; 2 “ git branch”指令后面没有接任何参数时，会列出当前文档库中正在开发的所有分支。 git branch 新分支的名称己经存在的分支 从特定的分支，再长出另一个新的分支。 git branch -a 列出丈档库和远程文档库中所有的分支。 git branch -d 要删除的分支名称 删除指定的分支。必须先切换到另一个分支,才能执行这个指令。 git branch -D 要删除的分支名称 在一般情况下，分支应该先合并到另一个分支，之后才能够被删除。如果我们要删除还 没有合并的分支， Git 会显示错误信息，并且停止删除分支的操作。如果确定要删除还 没有合并的分支，可以使用“－D ”选项，要求Git 强制执行删除分支的操作。 git branch --list 分支名称样板 显示符合“分支名称样板”的所有分支，例如以下指令范例会显示所有以“ bug/”开头 的分支：git branch --list bug/* git branch -m 新的分支名称 更改分支的名称。必须先切换到该分支，才能够执行这个指令。 git checkout 文件1 文件2 ... 或是 git checkout . Git 会先找索引中有没有该文件，如果有就把它取出：如果没有，就从最新的commit 节点开始，按照时间顺序往前寻找，然后取出第一个找到的文件版本，每一个文件都用 同样的方式处理。如果要取出文档库中全部文件的最新版本，可以执行“ git checkout .”’ git checkout commit 节点标识符或标签[文件1 文件2..] 从Git 文档库的commit 节点取出指定的文件。如果取出的文件和当前文档库中最新 commit 的文件内容不同，这个取出的文件内容会自动记录在Git 索引中。下次执行"git commit" 指令时，这个取出的文件内容就会存入文档库中成为新的版本。如果要避免这 种情况发生，可以在执行“git checkout"指令之后，立刻执行“ git reset HEAD ”来清除 Git 索引。 git checkout 分支名称 将当前操作的分支切换到指定的分支。 git checkout -f 分支名称 在切换分支的时候， Git 会先对比丈档库中当前分支的文件内容是否和将要切换过去的 分支的文件内容相同。针对内容不一样的文件， Git 需要从文档库中取出该文件，这是 为了让文件夹中的文件符合分支原来的状态。但是为了避免数据遗失，当Git 要覆盖 文件夹中的文件时，会检查该文件的内容是否已经加入文档库。如果还没有加入， Git 会显示警告信息，并且停止执行，以免资料遗失。如果我们确定不想保留这些已经修改 却还没有加入文档库的文件，可以加入“－f”选项，这样Git 就会强制覆盖修改后的文件。 git checkout -b 新分支的名称[commit 节点标识符或是标签] 创建指定的分支，然后切换到新创建的分支。这个指令等同于先执行“ git branch 新分支的 名称commit 节点标识符或是标签” ， 接着再执行“ git checkout 新分支的名称”。如果最 后指定了commit 节点标识符或是标签，就会从该节点“长”出分支。如果没有指定commit 节点，就会从最新的commit “ 长”出分支。 git cherry-pick -n commit 节点标识或标签 把指定的commit节点的文件版本合并到文件夹中的文件。在默认情况下 ，执行这个指令会创建一个新的commit 节点。如果不想要创建新节 或标签点， 则可以加上"－n"选项。执行这个指令之前， 文件夹中被修改的文件必须先存入Git 文档库，否则会出现警告信息，并且停止执行。 git clone “远程Git 文档库”的路径 从“远程Git 文档库”复制一个“本地Git或是文档库”到我们的计算机。 第一种方式适用的情况是“远程Git 文档库”在我们自己的计算机上。 git clone --bare[程序项目文件夹名称][“远程Git文档库”的路径] 从“本地Git 文档库”复制出Bare 类型的“远程Git 文档库" 我们通常会帮Bare类型的文档库加上扩展名".git"，例如game.git。 git commit -m ‘这次操作的说明’ [--author=‘操作者姓名 [email 邮箱]’] 把当前Git 索引的内容送进文档库存储。每一次执行commit 一定要附加说明和操作者 信息。如果没有使用“－m ” 选项， Git 会启动文字编辑程序让我们输入操作说明。默认 的文字编辑程序是vi ，我们可以使用“ git config ”指令设置其他的文字编辑程序，详 细操作方式请参考第2 单元的说明。除了使用“--author”选项输入操作者信息以 外，也可以把操作者的信息记录在Git 配置文件中， 这样就不用加上“--author” 选项. git commit -a -m ‘这次操作的说明’[--author=‘操作者姓名[email 邮箱]’] 或是 git commit --all -m ‘这次操作的说明’[--author=‘操作者姓名[email 邮箱]’] 这个指令的效果等同于先执行“git add -u"再执行“ git commit -m ‘这次操作的说明’ --author=‘操作者姓名[email 邮箱]’”。首先对比当前文件夹中的文件内容和Git 文 档库中的文件内容，把有修改的部分和删除的文件加到Git 索引，然后执行commit。这 个指令不会把新的文件加到Git 文档库。 git commit --amend -m ‘新的操作说明’--author=‘操作者姓名[email 邮箱]’) 修改最近一次commit 节点的操作说明，或是操作者信息 git config -l 或者 git config --global -l 或者 git config --system -l 只有使用“ －l”选项时会显示三个不同层级的配置文件中所有的设置项。低优先权配置 文件的设置项会先显示，最高优先权配置文件的设置项显示在最后。 加入“--global ” 选顶时会显示优先权配置文件中的设置项，也就是登录账号的home 或是directory 里面的.gitconfig 文件中的设置。 加入“--system ”选项时会报示最低优先权配置文件中的设置顶，也就是Git程序安装文 件夹里面的etc\gitconfig 文件夹的设置。 git config 设置项名称‘设置值’ 或者 git config --global 设置项名称‘设置值’ 或者 git config --system 设置项名称‘设置值‘ 在Git 配置文件中加入或是修改设置。如果“设置值”中没有空格，可以省略单引号。 如果没有使用任何选项，表示要将设置项写到当前操作中的Git文档库中的配置文件。 如果加入“--global ”选项，表示要将设置项写到登录账号的home directory 里面 的.gitconfig 配置文件中．如果加入“--system”选项，表示要将设置项 写到Git 程序安装文件夹里面的etc\gitconfig配置文件中。 Git diff 文件名 判断该文件是否发成冲突或者解决冲突是否成功 Git fetch 或者 git fetch --all 从“远程Git 文档库”取回当前所在分支的最新数据。完成这项工作之后，我们计算机 上的“远程Git 文档库”的当前分支状态就和实际的情况一致。如果想要一次获取全部 分支最新的数据，可以加上“－all ”选项。 Git init 在当前的文件夹创建一个Git 文档库。如果这个文件夹己经有Git 文档库，这个指令就 不会再重新创建，也不会修改其中的内容。Git 文档库其实是名称叫作“ .git ”的子文件 夹，默认它会被隐藏起来，我们可以改变文件夹的查看选项让它显示出来。如果删除这 个子文件夹， Git 文档库的内容就会全部消失。 Git init -bare Git 文档库文件夹名称 创建Bare 类型的Git 文档库。我们通常会帮Bare 类型的Git 文档库加上扩展名“ .git ”， 例如game.git 。 git log 按照时间顺序，从最近一次的commit 开始，往前列出每一次commit 的信息，包括标识 符、执行人、日期利时间以及说明。 Git log -after=’公元年－月－日 时间’[--before＝’公元年－月－日 时间’] 指定要显示某一段时间期间的commit 的信息。“ after＂可以换成“-since ”，“--before ” 可以换成“-until"。 Git log -author=‘人名’ 只显示特定人的commit 节点信息。 git log --graph --online --all --decorate 加上“--graph ” 选项会用文本模式排列出commit 节点的演进图．加上’--oneline ”选 项会用最精简的方式显示．加上“ --all ”选项会显示所有分支的commit信息。 加上“--decorate ” 选项表示要标示分支的名称。 git log --stat 或者 git log --shortstat 或者 git log --numstat 显示每一个commit 更改程序代码和文件的情况，包括有多少文件被修改了、 增加了几行程序代码和删除了几行程序代码。 git ls-file 列出当前Git 文档库中的文件列表。 git ls-remote 列出“本地Git 文档库”对应的所有“远程Git 文档库” git merge 分支名称 把指定的分支合并到当前所在的分支。 git merge --abort 合并的过程发生冲突之后，执行这个指令可以放弃合并。Git 文档库和文件夫中的文件 内容都会恢复到未执行合并前的状态。 git merge --no-ff 分支名称 “--no-ff”逃项表示不要使用fast-forward merge 。 git mv 原来的文件名 新文件名 更改文件夹巾的文件名，或是子文件夹名称，然后把它记录在Git 索引。接着只要执行 “git commit"指令，就可以将更改存入文档库。 git pull 或者 git pull -all "git pull"指令会执行两项工作： 1. 从“远程Git 文档库”取回当前所在分支的最新数据。完成这项工作之后，计算机 上的“远程Git 文档库”的当前分支状态就和实际的情况一致。如果想要一次获取全部 分支最新的数据，可以加上“--all ”选项。2. 把“远程Git 文档库”的分支合并到“本 地Git 文档库”的分支。 git pull --rebase 或者 git pull -r 把“ git pull ＂指令的第二个步骤换成“ git rebase” （原来是“ git merge" ）。 git push 执行这个指令时，屏幕画面会显示一段信息，提示我们需要在Git 的配置文件中如入 push.default 的设置。我们可以执行下列指令，指定push.default 为matching: git config --global push.default matching 这样的话，如果只下达"git push"指令，后面没有加上任何参数， Git 会 把“本地Git文档库”中所有曾经传送给“远程Git 文档库”的分支，都一并更新. 或者，可以执行下列指令，指定push.default 为simple: git config --global push.default simple 这样的话，如果只下达“ git push ”指令，后面没有加上任何参数。Git 会检查当前所在 的分支是否在配置文件中记录了它对应的“远程Git 文档库”的分支。如果有，就会 传送更新给“远程Git 文档库”。否则，就不会执行更新。和matching 的设置相比之下， simple 是比较安全的做法。因为它只会更新当前所在的分支，不会一次更新全部的分支。 git push origin 分支名称 把指定分支的最新状态送到origin 属性所对应的“远程Git 文档库”。执行这个指令不 会在配置文件中记录“本地Git 文档库”的分支和“远程Git 文档库”的分支之间的对 应关系． git push “远程Git 文档库”的url 分支名称 把指定分支的最新状态送到指定的“远程Git文档库”。执行这个指令不会在配置文件中 记录“本地Git 文档库”的分支和“远程Git文档库”的分支之间的对应关系。 git push --all 把“本地Git 文档库”中所有的分支传送到“远程Git 文档库”。 git push “远程Git 文档库”的名称 --delete[分支名称] 删除“远程Git 文档库”中的指定分支。 git rebase 分支名称 把指定分支的修改运用到当前的分支。当前分支会变成从指定的分支的HEAD 节点长 出来。 git rebase --abort 如果执行rebase 指令后出现冲突的情况，可以使用这个指令取消rebase 的操作。 Git 文档库会恢复到还没有执行rebase 之前的状态。 git rebase --continue 执行rebase 指令后出现冲突的情况，而且我们己经编辑好发生冲突的文件， 接着就可以执行“git add”指令， 把新的文件内容加入Git 索引， 最后再执行这个指令，完成rebase的操作。 git reflog HEAD 或是任何分支的名称 显示HEAD 或是任何分支变动的历史记录。如果不加任何参数，默认会列出HEAD 变动 的历史记录。 git remote -v 显示和“远程Git 文档库”相关的设置。 git remote add [“远程Git 文档库”的名称] [“远程Git 文档库”的url] 在“本地Git 文档库”的配置文件中加入指定的“远程Git 文档库”名称， 并且设置它 的url git remote rm “远程Git 文档库”的名称 或者 git remote remove “远程Git 文档库”的名称 删除“本地Git 文档序”的配置文件指定的“远程Git 文挡库”名称。 一旦删除“远程Git 文档库”的名称，所有属于它的追踪 分支也会一并消失。如果要再还原回来，只要再执行"git remote add"指令 和“ git remote update"指令即可。 git remote rename 旧名称 新名称 改变“远程Git 文档库”的名称。改变“远程Git 文档库”的名称之后， 和它相关的远程追踪分支的名称也会自动更新。 git reset HEAD 文件名 将指定文件的内容从Git 索引中删除。如果没有加上文件名，则会清除 Git 索引中所有的内容。 git reset --soft commit 节点标识符或是标签 或者 git reset --mixed commit 节点标识符或是标签 或者 git reset --hard commit 节点标识符或是标签 将Git 文档库“消磁”，让Git 文挡库恢复到某一个commit 节点的状态， 如果使用“--soft ＂选项，表示只有文档库中的数据会更改， Git 索引和文件夹中的文件都不会受到影响。如果使用“--mixed ”选项（这是默 认的选项），表示Git 索引也会恢复到指定节点的状态，但是文件夹中的文件仍然不会 受到影响。如果使用“--hard ” 选项，则文档库、Git索引和文件夹中的文件都会恢复到 指定节点的状态。 git revert commit 节点 回到指定的commit 节点的前一个节点的文件状态。执行完毕后会新增一个commit 节 点。请注意和“ git reset ”指令的差别。 git revert --abort 如果执行“git revert”指令的时候发生冲突，可以执行这个指令取消revert 的操作。 git rm 文件名 Git 会执行两项检查：Git 索引中有没有该文件的内容（也就是 刚刚有没有执行过“git add 文件名”），如果有，表示这个文件的内容和文档库中的不 一样，为了避免遗失数据， Git 会显示提醒信息，然后放弃执行： 2. 文件夹中的文件内容是不是和文档库中的一样，如果不一样， Git 同样会显示提醒 信息，然后放弃执行，以免遗失数据。如果通过以上两项检查， Git 会马上删除文 件夹中的文件，然后在索引中记录要从文档库中删除该文件。最后必须再执行"git commit"指令才会真正从文档库中删除文件。 git rm --cached 文件名 把指定的文件从tracked 状态变成untracked状态，也就是说从此以后不需要在文档库中 更新这个文件， 而且Git 索引中这个文件的内容也会被删除，但是Git 不会从文件夹中 删除这个文件(这是加上"--cached”选项最大的差别)。 git shortlog 按照人名的字母顺序，列出每－个人执行commit 的次数和说明。 加上“--numbered ”选项（或是“ －n " ）可以按照commit 次数，由高至低按序排列。 如果不需要显示commit 说明，可以加上"--summary ＂选项（或是“－s ”）。 git show commit 节点标识符或是标签 显示特定commit 节点的详细信息。commit节点标识符是一组很长的16 进制数字，指 定commit 节点标识符时，不需要将它完整列出。一般只要使用最前面4 个数字即可， Git 会向动找出对应的节点。如果找到超过一个以上的节点， Git 会显示错误信息， 这时候换长一点的数字就可以解决。 git show 文件名 显示指定文件最新版本的修改情况。也就是比较文件最新版本和前一个版本的差异。 git stash list 显示Git 暂存区的状态。 git stash pop 或者 git stash apply 取出暂存区的文件，将它们的内容合并到当前文件夹中的文件。 git stash save 这个指令会执行下列两项工作： 存储文件夹中被Git 追踪的文件和文档库中最新文件版本的差异： 把文件夹中被Git 追踪的文件还原成文档库中最新的文件版本。 git status 这个指令会执行以下3 项工作： ]. 检查Git 索引的内容，看看是否需要执行commit 存入文档库。 2. 对比文件夹中的文件和文档库中的文件，列出修改过的文件列表。 3. 列出untracked 状态的文件。 git tag 自定义的标签名称[commit 节点标识符或是标签] 指定的commit 节点贴上自定义的标签，之后就可以用这个自定义的标签来指定这个 commit 节点。 git tag -d commit 节点标签 删除自定义的commit 节点标签。 gitk --all 启动图形操作模式，如果加入“--all ”选项，表示要显示全部的分支，否则只会显示当前 操作中的分支。]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git学习（1）]]></title>
    <url>%2Fposts%2Ffc63c959.html</url>
    <content type="text"><![CDATA[1 git配置文件 1.1 git权限控制 git有三个不同的权限控制文件，高优先权的设置会覆盖低优先权的设置项，以下按照优先权从高到低介绍： 文件夹中&quot;.git&quot; 子文件夹中的confiig文件，这个配置具有最高优先权，但是这个配置文件只对他所在的文档有效。 登录账的home directory中的.gitconfig文件。这个配置文件只对此登陆账号有效。 git程序按照目录中的etc/gitconfig文件。 只有在前两个配置文件中没有设置的项这个配置传文件才会生效。这是公用的配置传文件，它对所有登录账号和所有git文档库都有效。 1.2 git config 指令的用法 要显示当前git的设置可以执行下面的指令： gitconfig -l 这个指令会显示三个配置文件中的所有设置项。他的顺序是先显示优先权最低的设置然后再显示高的。 当然我们也可以设置指定显示单一配置文件的内容： git config --system -l #显示git安装目录中的etc/gitconfig的设置 git config --global -l #显示登录账号 home directory 中的.gitconfig文件的配置 我们可以在配置文件中增加配置项，比如在执行&quot;git commit&quot; 的时候会提示你输入操作者的姓名和email，我们可以通过配置文件来设置以后就不用再输入，如果要记录在文档库中的配置文件，可以使用如下命令： git config user.name 'your name' git config user.email 'your email' 如果要记录在登录账号的home directory中的.gitconfig文件内可以使用如下指令： git config --global user.name 'your name' git config --global user.email 'your email' 如果要记录在git安装目录etc/gitconfig内可以执行如下指令： git config --system user.name 'your name' git config --system user.email 'your email' 如果要删除配置传文件中的配置项可以使用“–unset”命令: git config --unset user.name 如果要删除的是其他配置文件中的设置项，则视情况加入&quot;–global&quot;或者&quot;system&quot;选项。 补充说明：git指令的长和短 我们会发现有的指令使用一个连字符比如：&quot;-m&quot;有的却是两个，比如：&quot;–author&quot;，这是为啥呢。 其实使用一个连字符是简单表达形式的意思。我们也可以把它换成完整形式。比如&quot;git commit -m ‘comment’“的完整形式是&quot;git commit --message=‘comment’”。“git config --list&quot;的完整形式是&quot;git config --list”。 2 把git文件存入仓库 2.1 排除不需要加入文档库的文件 我们在上传文件到git仓库的时候并不能保证所有上传的文件都是我们需要的，比如编译器编译的文件以及一些资源文件，那么如何保证不把这些文件上传到git仓库呢。git为我们提供了一个文件:.gitignore。我们把需要忽略的文件一一列在这里即可。 比如我们不想上传.txt为结尾的文件到仓库，那么在.gitignore文件中写入&quot;.txt&quot;即可。.gitignore文件可以使用“#”表示注释，文件夹路径使用“/”，文件名可以使用通配符&quot;*&quot;。使用&quot;!&quot;表示排除。比如以下设置表示排除所有txt文件但是不包含note.txt 文件。 *.txt !note.txt 2.2 控制commit 我们知道在本地仓库修改完文件之后提交仓库有两个步骤： git add #提交修改文件到git索引 git commit #从本地git索引推到git仓库 但是如果我们在执行git add之后反悔了怎么办呢。这时候我们的分两种情况采取以下操作： 文档库中还没有任何文件，即执行&quot;git init&quot;之后没有执行过&quot;git commit&quot;： 使用git rm --cached 文件名 文档库中已经有文件，即我们已经执行过&quot;git commit&quot;命令： git reset HEAD 文件名 2.3 查看commit节点 git #启动图形查看模式 除了使用gitk查看commit节点信息，也可以使用&quot;git log&quot;命令达到同样目的。如果再加上&quot;–graph&quot;选项，会使用文本模式排列出commit节点的演进图。 显示最近一次代码提交记录： git show head 3 比较文件的差异和从git文档库取回文件 3.1 取出指定文件 git checkout commit 节点标识符或标签 文件名1 文件名2 #从文档库的任何一个节点取出指定文件 Tips： 文件夹中的文件会被取出的文件覆盖 注意： 如果执行 git checkout 的文件与当前库中最新commit的文件内容不同，那么这个文件会自动记录在git索引中。稍后如果我们执行了git commit命令，那么这个取出的文件会成为新的版本被存入仓库中。为了避免这种情况的发生我们在执行git checkout之后立刻执行&quot;git reset head&quot;命令清除git索引。 3.2 使用git mv命令改变文件或文件夹名字 git mv 原来的文件名 新文件名 4 获取git文档库的统计数据 4.1 git log 指令 这个指令会按照时间顺序从最近一次的commit接单开始往前列出每一次commit的数据，包括标识符、执行人、日期以及说明。该指令后面可以接参数，比如：–author=‘A’,即只列出提交人为A的提交记录。 指定显示某位提交者的提交信息： git log --author='A' 指定显示某一段时间内的提交信息： $ git log --after='2017-10-01 00:00:00' --before='2017-11-01 00:00:00' after表示指定的日期之后，before表示指定的日期之前，注意使用方式！ 以上指令只会显示指定日期期间的commit，如果加上&quot;–stat&quot;,或者&quot;–numstat&quot;或者&quot;–shortstat&quot;还可以显示每一个commit变更的情况。 比如我们执行： git log shortstat 可以看到会显示该次提交变更的基本信息。 Tips： 如果git指令显示的信息炒过了屏幕大小，屏幕会自动暂停，这时候可以使用如下按键继续操作： 按键 j 表示显示下一行； 空格表示显示下一页； 按键 h 表示显示操作说明； 按键 q 表示结束。 4.2 git shortlog指令 这个指令会按照名字的字母顺序，列出每一个人执行commit的次数和说明。 我们也可以按照commit的次数从高到低排列，只要加上&quot;–commit&quot;或者是使用缩写：&quot;-n&quot; 即可。如果不需要显示commit说明可以再加上&quot;–summary&quot;或者是使用缩写:&quot;-s&quot;。 git shortlog -n -s 4.3 git ls-files 指令 这个指令会列出当前git文档库中的文件列表： 我们也可以搭配&quot;xargs&quot;和&quot;wc&quot;这两个指令来计算程序中的代码行数： git ls-files | xargs wc -l 前面的数字就是该文件中的代码行数，如果你要计算行数总和可以使用如下指令： git ls-files | xargs cat | wc -l 5 建立分支和解决冲突 5.1 创立分支 创立分支使用git branch 指令： git branch &lt;new branch name&gt; commit &lt;节点标识符或者是标签&gt; 这个指令会根据后边参数的不同发挥不同的作用： 如果最后指定了commit节点标识符或者是标签就会从该节点开始的位置长出分支，即该节点之前提交的代码新分支都有，之后的代码新分支是没有的。如果没有指定commit标签那么默认是从最新节点的位置创立分支。 “git branch” 指令后面没有任何参数的时候会列出当前文档库中正在开发的所有分支。 建立分支之后可以使用&quot;git checkout &lt;分支名&gt;&quot; 这个指令来切换当前操作的分支。 5.2 删除分支 删除分支我们要注意的是在删除当前分支之前***必须先切换到另外的分支***，不然不能删除当前分支，删除分支使用如下指令： git branch -d &lt;要删除的分支&gt; 使用该命令需要保证要删除的分支已经合并到别的分支过，不然会报错 值得注意的是： 我们知道如果我们新建了一个分支一般都是为了方便多人协作，开发者分别在自己的dev分支上开发然后合并到master，上面这个命令有些反人类，如果当前要删除的分支没有合并到另一个分支，使用上面的命令会报错。需要把&quot;-d&quot;换成&quot;-D&quot;。 5.3 修改分支名 修改分支名需要注意的是你必须的先切换到要修改的分支之后才能操作： git branch -m &lt;新的分支出名&gt; 5.4 合并分支 合并分支使用如下命令： git merge 分支名 该命令会把指定的分支合并到当前分支。比如当前在master分支，想把A分支的代码合并到master使用命令： git merge A 即可。 如果合并之后你后悔了，想撤回可以使用&quot;git reset&quot;命令。我们想恢复到合并之前的状态执行如下操作： git reset --hard HEAD^ “–hard” 选项表示文件夹中的文件也要一起恢复。 5.5 解决冲突 我们先来想一下什么情况会发生冲突： 假如有个一master分支，你在master节点的最新节点新建了一个分支branch1； 然后你在branch1分支上新增了一个文件：1.txt,修改了一个文件：test.txt； 在你准备合并branch1到master之前，你的工友小B合并了他的分支branch2到master，新增了4个文件，也修改了test.txt文件； 然而你并不知情，你也提交了你的分支到master，这时候系统会告诉你你合并不了，因为检测目前的master和你当时新建分支branch1的时候从master最新节点上得到的那一版不一样，这个时候就代表你发生冲突了，需要手动合并。 git checkout master git merge branch1 然后就会提示代码冲突了。 一般冲突的部分会用: &lt;&lt;&lt;&lt;&lt;&lt;&lt;HEAD 冲突内容 ======= &gt;&gt;&gt;&gt;&gt;&gt;&gt;draft 来表示，所以需要你手动的去合并冲突，解决完了冲突之后执行如下指令提交代码： git diff 执行完成如果没有报错说明已经解决冲突。之后我们可以执行如下指令把我们的代码提交到master分支： git add . git commit -m '合并branch1到master' 6 使用Rebase指令更新分支的起始点 之前我们说的合并分支的时候都在说这样的情况： 有主分支master，我们在master的最新节点上创建了新分支AAA。 我们在AAA分支上做了更新，然后推送到master节点与master节点合并。 那么现在我们要说的是另一种情况： 有主分支master，我们在master的最新节点上创建了新分支AAA； 我们一直在AAA分支上做开发，这个过程的持续很久； 在AAA上做开发的过程中，别的程序员也在master的最新节点上创建了新的分支BBB，他开发完成很快就提交代码到master； 这个时候你在AAA分支上开发，为了保证代码的实效性，你必须的拉master的最新代码到你这里。 见下图： 我们执行如下命令将master合并到AAA： # git checkout AAA # git merge master 执行合并以后可能master会继续有更新，而我们的AAA分支也一直在开发，那么我们的节点图可能一直跟上图这样不停的交叉，到最后可能会看起来越来越复杂。当然这样并没有错，只是看起来不是那么简洁。我们可以用另一种方式让我们的节点图看起来更加简洁和直观：用rebase指令取代merge指令。 rebase指令的功能和merge一样，但是他在节点的处理上略有不同，我们用一个图来说明： 这幅图我们对比上一幅可以看出： rebase指令执行的合并会把以前从commit节点长出来的分支AAA合并到master的最新节点，然后从master的最新节点长出来。但是master和AAA节点的代码其实还是各自不同，AAA包含master的所有代码，master不包含AAA从master最新节点&quot;长出来&quot;之后的代码。图虽然不同，但是功能没有任何区别。 通过上面这两幅图大家是不是更加清楚这两个指定的区别。下面我们来说一下rebase指令的用法。 使用rebase很简单，就把以前使用merge的地方换成rebase即可： # git checkout AAA # git rebase master 与merge一样rebase也会面临发生冲突的情况，你可以使用上面提供的方法手动修改冲突然后执行 “git diff” 检测是否修改完成所有的冲突，最后提交合并。 假如你在使用rebase提交合并的时候发生冲突，这时候你决定取消合并，你可以执行如下命令： # git rebase --abort 执行如上命令一定是在还没有完成rebase之前，如果你提交合并成功了执行上述命令是不生效的。 接下来我们说如果执行rebase完全没有错误但是你又反悔了该如何操作。 如果是上述情况，我们可以使用&quot;git reset&quot;命令来恢复，但是有一个条件是:我们必须先找到执行rebase指令之前head所在的commit节点。我们知道head表示当前工作分支的最新commit节点，那我们如何找到历史的那个head节点呢？ git提供了一个指令： # git reflog head或者是任何分支的分支名 我们可以使用这个指令查询head或是任何分支的变动情况。该命令得出的结果是按照时间倒序排列。]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java并发编程（二十六）----ThreadLocal的使用]]></title>
    <url>%2Fposts%2Ff58afb00.html</url>
    <content type="text"><![CDATA[其实ThreadLocal很多接触过多线程的同学都可能会很陌生，他不像current包里面那些耳熟能详的api一样在我们面前经常出现，更多的他作为一个本地类出现在系统设计里面。我们可以说一下Spring，Spring的事务管理器通过AOP切入业务代码，在进入业务代码前，会根据对应的事务管理器提取出相应的事务对象，假如事务管理器是DataSourceTransactionManager，就会从DataSource中获取一个连接对象，通过一定的包装后将其保存在ThreadLocal中。并且Spring也将DataSource进行了包装，重写了其中的getConnection()方法，或者说该方法的返回将由Spring来控制，这样Spring就能让线程内多次获取到的Connection对象是同一个。 为什么要放在ThreadLocal里面呢？因为Spring在AOP后并不能向应用程序传递参数，应用程序的每个业务代码是事先定义好的，Spring并不会要求在业务代码的入口参数中必须编写Connection的入口参数。此时Spring选择了ThreadLocal，通过它保证连接对象始终在线程内部，任何时候都能拿到，此时Spring非常清楚什么时候回收这个连接，也就是非常清楚什么时候从ThreadLocal中删除这个元素 从名字上看我们很容易误解，ThreadLocal，本地线程。local有当地的，本地的，局部的意思，这里说的是局部线程，意思是线程的局部变量。我们知道synchronized是独占锁，同一时间只能有一个线程操作被锁住的代码大家排队等待，典型的以时间换空间的策略。那如果我们空间很足时间不够该怎么办呢，ThreadLocal就该派上用场了。ThreadLocal作为线程的局部变量，会为这个线程创建独立的变量副本，在线程的内部，他所创建的对象相当于全局对象。 说到这里，大家是不是还是没有分清楚ThreadLocal和synchronized有什么区别，下面我们来讲。 ThreadLocal 不是用来解决共享对象的多线程访问问题的，上面说了ThreadLocal是线程的局部变量。一般情况下，通过ThreadLocal.set() 到线程中的对象是该线程自己使用的对象，其他线程是不需要访问的，也访问不到的。各个线程中访问的是不同的对象。 ThreadLocal使得各线程能够保持各自独立的一个对象，并不是通过ThreadLocal.set()来实现的，而是通过每个线程中的new 对象 的操作来创建的对象，每个线程创建一个，不是什么对象的拷贝或副本。通过ThreadLocal.set()将这个新创建的对象的引用保存到各线程的自己的一个map中，每个线程都有这样一个map，执行ThreadLocal.get()时，各线程从自己的map中取出放进去的对象，因此取出来的是各自自己线程中的对象，ThreadLocal实例是作为map的key来使用的。 如果ThreadLocal.set()进去的东西本来就是多个线程共享的同一个对象，那么多个线程的ThreadLocal.get()取得的还是这个共享对象本身，还是有并发访问问题。 我们来看一个例子： 1234567891011121314private static final ThreadLocal threadSession = new ThreadLocal(); public static Session getSession() throws InfrastructureException &#123; Session s = (Session) threadSession.get(); try &#123; if (s == null) &#123; s = getSessionFactory().openSession(); threadSession.set(s); &#125; &#125; catch (HibernateException ex) &#123; throw new InfrastructureException(ex); &#125; return s; &#125; 在getSession()方法中，首先判断当前线程中有没有放进去session，如果还没有，那么通过sessionFactory().openSession()来创建一个session，再将session set到线程中，实际是放到当前线程的ThreadLocalMap这个map中，这时，对于这个session的唯一引用就是当前线程中的那个ThreadLocalMap，而threadSession作为这个值的key，要取得这个session可以通过threadSession.get()来得到，里面执行的操作实际是先取得当前线程中的ThreadLocalMap，然后将threadSession作为key将对应的值取出。上面我们也讲过每个线程进来创建threadSession 的时候，这个threadSession 只属于他一个人所有，别的线程无法共享到他自己创建的ThreadLocal。这就避免了所有线程共享同一个对象的问题。并且该session创建完成之后，我们不必走到哪里都携带着session这个参数，走到哪里传递到哪里。需要使用的时候只需要从threadlocal中取出即可。这也是极其省事的。 我们可以举一个 例子来说明ThreadLocal不是用来解决对象共享访问问题的，而是为了处理在多线程环境中，某个方法处理一个业务，需要递归依赖其他方法时，而要在这些方法中共享参数的问题。例如有方法a()，在该方法中调用了方法b()，而在b方法中又调用了方法c()，即a–&gt;b—&gt;c，如果a，b，c都需要使用用户对象，那么我们常用做法就是a(User user)–&gt;b(User user)—c(User user)。但是如果使用ThreadLocal我们就可以用另外一种方式解决： 在某个接口中定义一个静态的ThreadLocal 对象，例如 public static ThreadLocal threadLocal=new ThreadLocal (); 然后让a，b，c方法所在的类假设是类A，类B，类C都实现1中的接口 在调用a时，使用A.threadLocal.set(user) 把user对象放入ThreadLocal环境 这样我们在方法a，方法b，方法c可以在不用传参数的前提下，在方法体中使用threadLocal.get()方法就可以得到user对象。 上面我们说到ThreadLocal的使用，也说了ThreadLocal里面有一个ThreadLocalMap 用于存储当前线程的对象，下面我们简单的看一下源码来理解一下这个过程。先上类图： ThreadLocal里面有一个内部类ThreadLocalMap，在ThreadLocal内部又装了一个Entry，他继承了WeakReference，我们来看一下Entry： 123456789static class Entry extends WeakReference&lt;ThreadLocal&gt; &#123; /** The value associated with this ThreadLocal. */ Object value; Entry(ThreadLocal k, Object v) &#123; super(k); value = v; &#125;&#125; Entry对象其实还是ThreadLocal类型的，这里我们看到ThreadLocal用了一个WeakReference包装是为了保证该ThreadLocal对象在没有被引用的时候能够及时的被gc掉。 下面再看一下ThreadLocal的get和set方法： 12345678910111213141516171819202122232425public void set(T value) &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value);&#125;void createMap(Thread t, T firstValue) &#123; t.threadLocals = new ThreadLocalMap(this, firstValue);&#125;ThreadLocalMap getMap(Thread t) &#123; return t.threadLocals;&#125;private void set(ThreadLocal key, Object value) &#123; Entry[] tab = table; int len = tab.length; int i = key.threadLocalHashCode &amp; (len-1); ... ... ...&#125; 在set方法中t.threadLocals只要不为空，便创建map对象，我们看到set方法中的key是ThreadLocal，即thread调用ThreadLocal.get()方法既可得到当前thread的threadLocal对象里面的ThreadLocalMap的值！是不是有点绕，是不是不知道为什么当前线程能调用ThreadLocal，我们看一下上面的getMap()方法，返回值是：t.threadLocals，这个t即当前线程，在Thread类里面有一个threadLocals对象，我们可以跟过去看一下，在这里限于篇幅，就只上相关的源码： 123456789public class Thread implements Runnable &#123; ThreadLocal.ThreadLocalMap threadLocals = null; ThreadLocal.ThreadLocalMap inheritableThreadLocals = null; Thread parent = currentThread(); if (parent.inheritableThreadLocals != null) this.inheritableThreadLocals = ThreadLocal.createInheritedMap(parent.inheritableThreadLocals);&#125; 下面方法是ThreadLocal中的： 123static ThreadLocalMap createInheritedMap(ThreadLocalMap parentMap) &#123; return new ThreadLocalMap(parentMap);&#125; 我们在源码中看到threadLocals并未进行赋值，他一直都是一个空对象，为什么这么做呢，我们接着看下面的get方法： 123456789101112131415161718192021public T get() &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) &#123; ThreadLocalMap.Entry e = map.getEntry(this); if (e != null) return (T)e.value; &#125; return setInitialValue();&#125;private T setInitialValue() &#123; T value = initialValue(); Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value); return value;&#125; 在get方法中，如果一个线程当前并未使用ThreadLocal对象那么getMap(t)必然是空，那我们就得想了，难道在Thread类中创建一个空对象threadLocals就这么空着？哈哈，当然不是啦，我也着急了。所以就进入了下面的setInitialValue()方法啦，这里的getMap(t)当然还是空的，那进入createMap(t, value)呗： 123void createMap(Thread t, T firstValue) &#123; t.threadLocals = new ThreadLocalMap(this, firstValue);&#125; 终于在这里拨开云雾见月明！妈妈再也不用担心threadLocals没有人要了！上面分析的比较乱，大家就将就看，用一句话总结那就是： 在Thread类中有一个对象是threadLocals，如果在该线程运行中有ThreadLocal创建threadLocals会去找到他的！获得你在ThreadLocal中存储的值！ 上面我们已经详细分析了ThreadLocal的使用和实现，那么在真实的环境中使用它有什么弊端没呢。其实使用中还真的是有很多问题的。 我们知道ThreadLocal是和当前线程绑定的，即他的生命周期是和当前线程共存，当线程结束，ThreadLocal内部的Entity对象才会被gc回收。 下面我说一个场景大家看会带来什么样的后果：如果现在是线程池对象使用了ThreadLocal来保存变量会发生什么？大家知道线程池的主要目的是为了线程复用，那么线程池中的线程基本不会结束，与jvm的生命周期是一致的。那这个时候谁知道一个携带了ThreadLocal的线程会什么时候结束呢。长久以往必然造成内存泄露。 另外我们再说一个关于忘记释放的问题。如果你在线程刚开始进来的时候就载入了ThreadLocal用来保存变量，假设你的程序设计的不是很健壮，你忘记了写remove()。这个时候事情就来了。再假设你在ThreadLocal中存放了map对象，真实的业务中Map对象也许包含了很多数据，随着时间流逝，内存中的无用对象越来越多，内存泄露是必然的。 关于ThreadLocal的内容我们就讲到这里，其实里面还有很多值得我们深究的东西，慢慢一点点的去看吧！]]></content>
      <categories>
        <category>多线程与并发编程</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java并发编程（二十三）----(JUC集合)ConcurrentSkipListMap介绍]]></title>
    <url>%2Fposts%2F31315379.html</url>
    <content type="text"><![CDATA[Queue除了前面介绍的实现外，还有一种双向的Queue实现Deque。这种队列允许在队列头和尾部进行入队出队操作，因此在功能上比Queue显然要更复杂。 1 LinkedBlockingDeque 我们来看一下该类中的成员变量： 1234567891011121314151617181920212223public class LinkedBlockingDeque&lt;E&gt; extends AbstractQueue&lt;E&gt; implements BlockingDeque&lt;E&gt;, java.io.Serializable &#123; private static final long serialVersionUID = -387911632671998426L; static final class Node&lt;E&gt; &#123; E item; Node&lt;E&gt; prev; Node&lt;E&gt; next; Node(E x) &#123; item = x; &#125; &#125; transient Node&lt;E&gt; first; transient Node&lt;E&gt; last; private transient int count; private final int capacity; final ReentrantLock lock = new ReentrantLock(); private final Condition notEmpty = lock.newCondition(); private final Condition notFull = lock.newCondition(); &#125; 有一个内部类Node，该类用来标记queue的节点，capacity最大为Integer.MAX_VALUE。然后使用了独占锁和条件机制来保证线程安全和进行阻塞控制。从上面的结构上我们可以看出： 要想支持阻塞功能，队列的容量一定是固定的，否则无法在入队的时候挂起线程。也就是capacity是final类型的。 既然是双向链表，每一个结点就需要前后两个引用，这样才能将所有元素串联起来，支持双向遍历。也即需要prev/next两个引用。 双向链表需要头尾同时操作，所以需要first/last两个节点，当然可以参考LinkedList那样采用一个节点的双向来完成，那样实现起来就稍微麻烦点。 既然要支持阻塞功能，就需要锁和条件变量来挂起线程。这里使用一个锁两个条件变量来完成此功能。 上面对LinkedBlockingDeque的结构做了说明，那么原理就很清晰了，无非是用一个独占锁来保持线程安全，然后用Condition来做阻塞操作。双向链表的操作大家都很熟悉就不做过多解释。 2 ConcurrentLinkedDeque ConcurrentLinkedDeque是JSR166y中新增的一个无界并发Deque实现，基于已链接节点的、任选范围的双端队列。在迭代时，队列保持弱一致性，但不会抛出ConcurrentModificationException异常。 我们看一下类的成员变量： 12345678910111213141516171819public class ConcurrentLinkedDeque&lt;E&gt; extends AbstractCollection&lt;E&gt; implements Deque&lt;E&gt;, java.io.Serializable &#123; private transient volatile Node&lt;E&gt; head; private transient volatile Node&lt;E&gt; tail; private static final Node&lt;Object&gt; PREV_TERMINATOR, NEXT_TERMINATOR; @SuppressWarnings("unchecked") Node&lt;E&gt; prevTerminator() &#123; return (Node&lt;E&gt;) PREV_TERMINATOR; &#125; @SuppressWarnings("unchecked") Node&lt;E&gt; nextTerminator() &#123; return (Node&lt;E&gt;) NEXT_TERMINATOR; &#125;&#125; 我们看到成员变量里面有头节点和尾节点，然后是节点的引用PREV_TERMINATOR, NEXT_TERMINATOR， 双向链表的结构都是一样的。ConcurrentLinkedDeque不是阻塞队列所以没有用到条件原语。我们在成员变量里面没有看到使用ReentrantLock,因为所有的操作都是使用原子操作，避免了使用独占锁造成性能问题。]]></content>
      <categories>
        <category>多线程与并发编程</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java并发编程（二十四）----(JUC集合)ArrayBlockingQueue和LinkedBlockingQueue介绍]]></title>
    <url>%2Fposts%2Fed05b021.html</url>
    <content type="text"><![CDATA[这一节我们来了解阻塞队列（BlockingQueue），BlockingQueue接口定义了一种阻塞的FIFO queue，每一个BlockingQueue都有一个容量，当容量满时往BlockingQueue中添加数据时会造成阻塞，当容量为空时取元素操作会阻塞。首先我们来看ArrayBlockingQueue和LinkedBlockingQueue. 1 ArrayBlockingQueue ArrayBlockingQueue是一个用数组实现的有界阻塞队列。此队列按照先进先出（FIFO）的原则对元素进行排序。默认情况下不保证访问者公平的访问队列，所谓公平访问队列是指阻塞的所有生产者线程或消费者线程，当队列可用时，可以按照阻塞的先后顺序访问队列，即先阻塞的生产者线程，可以先往队列里插入元素，先阻塞的消费者线程，可以先从队列里获取元素。通常情况下为了保证公平性会降低吞吐量。 我们看他的构造函数实现： 1234//默认是非公平的，初始指定队列容量public ArrayBlockingQueue(int capacity) &#123; this(capacity, false);&#125; //该构造方法可以设置队列的公平性。当然如果为公平的，则对性能会产生影响 //访问者的公平性是使用可重入锁实现的 12345678public ArrayBlockingQueue(int capacity, boolean fair) &#123; if (capacity &lt;= 0) throw new IllegalArgumentException(); this.items = new Object[capacity]; lock = new ReentrantLock(fair); notEmpty = lock.newCondition(); notFull = lock.newCondition();&#125; 使用很简单我们直接看一个实例： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283public class ProducerConsumerTest &#123; public static void main(String[] args) &#123; final BlockingQueue&lt;Integer&gt; blockingQueue = new ArrayBlockingQueue&lt;Integer&gt;(3); ExecutorService service = Executors.newFixedThreadPool(10); for(int i = 0;i&lt;4;i++)&#123; service.execute(new ProducerAndConsumer(blockingQueue)); &#125; &#125;&#125;class ProducerAndConsumer implements Runnable&#123; private boolean flag = false; private Integer j = 1; private Lock lock = new ReentrantLock(); Condition pro_con = lock.newCondition(); Condition con_con = lock.newCondition(); private BlockingQueue&lt;Integer&gt; blockingQueue; public ProducerAndConsumer(BlockingQueue&lt;Integer&gt; blockingQueue)&#123; this.blockingQueue= blockingQueue; &#125; //生产 public void put()&#123; try &#123; lock.lock(); while(flag) pro_con.await(); System.out.println("正在准备放入数据。。。"); Thread.sleep(new Random().nextInt(10)*100); Integer value = new Random().nextInt(30); blockingQueue.put(value); System.out.println(Thread.currentThread().getName()+" 放入的数据 "+value); flag = true; con_con.signal(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally&#123; lock.unlock(); &#125; &#125; public void get()&#123; try &#123; lock.lock(); while(!flag) con_con.await(); System.out.println("正在准备取数据。。。"); Thread.sleep(new Random().nextInt(10)*1000); System.out.println(Thread.currentThread().getName()+" 取到的数据为"+blockingQueue.take()); flag = false; pro_con.signal(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally&#123; lock.unlock(); &#125; &#125; @Override public void run() &#123; while(true)&#123; if(j==1)&#123; put(); &#125; else&#123; get(); &#125; j=(j+1)%2; &#125; &#125;&#125; 输出为： 正在准备放入数据。。。 正在准备放入数据。。。 正在准备放入数据。。。 正在准备放入数据。。。 pool-1-thread-2 放入的数据 13 正在准备取数据。。。 pool-1-thread-3 放入的数据 4 正在准备取数据。。。 pool-1-thread-3 取到的数据为13 正在准备放入数据。。。 pool-1-thread-1 放入的数据 11 正在准备取数据。。。 pool-1-thread-4 放入的数据 26 正在准备取数据。。。 pool-1-thread-1 取到的数据为4 正在准备放入数据。。。 pool-1-thread-2 取到的数据为11 正在准备放入数据。。。 pool-1-thread-3 放入的数据 18 正在准备取数据。。。 ... ... 2 LinkedBlockingQueue LinkedBlockingQueue是一个用链表实现的有界阻塞队列。此队列的默认和最大长度为Integer.MAX_VALUE。此队列按照先进先出的原则对元素进行排序。 先看一下他的构造函数： 123456789public LinkedBlockingQueue() &#123; this(Integer.MAX_VALUE); //MAX_VALUE=2147483647&#125;public LinkedBlockingQueue(int capacity) &#123; if (capacity &lt;= 0) throw new IllegalArgumentException(); this.capacity = capacity; last = head = new Node&lt;E&gt;(null);&#125; 我们还是直接开看一个例子： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596public class BlockingQueueTest &#123; /** * 定义装苹果的篮子 */ public static class Basket &#123; // 篮子，能够容纳3个苹果 // BlockingQueue&lt;String&gt; basket = new ArrayBlockingQueue&lt;String&gt;(3); BlockingQueue&lt;String&gt; basket = new LinkedBlockingQueue&lt;String&gt;(3); // 生产苹果，放入篮子 public void produce() throws InterruptedException &#123; // put方法放入一个苹果，若basket满了，等到basket有位置 basket.put("An apple"); &#125; // 消费苹果，从篮子中取走 public String consume() throws InterruptedException &#123; // get方法取出一个苹果，若basket为空，等到basket有苹果为止 return basket.take(); &#125; &#125; // 测试方法 public static void testBasket() &#123; // 建立一个装苹果的篮子 final Basket basket = new Basket(); // 定义苹果生产者 class Producer implements Runnable &#123; public String instance = ""; public Producer(String a) &#123; instance = a; &#125; public void run() &#123; try &#123; while (true) &#123; // 生产苹果 System.out.println("生产者准备生产苹果：" + instance); basket.produce(); System.out.println("! 生产者生产苹果完毕：" + instance); // 休眠300ms Thread.sleep(300); &#125; &#125; catch (InterruptedException ex) &#123; &#125; &#125; &#125; // 定义苹果消费者 class Consumer implements Runnable &#123; public String instance = ""; public Consumer(String a) &#123; instance = a; &#125; public void run() &#123; try &#123; while (true) &#123; // 消费苹果 System.out.println("消费者准备消费苹果：" + instance); basket.consume(); System.out.println("! 消费者消费苹果完毕：" + instance); // 休眠1000ms Thread.sleep(1000); &#125; &#125; catch (InterruptedException ex) &#123; &#125; &#125; &#125; ExecutorService service = Executors.newCachedThreadPool(); Producer producer = new Producer("P1"); Producer producer2 = new Producer("P2"); Consumer consumer = new Consumer("C1"); service.submit(producer); service.submit(producer2); service.submit(consumer); // 程序运行3s后，所有任务停止 try &#123; Thread.sleep(3000); &#125; catch (InterruptedException e) &#123; &#125; service.shutdownNow(); &#125; public static void main(String[] args) &#123; BlockingQueueTest.testBasket(); &#125;&#125; 输出为： 生产者准备生产苹果：P1 消费者准备消费苹果：C1 ! 生产者生产苹果完毕：P1 生产者准备生产苹果：P2 ! 消费者消费苹果完毕：C1 ! 生产者生产苹果完毕：P2 生产者准备生产苹果：P2 ! 生产者生产苹果完毕：P2 生产者准备生产苹果：P1 ! 生产者生产苹果完毕：P1 生产者准备生产苹果：P2 生产者准备生产苹果：P1 消费者准备消费苹果：C1 ! 消费者消费苹果完毕：C1 ! 生产者生产苹果完毕：P2 生产者准备生产苹果：P2 消费者准备消费苹果：C1 ! 消费者消费苹果完毕：C1 ! 生产者生产苹果完毕：P1 生产者准备生产苹果：P1 消费者准备消费苹果：C1 ! 消费者消费苹果完毕：C1 ! 生产者生产苹果完毕：P2 Process finished with exit code 0]]></content>
      <categories>
        <category>多线程与并发编程</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java并发编程（二十三）----(JUC集合)ConcurrentSkipListMap介绍]]></title>
    <url>%2Fposts%2F31315379.html</url>
    <content type="text"><![CDATA[ConcurrentSkipListMap提供了一种线程安全的并发访问的排序映射表。内部是SkipList（跳表）结构实现，在理论上能够在O(log(n))时间内完成查找、插入、删除操作。 1 理解SkipList 要想弄明白ConcurrentSkipListMap,我们的先明白他的数据结构实现，先来看SkipList。 Skip List是一种随机化的数据结构，基于并联的链表，其效率可比拟于二叉查找树（对于大多数操作需要O(log n)平均时间）。基本上，跳跃列表是对有序的链表增加上附加的前进链接，增加是以随机化的方式进行的，所以在列表中的查找可以快速的跳过部分列表(因此得名)。所有操作都以对数随机化的时间进行。SkipList可以很好解决有序链表查找特定值的困难。 Skip List定义： 一个跳表，应该具有以下特征： 一个跳表应该有几个层（level）组成； 跳表的第一层包含所有的元素； 每一层都是一个有序的链表； 如果元素x出现在第i层，则所有比i小的层都包含x； 第i层的元素通过一个down指针指向下一层拥有相同值的元素； 在每一层中，-1和1两个元素都出现(分别表示INT_MIN和INT_MAX)； Top指针指向最高层的第一个元素。 构建有序链表： 一个跳表如下： Skip List构造步骤： 给定一个有序的链表。 选择链表中最大和最小的元素，然后从其他元素中按照一定算法（随机）随即选出一些元素，将这些元素组成有序链表。这个新的链表称为一层，原链表称为其下一层。 为刚选出的每个元素添加一个指针域，这个指针指向下一层中值同自己相等的元素。Top指针指向该层首元素 重复2、3步，直到不再能选择出除最大最小元素以外的元素。 从上图可以看到，跳表具有以下几种特性： 由很多层组成，level越高的层节点越少，最后一层level用有所有的节点数据 每一层的节点数据也都是有顺序的 上面层的节点肯定会在下面层中出现 每个节点都有两个指针，分别是同一层的下一个节点指针和下一层节点的指针 使用跳表查询元素的时间复杂度是O(log n)，跟红黑树一样。查询效率还是不错的，但是跳表的存储容量变大了，本来一共只有10个节点的数据，使用跳表之后变成了21个节点。 所以跳表是一种使用”空间换时间”的概念用来提高查询效率的链表，开源软件Redis、LevelDB都使用到了跳表。跳表相比B树，红黑树，AVL树时间复杂度一样，但是耗费更多存储空间，但是跳表的优势就是它相比树，实现简单，不需要考虑树的一些rebalance问题。 2 ConcurrentSkipListMap探索 ConcurrentSkipListMap包含了很多内部类，内部类的框架图如下： ConcurrentSkipListMap在原始链表的基础上增加了跳表的结构，所以需要两个额外的内部类来封装链表的节点，以及跳表的节点——Node和Index。 同ConcurrentHashMap的Node节点一样，key为final，是不可变的，value和next通过volatile修饰保证内存可见性。 Index：跳表的节点： 12345static class Index&lt;K,V&gt; &#123; final Node&lt;K,V&gt; node; final Index&lt;K,V&gt; down; volatile Index&lt;K,V&gt; right;&#125; Node：链表的节点： 12345static final class Node&lt;K,V&gt; &#123; final K key; volatile Object value; volatile Node&lt;K,V&gt; next;&#125; Index封装了跳表需要的结构，首先node包装了链表的节点，down指向下一层的节点（不是Node，而是Index），right指向同层右边的节点。node和down都是final的，说明跳表的节点一旦创建，其中的值以及所处的层就不会发生变化（因为down不会变化，所以其下层的down都不会变化，那他的层显然不会变化）。Node和Index内部都提供了用于CAS原子更新的AtomicReferenceFieldUpdater对象，该对象前面讲Atomic原子类的时候已经讲过,原理和机制将不再介绍。 下面我们还是着重介绍ConcurrentSkipListMap的get、put和remove方法。在介绍这三个方法之前我们先看一下这三个方法都会用到的一个辅助方法： 123456789101112131415161718192021222324private Comparable&lt;? super K&gt; comparable(Object key) throws ClassCastException &#123; if (key == null) throw new NullPointerException(); //有两种封装方法，如果在构造时指定了comparator，则使用comparator封装key // 如果没有指定comparator，则key必须是一个继承自Comparable接口的类，否则会抛出ClassCastException // 所以ConcurrentSkipListMap的key要么是继承自Comparable接口的类，如果不是的话需要显示提供comparator进行比较 if (comparator != null) return new ComparableUsingComparator&lt;K&gt;((K)key, comparator); else return (Comparable&lt;? super K&gt;)key; &#125;static final class ComparableUsingComparator&lt;K&gt; implements Comparable&lt;K&gt; &#123; final K actualKey; final Comparator&lt;? super K&gt; cmp; ComparableUsingComparator(K key, Comparator&lt;? super K&gt; cmp) &#123; this.actualKey = key; this.cmp = cmp; &#125; public int compareTo(K k2) &#123; return cmp.compare(actualKey, k2); &#125;&#125; ConcurrentSkipListMap的key必须是能够比较的，这样来确保线程安全。 我们再来看一下get方法： 1234567891011121314151617181920public V get(Object key) &#123; return doGet(key);&#125;private V doGet(Object okey) &#123; Comparable&lt;? super K&gt; key = comparable(okey); /* * Loop needed here and elsewhere in case value field goes * null just as it is about to be returned, in which case we * lost a race with a deletion, so must retry. */ for (;;) &#123; Node&lt;K,V&gt; n = findNode(key); if (n == null) return null; Object v = n.value; if (v != null) return (V)v; &#125;&#125; 可见在get方法中调用了doGet()来进行取值操作，首先调用了comparable（key）方法来确保该次取值的安全性，后面再一个死循环中持续进行 findNode(key)操作。 再看一下put方法： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657public V put(K key, V value) &#123; if (value == null) throw new NullPointerException(); return doPut(key, value, false);&#125;private V doPut(K kkey, V value, boolean onlyIfAbsent) &#123; Comparable&lt;? super K&gt; key = comparable(kkey); for (;;) &#123; // 从跳表中查找最接近指定key的节点：该节点的key小于等于指定key，且处于最底层 Node&lt;K,V&gt; b = findPredecessor(key); Node&lt;K,V&gt; n = b.next; //新节点插入在b与n之间 for (;;) &#123; //n==null则说明b是链表的最后一个节点，则新节点直接插入到链表尾部即可 if (n != null) &#123; Node&lt;K,V&gt; f = n.next; if (n != b.next) // 此处增加判断，避免链表结构已被修改(针对节点b) break; Object v = n.value; if (v == null) &#123; // n节点已经被删除 n.helpDelete(b, f);b和f分别为n的前驱和后继节点 break; &#125; // 这里如果v==n说明n是一个删除标记，用来标记其前继节点已被删除，即b已被删除 if (v == n || b.value == null) // b is deleted break; int c = key.compareTo(n.key); // 如果指定key&gt;n的key，则判断下一个节点，直到n==null，或者指定key&lt;n的key if (c &gt; 0) &#123; b = n; n = f; continue; &#125; // 相等，则更新value即可，更新失败，就再来一次，一直到成功为止 if (c == 0) &#123; if (onlyIfAbsent || n.casValue(v, value)) return (V)v; else break; // restart if lost race to replace value &#125; // else c &lt; 0; fall through &#125; // 创建一个节点，next指向n Node&lt;K,V&gt; z = new Node&lt;K,V&gt;(kkey, value, n); // 将b的next指向新创建的节点，则新的链表为：b--&gt;new--&gt;n，即将新节点插入到b和n之间 if (!b.casNext(n, z)) break; // restart if lost race to append to b // 随机计算一个层级 int level = randomLevel(); if (level &gt; 0) // 将z插入到该层级 insertIndex(z, level); return null; &#125; &#125;&#125; 代码中已经附上了大量的注释，这里再简单的梳理下流程。首先put()方法是调用内部的doPut()方法。Comparable&lt; ? super K&amp;&gt; key = comparable(kkey);这一句将key封装成一个Comparable对象，上面已经介绍了comparable这个方法。接着进入到死循环，循环第一步是调用findPredecessor(key)方法，该方法返回一个key最接近指定key的节点(最接近指的是小于等于)，该节点是处于最底层的，下面介绍下这个方法的逻辑。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546/**在跳表中查找节点的key小于指定key，且处于最底层的节点，即找到指定key的前继节点*基本逻辑是从head(跳表的最高层链表的头结点)开始自右开始查找，当找到该层链表的最*接近且小于指定key的节点时，往下开始查找，*最终找到最底层的那个节点*/private Node&lt;K,V&gt; findPredecessor(Comparable&lt;? super K&gt; key) &#123; if (key == null) throw new NullPointerException(); // don't postpone errors for (;;) &#123; // head是跳表的最高层链表的头结点 Index&lt;K,V&gt; q = head; Index&lt;K,V&gt; r = q.right;// head的右边节点 for (;;) &#123; // r==null说明该层链表已经查找到头，且未找到符合条件的节点，需开始往下查找 if (r != null) &#123; Node&lt;K,V&gt; n = r.node;// r的数据节点 K k = n.key; if (n.value == null) &#123;// n的value为null，说明该节点已被删除 // 将该节点从链表移除，通过将其(n)前置节点的right指向其(n)的后置节点 if (!q.unlink(r)) break; // restart r = q.right; // reread r 移除value==null的n节点之后，继续从n的下一个节点查找 continue; &#125; // 比较当前查找的节点的key与指定key，如果小于指定key，则继续查找， // 大于等于key则q即为该层链表最接近指定key的 if (key.compareTo(k) &gt; 0) &#123; q = r; r = r.right; continue; &#125; &#125; // 到这里有两种情况： //1)该层链表已经查找完，仍未找到符号条件的节点 //2)找到一个符合条件的节点 // 开始往下一层链表进行查找 Index&lt;K,V&gt; d = q.down; if (d != null) &#123; // 从下层对应位置继续查找 q = d; r = d.right; &#125; else // 如果无下层链表则直接返回当前节点的node return q.node; &#125; &#125;&#125; 该方法的查找逻辑是：从head(跳表的最高层链表的头结点)开始自右开始查找，当找到该层链表的最接近且小于指定key的节点时，往下开始查找，最终找到最底层的那个节点。具体的代码可以看注释，应该说的挺明白的了，针对Put方法，这个方法返回的节点就是将要插入的节点的前继节点，即新节点将插到该节点后面。下面是查找的示意图: 所有的修改操作都是使用CAS，只要失败就会重试，直至成功，所以就算多线程并发操作也不会出现错误，而且通过CAS避免了使用锁，性能比用锁好很多。 接下来在看一下remove： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public V remove(Object key) &#123; return doRemove(key, null);&#125;final V doRemove(Object okey, Object value) &#123; Comparable&lt;? super K&gt; key = comparable(okey); for (;;) &#123; // 从跳表中查找最接近指定key的节点：该节点的key小于等于指定key，且处于最底层 Node&lt;K,V&gt; b = findPredecessor(key); Node&lt;K,V&gt; n = b.next; for (;;) &#123; if (n == null) return null; //获取n节点的下一个节点 Node&lt;K,V&gt; f = n.next; if (n != b.next) // inconsistent read break; Object v = n.value; if (v == null) &#123; // n is deleted n.helpDelete(b, f); break; &#125; if (v == n || b.value == null) // b is deleted break; int c = key.compareTo(n.key); if (c &lt; 0) return null; if (c &gt; 0) &#123;//将该节点移除 b = n; n = f; continue; &#125; if (value != null &amp;&amp; !value.equals(v)) return null; if (!n.casValue(v, null)) break; if (!n.appendMarker(f) || !b.casNext(n, f)) findNode(key); // Retry via findNode else &#123; findPredecessor(key); // Clean index if (head.right == null) tryReduceLevel(); &#125; return (V)v; &#125; &#125;&#125; 说明：doRemove函数的处理流程如下。 ① 根据key值找到前驱结点，查找的过程会删除一个标记为删除的结点。 ② 从前驱结点往后查找该结点。 ③ 在该结点后面添加一个marker结点，若添加成功，则将该结点的前驱的后继设置为该结点之前的后继。 ④ 头结点的next域是否为空，若为空，则减少层级。]]></content>
      <categories>
        <category>多线程与并发编程</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java并发编程（二十三）----(JUC集合)ConcurrentSkipListMap介绍]]></title>
    <url>%2Fposts%2F31315379.html</url>
    <content type="text"><![CDATA[这一节我们来看一下并发的Map，ConcurrentHashMap和ConcurrentSkipListMap。ConcurrentHashMap通常只被看做并发效率更高的Map，用来替换其他线程安全的Map容器，比如Hashtable和Collections.synchronizedMap。ConcurrentSkipListMap提供了一种线程安全的并发访问的排序映射表。内部是SkipList（跳表）结构实现，在理论上能够在O(log(n))时间内完成查找、插入、删除操作。 1 ConcurrentHashMap简介 ConcurrentHashMap是一个线程安全的HashTable，它的主要功能是提供了一组和HashTable功能相同但是线程安全的方法。ConcurrentHashMap可以做到读取数据不加锁，并且其内部的结构可以让其在进行写操作的时候能够将锁的粒度保持地尽量地小，不用对整个ConcurrentHashMap加锁。 为了更好的理解 ConcurrentHashMap 高并发的具体实现，让我们先探索它的结构模型。 ConcurrentHashMap 类中包含两个静态内部类 HashEntry 和 Segment。HashEntry 用来封装映射表的键 / 值对；Segment 用来充当锁的角色，每个 Segment 对象守护整个散列映射表的若干个桶。每个桶是由若干个 HashEntry 对象链接起来的链表。一个 ConcurrentHashMap 实例中包含由若干个 Segment 对象组成的数组。 1.1 HashEntry类： HashEntry 用来封装散列映射表中的键值对。在 HashEntry 类中，key，hash 和 next 域都被声明为 final 型，value 域被声明为 volatile 型 ： 1234567891011121314151617181920212223242526272829static final class HashEntry&lt;K, V&gt; &#123; final int hash; final K key; volatile V value; volatile ConcurrentHashMap.HashEntry&lt;K, V&gt; next; static final Unsafe UNSAFE; static final long nextOffset; HashEntry(int var1, K var2, V var3, ConcurrentHashMap.HashEntry&lt;K, V&gt; var4) &#123; this.hash = var1; this.key = var2; this.value = var3; this.next = var4; &#125; final void setNext(ConcurrentHashMap.HashEntry&lt;K, V&gt; var1) &#123; UNSAFE.putOrderedObject(this, nextOffset, var1); &#125; static &#123; try &#123; UNSAFE = Unsafe.getUnsafe(); Class var0 = ConcurrentHashMap.HashEntry.class; nextOffset = UNSAFE.objectFieldOffset(var0.getDeclaredField("next")); &#125; catch (Exception var1) &#123; throw new Error(var1); &#125; &#125; &#125; 1.2 Segment类： Segment继承了ReentrantLock，表明每个segment都可以当做一个锁。Segment 中包含HashEntry 的数组，其可以守护其包含的若干个桶（HashEntry的数组）。Segment 在某些意义上有点类似于 HashMap了，都是包含了一个数组，而数组中的元素可以是一个链表。 12345678910111213141516171819202122static final class Segment&lt;K, V&gt; extends ReentrantLock implements Serializable &#123; private static final long serialVersionUID = 2249069246763182397L; static final int MAX_SCAN_RETRIES = Runtime.getRuntime().availableProcessors() &gt; 1?64:1; /** * table 是由 HashEntry 对象组成的数组 * 如果散列时发生碰撞，碰撞的 HashEntry 对象就以链表的形式链接成一个链表 * table 数组的数组成员代表散列映射表的一个桶 * 每个 table 守护整个 ConcurrentHashMap 包含桶总数的一部分 * 如果并发级别为 16，table 则守护 ConcurrentHashMap 包含的桶总数的 1/16 */ transient volatile ConcurrentHashMap.HashEntry&lt;K, V&gt;[] table; transient int count; //Segment中元素的数量 transient int modCount; //对table的大小造成影响的操作的数量（比如put或者remove操作） transient int threshold; //阈值，Segment里面元素的数量超过这个值依旧就会对Segment进行扩容 final float loadFactor; //负载因子，用于确定threshold Segment(float var1, int var2, ConcurrentHashMap.HashEntry&lt;K, V&gt;[] var3) &#123; this.loadFactor = var1; this.threshold = var2; this.table = var3; &#125; &#125; ConcurrentHashMap 的成员变量中，包含了一个 Segment 的数组（final Segment&lt;K,V&gt;[] segments;），而 Segment 是 ConcurrentHashMap 的内部类，然后在 Segment 这个类中，包含了一个 HashEntry 的数组（transient volatile HashEntry&lt;K,V&gt;[] table;）。而 HashEntry 也是 ConcurrentHashMap 的内部类。HashEntry 中，包含了 key 和 value 以及 next 指针（类似于 HashMap 中 Entry），所以 HashEntry 可以构成一个链表。 所以通俗的讲，ConcurrentHashMap 数据结构为一个 Segment 数组，Segment 的数据结构为 HashEntry 的数组，而 HashEntry 存的是我们的键值对，可以构成链表。 1.3 ConcurrentHashMap结构图 ConcurrentHashMap引入了分割，并提供了HashTable支持的所有的功能。在ConcurrentHashMap中，支持多线程对Map做读操作，并且不需要任何的blocking。这得益于CHM将Map分割成了不同的部分，在执行更新操作时只锁住一部分。根据默认的并发级别(concurrency level)，Map被分割成16个部分，并且由不同的锁控制。这意味着，同时最多可以有16个写线程操作Map。试想一下，由只能一个线程进入变成同时可由16个写线程同时进入(读线程几乎不受限制)，性能的提升是显而易见的。但由于一些更新操作，如put(),remove(),putAll(),clear()只锁住操作的部分，所以在检索操作不能保证返回的是最新的结果。 ConcurrentHashMap默认的并发级别是16，但可以在创建CHM时通过构造函数改变。毫无疑问，并发级别代表着并发执行更新操作的数目，所以如果只有很少的线程会更新Map，那么建议设置一个低的并发级别。另外，ConcurrentHashMap还使用了ReentrantLock来对segments加锁。 经过前面的铺垫我们来正式对ConcurrentHashMap的使用进行剖析，重点关注get、put、remove这三个操作。 首先来看一下get的操作： 123456789101112131415161718public V get(Object var1) &#123; int var4 = this.hash(var1); long var5 = (long)((var4 &gt;&gt;&gt; this.segmentShift &amp; this.segmentMask) &lt;&lt; SSHIFT) + SBASE; ConcurrentHashMap.Segment var2; if((var2 = (ConcurrentHashMap.Segment)UNSAFE.getObjectVolatile(this.segments, var5)) != null) &#123; ConcurrentHashMap.HashEntry[] var3 = var2.table; if(var2.table != null) &#123; for(ConcurrentHashMap.HashEntry var7 = (ConcurrentHashMap.HashEntry)UNSAFE.getObjectVolatile(var3, ((long)(var3.length - 1 &amp; var4) &lt;&lt; TSHIFT) + TBASE); var7 != null; var7 = var7.next) &#123; Object var8 = var7.key; if(var7.key == var1 || var7.hash == var4 &amp;&amp; var1.equals(var8)) &#123; return var7.value; &#125; &#125; &#125; &#125; return null;&#125; 根据key，计算出hashCode； 根据步骤1计算出的hashCode定位segment，如果segment不为null &amp;&amp; segment.table也不为null，跳转到步骤3，否则，返回null，该key所对应的value不存在； 根据hashCode定位table中对应的hashEntry，遍历hashEntry，如果key存在，返回key对应的value； 步骤3结束仍未找到key所对应的value，返回null，该key锁对应的value不存在。 ConcurrentHashMap的get操作高效之处在于整个get操作不需要加锁。如果不加锁，ConcurrentHashMap的get操作是如何做到线程安全的呢？原因是volatile，所有的value都定义成了volatile类型，（上面介绍HashEntry类源码中提到：volatile V value）volatile可以保证线程之间的可见性，这也是用volatile替换锁的经典应用场景。 再来看一下put操作： 1234567891011121314public V put(K var1, V var2) &#123; if(var2 == null) &#123; throw new NullPointerException(); &#125; else &#123; int var4 = this.hash(var1); int var5 = var4 &gt;&gt;&gt; this.segmentShift &amp; this.segmentMask; ConcurrentHashMap.Segment var3; if((var3 = (ConcurrentHashMap.Segment)UNSAFE.getObject(this.segments, (long)(var5 &lt;&lt; SSHIFT) + SBASE)) == null) &#123; var3 = this.ensureSegment(var5); &#125; return var3.put(var1, var4, var2, false); &#125;&#125; 我们看到在第7行定义了一个Segment类型的 var3，然后调用了Segment的put方法存入map，我们不妨来看一下Segment的put方法： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354final V put(K var1, int var2, V var3, boolean var4) &#123; //1.获取锁，保证put操作的线程安全； ConcurrentHashMap.HashEntry var5 = this.tryLock()?null:this.scanAndLockForPut(var1, var2, var3); Object var6; try &#123; ConcurrentHashMap.HashEntry[] var7 = this.table; int var8 = var7.length - 1 &amp; var2; //2.定位到HashEntry数组中具体的HashEntry ConcurrentHashMap.HashEntry var9 = ConcurrentHashMap.entryAt(var7, var8); ConcurrentHashMap.HashEntry var10 = var9; //3.遍历HashEntry链表，假若待插入key已存在： //需要更新key所对应value（!onlyIfAbsent），更新oldValue -&gt; newValue，跳转到步骤5； //否则，直接跳转到步骤5； while(true) &#123; if(var10 == null) &#123; if(var5 != null) &#123; var5.setNext(var9); &#125; else &#123; var5 = new ConcurrentHashMap.HashEntry(var2, var1, var3, var9); &#125; int var15 = this.count + 1; if(var15 &gt; this.threshold &amp;&amp; var7.length &lt; 1073741824) &#123; this.rehash(var5); &#125; else &#123; ConcurrentHashMap.setEntryAt(var7, var8, var5); &#125; ++this.modCount; this.count = var15; var6 = null; break; &#125; //4.遍历完HashEntry链表，key不存在，插入HashEntry节点，oldValue = null，跳转到步骤5 Object var11 = var10.key; if(var10.key == var1 || var10.hash == var2 &amp;&amp; var1.equals(var11)) &#123; var6 = var10.value; if(!var4) &#123; var10.value = var3; ++this.modCount; &#125; break; &#125; var10 = var10.next; &#125; &#125; finally &#123; //5.释放锁，返回oldValue this.unlock(); &#125; return var6;&#125; 上面代码中已经做出解析，需要知道的是Segment的HashEntry数组采用开链法来处理冲突，我们知道散列最大的局限性就是空间利用率低，例如载荷因子为0.7，那么仍有0.3的空间未被利用。使用开链法可以使载荷因子为1，每个链上都挂常数个数据，对于哈希表的开链法来说，其开的空间都是按素数个依次往后开的空间，所以put操作的效率很高。 再来看一下remove操作： 12345public V remove(Object var1) &#123; int var2 = this.hash(var1); ConcurrentHashMap.Segment var3 = this.segmentForHash(var2); return var3 == null?null:var3.remove(var1, var2, (Object)null);&#125; 仍旧是调用了Segment的remove方法： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748final V remove(Object var1, int var2, Object var3) &#123; //获取锁 if(!this.tryLock()) &#123; this.scanAndLock(var1, var2); &#125; Object var4 = null; try &#123; ConcurrentHashMap.HashEntry[] var5 = this.table; int var6 = var5.length - 1 &amp; var2; ConcurrentHashMap.HashEntry var7 = ConcurrentHashMap.entryAt(var5, var6); ConcurrentHashMap.HashEntry var10; for(ConcurrentHashMap.HashEntry var8 = null; var7 != null; var7 = var10) &#123; // 所有处于待删除节点之后的节点原样保留在链表中 var10 = var7.next; Object var9 = var7.key; //找到要删除的节点 if(var7.key == var1 || var7.hash == var2 &amp;&amp; var1.equals(var9)) &#123; // 所有处于待删除节点之前的节点被克隆到新链表中 Object var11 = var7.value; if(var3 != null &amp;&amp; var3 != var11 &amp;&amp; !var3.equals(var11)) &#123; break; &#125; if(var8 == null) &#123; ConcurrentHashMap.setEntryAt(var5, var6, var10); &#125; else &#123; var8.setNext(var10); &#125; ++this.modCount; --this.count; // 把桶链接到新的头结点 // 新的头结点是原链表中，删除节点之前的那个节点 var4 = var11; break; &#125; var8 = var7; &#125; &#125; finally &#123; this.unlock(); &#125; return var4;&#125; 我们来看两张图，执行删除前的原链表： 删除之后的链表： 从上图可以看出，删除节点 C 之后的所有节点原样保留到新链表中；删除节点 C 之前的每个节点被克隆到新链表中，注意：它们在新链表中的链接顺序被反转了。 在执行 remove 操作时，原始链表并没有被修改，也就是说：读线程不会受同时执行 remove 操作的并发写线程的干扰。 综合上面的分析我们可以看出，写线程对某个链表的结构性修改不会影响其他的并发读线程对这个链表的遍历访问。 总结 ConcurrentHashMap 允许并发的读和线程安全的更新操作 在执行写操作时，ConcurrentHashMap 只锁住部分的Map 并发的更新是通过内部根据并发级别将Map分割成小部分实现的 高的并发级别会造成时间和空间的浪费，低的并发级别在写线程多时会引起线程间的竞争 ConcurrentHashMap 的所有操作都是线程安全 ConcurrentHashMap 返回的迭代器是弱一致性，fail-safe并且不会抛出ConcurrentModificationException异常 ConcurrentHashMap 不允许null的键值 ConcurrentHashMap 是一个并发散列映射表的实现，它允许完全并发的读取，并且支持给定数量的并发更新。相比于 HashTable 和用同步包装器包装的 HashMap（Collections.synchronizedMap(new HashMap())），ConcurrentHashMap 拥有更高的并发性。在 HashTable 和由同步包装器包装的 HashMap 中，使用一个全局的锁来同步不同线程间的并发访问。同一时间点，只能有一个线程持有锁，也就是说在同一时间点，只能有一个线程能访问容器。这虽然保证多线程间的安全并发访问，但同时也导致对容器的访问变成串行化的了。]]></content>
      <categories>
        <category>多线程与并发编程</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java并发编程（二十一）----(JUC集合)CopyOnWriteArraySet和ConcurrentSkipListSet介绍]]></title>
    <url>%2Fposts%2F5ee7a4a7.html</url>
    <content type="text"><![CDATA[这一节我们来接着介绍JUC集合：CopyOnWriteArraySet和ConcurrentSkipListSet。从名字上来看我们知道CopyOnWriteArraySet与上一节讲到的CopyOnWriteArrayList一样是动态数组实现;ConcurrentSkipListSet是线程安全的有序的集合，适用于高并发的场景。下面我们深入细致的分析一下他们的用法。 1 CopyOnWriteArraySet简介 它是线程安全的无序的集合，可以将它理解成线程安全的HashSet。对其所有操作使用内部 CopyOnWriteArrayList 的 Set。因此，它共享以下相同的基本属性： 它最适合于具有以下特征的应用程序：set 大小通常保持很小，只读操作远多于可变操作，需要在遍历期间防止线程间的冲突。 它是线程安全的。 因为通常需要复制整个基础数组，所以可变操作（add、set 和 remove 等等）的开销很大。 迭代器不支持可变 remove操作。 使用迭代器进行遍历的速度很快，并且不会与其他线程发生冲突。在构造迭代器时，迭代器依赖于不变的数组快照。 我们看一下CopyOnWriteArraySet的类体： 1234public class CopyOnWriteArraySet&lt;E&gt; extends AbstractSet&lt;E&gt; implements Serializable &#123; private static final long serialVersionUID = 5457747651344034263L; private final CopyOnWriteArrayList&lt;E&gt; al = new CopyOnWriteArrayList();&#125; 该类有一个成员变量CopyOnWriteArrayList。CopyOnWriteArraySet其所有操作使用内部 CopyOnWriteArrayList 的 Set，所以他的所有的方法其实都是引用了CopyOnWriteArrayList的方法来完成的。有关CopyOnWriteArrayList的使用说明我们在上一节里已经介绍的很详细，有兴趣可以查看上一节里的源码介绍。 2 ConcurrentSkipListSet简介 ConcurrentSkipListSet是线程安全的有序的集合，适用于高并发的场景。他是一个基于 ConcurrentSkipListMap 的可缩放并发 NavigableSet 实现。下面我们看一下方法体： 12345678910111213public class ConcurrentSkipListSet&lt;E&gt; extends AbstractSet&lt;E&gt; implements NavigableSet&lt;E&gt;, Cloneable, java.io.Serializable &#123; private static final long serialVersionUID = -2479143111061671589L; private final ConcurrentNavigableMap&lt;E,Object&gt; m; public ConcurrentSkipListSet() &#123; m = new ConcurrentSkipListMap&lt;E,Object&gt;(); &#125;&#125; (1) ConcurrentSkipListSet继承于AbstractSet。因此，它本质上是一个集合。 (2) ConcurrentSkipListSet实现了NavigableSet接口。因此，ConcurrentSkipListSet是一个有序的集合。 (3) ConcurrentSkipListSet是通过ConcurrentSkipListMap实现的。它包含一个ConcurrentNavigableMap对象m，而m对象实际上是ConcurrentNavigableMap的实现类ConcurrentSkipListMap的实例。ConcurrentSkipListMap中的元素是key-value键值对；而ConcurrentSkipListSet是集合，它只用到了ConcurrentSkipListMap中的key！ 由源码中我们能看到ConcurrentSkipListSet内部所有操作都是在内部由ConcurrentSkipListMap完成。本节我们先不介绍ConcurrentSkipListMap，下节讲到map的时候再细说。]]></content>
      <categories>
        <category>多线程与并发编程</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java并发编程（十九）----(JUC集合)总体框架介绍]]></title>
    <url>%2Fposts%2Fc00aa264.html</url>
    <content type="text"><![CDATA[本节我们将继续学习JUC包中的集合类，我们知道jdk中本身自带了一套非线程安全的集合类，我们先温习一下java集合包里面的集合类，然后系统的看一下JUC包里面的集合类到底有什么不同。 1 java集合类 java集合类里面主要包含两大类：一类是Collection接口下的List、Set和Queue接口，一类是Map接口。 1.1 List的主要实现类包括： ArrayList：数组实现的队列，它是一个动态数组；它不是线程安全的，只适用于单线程； LinkedList：双重链表实现的队列，它也不是线程安全的，只适用于单线程； Stack：表示后进先出（LIFO）的对象堆栈，继承于Vector，他是线程安全的； Vector：可以实现可增长的对象数组，与数组一样，它包含可以使用整数索引进行访问的组件。此类是线程安全的。 1.2 Set的主要实现类包括： HashSet：由哈希表（实际上是一个 HashMap 实例）支持。该集合元素不可重复，它不保证 set 的迭代顺序；特别是它不保证该顺序恒久不变，并且此实现不是同步的； TreeSet：基于 TreeMap 的 NavigableSet 实现，也是一个没有重复元素的集合，不过和HashSet不同的是，TreeSet中的元素是有序的；该类的实现也不是同步的； LinkedHashSet：元素是有序的，维护着一个运行于所有条目的双重链接列表；此实现不是同步的； 1.3 Map的主要实现类包括： HashMap：基于哈希表的 Map 接口的实现。此实现不是同步的； TreeMap：基于红黑树（Red-Black tree）的 NavigableMap 实现，该类的&lt;k,v&gt;是有序的此实现不是同步的； HashTable：与HashMap一样都是基于哈希表的Map实现，但是此类是线程安全的； WeakHashMap：以弱键 实现的基于哈希表的 Map。在 WeakHashMap 中，当某个键不再正常使用时，将自动移除其条目。该类是非线程安全的。 上面这些是传统的java集合类，他们大多数都不是线程安全的，所以在同步并发中对他们的使用率并不高，为了线程安全以及开发人员在使用集合工具的同时不必去维护线程安全，Doug Lea在JUC(java.util.concurrent)包中添加了java集合包中单线程类的对应的支持高并发的类。下面我们一起来看一下这些并发集合类。 2 JUC集合类 2.1 List的主要实现类包括： CopyOnWriteArrayList：相当于线程安全的ArrayList，它实现了List接口，他是线程安全的。 2.2 Set的主要实现类包括： CopyOnWriteArraySet：相当于线程安全的HashSet,内部使用 CopyOnWriteArrayList 。 ConcurrentSkipListSet:一个基于 ConcurrentSkipListMap 的可缩放并发 NavigableSet 实现，内部排序是有序的。 2.3 Map的主要实现类包括： ConcurrentHashMap：支持获取的完全并发和更新的所期望可调整并发的哈希表。 ConcurrentSkipListMap：可缩放的并发 ConcurrentNavigableMap 实现，内部排序是有序的Map，该类为线程安全的。 2.4 Queue的主要实现类包括： ArrayBlockingQueue：一个由数组支持的有界阻塞队列。此队列按 FIFO（先进先出）原则对元素进行排序； LinkedBlockingQueue：一个基于已链接节点的、范围任意的 blocking queue。此队列按 FIFO（先进先出）排序元素； LinkedBlockingDeque：一个基于已链接节点的、任选范围的阻塞双端队列； ConcurrentLinkedQueue：一个基于链接节点的无界线程安全队列。此队列按照 FIFO（先进先出）原则对元素进行排序； ConcurrentLinkedDeque：是双向链表实现的无界队列，该队列同时支持FIFO和FILO两种操作方式。 下一节我们将一起详细的来分析JUC中的集合工具的使用和原理。]]></content>
      <categories>
        <category>多线程与并发编程</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java并发编程（十八）----(线程池)java线程池框架Fork-Join]]></title>
    <url>%2Fposts%2F5f5c40c1.html</url>
    <content type="text"><![CDATA[还记得我们在初始介绍线程池的时候提到了Executor框架的体系，到现在为止我们只有一个没有介绍，与ThreadPoolExecutor一样继承与AbstractExecutorService的ForkJoinPool.Fork/Join框架是Java7提供了的一个用于并行执行任务的框架， 是一个把大任务分割成若干个小任务，最终汇总每个小任务结果后得到大任务结果的框架。 我们通过表面的意思去理解ForkJoin框架：Fork即把一个大任务切割成若干部分并行执行，join即把这些被切分的任务的执行结果合并一起汇总，我们可以用下图来表示： Fork / Join的逻辑很简单： （1）将每个大任务分离（fork）为较小的任务; （2）在单独的线程中处理每个任务（如果必要，将它们分离成更小的任务）; （3）加入结果。 Fork/Join框架的核心是由下列两个类组成的。 ①ForkJoinPool：这个类实现了ExecutorService接口和工作窃取算法（Work-Stealing Algorithm）。它管理工作者线程，并提供任务的状态信息，以及任务的执行信息。 ②ForkJoinTask：这个类是一个将在ForkJoinPool中执行的任务的基类。 理解一个概念的最好方法是在实践中体会他，我们先写一个小程序，在此基础上一点一点来分析： 123456789101112131415161718192021222324252627282930313233343536public class ForkJoinPoolTest &#123; public static void main(String[] args) throws InterruptedException &#123; ForkJoinPool pool = new ForkJoinPool(); pool.submit(new PrintTask(1,100)); pool.awaitTermination(2, TimeUnit.SECONDS);//阻塞当前线程直到 ForkJoinPool 中所有的任务都执行结束 pool.shutdown(); &#125;&#125;class PrintTask extends RecursiveAction&#123; private int start; private int end; private int num; final int MAX = 50; public PrintTask(int start, int end) &#123; this.start = start; this.end = end; &#125; @Override protected void compute() &#123; if(end - start &lt; 50)&#123; for(int i = start;i &lt;= end; i++)&#123; num += i; &#125; System.out.println("当前任务结果为： "+num); &#125;else&#123; int mid = (end + start)/2; PrintTask left = new PrintTask(start,mid); PrintTask right = new PrintTask(mid+1,end); left.fork(); right.fork(); &#125; &#125;&#125; 结果为： 1234当前任务结果为： 3775当前任务结果为： 1275Process finished with exit code 0 我们通过结果可以看到当前任务被分裂为两个子任务去执行。而执行任务的类继承了RecursiveAction这个类，那他到底在Fork-Join框架中发挥什么作用呢？我们不妨看一下： 首先我们来看一下Fork-Join框架提交任务的方法仍旧还是submit和execute： void execute(ForkJoinTask&lt;?&gt; task) //安排（异步）执行给定任务 void execute(Runnable task) //在未来的某个时候执行给定的命令 &lt;T&gt; ForkJoinTask&lt;T&gt; submit(Callable&lt;T&gt; task) //执行一个有返回值得任务， 返回一个Future类型的实例代表任务的结果 &lt;T&gt; ForkJoinTask&lt;T&gt; submit(ForkJoinTask&lt;T&gt; task) //提交一个ForkJoinTask类型的任务 ForkJoinTask&lt;?&gt; submit(Runnable task) //提交一个Runnable类型的任务，返回一个 Future类型的实例代表任务结果 &lt;T&gt; ForkJoinTask&lt;T&gt; submit(Runnable task, T result) //提交一个Runnable类型的任务，返回一个Future类型的实例代表任务结果 由execute和submit的参数我们可以看到Fork-join框架可以提交ForkJoinTask，Callable和Runnable类型的任务。这个ForkJoinTask我们之前没见过，先来看一下： 12public abstract class ForkJoinTask&lt;V&gt; implements Future&lt;V&gt;, Serializable &#123;&#125; 我们看到ForkJoinTask实现了Future接口，一个ForkJoinTask是一个轻量级的Future。对ForkJoinTask效率源于一组限制（这只是部分静态强制执行）反映其用途作为计算任务计算纯函数或纯粹孤立的对象操作。主要的协调机制fork()，安排异步执行，而不进行join()，直到任务的结果已经计算。通常我们并不直接继承 ForkJoinTask，它包含了太多的抽象方法。针对特定的问题，我们可以选择 ForkJoinTask 的不同子类来完成任务： RecursiveAction：用于任务没有返回结果的场景。 RecursiveTask：用于任务有返回结果的场景。 上面的例子中我们就是继承了RecursiveAction子类用于没有返回结果的场景，下面我们再看一下RecursiveTask用于有返回结果的场景： 123456789101112131415161718192021222324252627282930313233343536373839404142public class TestRecursiveTask &#123; public static void main(String[] args) &#123; Integer result = 0; ForkJoinPool pool = new ForkJoinPool(); Future&lt;Integer&gt; future = pool.submit(new SumTask(30)); try &#123; result = future.get(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (ExecutionException e) &#123; e.printStackTrace(); &#125; System.out.println(result+"==========================="); &#125;&#125;class SumTask extends RecursiveTask&lt;Integer&gt; &#123; int num; public SumTask(int num) &#123; this.num = num; &#125; @Override protected Integer compute() &#123; if(num &lt;= 20)&#123; try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println("生产完成"+num+"个产品"); return num; &#125;else&#123; SumTask task1 = new SumTask(20); SumTask task2 = new SumTask(num - 20); task1.fork(); task2.fork(); return task1.join() + task2.join(); &#125; &#125;&#125; 结果为： 生产完成20个产品 生产完成10个产品 30=========================== Process finished with exit code 0 我们看到继承RecursiveTask类指定了返回值类型为Integer，在compute方法中的返回值类型即为Integer类型。 从以上的例子中可以看到，通过使用 Fork/Join 模式，软件开发人员能够方便地利用多核平台的计算能力。尽管还没有做到对软件开发人员完全透明，Fork/Join 模式已经极大地简化了编写并发程序的琐碎工作。对于符合 Fork/Join 模式的应用，软件开发人员不再需要处理各种并行相关事务，例如同步、通信等，以难以调试而闻名的死锁和 data race 等错误也就不会出现，提升了思考问题的层次。你可以把 Fork/Join 模式看作并行版本的 Divide and Conquer 策略，仅仅关注如何划分任务和组合中间结果，将剩下的事情丢给 Fork/Join 框架。]]></content>
      <categories>
        <category>多线程与并发编程</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java并发编程（十八）----(线程池)java线程池框架Fork-Join]]></title>
    <url>%2Fposts%2F5f5c40c1.html</url>
    <content type="text"><![CDATA[这一节开始我们正式来介绍JUC集合类。我们按照List、Set、Map、Queue的顺序来进行介绍。这一节我们来看一下CopyOnWriteArrayList。 1 CopyOnWriteArrayList介绍 CopyOnWriteArrayList是ArrayList 的一个线程安全的变体，其中所有可变操作（add、set 等等）都是通过对底层数组进行一次新的复制来实现的。 与ArrayList不同处就在于是否会拷贝数组和加锁。 CopyOnWriteArrayList顾名思义就是写时复制的ArrayList，其意思就是在修改容器的元素时，并不是直接在原数组上修改，而是先拷贝了一份数组，然后在拷贝的数组上进行修改，修改完后将其引用赋值给原数组的引用。这样体现了读写分离，这样无论在任何时候我们都可以对容器进行读取。 2 CopyOnWriteArrayList源码分析 我们看一下CopyOnWriteArrayList的类声明部分： 123456789public class CopyOnWriteArrayList&lt;E&gt; implements List&lt;E&gt;, RandomAccess, Cloneable, java.io.Serializable &#123; /** The lock protecting all mutators */ transient final ReentrantLock lock = new ReentrantLock(); /** The array, accessed only via getArray/setArray. */ private volatile transient Object[] array;&#125; 它实现了List接口，所以实现了Collection的功能，另外我们看到还有两个类成员变量lock和array， 在后面的源码分析中我们能看到CopyOnWriteArrayList是线程安全的使用动态数组操作机制实现的List。 所谓动态数组操作机制：即通过volatile修饰的Object类型数组来进行数组的CRUD操作。在进行add,set,remove等可变操作的时候，都会先新建一个数组把更新的值赋给该数组，然后再传递给上面的array数组来保持该次操作的可见性。这也是CopyOnWriteArrayList命名的由来。这一般需要很大的开销，但是当遍历操作的数量大大超过可变操作的数量时，即在进行读操作时的效率要远远高于写或是修改操作，这种方法可能比其他替代方法更 有效。 CopyOnWriteArrayList的线程安全实现：我们能看到是通过一个全局的Lock和volatile修饰的array来实现的。在进行add,remove,set等可变操作的时候通过赋值给array我们总能保证该变量的内存可见性，其他的线程每次总能读到最新的array变量；同样在每次进行add,remove,set等可变操作时候都会在操作的一开始加入独占锁，操作结束释放锁，以保证本次操作的安全性。 下面我们就上述分析来看一下他的部分源码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364public void add(int index, E element) &#123; final ReentrantLock lock = this.lock; //加锁 lock.lock(); try &#123; Object[] elements = getArray(); int len = elements.length; if (index &gt; len || index &lt; 0) throw new IndexOutOfBoundsException("Index: "+index+", Size: "+len); Object[] newElements; int numMoved = len - index; if (numMoved == 0) //如果是在最后一个位置增加就把该数组赋值一份然后新增一个长度 newElements = Arrays.copyOf(elements, len + 1); else &#123; //否则新建数组，然后将"volatile数组中被删除元素之外的其它元素“拷贝到新数组中；最后，将新数组赋值给”volatile数组"。 newElements = new Object[len + 1]; System.arraycopy(elements, 0, newElements, 0, index); System.arraycopy(elements, index, newElements, index + 1,numMoved); &#125; newElements[index] = element; setArray(newElements); //拷贝 &#125; finally &#123; lock.unlock(); &#125;&#125;public E set(int index, E element) &#123; final ReentrantLock lock = l.lock;//加锁 lock.lock(); try &#123; rangeCheck(index); checkForComodification(); E x = l.set(index+offset, element); expectedArray = l.getArray();//拷贝 return x; &#125; finally &#123; lock.unlock(); &#125;&#125;public E remove(int index) &#123; final ReentrantLock lock = this.lock; //加锁 lock.lock(); try &#123; Object[] elements = getArray(); int len = elements.length; E oldValue = get(elements, index); int numMoved = len - index - 1; // 如果被删除的是最后一个元素，则直接通过Arrays.copyOf()进行处理，而不需要新建数组。 if (numMoved == 0) setArray(Arrays.copyOf(elements, len - 1)); // 否则，新建数组，然后将"volatile数组中被删除元素之外的其它元素“拷贝到新数组中；最后，将新数组赋值给”volatile数组"。 else &#123; Object[] newElements = new Object[len - 1]; System.arraycopy(elements, 0, newElements, 0, index); System.arraycopy(elements, index + 1, newElements, index,numMoved); setArray(newElements); //拷贝 &#125; return oldValue; &#125; finally &#123; lock.unlock(); &#125;&#125; 由上面源码部分我们可以看到CopyOnWriteArrayList在修改原数组的过程中比ArrayList多做了2件事： 1、加锁：保证我在修改数组的时候，其他人不能修改。 2、拷贝数组：无论是哪个方法，发现都需要拷贝数组。 上面的两件事就确保了CopyOnWriteArrayList在多线程的环境下可以应对自如。 我们再来看一下他的迭代器的实现： 123public Iterator&lt;E&gt; iterator() &#123; return new COWIterator&lt;E&gt;(getArray(), 0);&#125; 我们看到迭代器里面调用了COWIterator这个类，下面来看一下他的源码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556private static class COWIterator&lt;E&gt; implements ListIterator&lt;E&gt; &#123; /** Snapshot of the array */ private final Object[] snapshot; /** Index of element to be returned by subsequent call to next. */ private int cursor; private COWIterator(Object[] elements, int initialCursor) &#123; cursor = initialCursor; snapshot = elements; &#125; public boolean hasNext() &#123; return cursor &lt; snapshot.length; &#125; public boolean hasPrevious() &#123; return cursor &gt; 0; &#125; @SuppressWarnings("unchecked") public E next() &#123; if (! hasNext()) throw new NoSuchElementException(); return (E) snapshot[cursor++]; &#125; @SuppressWarnings("unchecked") public E previous() &#123; if (! hasPrevious()) throw new NoSuchElementException(); return (E) snapshot[--cursor]; &#125; public int nextIndex() &#123; return cursor; &#125; public int previousIndex() &#123; return cursor-1; &#125; //不支持remove方法 public void remove() &#123; throw new UnsupportedOperationException(); &#125; //不支持set方法 public void set(E e) &#123; throw new UnsupportedOperationException(); &#125; //不支持add方法 public void add(E e) &#123; throw new UnsupportedOperationException(); &#125;&#125; 我们可以看到COWSubListIterator不支持修改元素的操作。例如，对于remove(),set(),add()等操作，COWSubListIterator都会抛出异常！ CopyOnWriteArrayList的迭代器并不是快速失败的，也就是说并不会抛出ConcurrentModificationException异常。这是因为他在修改的时候，是针对与拷贝数组而言的，对于原数组没有任何影响。我们可以看出迭代器里面没有锁机制，所以只提供读取，而不支持添加修改和删除（抛出UnsupportedOperationExcetion）。 3 CopyOnWriteArrayList使用示例 上面我们具体的分析了CopyOnWriteArrayList的线程安全机制和实现机制，我们再来就他的使用做一个相应的说明： 123456789101112131415161718192021222324252627282930313233public class TestCopyOnWriteArrayList &#123; // fixme: list是ArrayList对象时，程序会出错。 private static List&lt;String&gt; list = new ArrayList&lt;String&gt;(); /*private static List&lt;String&gt; list = new CopyOnWriteArrayList&lt;String&gt;();*/ public static void main(String[] args) &#123; ExecutorService executor = Executors.newFixedThreadPool(20); for(int i=0;i&lt;100;i++)&#123; executor.execute(new TestList("aa")); &#125; &#125; private static void printAll() &#123; String value = null; Iterator iter = list.iterator(); while(iter.hasNext()) &#123; value = (String)iter.next(); System.out.print(value+", "); &#125; System.out.println(); &#125; private static class TestList extends Thread &#123; TestList(String name) &#123; super(name); &#125; @Override public void run() &#123; String val = Thread.currentThread().getName(); list.add(val); printAll(); &#125; &#125;&#125; 运行上程序，当list是ArrayList对象时，程序会出错，报出java.util.ConcurrentModificationException类型异常；当使用CopyOnWriteArrayList对象时，程序可以完成iterator遍历操作。]]></content>
      <categories>
        <category>多线程与并发编程</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java并发编程（十七）----(线程池)java线程池架构和原理]]></title>
    <url>%2Fposts%2Fb9ed0404.html</url>
    <content type="text"><![CDATA[前面我们简单介绍了线程池的使用，但是对于其如何运行我们还不清楚，Executors为我们提供了简单的线程工厂类，但是我们知道ThreadPoolExecutor是线程池的具体实现类。我们先从他开始分析。 1 ThreadPoolExecutor初探 ThreadPoolExecutor一共有3个构造方法，我们来看一下其中看起来比较复杂的这个： 123456789101112131415161718192021public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) &#123; if (corePoolSize &lt; 0 || maximumPoolSize &lt;= 0 || maximumPoolSize &lt; corePoolSize || keepAliveTime &lt; 0) throw new IllegalArgumentException(); if (workQueue == null || threadFactory == null || handler == null) throw new NullPointerException(); this.corePoolSize = corePoolSize; this.maximumPoolSize = maximumPoolSize; this.workQueue = workQueue; this.keepAliveTime = unit.toNanos(keepAliveTime); this.threadFactory = threadFactory; this.handler = handler;&#125; 看起来参数是挺多的，我们不妨耐心看看参数都是什么意思： corePoolSize：核心池的大小，默认情况下，在创建了线程池后，线程池中的线程数为0，当有任务来之后，就会创建一个线程去执行任务，当线程池中的线程数目达到corePoolSize后，就会把到达的任务放到缓存队列当中； maximumPoolSize：线程池最大线程数，它表示在线程池中最多能创建多少个线程； keepAliveTime：表示线程没有任务执行时最多保持多久时间会终止。默认情况下，只有当线程池中的线程数大于corePoolSize时，keepAliveTime才会起作用，直到线程池中的线程数不大于corePoolSize，即当线程池中的线程数大于corePoolSize时，如果一个线程空闲的时间达到keepAliveTime，则会终止，直到线程池中的线程数不超过corePoolSize。 unit：参数keepAliveTime的时间单位，有7种取值，在TimeUnit类中有7种静态属性： 1234567TimeUnit.DAYS; //天TimeUnit.HOURS; //小时TimeUnit.MINUTES; //分钟TimeUnit.SECONDS; //秒TimeUnit.MILLISECONDS; //毫秒TimeUnit.MICROSECONDS; //微妙TimeUnit.NANOSECONDS; //纳秒 workQueue：一个阻塞队列，用来存储等待执行的任务，这个参数的选择也很重要，会对线程池的运行过程产生重大影响，一般来说，这里的阻塞队列有以下几种选择： 123ArrayBlockingQueue;LinkedBlockingQueue;SynchronousQueue; threadFactory：是构造Thread的方法，你可以自己去包装和传递，主要实现newThread方法即可； handler：表示当拒绝处理任务时的策略，也就是参数maximumPoolSize达到后丢弃处理的方法，java提供了4种丢弃处理的方法，当然你也可以自己根据实际情况去重写，主要是要实现接口：RejectedExecutionHandler中的方法： public void rejectedExecution(Runnabler, ThreadPoolExecutor e) java默认的是使用：AbortPolicy，他的作用是当出现这中情况的时候会抛出一个异常；有以下四种取值： ①ThreadPoolExecutor.AbortPolicy:丢弃任务并抛出RejectedExecutionException异常。 ②ThreadPoolExecutor.DiscardPolicy：也是丢弃任务，但是不抛出异常。 ③ThreadPoolExecutor.DiscardOldestPolicy：丢弃队列最前面的任务，然后重新尝试执行任务（重复此过程） ④ThreadPoolExecutor.CallerRunsPolicy：由调用线程处理该任务 上面说了ThreadPoolExecutor的构造方法，我们继续看他的类的关系： 123public class ThreadPoolExecutor extends AbstractExecutorService &#123;&#125; 由源码我们看出ThreadPoolExecutor继承了AbstractExecutorService类，我们知道AbstractExecutorService是一个抽象类，它实现了ExecutorService接口。AbstractExecutorService存在的目的是为ExecutorService中的函数接口提供了默认实现。 12public abstract class AbstractExecutorService implements ExecutorService &#123;&#125; 由上我们知道AbstractExecutorService又实现了ExecutorService接口，而ExecutorService是Executor实现类的最直接接口。 12public interface ExecutorService extends Executor &#123;&#125; 由此我们似乎可以明白他们之间的关系： Executor是一个顶层接口，在它里面只声明了一个方法execute(Runnable)； ExecutorService接口继承了Executor接口，并声明了一些方法：submit、invokeAll、invokeAny以及shutDown等； 抽象类AbstractExecutorService实现了ExecutorService接口，基本实现了ExecutorService中声明的所有方法； ThreadPoolExecutor继承了类AbstractExecutorService，成为线程池的具体实现类。 2 线程池的实现 上面我们从ThreadPoolExecutor的构造方法出发提到了线程池的状态，执行，初始化，排队策略等等，下面我们就从这些方面入手，看看线程池的原理。 3 线程池初始化 默认情况下，创建线程池之后，线程池中是没有线程的，需要提交任务（execute或者submit）之后才会创建线程。在实际中如果需要线程池创建之后立即创建线程，可以通过以下两个方法办到： prestartCoreThread()：初始化一个核心线程 prestartAllCoreThreads()：初始化所有核心线程 下面是这两个方法的实现： 1234567891011121314151617public boolean prestartCoreThread() &#123; return workerCountOf(ctl.get()) &lt; corePoolSize &amp;&amp; addWorker(null, true);&#125;----------------------------------------------------------public int prestartAllCoreThreads() &#123; int n = 0; while (addWorker(null, true)) ++n; return n;&#125;----------------------------------------------------- 我们注意到上面两个方法都调用了addWorker方法，我们看一下实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667private boolean addWorker(Runnable firstTask, boolean core) &#123; retry: for (;;) &#123; int c = ctl.get(); int rs = runStateOf(c); // Check if queue empty only if necessary. if (rs &gt;= SHUTDOWN &amp;&amp; ! (rs == SHUTDOWN &amp;&amp; firstTask == null &amp;&amp; ! workQueue.isEmpty())) return false; for (;;) &#123; int wc = workerCountOf(c); if (wc &gt;= CAPACITY || wc &gt;= (core ? corePoolSize : maximumPoolSize)) return false; if (compareAndIncrementWorkerCount(c)) break retry; c = ctl.get(); // Re-read ctl if (runStateOf(c) != rs) continue retry; // else CAS failed due to workerCount change; retry inner loop &#125; &#125; boolean workerStarted = false; boolean workerAdded = false; Worker w = null; try &#123; final ReentrantLock mainLock = this.mainLock; w = new Worker(firstTask); final Thread t = w.thread; if (t != null) &#123; mainLock.lock(); try &#123; // Recheck while holding lock. // Back out on ThreadFactory failure or if // shut down before lock acquired. int c = ctl.get(); int rs = runStateOf(c); if (rs &lt; SHUTDOWN || (rs == SHUTDOWN &amp;&amp; firstTask == null)) &#123; if (t.isAlive()) // precheck that t is startable throw new IllegalThreadStateException(); workers.add(w); int s = workers.size(); if (s &gt; largestPoolSize) largestPoolSize = s; workerAdded = true; &#125; &#125; finally &#123; mainLock.unlock(); &#125; if (workerAdded) &#123; t.start(); workerStarted = true; &#125; &#125; &#125; finally &#123; if (! workerStarted) addWorkerFailed(w); &#125; return workerStarted;&#125; 这个方法还是挺好理解：上面的retry是对当前线程池状态进行检查，如果当前线程池未初始化或者未分配则返回false； 往下是初始化firstTask，我们看到在56行把初始化的firstTask加入workers集合，该集合定义为： 1private final HashSet&lt;Worker&gt; workers = new HashSet&lt;Worker&gt;(); 集合中包含当前所有的工作线程。 看完addWorker的实现，那么上面的prestartCoreThread和prestartAllCoreThreads我们就很好理解，前一个是向当前工作线程池中加入一个工作线程，后一个是循环N次。 4 线程池的执行 通常你得到线程池后，会调用其中的：submit方法或execute方法去操作；其实你会发现，submit方法最终会调用execute方法来进行操作，只是他提供了一个Future来托管返回值的处理而已，当你调用需要有返回值的信息时，你用它来处理是比较好的；这个Future会包装对Callable信息，并定义一个Sync对象（），当你发生读取返回值的操作的时候，会通过Sync对象进入锁，直到有返回值的数据通知。 我们先看一下submit方法的源码： 123456public Future&lt;?&gt; submit(Runnable task) &#123; if (task == null) throw new NullPointerException(); RunnableFuture&lt;Void&gt; ftask = newTaskFor(task, null); execute(ftask); return ftask;&#125; 我们看到在源码的第4行实际上是调用了execute()方法来处理包装的RunnableFuture。下面是execute方法的源码： 12345678910111213141516171819public void execute(Runnable command) &#123; if (command == null) throw new NullPointerException(); int c = ctl.get(); if (workerCountOf(c) &lt; corePoolSize) &#123; if (addWorker(command, true)) return; c = ctl.get(); &#125; if (isRunning(c) &amp;&amp; workQueue.offer(command)) &#123; int recheck = ctl.get(); if (! isRunning(recheck) &amp;&amp; remove(command)) reject(command); else if (workerCountOf(recheck) == 0) addWorker(null, false); &#125; else if (!addWorker(command, false)) reject(command);&#125; 第一个if为非空判断; 第二个if中的workerCountOf()方法拿到ctl中存储的当前线程总数，如果小于corePoolSize，那么就会走到addWorker()方法中，如果成功创建了Worker的话，那么返回true，直接return，否则重新通过cas拿一次c; 第三个if中判断当前的线程池是否处于RUNNING状态，如果是，并且workQueue.offer加入队列成功话，那么就重新拿出来一次ctl，再判断如果加入队列之后，线程池如果不是处于RUNNING的状态，并且从队列中remove成功的话，那么就会执行reject操作；判断当前线程数是否为0，如果为0的话，那么就调用addWorker(null,false)，否则如果非Running状态或者加入队列失败的话，那么就会调用addWorker(command,false)如果返回false，说明没有添加成功，就会执行reject操作。 5 任务缓存队列 我们还记得ThreadPoolExecutor的构造函数中有一个参数workQueue，它用来存放等待执行的任务。 workQueue的类型为BlockingQueue，通常可以取下面三种类型： 1）ArrayBlockingQueue：基于数组的先进先出队列，此队列创建时必须指定大小； 2）LinkedBlockingQueue：基于链表的先进先出队列，如果创建时没有指定此队列大小，则默认为Integer.MAX_VALUE； 3）synchronousQueue：这个队列比较特殊，它不会保存提交的任务，而是将直接新建一个线程来执行新来的任务。 6 线程池的关闭 ThreadPoolExecutor提供了两个方法，用于线程池的关闭，分别是shutdown()和shutdownNow()，其中： shutdown()：不会立即终止线程池，而是要等所有任务缓存队列中的任务都执行完后才终止，但再也不会接受新的任务 shutdownNow()：立即终止线程池，并尝试打断正在执行的任务，并且清空任务缓存队列，返回尚未执行的任务 7 线程池容量的动态调整 ThreadPoolExecutor提供了动态调整线程池容量大小的方法：setCorePoolSize()和setMaximumPoolSize() setCorePoolSize：设置核心池大小 setMaximumPoolSize：设置线程池最大能创建的线程数目大小 当上述参数从小变大时，ThreadPoolExecutor进行线程赋值，还可能立即创建新的线程来执行任务。 下面我们看一个小例子： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081public class ThreadPoolExecutorTest &#123; private static int produceTaskSleepTime = 2; private static int produceTaskMaxNumber = 10; public static void main(String[] args) &#123; // 构造一个线程池 ThreadPoolExecutor threadPool = new ThreadPoolExecutor(2, 4, 3, TimeUnit.SECONDS, new ArrayBlockingQueue&lt;Runnable&gt;(3), new ThreadPoolExecutor.DiscardOldestPolicy()); for (int i = 1; i &lt;= produceTaskMaxNumber; i++) &#123; try &#123; String task = "task-- " + i; System.out.println("创建任务并提交到线程池中：" + task); threadPool.execute(new ThreadPoolTask(task)); System.out.println("线程池中线程数目："+threadPool.getPoolSize()+"，队列中等待执行的任务数目："+ threadPool.getQueue().size()+"，已执行完毕的任务数目："+threadPool.getCompletedTaskCount()); Thread.sleep(produceTaskSleepTime); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125;class ThreadPoolTask implements Runnable, Serializable &#123; private Object attachData; ThreadPoolTask(Object tasks) &#123; this.attachData = tasks; &#125; public void run() &#123; System.out.println("开始执行任务：" + attachData); attachData = null; &#125; public Object getTask() &#123; return this.attachData; &#125;&#125;结果为：创建任务并提交到线程池中：task-- 1开始执行任务：task-- 1线程池中线程数目：1，队列中等待执行的任务数目：0，已执行完毕的任务数目：0创建任务并提交到线程池中：task-- 2线程池中线程数目：2，队列中等待执行的任务数目：0，已执行完毕的任务数目：1开始执行任务：task-- 2创建任务并提交到线程池中：task-- 3线程池中线程数目：2，队列中等待执行的任务数目：1，已执行完毕的任务数目：2开始执行任务：task-- 3创建任务并提交到线程池中：task-- 4线程池中线程数目：2，队列中等待执行的任务数目：1，已执行完毕的任务数目：3开始执行任务：task-- 4创建任务并提交到线程池中：task-- 5线程池中线程数目：2，队列中等待执行的任务数目：1，已执行完毕的任务数目：4开始执行任务：task-- 5创建任务并提交到线程池中：task-- 6线程池中线程数目：2，队列中等待执行的任务数目：1，已执行完毕的任务数目：5开始执行任务：task-- 6创建任务并提交到线程池中：task-- 7线程池中线程数目：2，队列中等待执行的任务数目：1，已执行完毕的任务数目：6开始执行任务：task-- 7创建任务并提交到线程池中：task-- 8线程池中线程数目：2，队列中等待执行的任务数目：1，已执行完毕的任务数目：7开始执行任务：task-- 8创建任务并提交到线程池中：task-- 9开始执行任务：task-- 9线程池中线程数目：2，队列中等待执行的任务数目：0，已执行完毕的任务数目：8创建任务并提交到线程池中：task-- 10开始执行任务：task-- 10线程池中线程数目：2，队列中等待执行的任务数目：0，已执行完毕的任务数目：9 由结果我们可以看到当线程池中线程的数目大于2时，便将任务放入任务缓存队列里面，当任务缓存队列满了之后，便创建新的线程。]]></content>
      <categories>
        <category>多线程与并发编程</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java并发编程（十六）----(线程池)java线程池的使用]]></title>
    <url>%2Fposts%2Fe7e539b9.html</url>
    <content type="text"><![CDATA[上节我们简单介绍了线程池，这次我们就来使用一下。Executors提供四种线程池，分别是：newCachedThreadPool，newFixedThreadPool ，newScheduledThreadPool ，newSingleThreadExecutor 。下面我们分别来使用下。 1 newSingleThreadExecutor 创建一个单线程的线程池。这个线程池只有一个线程在工作，也就是相当于单线程串行执行所有任务。如果这个唯一的线程因为异常结束，那么会有一个新的线程来替代它。此线程池保证所有任务的执行顺序按照任务的提交顺序执行。 我们来看一个小例子： 12345678910111213public class newSingleThreadExecutorTest &#123; public static void main(String[] args) &#123; ExecutorService ex = Executors.newSingleThreadExecutor(); for(int i=0;i&lt;10;i++)&#123; ex.execute(new Runnable() &#123; @Override public void run() &#123; System.out.println(Thread.currentThread().getName()); &#125; &#125;); &#125; &#125;&#125; 输出为： pool-1-thread-1 pool-1-thread-1 pool-1-thread-1 pool-1-thread-1 pool-1-thread-1 pool-1-thread-1 pool-1-thread-1 pool-1-thread-1 pool-1-thread-1 pool-1-thread-1 由输出结果可以看出始终只有一个线程在工作。 2 newFixedThreadPool 创建固定大小的线程池。每次提交一个任务就创建一个线程，直到线程达到线程池的最大大小。线程池的大小一旦达到最大值就会保持不变，如果某个线程因为执行异常而结束，那么线程池会补充一个新线程。 我们来看一个小例子： 12345678910111213public class newFixedThreadPoolTest &#123; public static void main(String[] args) &#123; ExecutorService ex = Executors.newFixedThreadPool(5); for(int i=0;i&lt;10;i++)&#123; ex.execute(new Runnable() &#123; @Override public void run() &#123; System.out.println(Thread.currentThread().getName()); &#125; &#125;); &#125; &#125;&#125; 输出为： pool-1-thread-1 pool-1-thread-2 pool-1-thread-2 pool-1-thread-5 pool-1-thread-3 pool-1-thread-3 pool-1-thread-3 pool-1-thread-3 pool-1-thread-3 pool-1-thread-4 我们启动了10个线程，但是池中只有5个线程工作，所以结果中最多只有5个线程。 3 newCachedThreadPool 创建一个可缓存的线程池。如果线程池的大小超过了处理任务所需要的线程， 那么就会回收部分空闲（60秒不执行任务）的线程，当任务数增加时，此线程池又可以智能的添加新线程来处理任务。此线程池不会对线程池大小做限制，线程池大小完全依赖于操作系统（或者说JVM）能够创建的最大线程大小。 我们来看一个小例子： 123456789101112131415161718public class newCachedThreadPoolTest &#123; public static void main(String[] args) &#123; ExecutorService ex = Executors.newCachedThreadPool(); for(int i=0;i&lt;10;i++)&#123; ex.execute(new Runnable() &#123; @Override public void run() &#123; System.out.println(Thread.currentThread().getName()); &#125; &#125;); try &#123; Thread.sleep(6000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 输出为： pool-1-thread-1 pool-1-thread-1 pool-1-thread-1 pool-1-thread-1 pool-1-thread-1 pool-1-thread-1 pool-1-thread-1 pool-1-thread-1 pool-1-thread-1 pool-1-thread-1 线程池为无限大，当执行第二个任务时第一个任务已经完成，会复用执行第一个任务的线程，而不用每次新建线程。但是如果执行第二个任务时第一个任务没有完成则又是另一番景象，我们把上面的例子稍稍改一下就有所不同： 123456789101112131415161718public class newCachedThreadPoolTest &#123; public static void main(String[] args) &#123; ExecutorService ex = Executors.newCachedThreadPool(); for(int i=0;i&lt;10;i++)&#123; ex.execute(new Runnable() &#123; @Override public void run() &#123; System.out.println(Thread.currentThread().getName()); try &#123; Thread.sleep(6000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;); &#125; &#125;&#125; 输出为： pool-1-thread-1 pool-1-thread-3 pool-1-thread-2 pool-1-thread-5 pool-1-thread-4 pool-1-thread-6 pool-1-thread-7 pool-1-thread-8 pool-1-thread-9 pool-1-thread-10 第一个任务在执行的时候等待了6秒，所以此时第二个任务执行的时候则是新建一个线程来执行。 4 newScheduledThreadPool 创建一个大小无限的线程池。此线程池支持定时以及周期性执行任务的需求。 在上一篇类类的关系图中我们可以看到该方法直接实现了ScheduledExecutorService接口，而该接口相当于提供了”延时”和”周期执行”功能的ExecutorService，再来看一下该方法的源码： 1234public static ScheduledExecutorService newScheduledThreadPool( int corePoolSize, ThreadFactory threadFactory) &#123; return new ScheduledThreadPoolExecutor(corePoolSize, threadFactory);&#125; 返回值是ScheduledExecutorService类型的，与其他3个方法不同，需要注意。我们来看一个小例子： 123456789101112131415161718public class newScheduledThreadPoolTest &#123; public static void main(String[] args) &#123; ScheduledExecutorService ex = Executors.newScheduledThreadPool(5); for(int i=0;i&lt;10;i++)&#123; ex.schedule(new Runnable() &#123; //定时执行的线程池 @Override public void run() &#123; System.out.println(Thread.currentThread().getName()); try &#123; Thread.sleep(6000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;,2, TimeUnit.SECONDS); &#125; &#125;&#125; 输出结果为： pool-1-thread-2 pool-1-thread-4 pool-1-thread-1 pool-1-thread-5 pool-1-thread-3 pool-1-thread-2 pool-1-thread-3 pool-1-thread-5 pool-1-thread-1 pool-1-thread-4 启动后会延迟2s之后才开始执行。 我们再来看一个周期性执行的例子： 123456789101112131415161718public class newScheduledThreadPoolTest &#123; public static void main(String[] args) &#123; ScheduledExecutorService ex = Executors.newScheduledThreadPool(5); for(int i=0;i&lt;10;i++)&#123; ex.scheduleAtFixedRate(new Runnable() &#123; //延迟3s后每2s周期性执行一次，不停 @Override public void run() &#123; System.out.println(Thread.currentThread().getName()); try &#123; Thread.sleep(6000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;,3,2, TimeUnit.SECONDS); &#125; &#125;&#125; 输出为： pool-1-thread-3 pool-1-thread-4 pool-1-thread-2 pool-1-thread-5 pool-1-thread-1 ... newScheduledThreadPool中有很多另外3个类中没有的方法，我们来看一下： shedule(Runnable command, long delay, TimeUnit unit): 延迟一定时间后执行Runnable任务； schedule(Callable callable, long delay, TimeUnit unit): 延迟一定时间后执行Callable任务； scheduleAtFixedRate(Runnable command, long initialDelay, long period, TimeUnit unit): 延迟一定时间后，以间隔period时间的频率周期性地执行任务； scheduleWithFixedDelay(Runnable command, long initialDelay, long delay,TimeUnit unit): 与scheduleAtFixedRate()方法很类似，但是不同的是scheduleWithFixedDelay()方法的周期时间间隔是以上一个任务执行结束到下一个任务开始执行的间隔，而scheduleAtFixedRate()方法的周期时间间隔是以上一个任务开始执行到下一个任务开始执行的间隔，也就是这一些任务系列的触发时间都是可预知的。 由上我们看到ScheduledExecutorService在执行定时任务方面还是挺强大的。线程池的使用我们就到这里，其实用了这么多我们只是在调用别人写好的方法，但是对于线程池是如何实现的我们还是未知的，下一节我们就深入的去分析线程池的实现，看看到底有什么高深莫测。]]></content>
      <categories>
        <category>多线程与并发编程</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[（十五）----(线程池)java线程池简介]]></title>
    <url>%2Fposts%2Fd72ca9c5.html</url>
    <content type="text"><![CDATA[好的软件设计不建议手动创建和销毁线程。线程的创建和销毁是非常耗 CPU 和内存的，因为这需要 JVM 和操作系统的参与。64位 JVM 默认线程栈是大小1 MB。这就是为什么说在请求频繁时为每个小的请求创建线程是一种资源的浪费。线程池可以根据创建时选择的策略自动处理线程的生命周期。重点在于：在资源（如内存、CPU）充足的情况下，线程池没有明显的优势，否则没有线程池将导致服务器崩溃。有很多的理由可以解释为什么没有更多的资源。例如，在拒绝服务（denial-of-service）攻击时会引起的许多线程并行执行，从而导致线程饥饿（thread starvation）。除此之外，手动执行线程时，可能会因为异常导致线程死亡，程序员必须记得处理这种异常情况。这时我们需要一个管理线程的工具----线程池应运而生。 在 Java 5 之后，并发编程引入了一堆新的启动、调度和管理线程的API。Executor 框架便是 Java 5 中引入的，其内部使用了线程池机制，它在 java.util.cocurrent 包下，通过该框架来控制线程的启动、执行和关闭，可以简化并发编程的操作。因此，在 Java 5之后，通过 Executor 来启动线程比使用 Thread 的 start 方法更好，除了更易管理，效率更好（用线程池实现，节约开销）外，还有关键的一点：有助于避免 this 逃逸问题——如果我们在构造器中启动一个线程，因为另一个任务可能会在构造器结束之前开始执行，此时可能会访问到初始化了一半的对象用 Executor 在构造器中。 1 Executor简介 我们先看一下Execotor框架的体系： Executor: 所有线程池的接口,只有一个方法。 void execute(Runnable command); ExecutorService: 增加Executor的行为，是Executor实现类的最直接接口。 AbstractExecutorService：AbstractExecutorService是一个抽象类，它实现了ExecutorService接口。AbstractExecutorService存在的目的是为ExecutorService中的函数接口提供了默认实现。 ScheduledExecutorService：ScheduledExecutorService是一个接口，它继承于ExecutorService。它相当于提供了&quot;延时&quot;和&quot;周期执行&quot;功能的ExecutorService。ScheduledExecutorService提供了相应的函数接口，可以安排任务在给定的延迟后执行，也可以让任务周期的执行。 ForkJoinPool ：ForkJoinPool 是 Java SE 7 新功能“分叉/结合框架”的核心类，专用于需要将一个任务不断分解成子任务（分叉），再不断进行汇总得到最终结果（结合）的计算过程。比起传统的线程池类ThreadPoolExecutor，ForkJoinPool 实现了工作窃取算法，使得空闲线程能够主动分担从别的线程分解出来的子任务，从而让所有的线程都尽可能处于饱满的工作状态，提高执行效率。 ScheduledThreadPoolExecutor：ScheduledThreadPoolExecutor继承于ThreadPoolExecutor，并且实现了ScheduledExecutorService接口。它相当于提供了&quot;延时&quot;和&quot;周期执行&quot;功能的ScheduledExecutorService。ScheduledThreadPoolExecutor类似于Timer，但是在高并发程序中，ScheduledThreadPoolExecutor的性能要优于Timer。 Executors：Executors是个静态工厂类。它通过静态工厂方法返回ExecutorService、ScheduledExecutorService、ThreadFactory 和 Callable 等类的对象。 ThreadPoolExecutor：线程池的具体实现类,一般用的各种线程池都是基于这个类实现的。 构造方法如下： 12345678public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue) &#123; this(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue, Executors.defaultThreadFactory(), defaultHandler);&#125; 参数： corePoolSize - 池中所保存的线程数，包括空闲线程。 maximumPoolSize - 池中允许的最大线程数。 keepAliveTime - 当线程数大于核心时，此为终止前多余的空闲线程等待新任务的最长时间。 unit - keepAliveTime 参数的时间单位。 workQueue - 执行前用于保持任务的队列。此队列仅保持由 execute 方法提交的 Runnable 任务。 该方法作用为：用给定的初始参数和默认的线程工厂及被拒绝的执行处理程序创建新的 ThreadPoolExecutor。但是使用 Executors 工厂方法比使用此通用构造方法方便得多。（所以一般不用这个方法） 由该方法我们可以看出一般线程池的工作方式为： ①线程池刚创建时，里面没有一个线程。任务队列是作为参数传进来的。不过，就算队列里面有任务，线程池也不会马上执行它们。 ②当调用 execute() 方法添加一个任务时，线程池会做如下判断： ⒈如果正在运行的线程数量小于 corePoolSize，那么马上创建线程运行这个任务； ⒉如果正在运行的线程数量大于或等于 corePoolSize，那么将这个任务放入队列； ⒊如果这时候队列满了，而且正在运行的线程数量小于 maximumPoolSize，那么还是要创建非核心线程立刻运行这个任务； ⒋如果队列满了，而且正在运行的线程数量大于或等于 maximumPoolSize，那么线程池会抛出异常RejectExecutionException。 ⒌当一个线程完成任务时，它会从队列中取下一个任务来执行。 ⒍当一个线程无事可做，超过一定的时间（keepAliveTime）时，线程池会判断，如果当前运行的线程数大于 corePoolSize，那么这个线程就被停掉。所以线程池的所有任务完成后，它最终会收缩到 corePoolSize 的大小。 2 线程池 2.1 线程池的作用： 线程池作用就是限制系统中执行线程的数量。 根据系统的环境情况，可以自动或手动设置线程数量，达到运行的最佳效果；少了浪费了系统资源，多了造成系统拥挤效率不高。用线程池控制线程数量，其他线程排队等候。一个任务执行完毕，再从队列的中取最前面的任务开始执行。若队列中没有等待进程，线程池的这一资源处于等待。当一个新任务需要运行时，如果线程池中有等待的工作线程，就可以开始运行了；否则进入等待队列。 2.2 为什么要用线程池: 1.减少了创建和销毁线程的次数，每个工作线程都可以被重复利用，可执行多个任务。 2.可以根据系统的承受能力，调整线程池中工作线线程的数目，防止因为消耗过多的内存，而把服务器累趴下(每个线程需要大约1MB内存，线程开的越多，消耗的内存也就越大，最后死机)。 Java里面线程池的顶级接口是Executor，但是严格意义上讲Executor并不是一个线程池，而只是一个执行线程的工具。真正的线程池接口是ExecutorService。 要配置一个线程池是比较复杂的，尤其是对于线程池的原理不是很清楚的情况下，很有可能配置的线程池不是较优的，因此在Executors类里面提供了一些静态工厂，生成一些常用的线程池。 1.newSingleThreadExecutor 创建一个单线程的线程池。这个线程池只有一个线程在工作，也就是相当于单线程串行执行所有任务。如果这个唯一的线程因为异常结束，那么会有一个新的线程来替代它。此线程池保证所有任务的执行顺序按照任务的提交顺序执行。 2.newFixedThreadPool 创建固定大小的线程池。每次提交一个任务就创建一个线程，直到线程达到线程池的最大大小。线程池的大小一旦达到最大值就会保持不变，如果某个线程因为执行异常而结束，那么线程池会补充一个新线程。 3.newCachedThreadPool 创建一个可缓存的线程池。如果线程池的大小超过了处理任务所需要的线程， 那么就会回收部分空闲（60秒不执行任务）的线程，当任务数增加时，此线程池又可以智能的添加新线程来处理任务。此线程池不会对线程池大小做限制，线程池大小完全依赖于操作系统（或者说JVM）能够创建的最大线程大小。 4.newScheduledThreadPool 创建一个大小无限的线程池。此线程池支持定时以及周期性执行任务的需求。 下节我们分别介绍这些工厂类。]]></content>
      <categories>
        <category>多线程与并发编程</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java并发编程（十四）----(JUC原子类)对象的属性修改类型介绍]]></title>
    <url>%2Fposts%2F58721c94.html</url>
    <content type="text"><![CDATA[今天我们介绍原子类的最后一个类型----对象的属性修改类型: AtomicIntegerFieldUpdater,AtomicLongFieldUpdater,AtomicReferenceFieldUpdater。有了这几个方法，普通的变量也能享受原子操作了。 1 开胃菜 由API我们知道AtomicIntegerFieldUpdater，AtomicLongFieldUpdater，AtomicReferenceFieldUpdater通过反射原子更新对象的字段,既然他们的作用是更新字段我们知道有些类型的字段是不可被更新的，所以被更新的字段是有一定的要求： 1. 必须是volatile类型（volatile是线程可见变量，保存在Jvm的主内存中，而不是线程的工作内存里面）， 2. 字段的描述类型（修饰符public/protected/default/private）是调用者与操作对象字段的关系一致， 3. 只能是实例变量，不能是类变量，也就是说不能加static关键字， 4. 只能是可修改变量，不能使final变量，因为final的语义就是不可修改。实际上final的语义和volatile是有冲突的，这两个关键字不能同时存在， 5. 对于AtomicIntegerFieldUpdater和AtomicLongFieldUpdater只能修改int/long类型的字段，不能修改其包装类型（Integer/Long）。如果要修改包装类型就需要使用AtomicReferenceFieldUpdater。 2 使用它 上面我们说了这几个类的作用是让普通类型的字段也能享受到原子操作，假如原本有一个变量是int型，并且很多地方都应用了这个变量，但是在某个场景下，想让int型变成AtomicInteger，但是如果直接改类型，就要改其他地方的应用。AtomicIntegerFieldUpdater就是为了解决这样的问题产生的。 AtomicIntegerFieldUpdater，AtomicLongFieldUpdater分别是对int和long类型的字段操作，AtomicReferenceFieldUpdater是对引用型的对象操作，并且在API中他们的操作方法与普通的AtomicInteger差不多，所以方法我就不再罗列，我们就直接使用吧。 我们来看AtomicIntegerFieldUpdater的例子： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748/*** allscore 如果和 score 的结果相同则说明线程是安全的*/public class AtomicIntegerFieldUpdaterTest &#123; public final static AtomicIntegerFieldUpdater&lt;AA&gt; vv = AtomicIntegerFieldUpdater.newUpdater(AA.class, "score"); //newUpdater方法为AA类中的score 对象创造一个更新器 public static AtomicInteger allscore = new AtomicInteger(0); public static void main(String[] args) throws InterruptedException &#123; final AA stu = new AA(); Thread[] t = new Thread[10000]; for (int i = 0; i &lt; 10000; i++) &#123; t[i] = new Thread() &#123; @Override public void run() &#123; if(Math.random()&gt;0.4) &#123; vv.incrementAndGet(stu); allscore.incrementAndGet(); &#125; &#125; &#125;; t[i].start(); &#125; for (int i = 0; i &lt; 10000; i++) &#123; t[i].join(); &#125; System.out.println("score="+stu.getScore()); System.out.println("allscore="+allscore); &#125;&#125;class AA&#123; int id; volatile int score; public int getScore() &#123; return score; &#125; public void setScore(int score) &#123; this.score = score; &#125;&#125; 输出结果： score=6032 allscore=6032 AtomicIntegerFieldUpdater包装过的int类型的score与 AtomicInteger 的allscore输出的值是一样的，足以见他们所起到的作用是一样。 我们说了AtomicIntegerFieldUpdater,那么AtomicLongFieldUpdater与它的用法大同小异，就不再说明。我们说这几个类是基于反射的实用工具，那么到底是怎么个反射法呢，我们不妨看看源码体验一下，上面用到了AtomicIntegerFieldUpdater.newUpdater()方法来指定类中的字段，我们不妨看看这个newUpdater是怎么执行的： newUpdater（）方法： 1234@CallerSensitivepublic static &lt;U&gt; AtomicIntegerFieldUpdater&lt;U&gt; newUpdater(Class&lt;U&gt; tclass, String fieldName) &#123; return new AtomicIntegerFieldUpdaterImpl&lt;U&gt;(tclass, fieldName, Reflection.getCallerClass());&#125; 我们看到在newUpdater方法上有一个注解：@CallerSensitive，关于这个注解我们可以探究一天的，暂时先埋一个伏笔哈，我们直接跟进去AtomicIntegerFieldUpdaterImpl方法： 12345678910111213141516171819202122232425AtomicIntegerFieldUpdaterImpl(Class&lt;T&gt; tclass, String fieldName, Class&lt;?&gt; caller) &#123; Field field = null; int modifiers = 0; try &#123; field = tclass.getDeclaredField(fieldName); modifiers = field.getModifiers(); sun.reflect.misc.ReflectUtil.ensureMemberAccess( caller, tclass, null, modifiers); sun.reflect.misc.ReflectUtil.checkPackageAccess(tclass); &#125; catch (Exception ex) &#123; throw new RuntimeException(ex); &#125; Class fieldt = field.getType(); if (fieldt != int.class) throw new IllegalArgumentException("Must be integer type"); if (!Modifier.isVolatile(modifiers)) throw new IllegalArgumentException("Must be volatile type"); this.cclass = (Modifier.isProtected(modifiers) &amp;&amp; caller != tclass) ? caller : null; this.tclass = tclass; offset = unsafe.objectFieldOffset(field);&#125; 我们能看到该类里面都是我们常见到的反射的机制，除了sun.reflect.misc.ReflectUtil这个包里面的我们没用到以外。 我们再看一下AtomicReferenceFieldUpdater的使用： 123456789101112131415161718192021222324252627282930313233343536373839404142public class AtomicReferenceFieldUpdaterTest &#123; public static void main(String[] args) &#123; TestAA testAA = new TestAA("xiaoming","nv",12); AtomicReferenceFieldUpdater Updater = AtomicReferenceFieldUpdater.newUpdater(TestAA.class,String.class,"name")； Updater.compareAndSet(testAA,testAA.name,"liming"); System.out.println(testAA.getName()); &#125;&#125;class TestAA&#123; volatile String name; volatile String sex; volatile int age; public TestAA(String name, String sex, int age) &#123; this.name = name; this.sex = sex; this.age = age; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public String getSex() &#123; return sex; &#125; public void setSex(String sex) &#123; this.sex = sex; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125;&#125; 输出为： liming Process finished with exit code 0]]></content>
      <categories>
        <category>多线程与并发编程</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java并发编程（十三）----(JUC原子类)引用类型介绍（CAS和ABA的介绍）]]></title>
    <url>%2Fposts%2F579efe75.html</url>
    <content type="text"><![CDATA[这一节我们将探讨引用类型原子类：AtomicReference, AtomicStampedRerence, AtomicMarkableReference。AtomicReference的使用非常简单，根据API我们就可以知道如何用，但是后两个从名字上看起来感觉是很难的样子，其实只是他的样子长得有点吓人，并且确实发挥了很大的作用（解决了ABA问题）。所以并没有那么可怕，就让我们一起来克服困难吧。 1 AtomicReference简介 AtomicReference的使用非常简单，首先我们来看一下他的方法： 构造函数： AtomicReference() //使用 null 初始值创建新的 AtomicReference。 AtomicReference(V initialValue) //使用给定的初始值创建新的 AtomicReference。 方法： boolean compareAndSet(V expect, V update) //如果当前值 == 预期值，则以原子方式将该值设置为给定的更新值。 V get() //获取当前值。 V getAndSet(V newValue) //以原子方式设置为给定值，并返回旧值。 void lazySet(V newValue) //最终设置为给定值。 void set(V newValue) //设置为给定值。 String toString() //返回当前值的字符串表示形式。 boolean weakCompareAndSet(V expect, V update) // 如果当前值 == 预期值，则以原子方式将该值设置为给定的更新值。 下面我们看一个例子来了解下使用方法： 1234567891011121314151617public class AtomicReferenceTest &#123; public static void main(String[] args) &#123; AtomicReference atomic1 = new AtomicReference(); atomic1.set("aaa"); atomic1.set(new StringBuffer("str")); Map map = new HashMap(); map.put("name","xiaoming"); atomic1.set(map); System.out.println(atomic1.get()); String[] s = &#123;"aa","bb","cc"&#125;; AtomicReference atomic2 = new AtomicReference(s); System.out.println(atomic2.get()); atomic2.set(atomic1); System.out.println(atomic2.get()); &#125;&#125; 输出结果： {name=xiaoming} [Ljava.lang.String;@766e119d {name=xiaoming} Process finished with exit code 0 由上面程序我们可以看到：AtomicReference可以set任何类型的值并且都是以原子的形式操作的。 2 CAS和ABA 在介绍AtomicStampedRerence, AtomicMarkableReference之前我们的先谈一谈CAS和ABA的问题，因为这两个类的设计就是为了避免CAS操作中的ABA问题而设计的. 2.1 CAS 原子操作的基石 我们知道之前我们学过的Synchronized是一种独占锁。一个线程获得该锁，那么其余的线程只能等待该线程释放锁才能获得。这其实是一种悲观锁的形式。那么乐观锁是如何实现的呢？乐观锁是每次都不加锁，假设完成某项任务没有冲突。如果因为冲突失败那就重试，直到成功为止。 今天我们要说的CAS就是乐观锁的实现机制。可用于在多线程编程中实现不被打断的数据交换操作，从而避免多线程同时改写某一数据时由于执行顺序不确定性以及中断的不可预知性产生的数据不一致问题。 该操作通过将内存中的值与指定数据进行比较，当数值一样时将内存中的数据替换为新的值。 CAS:Compare and Swap，这个操作用C语言来描述就是下面这个样子（代码来自Wikipedia的Compare And Swap词条）： 1234567int compare_and_swap (int* reg, int oldval, int newval)&#123; int old_reg_val = *reg; if (old_reg_val == oldval) *reg = newval; return old_reg_val;&#125; 意思就是说，看一看内存*reg里的值是不是oldval，如果是的话，则对其赋值newval。 CAS有3个操作数，内存值V，旧的预期值A，要修改的新值B。当且仅当预期值A和内存值V相同时，将内存值V修改为B，否则什么都不做。 在使用上，通常会记录下某块内存中的旧值，通过对旧值进行一系列的操作后得到新值，然后通过CAS操作将新值与旧值进行交换。如果这块内存的值在这期间内没被修改过，则旧值会与内存中的数据相同，这时CAS操作将会成功执行 使内存中的数据变为新值。如果内存中的值在这期间内被修改过，则一般来说旧值会与内存中的数据不同，这时CAS操作将会失败，新值将不会被写入内存。 2.2 原子操作 虽然我们是在用java语言去执行原子操作，但是最终还是对应到处理器上去执行。那么在处理器上是如何执行原子操作的呢？ 32位IA-32处理器使用基于对缓存加锁或总线加锁的方式来实现多处理器之间的原子操作。 第一个机制是通过总线锁保证原子性。。如果多个处理器同时对共享变量进行读改写（i就是经典的读改写操作）操作，那么共享变量就会被多个处理器同时进行操作，这样读改写操作就不是原子的，操作完之后共享变量的值会和期望的不一致，举个例子：如果i=1,我们进行两次i操作，我们期望的结果是3，但是有可能结果是2。 原因是有可能多个处理器同时从各自的缓存中读取变量i，分别进行加一操作，然后分别写入系统内存当中。那么想要保证读改写共享变量的操作是原子的，就必须保证CPU1读改写共享变量的时候，CPU2不能操作缓存了该共享变量内存地址的缓存。 处理器使用总线锁就是来解决这个问题的。所谓总线锁就是使用处理器提供的一个LOCK＃信号，当一个处理器在总线上输出此信号时，其他处理器的请求将被阻塞住,那么该处理器可以独占使用共享内存。 第二个机制是通过缓存锁定保证原子性。在同一时刻我们只需保证对某个内存地址的操作是原子性即可，但总线锁定把CPU和内存之间通信锁住了，这使得锁定期间，其他处理器不能操作其他内存地址的数据，所以总线锁定的开销比较大，最近的处理器在某些场合下使用缓存锁定代替总线锁定来进行优化。 上面说了cpu的原子操作是如何实现的，那么在java中一定也有相应的方法去驱动处理器来实现原子操作。JUC包中的原子类都是基于CAS来实现的，我们不妨以AtomicInteger为例来跟踪一下，看看到底是如何实现原子操作的。 首先我们能看到在AtomicInteger中的value值是这样定义的： 1private volatile int value; 我们再来看他的get方法： 123public final int get() &#123; return value;&#125; 直接返回value值，说明在无锁的情况下，通过volatile来做控制，保证值是可见的。 下面我们接着看一下i++在原子类中是怎么实现的： 1234567public final int getAndSet(int newValue) &#123; for (;;) &#123; int current = get(); if (compareAndSet(current, newValue)) return current; &#125;&#125; 能看到，首先把volatile修饰的内存中的原始值赋值给当前变量，为了下面compareAndSet方法拿内存中的值和新值比较进行CAS操作。所以关键就在于这个compareAndSet（）方法，我们接着进入这个方法： 123public final boolean compareAndSet(int expect, int update) &#123; return unsafe.compareAndSwapInt(this, valueOffset, expect, update);&#125; 我们看到compareAndSet方法中返回的是unsafe类的方法，我们知道java不能直接访问操作系统底层，而是通过本地方法来访问。Unsafe类提供了硬件级别的原子操作，这就与硬件相挂钩了。由此我们从java到处理器的通道就打通了。 具体的compareAndSwapInt()方法的实现属于JDK底层的实现，我们在此不多做说明，有兴趣的可以查阅相关资料，看java的底层c语言源码。大致的过程我们可以说明如下：CAS 操作包含三个操作数 —— 内存位置（V）、预期原值（A）和新值(B)。如果内存位置的值与预期原值相匹配，那么处理器会自动将该位置值更新为新值。否则，处理器不做任何操作。无论哪种情况，它都会在 CAS 指令之前返回该位置的值。CAS 有效地说明了“我认为位置 V 应该包含值 A；如果包含该值，则将 B 放到这个位置；否则，不要更改该位置，只告诉我这个位置现在的值即可。” Java并发包(java.util.concurrent)中大量使用了CAS操作,涉及到并发的地方都调用了sun.misc.Unsafe类方法进行CAS操作。 2.3 ABA问题 上面我们结合着处理器对原子操作的处理机制一起讲了java对原子操作的处理方式，那么难道这种方式就一定是完美的吗。下面我们举出一种情况大家分析看看： 进程P1在共享变量中读到值为A P1被抢占了，进程P2执行 P2把共享变量里的值从A改成了B，再改回到A，此时被P1抢占。 P1回来看到共享变量里的值没有被改变，于是继续执行。 你觉得这会出现什么问题呢？我们知道java底层代码都是用c语言实现的，虽然java中没有指针，但不代表java底层代码中没有使用。虽然P1以为变量值没有改变，继续执行了，但是这个会引发一些潜在的问题。ABA问题最容易发生在lock free 的算法中的，CAS首当其冲，因为CAS判断的是指针的地址（c语言的特性）。如果这个地址被重用了呢，问题就很大了。 现有一个用单向链表实现的堆栈，栈顶为A，这时线程T1已经知道A.next为B，然后希望用CAS将栈顶替换为B: 在T1执行上面这条指令之前，线程T2介入，将A、B出栈，再pushD、C、A，此时堆栈结构如下图，而对象B此时已经出栈： 在CAS中c语言的堆栈实现过程大致如下： 123456789101112131415161718push(node): curr := head old := curr node-&gt;next = curr while (old != (curr = CAS(&amp;head, curr, node))) &#123; old = curr node-&gt;next = curr &#125;pop(): curr := head old := curr next = curr-&gt;next while (old != (curr = CAS(&amp;head, curr, next))) &#123; old = curr next = curr-&gt;next &#125; return curr 假如，在pop函数中，next = curr-&gt;next 和 while之间，线程被切换走，此时轮到线程T1执行CAS操作，检测发现栈顶仍为A，所以CAS成功，栈顶变为B，但实际上B.next为null，即while此时还没有执行呢，所以此时的情况变为： 其中堆栈中只有B元素的引用地址，C和D组成的链表不再存在于堆栈中，平白无故就把C、D丢掉了。 如果这个例子你没有看懂的话，我可以再举出一个生活中的例子： 你拿着一个装满钱的手提箱在飞机场，此时过来了一个火辣性感的美女，然后她很暖昧地挑逗着你，并趁你不注意的时候，把用一个一模一样的手提箱和你那装满钱的箱子调了个包，然后就离开了，你看到你的手提箱还在那，于是就提着手提箱去赶飞机去了。 以上就是ABA问题。 3 解决ABA问题之道 因为CAS需要在操作值的时候检查下值有没有发生变化，如果没有发生变化则更新，但是如果一个值原来是A，变成了B，又变成了A，那么使用CAS进行检查时会发现它的值没有发生变化，但是实际上却变化了。ABA问题的解决思路就是使用版本号。在变量前面追加上版本号，每次变量更新的时候把版本号加一，那么A－B－A 就会变成1A-2B－3A。 为了避免CAS过程中的ABA问题，并发包提供了两个类，AtomicStampedReference和AtomicMarkableReference。前者相当于一个[引用,integer]的二元组，后者相当于一个[引用,boolean]的二元组。 AtomicStampedReference可用来作为带版本号的原子引用，而AtomicMarkableReference可用于表示已删除的节点。 这个类的compareAndSet方法作用是首先检查当前引用是否等于预期引用，并且当前标志是否等于预期标志，如果全部相等，则以原子方式将该引用和该标志的值设置为给定的更新值。 我们先来看一下AtomicStampedReference的方法，AtomicStampedReference 维护带有整数“标志”的对象引用，可以用原子方式对其进行更新： 构造方法： AtomicStampedReference(V initialRef, int initialStamp) //创建具有给定初始值的新 AtomicStampedReference 方法： boolean attemptStamp(V expectedReference, int newStamp) //如果当前引用 == 预期引用，则以原子方式将该标志的值设置为给定的更新值。 boolean compareAndSet(V expectedReference, V newReference, int expectedStamp, int newStamp) //如果当前引用 == 预期引用，并且当前标志等于预期标志， 则以原子方式将该引用和该标志的值设置为给定的更新值。 V get(int[] stampHolder) //返回该引用和该标志的当前值。 V getReference() //返回该引用的当前值。 int getStamp() // 返回该标志的当前值。 void set(V newReference, int newStamp) // 无条件地同时设置该引用和标志的值。 boolean weakCompareAndSet(V expectedReference, V newReference, int expectedStamp, int newStamp) //如果当前引用 == 预期引用，并且当前标志等于预期标志，则以原子方式将该引用和该标志的值设置为给定的更新值。 看来看一下AtomicMarkableReference的方法： 构造方法： AtomicMarkableReference(V initialRef, boolean initialMark) //创建具有给定初始值的新 AtomicMarkableReference。 方法： boolean attemptMark(V expectedReference, boolean newMark) //如果当前引用 == 预期引用，则以原子方式将该标记的值设置为给定的更新值。 boolean compareAndSet(V expectedReference, V newReference, boolean expectedMark, boolean newMark) //如果当前引用 == 预期引用，并且当前标记等于预期标记，那么以原子方式将引用和标记的值设置为给定的更新值。 V get(boolean[] markHolder) // 返回该引用和该标记的当前值。 V getReference() //返回该引用的当前值。 boolean isMarked() //返回该标记的当前值。 void set(V newReference, boolean newMark) //无条件地同时设置该引用和标记的值。 boolean weakCompareAndSet(V expectedReference, V newReference, boolean expectedMark, boolean newMark) //如果当前引用 == 预期引用，并且当前标记等于预期标记，那么以原子方式将引用和标记的值设置为给定的更新值。 AtomicMarkableReference类描述的一个(Object,Boolean)的对，可以原子的修改Object或者Boolean的值，这种数据结构在一些缓存或者状态描述中比较有用。这种结构在单个或者同时修改Object/Boolean的时候能够有效的提高吞吐量。 AtomicStampedReference类维护带有整数“标志”的对象引用，可以用原子方式对其进行更新。对比AtomicMarkableReference类的(Object,Boolean)，AtomicStampedReference维护的是一种类似(Object,int)的数据结构，其实就是对对象（引用）的一个并发计数。但是与AtomicInteger不同的是，此数据结构可以携带一个对象引用（Object），并且能够对此对象和计数同时进行原子操作。 下面我们就AtomicStampedReference 的使用举一个例子： 1234567891011121314151617181920212223242526public class AtomicStampedReferenceTest &#123; public static void main(String[] args) &#123; AtomicInteger atomicInt = new AtomicInteger(100); atomicInt.compareAndSet(100, 101); atomicInt.compareAndSet(101, 100); System.out.println("new value = " + atomicInt.get()); boolean result1 = atomicInt.compareAndSet(100, 101); System.out.println(result1); // result:true AtomicInteger v1 = new AtomicInteger(100); AtomicInteger v2 = new AtomicInteger(101); AtomicStampedReference&lt;AtomicInteger&gt; stampedRef = new AtomicStampedReference&lt;AtomicInteger&gt;(v1, 0); int stamp = stampedRef.getStamp(); stampedRef.compareAndSet(v1, v2, stampedRef.getStamp(), stampedRef.getStamp() + 1); System.out.println(stampedRef.getStamp()); stampedRef.compareAndSet(v2, v1, stampedRef.getStamp(), stampedRef.getStamp() + 1); System.out.println("new value = " + stampedRef.getReference()); boolean result2 = stampedRef.compareAndSet(v1, v2, stamp, stamp + 1); System.out.println(result2); // result:false &#125;&#125; 输出结果为： new value = 100 true 1 new value = 100 false Process finished with exit code 0 上面的输出结果可以看到AtomicInteger 执行cas操作成功，AtomicStampedReference执行cas操作失败。我们可以看到在24行compareAndSet(v1, v2, stamp, stamp + 1)中第三个参数期待的标志位值为1，但是经过上面两次的值变更，stampedRef.getStamp()已经是2了，所以此刻期待值与内存中的标志位值不符，操作失败。 AtomicMarkableReference的使用方法也是类似，在此就不另做介绍，大家可以多多使用才会知道这些类所带来的好处。]]></content>
      <categories>
        <category>多线程与并发编程</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java并发编程（十二）----(JUC原子类)数组类型介绍]]></title>
    <url>%2Fposts%2Ff9388b3f.html</url>
    <content type="text"><![CDATA[上一节我们介绍过三个基本类型的原子类，这次我们来看一下数组类型： AtomicIntegerArray, AtomicLongArray, AtomicReferenceArray。其中前两个的使用方式差不多，AtomicReferenceArray因为他的参数为引用数组，所以跟前两个的使用方式有所不同。 1 AtomicLongArray介绍 对于AtomicLongArray, AtomicIntegerArray我们还是只介绍一个，另一个使用方式大同小异。 我们先来看看AtomicLongArray的构造函数和方法： 构造函数： AtomicLongArray(int length) //创建给定长度的新 AtomicLongArray。 AtomicLongArray(long[] array) //创建与给定数组具有相同长度的新 AtomicLongArray，并从给定数组复制其所有元素。 方法： long addAndGet(int i, long delta) //以原子方式将给定值添加到索引 i 的元素。 boolean compareAndSet(int i, long expect, long update) //如果当前值 == 预期值，则以原子方式将该值设置为给定的更新值。 long decrementAndGet(int i) //以原子方式将索引 i 的元素减1。 long get(int i) //获取位置 i 的当前值。 long getAndAdd(int i, long delta) //以原子方式将给定值与索引 i 的元素相加。 long getAndDecrement(int i) //以原子方式将索引 i 的元素减 1。 long getAndIncrement(int i) //以原子方式将索引 i 的元素加 1。 long getAndSet(int i, long newValue) //以原子方式将位置 i 的元素设置为给定值，并返回旧值。 long incrementAndGet(int i) // 以原子方式将索引 i 的元素加1。 void lazySet(int i, long newValue)// 最终将位置 i 的元素设置为给定值。 int length() //返回该数组的长度。 void set(int i, long newValue) //将位置 i 的元素设置为给定值。 String toString() //返回数组当前值的字符串表示形式。 2 使用方式： 我们可以发现AtomicLongArray的使用方式和上一篇介绍的基本类型的原子类差不多，无非是换成了数组类型，另外方法里面的etAndAdd与ncrementAndGet我们要注意使用方式。 3 AtomicReferenceArray介绍 我们来看一下他的方法： 构造方法： AtomicReferenceArray(E[] array) //创建与给定数组具有相同长度的新 AtomicReferenceArray，并从给定数组复制其所有元素。 AtomicReferenceArray(int length) // 创建给定长度的新 AtomicReferenceArray。 方法: boolean compareAndSet(int i, E expect, E update) //如果当前值 == 预期值，则以原子方式将位置 i 的元素设置为给定的更新值。 E get(int i) //获取位置 i 的当前值。 E getAndSet(int i, E newValue) // 以原子方式将位置 i 的元素设置为给定值，并返回旧值。 void lazySet(int i, E newValue) //最终将位置 i 的元素设置为给定值。 int length() //返回该数组的长度。 void set(int i, E newValue) // 将位置 i 的元素设置为给定值。 String toString() //返回数组当前值的字符串表示形式。 boolean weakCompareAndSet(int i, E expect, E update) // 如果当前值 == 预期值，则以原子方式将位置 i 的元素设置为给定的更新值。 由上我们可以看到AtomicReferenceArray与前两个的方法相比少了很多。 下面我们通过一个小例子来看一下他的使用： 1234567891011121314151617public class AtomicReferenceArrayTest &#123; public static void main(String[] args) &#123; Long[] l = new Long[4]; String[] s = new String[4]; int[] i = new int[4]; Integer[] in = new Integer[4]; AtomicReferenceArray atomicReferenceArray = new AtomicReferenceArray(l); System.out.println(atomicReferenceArray.length()); System.out.println(atomicReferenceArray.get(2)); AtomicReferenceArray atomic = new AtomicReferenceArray(4); atomic.set(0,432141); atomic.set(2,"fsafefeq"); atomic.set(3,i); System.out.println(atomic.toString()); &#125;&#125; 输出结果为： exclude patterns: 4 null [432141, null, fsafefeq, [I@357b2b99] Process finished with exit code 0 说明： 1.当我们使用AtomicReferenceArray(E[] array)这个构造方法传入一个数组对象时，该数组对象必须是引用类型，int[]不可以，但是Integer[]的可以。 2.当我们使用AtomicReferenceArray(int length)这个构造函数的时候，只要为他指定了数组大小之后，你为数组的每一位设置什么值是没有要求的，类似于Map的形式。]]></content>
      <categories>
        <category>多线程与并发编程</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java并发编程（十一）----(JUC原子类)基本类型介绍]]></title>
    <url>%2Fposts%2F8ca24763.html</url>
    <content type="text"><![CDATA[上一节我们说到了基本原子类的简单介绍，这一节我们先来看一下基本类型: AtomicInteger, AtomicLong, AtomicBoolean。AtomicInteger和AtomicLong的使用方法差不多，AtomicBoolean因为比较简单所以方法比前两个都少，那我们这节主要挑AtomicLong来说，会使用一个，其余的大同小异。 1 原子操作与一般操作异同 我们在说原子操作之前为了有个对比为什么需要这些原子类而不是普通的基本数据类型就能满足我们的使用要求，那就不得不提原子操作不同的地方。 当你在操作一个普通变量时，你在Java实现的每个操作，在程序编译时会被转换成几个机器能读懂的指令。例如，当你分配一个值给变量，在Java你只使用了一个指令，但是当你编译这个程序时，这个指令就被转换成多个JVM 语言指令。这样子的话当你在操作多个线程且共享一个变量时，就会导致数据不一致的错误。 为了避免这样的问题，Java引入了原子变量。当一个线程正在操作一个原子变量时，即使其他线程也想要操作这个变量，类的实现中含有一个检查那步骤操作是否完成的机制。 基本上，操作获取变量的值，改变本地变量值，然后尝试以新值代替旧值。如果旧值还是一样，那么就改变它。如果不一样，方法再次开始操作。这个操作称为 Compare and Set（简称CAS，比较并交换的意思）。 原子变量不使用任何锁或者其他同步机制来保护它们的值的访问。他们的全部操作都是基于CAS操作。它保证几个线程可以同时操作一个原子对象也不会出现数据不一致的错误，并且它的性能比使用受同步机制保护的正常变量要好。 2 AtomicLong简介 由字面意义我们可以知道AtomicLong可以用原子方式更新的 long 值，下面我们看一下他的构造方法和一般方法： 构造方法： AtomicLong() //创建具有初始值 0 的新 AtomicLong。 AtomicLong(long initialValue) //创建具有给定初始值的新 AtomicLong。 方法： long addAndGet(long delta) //以原子方式将给定值添加到当前值。 boolean compareAndSet(long expect, long update) //如果当前值 == 预期值，则以原子方式将该值 设置为给定的更新值。 long decrementAndGet() //以原子方式将当前值减 1。 double doubleValue() //以 double 形式返回指定的数值。 float floatValue() //以 float 形式返回指定的数值。 long get() //获取当前值。 long getAndAdd(long delta) //以原子方式将给定值添加到当前值。 long getAndDecrement() //以原子方式将当前值减 1。 long getAndIncrement() //以原子方式将当前值加 1。 long getAndSet(long newValue)// 以原子方式设置为给定值，并返回旧值。 long incrementAndGet() //以原子方式将当前值加 1。 int intValue() // 以 int 形式返回指定的数值。 void lazySet(long newValue) //最后设置为给定值。 long longValue() // 以 long 形式返回指定的数值。 void set(long newValue) //设置为给定值。 String toString() // 返回当前值的字符串表示形式。 boolean weakCompareAndSet(long expect, long update) //如果当前值 == 预期值，则以原子方式将该值设置为给定的更新值。 3 使用AtomicLong 3.1 创建AtomicLong 创建AtomicLong的过程如下： 1AtomicLong atomicLong = new AtomicLong（）; 此示例创建一个初始值为0的AtomicLong 。 如果你想创建一个带有初始值的AtomicLong ，你可以这样做： 1AtomicLong atomicLong = new AtomicLong（123）; 此示例将值123作为参数传递给AtomicLong装订器，该装置将AtomicLong实例的初始值设置为123 。 3.2 获取AtomicLong值 您可以通过get()方法get() AtomicLong实例的值。 这里是一个AtomicLong.get()示例： 12AtomicLong atomicLong = new AtomicLong（123）;long theValue = atomicLong.get（）; 设置AtomicLong值 您可以通过set()方法set() AtomicLong实例的值。 这里是一个AtomicLong.set()示例： 123AtomicLong atomicLong = new AtomicLong（123）; atomicLong.set（234）; 此示例创建一个初始值为123的AtomicLong示例，然后在下一行中将其值设置为234 。 3.3 比较并设置AtomicLong值 AtomicLong类也有一个原子compareAndSet()方法。 此方法将AtomicLong实例的当前值与AtomicLong进行比较，如果这两个值相等， AtomicLong实例设置新值。 这里是一个AtomicLong.compareAndSet()示例： 1234AtomicLong atomicLong = new AtomicLong（123）;long expectedValue = 123;long newValue = 234;atomicLong.compareAndSet（expectedValue，newValue）; 此示例首先创建一个初始值为123的AtomicLong实例。 然后，它将AtomicLong的值与期望值123进行比较，如果它们相等，则AtomicLong的新值变为234 ; 3.4 添加到AtomicLong值 AtomicLong类包含几个方法，您可以使用这些方法向AtomicLong添加值并返回其值。这里我们要重点关注一下，因为这几个方法会如果我们使用不当会造成歧义。 这些方法是： addAndGet() getAndAdd() getAndIncrement() incrementAndGet() 第一种方法addAndGet()向AtomicLong添加一个数字，并在添加后返回其值。 第二种方法getAndAdd()还向AtomicLong添加一个数字，但返回AtomicLong在添加值之前的值。 您应该使用这两种方法中的哪一种取决于您的用例。 这里有两个例子： 123AtomicLong atomicLong = new AtomicLong（）;System.out.println（atomicLong.getAndAdd（10））;System.out.println（atomicLong.addAndGet（10））; 此示例将打印出值0和20 。 首先，示例在添加10之前获取AtomicLong的值。 它的值在加法之前为0.然后示例将10添加到AtomicLong ，并获取添加后的值。 该值现在为20。 您也可以通过这两种方法向AtomicLong添加负数。 结果实际上是一个减法。 方法getAndIncrement()和incrementAndGet()工作原理像getAndAdd()和addAndGet()但只是添加1到AtomicLong的值。 3.5 从AtomicLong值中减去 AtomicLong类还包含一些用于从AtomicLong值中以AtomicLong值的方法。 这些方法是： decrementAndGet() getAndDecrement() decrementAndGet()从AtomicLong值中减去1，并在AtomicLong后返回其值。 getAndDecrement()也从AtomicLong值中减去1，但返回AtomicLong在AtomicLong之前的值。 由上我们大致知道了AtomicLong的用法，AtomicBoolean，AtomicInteger也与它的用法差不多，我们看一下API他们各自的方法就知道该如何使用。]]></content>
      <categories>
        <category>多线程与并发编程</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java并发编程（十）----JUC原子类介绍]]></title>
    <url>%2Fposts%2Fdb147845.html</url>
    <content type="text"><![CDATA[今天我们来看一下JUC包中的原子类，所谓原子操作是指不会被线程调度机制打断的操作；这种操作一旦开始，就一直运行到结束，中间不会有任何 context switch （切换到另一个线程），原子操作可以是一个步骤，也可以是多个操作步骤，但是其顺序不可以被打乱，也不可以被切割而只执行其中的一部分。将整个操作视作一个整体是原子性的核心特征。 在atomic包中的这些原子类我们可以大致给他分类为： 基本类型: AtomicInteger, AtomicLong, AtomicBoolean ; 数组类型: AtomicIntegerArray, AtomicLongArray, AtomicReferenceArray ; 引用类型: AtomicReference, AtomicStampedRerence, AtomicMarkableReference ; 对象的属性修改类型: AtomicIntegerFieldUpdater, AtomicLongFieldUpdater, AtomicReferenceFieldUpdater 。 下一节我们详细的分析这些原子类。]]></content>
      <categories>
        <category>多线程与并发编程</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java并发编程（九）----(JUC)CyclicBarrier]]></title>
    <url>%2Fposts%2Ff5e73779.html</url>
    <content type="text"><![CDATA[上一篇我们介绍了CountDownlatch，我们知道CountDownlatch是“在完成一组正在其他线程中执行的操作之前，它允许一个或多个线程一直等待”，即CountDownLatch的作用是允许1或N个线程等待其他线程完成执行，而我们今天要介绍的CyclicBarrier则是允许N个线程相互等待。 1 CyclicBarrier简介 CyclicBarrier 的字面意思是可循环使用（Cyclic）的屏障（Barrier）。它要做的事情是，让一组线程到达一个屏障（也可以叫同步点）时被阻塞，直到最后一个线程到达屏障时，屏障才会开门，所有被屏障拦截的线程才会继续干活。 在JDK中对CyclicBarrier是这样说的“允许一组线程全部等待彼此到达公共屏障点的同步辅助。 循环障碍在涉及必须偶尔彼此等待的固定大小的线程程序中是有用的。屏障称为循环 ，因为它可以在等待线程释放后重新使用”。CountDownLatch的计数器无法被重置；CyclicBarrier的计数器可以被重置后使用，因此它被称为是循环的barrier。 我们先来看一下他的构造方法和使用方式： 构造函数： CyclicBarrier(int parties) //其参数表示屏障拦截的线程数量，每个线程调用await方法告 诉CyclicBarrier我已经到达了屏障，然后当前线程被阻塞。 CyclicBarrier (int parties, Runnable barrierAction) //创建一个新的CyclicBarrier ， 当给定数量的参与者（线程）等待它时，它将跳闸，当障碍跳闸时，它 将执行 给定的障碍动作(Runnable参数提供)，由最后一个线程进入障碍。 方法： int await() //在所有参与者都已经在此 barrier 上调用 await 方法之前，将一直等待。 方法之前将一直等待,或者超出了指定的等待时间。 int getNumberWaiting() //返回当前在屏障处等待的参与者数目。 int getParties() //返回要求启动此 barrier 的参与者数目。 boolean isBroken() //查询此屏障是否处于损坏状态。 void reset() //将屏障重置为其初始状态。 下面我们来看一个小程序了解一下CyclicBarrier的使用方式： 123456789101112131415161718192021222324public class CyclicBarrierTest &#123; static CyclicBarrier c = new CyclicBarrier(2); public static void main(String[] args) &#123; new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; c.await(); &#125; catch (Exception e) &#123; &#125; System.out.println(Thread.currentThread().getName()+"正在等待..."); &#125; &#125;).start(); try &#123; c.await(); &#125; catch (Exception e) &#123; &#125; System.out.println(Thread.currentThread().getName()+"正在等待..."); System.out.println("人够了，出发吧 当前有 "+c.getParties()+" 个人参与比赛"); &#125;&#125; 输出结果为： Thread-0正在等待... main正在等待... 人够了，出发吧 当前有 2 个人参与比赛 Process finished with exit code 0 在上面程序中如果我们把&quot;static CyclicBarrier c = new CyclicBarrier(2);&quot;中的参数2修改为3的话改程序中的线程Thread-0和main则会一直等待下去，因为CyclicBarrier是让一组线程到达一个屏障（也可以叫同步点）时被阻塞，直到最后一个线程到达屏障时，屏障才会开门，而这最后一个线程迟迟不来，所以屏障也不会被打开。 CyclicBarrier还提供一个更高级的构造函数CyclicBarrier(int parties, Runnable barrierAction)，用于在线程到达屏障时，优先执行barrierAction，方便处理更复杂的业务场景。我们来看一下示例： 12345678910111213141516171819202122232425262728293031public class CyclicBarrierTest &#123; static CyclicBarrier c = new CyclicBarrier(2,new PrioExecut()); public static void main(String[] args) &#123; new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; c.await(); &#125; catch (Exception e) &#123; &#125; System.out.println(Thread.currentThread().getName()+"正在等待..."); &#125; &#125;).start(); try &#123; c.await(); &#125; catch (Exception e) &#123; &#125; System.out.println(Thread.currentThread().getName()+"正在等待..."); System.out.println("人够了，出发吧 当前有 "+c.getParties()+" 个人参与比赛"); &#125;&#125; class PrioExecut implements Runnable&#123; @Override public void run() &#123; System.out.println("我会先跑5秒，不管你信不信！"); &#125;&#125; 执行结果为： 我会先跑5秒，不管你信不信！ Thread-0正在等待... main正在等待... 人够了，出发吧 当前有 2 个人参与比赛 Process finished with exit code 0 我们可以看到构造方法中的参数：new PrioExecut()中的线程会优先执行。 2 CyclicBarrier的应用场景 CyclicBarrier可以用于多线程计算数据，最后合并计算结果的应用场景。比如在支付业务中，我们可以按照事先划分好的片区的形式来统计日收支流水，然后根据片区的计算结果，使用Runnable barrierAction来进行汇总这是一个很好的实现。 3 CyclicBarrier和CountDownLatch的区别 在javadoc里面的描述是这样的： CountDownLatch: A synchronization aid that allows one or more threads to wait until a set of operations being performed in other threads completes. CyclicBarrier : A synchronization aid that allows a set of threads to all wait for each other to reach a common barrier point. 根据我的理解：对于CountDownLatch来说，重点是那个“一个线程”, 它在等待其余线程执行完毕他才能执行，而另外那N的线程在把“某个事情”做完之后可以继续等待，可以终止。比如上文说的跑步的例子，只有5位跑步者同时准备好了，裁判才能下令开始跑步；CyclicBarrier强调的是n个线程，大家相互等待，只要有一个没完成，所有人都得等着。 CountDownLatch的计数器无法被重置；CyclicBarrier的计数器可以使用reset() 方法重置。所以CyclicBarrier能处理更为复杂的业务场景，比如如果计算发生错误，可以重置计数器，并让线程们重新执行一次。]]></content>
      <categories>
        <category>多线程与并发编程</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java并发编程（七）----（JUC）ReadWriteLock]]></title>
    <url>%2Fposts%2Fde2aa32e.html</url>
    <content type="text"><![CDATA[前面我们已经分析过JUC包里面的Lock锁，ReentrantLock锁和semaphore信号量机制。Lock锁实现了比synchronized更灵活的锁机制，Reentrantlock是Lock的实现类，是一种可重入锁，都是每次只有一次线程对资源进行处理；semaphore实现了多个线程同时对一个资源的访问；今天我们要讲的ReadWriteLock锁将实现另外一种很重要的功能：读写分离锁。 假设你的程序中涉及到对一些共享资源的读和写操作，且写操作没有读操作那么频繁。在没有写操作的时候，两个线程同时读一个资源没有任何问题，所以应该允许多个线程能在同时读取共享资源。但是如果有一个线程想去写这些共享资源，就不应该再有其它线程对该资源进行读或写，也就是说：读-读能共存，读-写不能共存，写-写不能共存。这就需要一个读/写锁来解决这个问题。 1 ReadWriteLock简介 我们在JUC包可以看到ReadWriteLock是一个接口，他有一个实现类：ReentrantReadWriteLock，先让我们对读写访问资源的条件做个概述： - 读取： 没有线程正在做写操作，且没有线程在请求写操作。 - 写入： 没有线程正在做读写操作。 如果某个线程想要读取资源，只要没有线程正在对该资源进行写操作且没有线程请求对该资源的写操作即可。同样当有线程想要写资源，但是此刻有线程正在读取资源，那么此刻写资源的操作是不能继续下去的。 我们来看一个例子: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495public class ReadWriteLockTest2 &#123; public static void main(String[] args) &#123; final int threadCount = 2; final ExecutorService exService = Executors.newFixedThreadPool(threadCount); final ScoreBoard scoreBoard = new ScoreBoard(); exService.execute(new ScoreUpdateThread(scoreBoard)); exService.execute(new ScoreHealthThread(scoreBoard)); exService.shutdown(); &#125;&#125; class ScoreBoard &#123; private boolean scoreUpdated = false; private int score = 0; String health = "不可用"; final ReentrantReadWriteLock rrwl = new ReentrantReadWriteLock(); public String getMatchHealth() &#123; rrwl.readLock().lock(); if (scoreUpdated) &#123; rrwl.readLock().unlock(); rrwl.writeLock().lock(); try &#123; if (scoreUpdated) &#123; score = fetchScore(); scoreUpdated = false; &#125; rrwl.readLock().lock(); &#125; finally &#123; rrwl.writeLock().unlock(); &#125; &#125; try &#123; if (score % 2 == 0) &#123; health = "Bad Score"; &#125; else &#123; health = "Good Score"; &#125; &#125; finally &#123; rrwl.readLock().unlock(); &#125; return health; &#125; public void updateScore() &#123; try &#123; rrwl.writeLock().lock(); scoreUpdated = true; &#125; finally &#123; rrwl.writeLock().unlock(); &#125; &#125; private int fetchScore() &#123; Calendar calender = Calendar.getInstance(); return calender.get(Calendar.MILLISECOND); &#125;&#125;class ScoreHealthThread implements Runnable &#123; private ScoreBoard scoreBoard; public ScoreHealthThread(ScoreBoard scoreTable) &#123; this.scoreBoard = scoreTable; &#125; @Override public void run() &#123; for(int i= 0; i&lt; 5; i++) &#123; System.out.println("Match Health: "+ scoreBoard.getMatchHealth()); try &#123; Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125;class ScoreUpdateThread implements Runnable &#123; private ScoreBoard scoreBoard; public ScoreUpdateThread(ScoreBoard scoreTable) &#123; this.scoreBoard = scoreTable; &#125; @Override public void run() &#123; for(int i= 0; i &lt; 5; i++) &#123; System.out.println("Score Updated."); scoreBoard.updateScore(); try &#123; Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 打印结果： Score Updated. Match Health: Good Score Score Updated. Match Health: Good Score Score Updated. Match Health: Good Score Score Updated. Match Health: Good Score Score Updated. Match Health: Good Score 基本用法见上例，读写分离锁很好的控制了多个线程对同一个资源的访问。 2 ReentrantReadWriteLock 由名字我们可以看到读写锁也有可重入的实现类。ReentrantReadWriteLock具有关联的读取和写入锁定，可以重新获取锁定。它可表现为公平和不公平的模式两者。 默认行为是不公平的。 非公平锁的性能更好，虽然有可能读写器或写入器锁可以被推迟许多次，并且持续地尝试锁定。 在公平锁定的情况下，锁定请求按照最长等待的单个写入器锁或读取锁定组请求的顺序来完成，无论谁具有最长等待时间将获得对共享资源的锁定。 在重入ReentrantReadWriteLock可以写入锁定降级读锁。 这意味着如果线程已经获得写锁定，它可以将其锁从写降级到读锁。 顺序将是首先获得写锁定，执行写操作，然后获取读锁，然后解锁写锁，并且在读操作后最终解锁读锁。 ReentrantReadWriteLock 也是基于 AbstractQueuedSynchronizer 实现的，它具有下面这些属性： 获取顺序 此类不会将读取者优先或写入者优先强加给锁访问的排序。但是，它确实支持可选的公平 策略。 1.非公平模式（默认） 当非公平地（默认）构造时，未指定进入读写锁的顺序，受到 reentrancy 约束的限制。连续竞争的非公平锁可能无限期地推迟一个或多个 reader 或 writer 线程，但吞吐量通常要高于公平锁。 2.公平模式 当公平地构造线程时，线程利用一个近似到达顺序的策略来争夺进入。当释放当前保持的锁时，可以为等待时间最长的单个 writer 线程分配写入锁，如果有一组等待时间大于所有正在等待的 writer 线程 的 reader 线程，将为该组分配写入锁。 如果保持写入锁，或者有一个等待的 writer 线程，则试图获得公平读取锁（非重入地）的线程将会阻塞。直到当前最旧的等待 writer 线程已获得并释放了写入锁之后，该线程才会获得读取锁。当然，如果等待 writer 放弃其等待，而保留一个或更多 reader 线程为队列中带有写入锁自由的时间最长的 waiter，则将为那些 reader 分配读取锁。 试图获得公平写入锁的（非重入地）的线程将会阻塞，除非读取锁和写入锁都自由（这意味着没有等待线程）。（注意，非阻塞 ReentrantReadWriteLock.ReadLock.tryLock() 和 ReentrantReadWriteLock.WriteLock.tryLock() 方法不会遵守此公平设置，并将获得锁（如果可能），不考虑等待线程）。 重入 此锁允许 reader 和 writer 按照 ReentrantLock 的样式重新获取读取锁或写入锁。在写入线程保持的所有写入锁都已经释放后，才允许重入 reader 使用它们。 此外，writer 可以获取读取锁，但反过来则不成立。在其他应用程序中，当在调用或回调那些在读取锁状态下执行读取操作的方法期间保持写入锁时，重入很有用。如果 reader 试图获取写入锁，那么将永远不会获得成功。 锁降级 重入还允许从写入锁降级为读取锁，其实现方式是：先获取写入锁，然后获取读取锁，最后释放写入锁。但是，从读取锁升级到写入锁是不可能的。 锁获取的中断 读取锁和写入锁都支持锁获取期间的中断。 Condition 支持 写入锁提供了一个 Condition 实现，对于写入锁来说，该实现的行为与 ReentrantLock.newCondition() 提供的 Condition 实现对 ReentrantLock 所做的行为相同。当然，此 Condition 只能用于写入锁。读取锁不支持 Condition，readLock().newCondition() 会抛出 UnsupportedOperationException。 监测 此类支持一些确定是保持锁还是争用锁的方法。这些方法设计用于监视系统状态，而不是同步控制。 此类行为的序列化方式与内置锁的相同：反序列化的锁处于解除锁状态，无论序列化该锁时其状态如何。 下面的代码展示了如何利用重入来执行升级缓存后的锁降级（为简单起见，省略了异常处理）： 1234567891011121314151617181920212223242526class CachedData &#123; Object data; volatile boolean cacheValid; ReentrantReadWriteLock rwl = new ReentrantReadWriteLock(); void processCachedData() &#123; rwl.readLock().lock(); if (!cacheValid) &#123; // 在获得写锁之前必须释放读锁 rwl.readLock().unlock(); rwl.writeLock().lock(); // Recheck state because another thread might have acquired // write lock and changed state before we did. if (!cacheValid) &#123; data = ... cacheValid = true; &#125; //通过在释放写锁之前获得读锁来降级 rwl.readLock().lock(); rwl.writeLock().unlock(); // 解锁写锁，但是任然持有读锁 &#125; use(data); rwl.readLock().unlock(); &#125;&#125; 3 与互斥锁对比 互斥锁一次只允许一个线程访问共享数据，哪怕进行的是只读操作；读写锁允许对共享数据进行更高级别的并发访问：对于写操作，一次只有一个线程（write线程）可以修改共享数据，对于读操作，允许任意数量的线程同时进行读取。 与互斥锁相比，使用读写锁能否提升性能则取决于读写操作期间读取数据相对于修改数据的频率，以及数据的争用——即在同一时间试图对该数据执行读取或写入操作的线程数。]]></content>
      <categories>
        <category>多线程与并发编程</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java并发编程（七）----（JUC）ReadWriteLock]]></title>
    <url>%2Fposts%2Fde2aa32e.html</url>
    <content type="text"><![CDATA[CountDownLatch 是一个非常实用的多线程控制工具类。&quot; Count Down &quot; 在英文中意为倒计数， Latch 为门问的意思。如果翻译成为倒计数门阀， 我想大家都会觉得不知所云吧! 因此，这里简单地称之为倒计数器。在这里， 门问的含义是:把门锁起来，不让里面的线程跑出来。因此，这个工具通常用来控制线程等待，它可以让某一个线程等待直到倒计时结束， 再开始执行。 CountDown Latch 的构造函数接收一个整数作为参数，即当前这个计数器的计数个数。 1public CountDownLatch(int count) CountDownLatch是一个同步辅助类，在完成一组正在其他线程中执行的操作之前，它允许一个或多个线程一直等待。一个CountDownLatch初始化为给定的计数 。 调用await方法阻塞，直到当前计数为零，在调用countDown()方法之后，所有等待的线程被释放，任何后续调用await立即返回。 这是一次性的现象 - 计数不能重置。 如果需要重置计数，考虑使用CyclicBarrier ，CyclicBarrier的计数器可以被重置后使用，因此它被称为是循环的barrier。 主要方法： // 使当前线程在锁存器倒计数至零之前一直等待，除非线程被中断。 void await() // 使当前线程在锁存器倒计数至零之前一直等待，除非线程被中断或超出了指定的等待时间。 boolean await(long timeout, TimeUnit unit) // 递减锁存器的计数，如果计数到达零，则释放所有等待的线程。 void countDown() // 返回当前计数。 long getCount() 我们来看一个例子： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public class TestCountDownLatch &#123; private static final int RUNNER_NUMBER = 5; // 运动员个数 private static final Random RANDOM = new Random(); public static void main(String[] args) &#123; // 用于判断发令之前运动员是否已经完全进入准备状态，需要等待5个运动员，所以参数为5 CountDownLatch readyLatch = new CountDownLatch(RUNNER_NUMBER); // 用于判断裁判是否已经发令，只需要等待一个裁判，所以参数为1 CountDownLatch startLatch = new CountDownLatch(1); for (int i = 0; i &lt; RUNNER_NUMBER; i++) &#123; Thread t = new Thread(new Runner((i + 1) + "号运动员", readyLatch, startLatch)); t.start(); &#125; try &#123; readyLatch.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; startLatch.countDown(); System.out.println("裁判：所有运动员准备完毕，开始..."); &#125; static class Runner implements Runnable &#123; private CountDownLatch readyLatch; private CountDownLatch startLatch; private String name; public Runner(String name, CountDownLatch readyLatch, CountDownLatch startLatch) &#123; this.name = name; this.readyLatch = readyLatch; this.startLatch = startLatch; &#125; public void run() &#123; int readyTime = RANDOM.nextInt(1000); System.out.println(name + "：我需要" + readyTime + "秒时间准备."); try &#123; Thread.sleep(readyTime); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(name + "：我已经准备完毕."); readyLatch.countDown(); try &#123; startLatch.await(); // 等待裁判发开始命令 &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(name + "：开跑..."); &#125; &#125;&#125; 打印结果： 1号运动员：我需要547秒时间准备. 2号运动员：我需要281秒时间准备. 4号运动员：我需要563秒时间准备. 5号运动员：我需要916秒时间准备. 3号运动员：我需要461秒时间准备. 2号运动员：我已经准备完毕. 3号运动员：我已经准备完毕. 1号运动员：我已经准备完毕. 4号运动员：我已经准备完毕. 5号运动员：我已经准备完毕. 裁判：所有运动员准备完毕，开始... 3号运动员：开跑... 2号运动员：开跑... 1号运动员：开跑... 4号运动员：开跑... 5号运动员：开跑... Process finished with exit code 0 注意：计数器必须大于等于0，只是等于0时候，计数器就是零，调用await方法时不会阻塞当前线程。]]></content>
      <categories>
        <category>多线程与并发编程</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java并发编程（六）----（JUC）Semaphore]]></title>
    <url>%2Fposts%2F7afb7337.html</url>
    <content type="text"><![CDATA[Semaphore,从字面意义上我们知道他是信号量的意思。在java中，一个计数信号量维护了一个许可集。Semaphore 只对可用许可的号码进行计数，并采取相应的行动。拿到信号量的线程可以进入代码，否则就等待。通过acquire()和release()获取和释放访问许可。 信号量Semaphore是一个控制访问多个共享资源的计数器，它本质上是一个“共享锁”。 Java并发提供了两种加锁模式：共享锁和独占锁。前面介绍的ReentrantLock就是独占锁。对于独占锁而言，它每次只能有一个线程持有，而共享锁则不同，它允许多个线程并行持有锁，并发访问共享资源。 独占锁它所采用的是一种悲观的加锁策略， 对于写而言为了避免冲突独占是必须的，但是对于读就没有必要了，因为它不会影响数据的一致性。如果某个只读线程获取独占锁，则其他读线程都只能等待了，这种情况下就限制了不必要的并发性，降低了吞吐量。而共享锁则不同，它放宽了加锁的条件，采用了乐观锁机制，它是允许多个读线程同时访问同一个共享资源的。 举一个生活中的例子，有一条单行道路口有一红绿灯在正常的绿灯时间内如果骑车速度都很平均只能过去20辆车，这就意味着排在前面的20辆肯定能过去红绿灯，后面的就只能等下一个绿灯了。但是如果这个时候有车不想过去这个路口它驶向了边上别的路，那么后面的车就有机会。下面我们来看一个简单的例子： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public class TestSemaphore &#123; public static void main(String[] args) &#123; final Semaphore semaphore = new Semaphore(5); ExecutorService executorService = Executors.newCachedThreadPool(); for(int i = 0;i&lt;10;i++)&#123; int j = 0; executorService.submit(new A("car"+(j++),semaphore),"Thread"+(j++)); //new Thread(new A("car"+(j++),semaphore),"Thread"+(j++)).start(); if(i == 5)&#123; try &#123; Thread.sleep(1000); System.out.println("最后还有"+semaphore.availablePermits()+"个许可可用"); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; System.out.println("最后还有"+semaphore.availablePermits()+"个许可可用"); &#125; &#125;class A implements Runnable&#123; String carName; private Semaphore semaphore; public A(String carName, Semaphore semaphore)&#123; this.carName = carName; this.semaphore = semaphore; &#125; public void getWay()&#123; System.out.println("this car is get the way" + Thread.currentThread().getName()); &#125; public void run() &#123; try &#123; if(semaphore.availablePermits() &gt; 0)&#123; semaphore.acquire(); getWay(); semaphore.release(); &#125;else&#123; System.out.println("请等待========"); &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;&#125;]]></content>
      <categories>
        <category>多线程与并发编程</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java并发编程（五）----（JUC）ReentrantLock]]></title>
    <url>%2Fposts%2Fd68e6b33.html</url>
    <content type="text"><![CDATA[上一节我们了解了Lock接口的一些简单的说明，知道Lock锁的常用形式，那么这节我们正式开始进入JUC锁（java.util.concurrent包下的锁，简称JUC锁）。下面我们来看一下Lock最常用的实现类ReentrantLock。 1 ReentrantLock简介 由单词意思我们可以知道这是可重入的意思。那么可重入对于锁而言到底意味着什么呢？简单来说，它有一个与锁相关的获取计数器，如果拥有锁的某个线程再次得到锁，那么获取计数器就加1，然后锁需要被释放两次才能获得真正释放。这模仿了 synchronized 的语义；如果线程进入由线程已经拥有的监控器保护的 synchronized 块，就允许线程继续进行，当线程退出第二个（或者后续） synchronized 块的时候，不释放锁，只有线程退出它进入的监控器保护的第一个 synchronized 块时，才释放锁。 1.1 公平锁与非公平锁 我们查看ReentrantLock的源码可以看到无参构造函数是这样的： 123public ReentrantLock() &#123; sync = new NonfairSync();&#125; NonfairSync()方法为一个非公平锁的实现方法，另外Reentrantlock还有一个有参的构造方法： 123public ReentrantLock(boolean fair) &#123; sync = fair ? new FairSync() : new NonfairSync();&#125; 它允许您选择想要一个 公平（fair）锁，还是一个 不公平（unfair）锁。公平锁使线程按照请求锁的顺序依次获得锁；而不公平锁则允许直接获取锁，在这种情况下，线程有时可以比先请求锁的其他线程先得到锁。 为什么我们不让所有的锁都公平呢？毕竟，公平是好事，不公平是不好的，不是吗？（当孩子们想要一个决定时，总会叫嚷“这不公平”。我们认为公平非常重要，孩子们也知道。）在现实中，公平保证了锁是非常健壮的锁，有很大的性能成本。要确保公平所需要的记帐（bookkeeping）和同步，就意味着被争夺的公平锁要比不公平锁的吞吐率更低。作为默认设置，应当把公平设置为 false ，除非公平对您的算法至关重要，需要严格按照线程排队的顺序对其进行服务。 下面我们先来看一个例子： 12345678910111213141516171819202122232425262728public class TestReentrantLock implements Runnable&#123; ReentrantLock lock = new ReentrantLock(); public void get() &#123; lock.lock(); System.out.println(Thread.currentThread().getId()); set(); lock.unlock(); &#125; public void set() &#123; lock.lock(); System.out.println(Thread.currentThread().getId()); lock.unlock(); &#125; @Override public void run() &#123; get(); &#125; public static void main(String[] args) &#123; TestReentrantLock ss = new TestReentrantLock(); new Thread(ss).start(); new Thread(ss).start(); new Thread(ss).start(); &#125;&#125; 运行结果： 10 10 12 12 11 11 Process finished with exit code 0 由结果我们可以看出同一个线程进入了同一个ReentrantLock锁两次。 2 condition条件变量 我们知道根类 Object 包含某些特殊的方法，用来在线程的 wait() 、 notify() 和 notifyAll() 之间进行通信。那么为了在对象上 wait 或 notify ，您必须持有该对象的锁。就像 Lock 是同步的概括一样， Lock 框架包含了对 wait 和 notify 的概括，这个概括叫作 条件（Condition）。 Condition 的方法与 wait 、 notify 和 notifyAll 方法类似，分别命名为 await 、 signal 和signalAll ，因为它们不能覆盖 Object 上的对应方法。 首先我们来计算一道题： 我们要打印1到9这9个数字，由A线程先打印1，2，3，然后由B线程打印4,5,6，然后再由A线程打印7，8，9. 这道题有很多种解法，我们先用Object的wait，notify方法来实现： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586public class WaitNotifyDemo &#123; private volatile int val = 1; private synchronized void printAndIncrease() &#123; System.out.println(Thread.currentThread().getName() +"prints " + val); val++; &#125; // print 1,2,3 7,8,9 public class PrinterA implements Runnable &#123; @Override public void run() &#123; while (val &lt;= 3) &#123; printAndIncrease(); &#125; // print 1,2,3 then notify printerB synchronized (WaitNotifyDemo.this) &#123; System.out.println("PrinterA printed 1,2,3; notify PrinterB"); WaitNotifyDemo.this.notify(); &#125; try &#123; while (val &lt;= 6) &#123; synchronized (WaitNotifyDemo.this) &#123; System.out.println("wait in printerA"); WaitNotifyDemo.this.wait(); &#125; &#125; System.out.println("wait end printerA"); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; while (val &lt;= 9) &#123; printAndIncrease(); &#125; System.out.println("PrinterA exits"); &#125; &#125; // print 4,5,6 after printA print 1,2,3 public class PrinterB implements Runnable &#123; @Override public void run() &#123; while (val &lt; 3) &#123; synchronized (WaitNotifyDemo.this) &#123; try &#123; System.out .println("printerB wait for printerA printed 1,2,3"); WaitNotifyDemo.this.wait(); System.out .println("printerB waited for printerA printed 1,2,3"); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; while (val &lt;= 6) &#123; printAndIncrease(); &#125; System.out.println("notify in printerB"); synchronized (WaitNotifyDemo.this) &#123; WaitNotifyDemo.this.notify(); &#125; System.out.println("notify end printerB"); System.out.println("PrinterB exits."); &#125; &#125; public static void main(String[] args) &#123; WaitNotifyDemo demo = new WaitNotifyDemo(); demo.doPrint(); &#125; private void doPrint() &#123; PrinterA pa = new PrinterA(); PrinterB pb = new PrinterB(); Thread a = new Thread(pa); a.setName("printerA"); Thread b = new Thread(pb); b.setName("printerB"); // 必须让b线程先执行，否则b线程有可能得不到锁，执行不了wait，而a线程一直持有锁，会先notify了 b.start(); a.start(); &#125;&#125; 运行结果为： printerB wait for printerA printed 1,2,3 printerA prints 1 printerA prints 2 printerA prints 3 PrinterA printed 1,2,3; notify PrinterB wait in printerA printerB waited for printerA printed 1,2,3 printerB prints 4 printerB prints 5 printerB prints 6 notify in printerB notify end printerB wait end printerA printerA prints 7 printerA prints 8 printerA prints 9 PrinterA exits PrinterB exits. Process finished with exit code 0 我们来分析一下上面的程序： 首先在main方法中我们看到是先启动了B线程，因为B线程持有wait()对象，而A线程则持有notify(),如果先启动A有可能会造成死锁的状态。 B线程启动以后进入run()方法： while (val &lt; 3) { synchronized (WaitNotifyDemo.this) { try { System.out.println(“printerB wait for printerA printed 1,2,3”); WaitNotifyDemo.this.wait(); System.out.println(“printerB waited for printerA printed 1,2,3”); } catch (InterruptedException e) { e.printStackTrace(); } } } while (val &lt;= 6) { printAndIncrease(); } 这里有一个while循环，如果val的值小于3，那么在WaitNotifyDemo的实例的同步块中调用WaitNotifyDemo.this.wait()方法，这里要注意无论是wait，还是notify，notifyAll方法都需要在其实例对象的同步块中执行，这样当前线程才能获得同步实例的同步控制权，如果不在同步块中执行wait或者notify方法会出java.lang.IllegalMonitorStateException异常。另外还要注意在wait方法两边的同步块会在wait执行完毕之后释放对象锁。 这样PrinterB就进入了等待状态，我们再看下PrinterA的run方法： 123456789101112131415161718192021while (val &lt;= 3) &#123; printAndIncrease(); &#125;// print 1,2,3 then notify printerBsynchronized (WaitNotifyDemo.this) &#123; System.out.println("PrinterA printed 1,2,3; notify PrinterB"); WaitNotifyDemo.this.notify();&#125;try &#123; while (val &lt;= 6) &#123; synchronized (WaitNotifyDemo.this) &#123; System.out.println("wait in printerA"); WaitNotifyDemo.this.wait(); &#125; &#125; System.out.println("wait end printerA");&#125; catch (InterruptedException e) &#123; e.printStackTrace();&#125; 这里首先打印了1、2、3，然后在同步块中调用了WaitNotifyDemo实例的notify方法，这样PrinterB就得到了继续执行的通知，然后PrinterA进入等待状态，等待PrinterB通知。 我们再看下PrinterB run方法剩下的代码： 12345678910while (val &lt;= 6) &#123; printAndIncrease();&#125;System.out.println("notify in printerB");synchronized (WaitNotifyDemo.this) &#123; WaitNotifyDemo.this.notify();&#125;System.out.println("notify end printerB");System.out.println("PrinterB exits."); PrinterB首先打印了4、5、6，然后在同步块中调用了notify方法，通知PrinterA开始执行。 PrinterA得到通知后，停止等待，打印剩下的7、8、9三个数字，如下是PrinterA run方法中剩下的代码： 123while (val &lt;= 9) &#123; printAndIncrease();&#125; 整个程序就分析完了，下面我们再来使用Condition来做这道题： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394public class TestCondition &#123; static class NumberWrapper &#123; public int value = 1; &#125; public static void main(String[] args) &#123; //初始化可重入锁 final Lock lock = new ReentrantLock(); //第一个条件当屏幕上输出到3 final Condition reachThreeCondition = lock.newCondition(); //第二个条件当屏幕上输出到6 final Condition reachSixCondition = lock.newCondition(); //NumberWrapper只是为了封装一个数字，一边可以将数字对象共享，并可以设置为final //注意这里不要用Integer, Integer 是不可变对象 final NumberWrapper num = new NumberWrapper(); //初始化A线程 Thread threadA = new Thread(new Runnable() &#123; @Override public void run() &#123; //需要先获得锁 lock.lock(); try &#123; System.out.println("threadA start write"); //A线程先输出前3个数 while (num.value &lt;= 3) &#123; System.out.println(num.value); num.value++; &#125; //输出到3时要signal，告诉B线程可以开始了 reachThreeCondition.signal(); &#125; finally &#123; lock.unlock(); &#125; lock.lock(); try &#123; //等待输出6的条件 reachSixCondition.await(); System.out.println("threadA start write"); //输出剩余数字 while (num.value &lt;= 9) &#123; System.out.println(num.value); num.value++; &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125; &#125;); Thread threadB = new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; lock.lock(); while (num.value &lt;= 3) &#123; //等待3输出完毕的信号 reachThreeCondition.await(); &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; try &#123; lock.lock(); //已经收到信号，开始输出4，5，6 System.out.println("threadB start write"); while (num.value &lt;= 6) &#123; System.out.println(num.value); num.value++; &#125; //4，5，6输出完毕，告诉A线程6输出完了 reachSixCondition.signal(); &#125; finally &#123; lock.unlock(); &#125; &#125; &#125;); //启动两个线程 threadB.start(); threadA.start(); &#125;&#125; 基本思路就是首先要A线程先写1，2，3，这时候B线程应该等待reachThredCondition信号，而当A线程写完3之后就通过signal告诉B线程“我写到3了，该你了”，这时候A线程要等嗲reachSixCondition信号，同时B线程得到通知，开始写4，5，6，写完4，5，6之后B线程通知A线程reachSixCondition条件成立了，这时候A线程就开始写剩下的7，8，9了。 我们可以看到上例中我们创建了两个Condition,在不同的情况下可以使用不同的Condition，与wait和notify相比提供了更细致的控制。 3 线程阻塞原语–LockSupport 我们一再提线程、锁等概念，但锁是如果实现的呢？又是如何知道当前阻塞线程的又是哪个对象呢？LockSupport是JDK中比较底层的类，用来创建锁和其他同步工具类的基本线程阻塞原语。 java锁和同步器框架的核心 AQS: AbstractQueuedSynchronizer，就是通过调用 LockSupport .park()和 LockSupport .unpark()实现线程的阻塞和唤醒 的。 LockSupport 很类似于二元信号量(只有1个许可证可供使用)，如果这个许可还没有被占用，当前线程获取许可并继 续 执行；如果许可已经被占用，当前线 程阻塞，等待获取许可。 LockSupport是针对特定线程来进行阻塞和解除阻塞操作的；而Object的wait()/notify()/notifyAll()是用来操作特定对象的等待集合的。 LockSupport的两个主要方法是park()和Unpark()，我们来看一下他们的实现： 123456789101112131415public static void park(Object blocker) &#123; Thread t = Thread.currentThread(); setBlocker(t, blocker); unsafe.park(false, 0L); setBlocker(t, null);&#125;public static void park() &#123; unsafe.park(false, 0L);&#125;public static void unpark(Thread thread) &#123; if (thread != null) unsafe.unpark(thread);&#125; 由源码我们可见在park方法内部首先获得当前线程然后阻塞当前线程，unpark方法传入一个可配置的线程来为该线程解锁。以“线程”作为方法的参数， 语义更清晰，使用起来也更方便。而wait/notify的实现使得“线程”的阻塞/唤醒对线程本身来说是被动的，要准确的控制哪个线程、什么时候阻塞/唤醒很困难， 要不随机唤醒一个线程（notify）要不唤醒所有的（notifyAll）。 下面我们来看一个例子： 1234567891011121314151617181920212223242526272829public class TestLockSupport &#123; public static Object u = new Object(); static ChangeObjectThread t1 = new ChangeObjectThread("t1"); static ChangeObjectThread t2 = new ChangeObjectThread("t2"); public static class ChangeObjectThread extends Thread &#123; public ChangeObjectThread(String name) &#123; super.setName(name); &#125; public void run() &#123; synchronized (u) &#123; System.out.println("in" + getName()); LockSupport.park(); &#125; &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; t1.start(); Thread.sleep(2000); t2.start(); LockSupport.unpark(t1); LockSupport.unpark(t2); t1.join(); t2.join(); &#125;&#125; 当我们把&quot;LockSupport.unpark(t1);&quot;这一句注掉的话我们会发现程序陷入死锁。而且我们看到再main方法中unpark是在t1和t2启动之后才执行，但是为什么t1启动之后，t2也启动了呢？注意，**unpark函数可以先于park调用。比如线程B调用unpark函数，给线程A发了一个“许可”，那么当线程A调用park时，它发现已经有“许可”了，那么它会马上再继续运行。**unpark函数为线程提供“许可(permit)”，线程调用park函数则等待“许可”。这个有点像信号量，但是这个“许可”是不能叠加的，“许可”是一次性的。比如线程B连续调用了三次unpark函数，当线程A调用park函数就使用掉这个“许可”，如果线程A再次调用park，则进入等待状态。 除了有定时阻塞的功能外,还支持中断影响,但是和其他接收中断函数不一样,他不会抛出 InterruptedException异常,他只会默默的返回,但是我们可以从Thread.Interrupted()等方法获得中断标记. 我们来看一个例子： 123456789101112131415161718192021222324252627282930public class TestLockSupport &#123; public static Object u = new Object(); static ChangeObjectThread t1 = new ChangeObjectThread("t1"); static ChangeObjectThread t2 = new ChangeObjectThread("t2"); public static class ChangeObjectThread extends Thread &#123; public ChangeObjectThread(String name) &#123; super.setName(name); &#125; public void run() &#123; synchronized (u) &#123; System.out.println("in " + getName()); LockSupport.park(); if (Thread.interrupted()) &#123; System.out.println(getName() + " 被中断了!"); &#125; &#125; System.out.println(getName() + " 执行结束"); &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; t1.start(); Thread.sleep(100); t2.start(); t1.interrupt(); LockSupport.unpark(t2); &#125;&#125; 输出： in t1 t1 被中断了! t1 执行结束 in t2 t2 执行结束 Process finished with exit code 0 由run方法中的终端异常捕获我们可以看到线程在中断时并没有抛出异常而是正常执行下去了。 关于LockSupport其实要介绍的东西还是很多，因为这个类实现了底层的一些方法，各种的锁实现都是这个基础上发展而来的。以后会专门用一个篇章来学习jdk内部的阻塞机制。说前面我们讲到Object的wait和notify，讲到Condition条件，讲到jdk中不对外部暴露的LockSupport阻塞原语，那么在JUC包中还有另外一个阻塞机制—信号量机制（Semaphore），下一节我们一起探讨一下。]]></content>
      <categories>
        <category>多线程与并发编程</category>
      </categories>
      <tags>
        <tag>学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java并发编程（四）----（JUC）Lock锁初探]]></title>
    <url>%2Fposts%2Ff4331185.html</url>
    <content type="text"><![CDATA[首先我们来回忆一下上一节讲过的synchronized关键字，该关键字用于给代码段或方法加锁，使得某一时刻它修饰的方法或代码段只能被一个线程访问。那么试想，当我们遇到这样的情况：当synchronized修饰的方法或代码段因为某种原因（IO异常或是sleep方法）被阻塞了，但是锁有没有被释放，那么其他线程除了等待以外什么事都做不了。当我们遇到这种情况该怎么办呢？我们今天讲到的Lock锁将有机会为此行使他的职责。 1 为什么需要Lock synchronized 是Java 语言层面的，是内置的关键字；Lock 则是JDK 5 的J.U.C(java/util/currrent)包中出现的一个类，在使用时，synchronized 同步的代码块可以由JVM自动释放；Lock 需要程序员在finally块中手工释放；synchronized是比较古老的实现机制，设计较早，有一些功能上的限制： ——它无法中断一个正在等候获得锁的线程 ——也无法通过投票得到锁，如果不想等下去，也就没法得到锁。 ——同步还要求锁的释放只能在与获得锁所在的堆栈帧相同的堆栈帧中进行 而且对多线程环境中，使用synchronized后，线程要么获得锁，执行相应的代码，要么无法获得锁处于等待状态，对于锁的处理不灵活。而Lock提供了多种基于锁的处理机制，比如： void lock()，获取一个锁，如果锁当前被其他线程获得，当前的线程将被休眠。 boolean tryLock()，尝试获取一个锁，如果当前锁被其他线程持有，则返回false，不会使当前线程休眠。 boolean tryLock(long timeout,TimeUnit unit)，如果获取了锁定立即返回true，如果别的线程正持有锁，会等待参数给定的时间，在等待的过程中，如果获取了锁定，就返回true，如果等待超时，返回false。 void lockInterruptibly()，如果获取了锁定立即返回，如果没有获取锁定，当前线程处于休眠状态，直到或者锁定，或者当前线程被别的线程中断。 可见lock比synchronized提供了更细的粒度、更灵活的控制。 2 初探Lock 在jdk1.5之后，并发包中新增了Lock接口(以及相关实现类)用来实现锁功能，其实真正的实现Lock接口的类就三个，ReentrantLock和ReentrantReadWriteLock的两个内部类（ReadLock和WriteLock实现了Lock的接口），下面我们来看一下Lock的类图： ReentrantLock：一个可重入的互斥锁，为lock接口的主要实现。 ReentrantReadWriteLock： ReadWriteLock、ReadWriteLock 维护了一对相关的锁，一个用于只读操作，另一个用于写入操作。 Semaphore：一个计数信号量。 Condition:锁的关联条件，目的是允许线程获取锁并且查看等待的某一个条件是否满足。 CyclicBarrier：一个同步辅助类，它允许一组线程互相等待，直到到达某个公共屏障点。 ①首先我们来看一下Lock的用法： 123456789Lock lock = new ReentrantLock();lock.lock();try&#123;//处理任务&#125;catch(Exception ex)&#123; &#125;finally&#123;lock.unlock(); //释放锁&#125; 正常使用Lock的用法最多就是这样，ReentrantLock是Lock的实现类们也是最常使用的。如果采用Lock，必须主动去释放锁，并且在发生异常时，不会自动释放锁。因此一般来说，使用Lock必须在try{}catch{}块中进行，并在finally块释放锁，以保证锁一定被被释放，防止死锁的发生。 ②我们也可以这样使用Lock： 123456789101112Lock lock = new ReentrantLock();if(lock.tryLock()) &#123; try&#123; //处理任务 &#125;catch(Exception ex)&#123; &#125;finally&#123; lock.unlock(); //释放锁 &#125; &#125;else &#123; //如果不能获取锁，则直接做其他事情 &#125; tryLock()方法是有返回值的，它表示用来尝试获取锁，如果获取成功，则返回true，如果获取失败（即锁已被其他线程获取）则返回false，也就说这个方法无论如何都会立即返回。在拿不到锁时不会一直在那等待。 这一节我们简单了解一下Lock接口，由于Lock锁的内容实在是太多，包括互斥锁，公平锁，非公平锁，共享锁以及相关的条件机制，信号量机制等等，我会一点点的把他们都啃下来，下面才是我们的重头戏。]]></content>
      <categories>
        <category>多线程与并发编程</category>
      </categories>
      <tags>
        <tag>学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netty学习(二）-Helloworld Netty]]></title>
    <url>%2Fposts%2F7ab746e7.html</url>
    <content type="text"><![CDATA[这一节我们来讲解Netty，使用Netty之前我们先了解一下Netty能做什么，无为而学，岂不是白费力气！ 1.使用Netty能够做什么 开发异步、非阻塞的TCP网络应用程序； 开发异步、非阻塞的UDP网络应用程序； 开发异步文件传输应用程序； 开发异步HTTP服务端和客户端应用程序； 提供对多种编解码框架的集成，包括谷歌的Protobuf、Jboss marshalling、Java序列化、压缩编解码、XML解码、字符串编解码等，这些编解码框架可以被用户直接使用； 提供形式多样的编解码基础类库，可以非常方便的实现私有协议栈编解码框架的二次定制和开发； 基于职责链模式的Pipeline-Handler机制，用户可以非常方便的对网络事件进行拦截和定制； 所有的IO操作都是异步的，用户可以通过Future-Listener机制主动Get结果或者由IO线程操作完成之后主动Notify结果，用户的业务线程不需要同步等待； IP黑白名单控制； 打印消息码流； 流量控制和整形； 性能统计； 基于链路空闲事件检测的心跳检测 2. Netty常用类讲解 在这里我们就一些我们常用到的类做大致的讲解，然后再写入门程序的时候大致知道每一行都讲了什么。 EventLoop,EventLoopGroup EventLoop目的是为Channel处理IO操作，一个EventLoop可以为多个Channel服务,EventLoopGroup会包含多个EventLoop。 BootStrap,ServerBootstrap 一个Netty应用通常由一个Bootstrap开始，它主要作用是配置整个Netty程序，串联起各个组件。 ChannelInitializer 当一个链接建立时，我们需要知道怎么来接收或者发送数据，当然，我们有各种各样的Handler实现来处理它，那么ChannelInitializer便是用来配置这些Handler，它会提供一个ChannelPipeline，并把Handler加入到ChannelPipeline。 Handler 为了支持各种协议和处理数据的方式，便诞生了Handler组件。Handler主要用来处理各种事件，这里的事件很广泛，比如可以是连接、数据接收、异常、数据转换等。 ChannelInboundHandler 一个最常用的Handler。这个Handler的作用就是处理接收到数据时的事件，也就是说，我们的业务逻辑一般就是写在这个Handler里面的，ChannelInboundHandler就是用来处理我们的核心业务逻辑。 Future 在Netty中所有的IO操作都是异步的，因此，你不能立刻得知消息是否被正确处理，但是我们可以过一会等它执行完成或者直接注册一个监听，具体的实现就是通过Future和ChannelFutures,他们可以注册一个监听，当操作执行成功或失败时监听会自动触发。总之，所有的操作都会返回一个ChannelFuture。 3. 第一个Helloworld 上面我们已经对常用类进行说明，下面我们就使用这些类来构建我们的第一个入门程序，本示例我使用的是maven来构建工程，如果你使用的是普通的项目则跳过第一步。 首先引入maven jar包： 12345&lt;dependency&gt; &lt;groupId&gt;io.netty&lt;/groupId&gt; &lt;artifactId&gt;netty-all&lt;/artifactId&gt; &lt;version&gt;4.1.5.Final&lt;/version&gt;&lt;/dependency&gt; 下面我们来写客户端： 12345678910111213141516171819202122232425262728293031323334353637383940public class HelloWorldClient &#123; private int port; private String address; public HelloWorldClient(int port,String address) &#123; this.port = port; this.address = address; &#125; public void start()&#123; EventLoopGroup group = new NioEventLoopGroup(); Bootstrap bootstrap = new Bootstrap(); bootstrap.group(group) .channel(NioSocketChannel.class) .handler(new ClientChannelInitializer()); try &#123; Channel channel = bootstrap.connect(address,port).sync().channel(); BufferedReader reader = new BufferedReader(new InputStreamReader(System.in)); for(;;)&#123; String msg = reader.readLine(); if(msg == null)&#123; continue; &#125; channel.writeAndFlush(msg + "\r\n"); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;finally &#123; group.shutdownGracefully(); &#125; &#125; public static void main(String[] args) &#123; HelloWorldClient client = new HelloWorldClient(7788,"127.0.0.1"); client.start(); &#125;&#125; ChannelInitializer用来配置处理数据的handler： 123456789101112131415161718public class ClientChannelInitializer extends ChannelInitializer&lt;SocketChannel&gt; &#123; protected void initChannel(SocketChannel socketChannel) throws Exception &#123; ChannelPipeline pipeline = socketChannel.pipeline(); /* * 这个地方的 必须和服务端对应上。否则无法正常解码和编码 * * 解码和编码 我将会在下一节为大家详细的讲解。暂时不做详细的描述 * * / pipeline.addLast("decoder", new StringDecoder()); pipeline.addLast("encoder", new StringEncoder()); // 我们自己的handler pipeline.addLast("handler", new HelloWorldClientHandler()); &#125;&#125; 写一个我们自己的handler，用自己的方式来处理数据： 12345678910111213141516public class HelloWorldClientHandler extends ChannelInboundHandlerAdapter &#123; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; System.out.println("server say : "+msg.toString()); &#125; @Override public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; System.out.println("Client is active"); &#125; @Override public void channelInactive(ChannelHandlerContext ctx) throws Exception &#123; System.out.println("Client is close"); &#125;&#125; 客户端我们写完了，下面开始写服务器端： 12345678910111213141516171819202122232425262728293031public class HelloWordServer &#123; private int port; public HelloWordServer(int port) &#123; this.port = port; &#125; public void start()&#123; EventLoopGroup bossGroup = new NioEventLoopGroup(); EventLoopGroup workGroup = new NioEventLoopGroup(); ServerBootstrap server = new ServerBootstrap().group(bossGroup,workGroup) .channel(NioServerSocketChannel.class) .childHandler(new ServerChannelInitializer()); try &#123; ChannelFuture future = server.bind(port).sync(); future.channel().closeFuture().sync(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;finally &#123; bossGroup.shutdownGracefully(); workGroup.shutdownGracefully(); &#125; &#125; public static void main(String[] args) &#123; HelloWordServer server = new HelloWordServer(7788); server.start(); &#125;&#125; 服务端的ChannelInitializer： 12345678910111213public class ServerChannelInitializer extends ChannelInitializer&lt;SocketChannel&gt; &#123; @Override protected void initChannel(SocketChannel socketChannel) throws Exception &#123; ChannelPipeline pipeline = socketChannel.pipeline(); // 字符串解码 和 编码 pipeline.addLast("decoder", new StringDecoder()); pipeline.addLast("encoder", new StringEncoder()); // 自己的逻辑Handler pipeline.addLast("handler", new HelloWordServerHandler()); &#125;&#125; 服务器端的handler: 123456789101112131415public class HelloWordServerHandler extends ChannelInboundHandlerAdapter &#123; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; System.out.println(ctx.channel().remoteAddress()+"===&gt;server: "+msg.toString()); ctx.write("received your msg"); ctx.flush(); &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; super.exceptionCaught(ctx, cause); ctx.close(); &#125;&#125; 上面服务器端和客户端的代码都已经写完，下面我们先启动服务端，然后启动客户端，程序中我是在客户端让手动输入，输入结束之后回车，服务器端即可接受数据。 客户端： 服务端：]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netty学习(五)-DelimiterBasedFrameDecoder]]></title>
    <url>%2Fposts%2F87e472dc.html</url>
    <content type="text"><![CDATA[上一节我们说了LineBasedframeDecoder来解决粘包拆包的问题，TCP以流的方式进行数据传输，上层应用协议为了对消息进行区分，一般采用如下4种方式： 消息长度固定，累计读取到消息长度总和为定长Len的报文之后即认为是读取到了一个完整的消息。计数器归位，重新读取。 将回车换行符作为消息结束符。 将特殊的分隔符作为消息分隔符，回车换行符是他的一种。 通过在消息头定义长度字段来标识消息总长度。 LineBasedframeDecoder属于第二种，今天我们要说的DelimiterBasedFrameDecoder和FixedLengthFrameDecoder属于第三种和第一种。DelimiterBasedFrameDecoder用来解决以特殊符号作为消息结束符的粘包问题，FixedLengthFrameDecoder用来解决定长消息的粘包问题。下面首先来用DelimiterBasedFrameDecoder来写一个例子，我们看一下效果然后接着分析用法。 1. DelimiterBasedFrameDecoder使用 服务端： 12345678910111213141516171819202122232425262728293031public class HelloWordServer &#123; private int port; public HelloWordServer(int port) &#123; this.port = port; &#125; public void start()&#123; EventLoopGroup bossGroup = new NioEventLoopGroup(); EventLoopGroup workGroup = new NioEventLoopGroup(); ServerBootstrap server = new ServerBootstrap().group(bossGroup,workGroup) .channel(NioServerSocketChannel.class) .childHandler(new ServerChannelInitializer()); try &#123; ChannelFuture future = server.bind(port).sync(); future.channel().closeFuture().sync(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;finally &#123; bossGroup.shutdownGracefully(); workGroup.shutdownGracefully(); &#125; &#125; public static void main(String[] args) &#123; HelloWordServer server = new HelloWordServer(7788); server.start(); &#125;&#125; 服务端ServerChannelInitializer： 123456789101112131415public class ServerChannelInitializer extends ChannelInitializer&lt;SocketChannel&gt; &#123; @Override protected void initChannel(SocketChannel socketChannel) throws Exception &#123; ChannelPipeline pipeline = socketChannel.pipeline(); ByteBuf delimiter = Unpooled.copiedBuffer("\t".getBytes()); pipeline.addLast("framer", new DelimiterBasedFrameDecoder(2048,delimiter)); // 字符串解码 和 编码 pipeline.addLast("decoder", new StringDecoder()); pipeline.addLast("encoder", new StringEncoder()); // 自己的逻辑Handler pipeline.addLast("handler", new ServerHandler()); &#125;&#125; 服务端handler： 1234567891011121314public class ServerHandler extends ChannelInboundHandlerAdapter &#123; private int counter; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; String body = (String)msg; System.out.println("server receive order : " + body + ";the counter is: " + ++counter); &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; super.exceptionCaught(ctx, cause); &#125;&#125; 客户端： 123456789101112131415161718192021222324252627282930313233public class HelloWorldClient &#123; private int port; private String address; public HelloWorldClient(int port,String address) &#123; this.port = port; this.address = address; &#125; public void start()&#123; EventLoopGroup group = new NioEventLoopGroup(); Bootstrap bootstrap = new Bootstrap(); bootstrap.group(group) .channel(NioSocketChannel.class) .handler(new ClientChannelInitializer()); try &#123; ChannelFuture future = bootstrap.connect(address,port).sync(); future.channel().closeFuture().sync(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;finally &#123; group.shutdownGracefully(); &#125; &#125; public static void main(String[] args) &#123; HelloWorldClient client = new HelloWorldClient(7788,"127.0.0.1"); client.start(); &#125;&#125; 客户端ClientChannelInitializer： 12345678910111213141516171819public class ClientChannelInitializer extends ChannelInitializer&lt;SocketChannel&gt; &#123; protected void initChannel(SocketChannel socketChannel) throws Exception &#123; ChannelPipeline pipeline = socketChannel.pipeline(); /* * 这个地方的 必须和服务端对应上。否则无法正常解码和编码 * * */ ByteBuf delimiter = Unpooled.copiedBuffer("\t".getBytes()); pipeline.addLast("framer", new DelimiterBasedFrameDecoder(2048,delimiter)); pipeline.addLast("decoder", new StringDecoder()); pipeline.addLast("encoder", new StringEncoder()); // 客户端的逻辑 pipeline.addLast("handler", new ClientHandler()); &#125;&#125; 客户端handler： 123456789101112131415161718192021222324252627282930313233343536373839404142434445public class ClientHandler extends ChannelInboundHandlerAdapter &#123; private byte[] req; private int counter; public ClientHandler() &#123; req = ("Unless required by applicable law or agreed to in writing, software\t" + " distributed under the License is distributed on an \"AS IS\" BASIS,\t" + " WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\t" + " See the License for the specific language governing permissions and\t" + " limitations under the License.This connector uses the BIO implementation that requires the JSSE\t" + " style configuration. When using the APR/native implementation, the\t" + " penSSL style configuration is required as described in the APR/native\t" + " documentation.An Engine represents the entry point (within Catalina) that processes\t" + " every request. The Engine implementation for Tomcat stand alone\t" + " analyzes the HTTP headers included with the request, and passes them\t" + " on to the appropriate Host (virtual host)# Unless required by applicable law or agreed to in writing, software\t" + "# distributed under the License is distributed on an \"AS IS\" BASIS,\t" + "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\t" + "# See the License for the specific language governing permissions and\t" + "# limitations under the License.# For example, set the org.apache.catalina.util.LifecycleBase logger to log\t" + "# each component that extends LifecycleBase changing state:\t" + "#org.apache.catalina.util.LifecycleBase.level = FINE\t" ).getBytes(); &#125; @Override public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; ByteBuf message; message = Unpooled.buffer(req.length); message.writeBytes(req); ctx.writeAndFlush(message); &#125; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; String buf = (String)msg; System.out.println("Now is : " + buf + " ; the counter is : "+ (++counter)); &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; ctx.close(); &#125;&#125; 输出如下： server receive order : Unless required by applicable law or agreed to in writing, software;the counter is: 1 server receive order : distributed under the License is distributed on an &quot;AS IS&quot; BASIS,;the counter is: 2 server receive order : WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.;the counter is: 3 server receive order : See the License for the specific language governing permissions and;the counter is: 4 server receive order : limitations under the License.This connector uses the BIO implementation that requires the JSSE;the counter is: 5 server receive order : style configuration. When using the APR/native implementation, the;the counter is: 6 server receive order : penSSL style configuration is required as described in the APR/native;the counter is: 7 server receive order : documentation.An Engine represents the entry point (within Catalina) that processes;the counter is: 8 server receive order : every request. The Engine implementation for Tomcat stand alone;the counter is: 9 server receive order : analyzes the HTTP headers included with the request, and passes them;the counter is: 10 server receive order : on to the appropriate Host (virtual host)# Unless required by applicable law or agreed to in writing, software;the counter is: 11 server receive order : # distributed under the License is distributed on an &quot;AS IS&quot; BASIS,;the counter is: 12 server receive order : # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.;the counter is: 13 server receive order : # See the License for the specific language governing permissions and;the counter is: 14 server receive order : # limitations under the License.# For example, set the org.apache.catalina.util.LifecycleBase logger to log;the counter is: 15 server receive order : # each component that extends LifecycleBase changing state:;the counter is: 16 server receive order : #org.apache.catalina.util.LifecycleBase.level = FINE;the counter is: 17 启动服务端和客户端，我们能看到服务端接收客户端发过来的消息一共分17次接收。那么为什么是17次呢？而且我们并没有使用在上一篇中解决拆包和粘包问题的LineBasedFrameDecoder，并且这次我们的消息每一行的末尾也换成了&quot;\t&quot;。下面就来讲解一下DelimiterBasedFrameDecoder的使用。 DelimiterBasedFrameDecoder是将特殊的字符作为消息的分隔符，本例中用到的是&quot;\t&quot;。而LineBasedFrameDecoder是默认将换行符&quot;\n&quot;作为消息分隔符。首先我们注意到在ServerChannelInitializer中我们在添加解码器时跟以前有点不一样： 12ByteBuf delimiter = Unpooled.copiedBuffer("\t".getBytes());pipeline.addLast("framer", new DelimiterBasedFrameDecoder(2048, delimiter)); 这里我们添加DelimiterBasedFrameDecoder解码器并且手动指定消息分隔符为：&quot;\t&quot;。我们可以看一下DelimiterBasedFrameDecoder的构造方法： 123public DelimiterBasedFrameDecoder(int maxFrameLength, boolean stripDelimiter, ByteBuf delimiter) &#123; this(maxFrameLength, stripDelimiter, true, delimiter);&#125; maxFrameLength：解码的帧的最大长度 stripDelimiter：解码时是否去掉分隔符 failFast：为true，当frame长度超过maxFrameLength时立即报TooLongFrameException异常，为false，读取完整个帧再报异常 delimiter：分隔符 这个时候大家应该明白了为什么服务端分17次收到消息。我们在消息的每一行都加了一个&quot;\t&quot;,自然解码器在度消息时遇到&quot;\t&quot;就会认为这是一条消息的结束。用这种方式我们可以把&quot;\t&quot;换成任何我们自定义的字符对象。换成&quot;\n&quot;也是可以的。 2. FixedLengthFrameDecoder使用 FixedLengthFrameDecoder是固定长度解码器，它能够按照指定的长度对消息进行自动解码。使用它也没有什么特别费力的事情，在ServerChannelInitializer类中添加： pipeline.addLast(new FixedLengthFrameDecoder(23));//参数为一次接受的数据长度 即可，同时也别忘了把刚才使用的DelimiterBasedFrameDecoder注释掉啊，不然达不到效果。]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netty学习(六)-LengthFieldBasedFrameDecoder解码器]]></title>
    <url>%2Fposts%2F2b5f20ba.html</url>
    <content type="text"><![CDATA[在TCP协议中我们知道当我们在接收消息时候，我们如何判断我们一次读取到的包就是整包消息呢，特别是对于使用了长连接和使用了非阻塞I/O的程序。上节我们也说了上层应用协议为了对消息进行区分一般采用4种方式。前面三种我们都说了，第四种是：通过在消息头定义长度字段来标识消息总长度。这个我们还没讲。当然Netty也提供了相应的解码器：LengthFieldBasedFrameDecoder。 大多数的协议（私有或者公有），协议头中会携带长度字段，用于标识消息体或者整包消息的长度，例如SMPP、HTTP协议等。由于基于长度解码需求 的通用性，Netty提供了LengthFieldBasedFrameDecoder，自动屏蔽TCP底层的拆包和粘 包问题，只需要传入正确的参数，即可轻松解决“读半包“问题。 我们先来看一下他的构造函数： 123456789public LengthFieldBasedFrameDecoder(ByteOrder byteOrder, int maxFrameLength, int lengthFieldOffset, int lengthFieldLength, int lengthAdjustment, int initialBytesToStrip, boolean failFast) &#123;&#125; byteOrder：表示字节流表示的数据是大端还是小端，用于长度域的读取； maxFrameLength：表示的是包的最大长度，超出包的最大长度netty将会做一些特殊处理； lengthFieldOffset：指的是长度域的偏移量，表示跳过指定长度个字节之后的才是长度域； lengthFieldLength：记录该帧数据长度的字段本身的长度； lengthAdjustment：该字段加长度字段等于数据帧的长度，包体长度调整的大小，长度域的数值表示的长度加上这个修正值表示的就是带header的包； initialBytesToStrip：从数据帧中跳过的字节数，表示获取完一个完整的数据包之后，忽略前面的指定的位数个字节，应用解码器拿到的就是不带长度域的数据包； failFast：如果为true，则表示读取到长度域，TA的值的超过maxFrameLength，就抛出一个 TooLongFrameException，而为false表示只有当真正读取完长度域的值表示的字节之后，才会抛出 TooLongFrameException，默认情况下设置为true，建议不要修改，否则可能会造成内存溢出。 LengthFieldBasedFrameDecoder定义了一个长度的字段来表示消息的长度，因此能够处理可变长度的消息。将消息分为消息头和消息体，消息头固定位置增加一个表示长度的字段，通过长度字段来获取整包的信息。LengthFieldBasedFrameDecoder继承了ByteToMessageDecoder，即转换字节这样的工作是由ByteToMessageDecoder来完成，而LengthFieldBasedFrameDecoder只用安心完成他的解码工作就好。Netty在解耦和方面确实做的不错。 既然我们知道了LengthFieldBasedFrameDecoder处理的是带有消息头和消息体的消息类型，那么我们完全可以来定义一个我们自己的消息，我们来写一个消息类： 1234567891011121314151617181920212223242526272829303132333435363738394041public class Message &#123; //消息类型 private byte type; //消息长度 private int length; //消息体 private String msgBody; public Message(byte type, int length, String msgBody) &#123; this.type = type; this.length = length; this.msgBody = msgBody; &#125; public byte getType() &#123; return type; &#125; public void setType(byte type) &#123; this.type = type; &#125; public int getLength() &#123; return length; &#125; public void setLength(int length) &#123; this.length = length; &#125; public String getMsgBody() &#123; return msgBody; &#125; public void setMsgBody(String msgBody) &#123; this.msgBody = msgBody; &#125;&#125; 我们先来写服务端： 1234567891011121314151617181920212223242526272829303132333435363738394041public class NewServer &#123; private static final int MAX_FRAME_LENGTH = 1024 * 1024; private static final int LENGTH_FIELD_LENGTH = 4; private static final int LENGTH_FIELD_OFFSET = 1; private static final int LENGTH_ADJUSTMENT = 0; private static final int INITIAL_BYTES_TO_STRIP = 0; private int port; public NewServer(int port) &#123; this.port = port; &#125; public void start()&#123; EventLoopGroup bossGroup = new NioEventLoopGroup(1); EventLoopGroup workerGroup = new NioEventLoopGroup(); try &#123; ServerBootstrap sbs = new ServerBootstrap() .group(bossGroup,workerGroup) .channel(NioServerSocketChannel.class) .localAddress(new InetSocketAddress(port)) .childHandler(new NewServerChannelInitializer(MAX_FRAME_LENGTH,LENGTH_FIELD_LENGTH,LENGTH_FIELD_OFFSET,LENGTH_ADJUSTMENT,INITIAL_BYTES_TO_STRIP)) .option(ChannelOption.SO_BACKLOG, 128) .childOption(ChannelOption.SO_KEEPALIVE, true); ChannelFuture future = sbs.bind(port).sync(); System.out.println("Server start listen at " + port ); future.channel().closeFuture().sync(); &#125; catch (Exception e) &#123; bossGroup.shutdownGracefully(); workerGroup.shutdownGracefully(); &#125; &#125; public static void main(String[] args) &#123; NewServer server = new NewServer(7788); server.start(); &#125;&#125; 注意到服务端我们在上面定义了5个参数，这5个参数是为了传入LengthFieldBasedFrameDecoder里面用的，因为我们的LengthFieldBasedFrameDecoder写在了NewServerChannelInitializer类里面，所以这几个参数采用可配置的方式也更符合可扩展性，我们分别说一下这几个参数定值的含义： MAX_FRAME_LENGTH = 1024 * 1024 ：这个没什么说的，消息体的最大长度； LENGTH_FIELD_LENGTH = 4 ：指的就是我们的Message类中的length的长度，int占4位 LENGTH_FIELD_OFFSET = 1 ：偏移多少位之后才是我们的消息体，因为我们消息头只有type一个参数，byte类型占1位，所以是1； LENGTH_ADJUSTMENT = 0 ：该字段加长度字段等于数据帧的长度，一般数据帧长度都是这样定义(即我们在设置Message中的length属性)，加入你的消息体是20位，再加上 LENGTH_FIELD_LENGTH就是24位，所以在此处为了正确的解析出消息体，需要偏移4位才能解析出消息体的正确位置，我们在发送的消息里面设置的就是消息体本身的长度，所以无需偏移。 INITIAL_BYTES_TO_STRIP = 0 ：这里我们也不需要跳过数据帧中的字节数，因为我们的消息体和长度是分别发送的，详情见下面EnCoder代码。 然后我们写ChannelInitializer： 12345678910111213141516171819202122232425public class NewServerChannelInitializer extends ChannelInitializer&lt;SocketChannel&gt; &#123; private final int MAX_FRAME_LENGTH; private final int LENGTH_FIELD_LENGTH; private final int LENGTH_FIELD_OFFSET; private final int LENGTH_ADJUSTMENT; private final int INITIAL_BYTES_TO_STRIP; public NewServerChannelInitializer(int MAX_FRAME_LENGTH, int LENGTH_FIELD_LENGTH, int LENGTH_FIELD_OFFSET, int LENGTH_ADJUSTMENT, int INITIAL_BYTES_TO_STRIP) &#123; this.MAX_FRAME_LENGTH = MAX_FRAME_LENGTH; this.LENGTH_FIELD_LENGTH = LENGTH_FIELD_LENGTH; this.LENGTH_FIELD_OFFSET = LENGTH_FIELD_OFFSET; this.LENGTH_ADJUSTMENT = LENGTH_ADJUSTMENT; this.INITIAL_BYTES_TO_STRIP = INITIAL_BYTES_TO_STRIP; &#125; @Override protected void initChannel(SocketChannel socketChannel) throws Exception &#123; ChannelPipeline pipeline = socketChannel.pipeline(); pipeline.addLast(new NewDecoder(MAX_FRAME_LENGTH,LENGTH_FIELD_LENGTH,LENGTH_FIELD_OFFSET,LENGTH_ADJUSTMENT,INITIAL_BYTES_TO_STRIP,false)); // 自己的逻辑Handler pipeline.addLast("handler", new NewServerHandler()); &#125;&#125; 上面用到了我们自己写的Decoder，接下来定义一个Decoder，继承LengthFieldBasedFrameDecoder，以方便我们做一些改写： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960public class NewDecoder extends LengthFieldBasedFrameDecoder &#123; /** * 我们在Message类中定义了type和length，这都放在消息头部 * type占1个字节，length占4个字节所以头部总长度是5个字节 */ private static final int HEADER_SIZE = 5; private byte type; private int length; private String msgBody; /** * * @param maxFrameLength 网络字节序，默认为大端字节序 * @param lengthFieldOffset 消息中长度字段偏移的字节数 * @param lengthFieldLength 数据帧的最大长度 * @param lengthAdjustment 该字段加长度字段等于数据帧的长度 * @param initialBytesToStrip 从数据帧中跳过的字节数 * @param failFast 如果为true，则表示读取到长度域，TA的值的超过maxFrameLength，就抛出一个 TooLongFrameException */ public NewDecoder(int maxFrameLength, int lengthFieldOffset, int lengthFieldLength, int lengthAdjustment, int initialBytesToStrip, boolean failFast) &#123; super(maxFrameLength, lengthFieldOffset, lengthFieldLength, lengthAdjustment, initialBytesToStrip, failFast); &#125; @Override protected Object decode(ChannelHandlerContext ctx, ByteBuf in) throws Exception &#123; if(in == null)&#123; return null; &#125; if(in.readableBytes() &lt; HEADER_SIZE)&#123; throw new Exception("错误的消息"); &#125; /** * 通过源码我们能看到在读的过程中 * 每读一次读过的字节即被抛弃 * 即指针会往前跳 */ type = in.readByte(); length = in.readByte(); if(in.readableBytes() &lt; length)&#123; throw new Exception("消息不正确"); &#125; ByteBuf buf = in.readBytes(length); byte[] b = new byte[buf.readableBytes()]; buf.readBytes(b); msgBody = new String(b,"UTF-8"); Message msg = new Message(type,length,msgBody); return msg; &#125;&#125; 在上面的NewDecoder中有一个HEADER_SIZE-消息头。上面也解释过了，我们在Message中定义的type和length分别占一个字节和4个字节（别问我为啥是4个哈）。所以我们的消息头就是5个字节啦。 接下来就是服务端的handler了： 12345678910public class NewServerHandler extends SimpleChannelInboundHandler&lt;Object&gt; &#123; @Override protected void channelRead0(ChannelHandlerContext channelHandlerContext, Object o) throws Exception &#123; if(o instanceof Message) &#123; Message msg = (Message)o; System.out.println("Client-&gt;Server:"+channelHandlerContext.channel().remoteAddress()+" send "+msg.getMsgBody()); &#125; &#125;&#125; 在handler中我们用来接收已经被NewDecoder解码过后的客户端发送过来的消息。 下面是客户端： 12345678910111213141516171819202122232425262728293031323334public class NewClient &#123; private int port; private String address; public NewClient(int port,String address) &#123; this.port = port; this.address = address; &#125; public void start()&#123; EventLoopGroup group = new NioEventLoopGroup(); Bootstrap bootstrap = new Bootstrap(); bootstrap.group(group) .channel(NioSocketChannel.class) .option(ChannelOption.TCP_NODELAY, true) .handler(new NewClientChannelInitializer()); try &#123; ChannelFuture future = bootstrap.connect(address,port).sync(); future.channel().closeFuture().sync(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;finally &#123; group.shutdownGracefully(); &#125; &#125; public static void main(String[] args) &#123; NewClient client = new NewClient(7788,"127.0.0.1"); client.start(); &#125;&#125; 客户端Initializer： 123456789public class NewClientChannelInitializer extends ChannelInitializer&lt;SocketChannel&gt; &#123; protected void initChannel(SocketChannel socketChannel) throws Exception &#123; ChannelPipeline pipeline = socketChannel.pipeline(); pipeline.addLast(new NewEncoder()); pipeline.addLast(new NewClientHandler()); &#125;&#125; 客户端中我们又定义了一个编码器NewEncoder，继承了MessageToByteEncoder，该类用于将文本信息转换为流： 123456789101112131415161718public class NewEncoder extends MessageToByteEncoder&lt;Message&gt; &#123; @Override protected void encode(ChannelHandlerContext channelHandlerContext, Message message, ByteBuf byteBuf) throws Exception &#123; if(message == null)&#123; throw new Exception("未获得消息内容"); &#125; String msgBody = message.getMsgBody(); byte[] b = msgBody.getBytes(Charset.forName("utf-8")); byteBuf.writeByte(message.getType()); byteBuf.writeByte(b.length); byteBuf.writeBytes(b); &#125;&#125; 接下来是我们的客户端handler： 123456789public class NewClientHandler extends ChannelInboundHandlerAdapter &#123; @Override public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; String m = "你好啊,Netty。昂昂"; Message msg = new Message((byte)0xCA, m.length(), m); ctx.writeAndFlush(msg); &#125;&#125; 注意到在handler中我们发送了一个Message对象。然后会由NewEncoder编码发送出去，服务端对消息解码获得消息头和消息体。分别启动服务端和客户端，打印结果为： 我们的消息就发送出去了。]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netty学习(七)-Netty编解码技术以及ProtoBuf和Thrift的介绍]]></title>
    <url>%2Fposts%2Ff693139f.html</url>
    <content type="text"><![CDATA[在前几节我们学习过处理粘包和拆包的问题，用到了Netty提供的几个解码器对不同情况的问题进行处理。功能很是强大。我们有没有去想这么强大的功能是如何实现的呢？背后又用到了什么技术？这一节我们就来处理这个问题。了解一下编码解码到底是如何处理的。 通常说的编码(Encoder)也就是发生在发送消息的时候需要将消息编译成字节对象，在Netty中即编译成ByteBuf对象。在java中我们将这种编译称之为序列化(Serializable),即将对象序列化为字节数组，然后用于传输或是持久化啊之类的。那么自然解码（Decoder）就是一个反序列化的过程，使用相应的编码格式对接收到的对做一个解码，以正确解析该对象。 1. java序列化的弱点 谈到序列化我们自然想到java提供的Serializable接口，在java中我们如果需要序列化只需要继承该接口就可以通过输入输出流进行序列化和反序列化。但是在提供很用户简单的调用的同时他也存在很多问题： 无法跨语言。当我们进行跨应用之间的服务调用的时候如果另外一个应用使用c语言来开发，这个时候我们发送过去的序列化对象，别人是无法进行反序列化的因为其内部实现对于别人来说完全就是黑盒。 序列化之后的码流太大。这个我们可以做一个实验还是上一节中的Message类，我们分别用java的序列化和使用二进制编码来做一个对比，下面我写了一个测试类： 123456789101112131415161718192021222324252627282930@Testpublic void testSerializable()&#123; String str = "哈哈,我是一条消息"; Message msg = new Message((byte)0xAD,35,str); ByteArrayOutputStream out = new ByteArrayOutputStream(); try &#123; ObjectOutputStream os = new ObjectOutputStream(out); os.writeObject(msg); os.flush(); byte[] b = out.toByteArray(); System.out.println("jdk序列化后的长度： "+b.length); os.close(); out.close(); ByteBuffer buffer = ByteBuffer.allocate(1024); byte[] bt = msg.getMsgBody().getBytes(); buffer.put(msg.getType()); buffer.putInt(msg.getLength()); buffer.put(bt); buffer.flip(); byte[] result = new byte[buffer.remaining()]; buffer.get(result); System.out.println("使用二进制序列化的长度："+result.length); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;&#125; 输出结果为： 我们可以看到差距是挺大的，目前的主流编解码框架序列化之后的码流也都比java序列化要小太多。 序列化效率差，这个我们也可以做一个对比，还是上面写的测试代码我们循环跑100000次对比一下时间： 123456789101112131415161718192021222324252627282930313233343536373839@Testpublic void testSerializable()&#123; String str = "哈哈,我是一条消息"; Message msg = new Message((byte)0xAD,35,str); ByteArrayOutputStream out = new ByteArrayOutputStream(); try &#123; long startTime = System.currentTimeMillis(); for(int i = 0;i &lt; 100000;i++)&#123; ObjectOutputStream os = new ObjectOutputStream(out); os.writeObject(msg); os.flush(); byte[] b = out.toByteArray(); /*System.out.println("jdk序列化后的长度： "+b.length);*/ os.close(); out.close(); &#125; long endTime = System.currentTimeMillis(); System.out.println("jdk序列化100000次耗时：" +(endTime - startTime)); long startTime1 = System.currentTimeMillis(); for(int i = 0;i &lt; 100000;i++)&#123; ByteBuffer buffer = ByteBuffer.allocate(1024); byte[] bt = msg.getMsgBody().getBytes(); buffer.put(msg.getType()); buffer.putInt(msg.getLength()); buffer.put(bt); buffer.flip(); byte[] result = new byte[buffer.remaining()]; buffer.get(result); /*System.out.println("使用二进制序列化的长度："+result.length);*/ &#125; long endTime1 = System.currentTimeMillis(); System.out.println("使用二进制序列化100000次耗时：" +(endTime1 - startTime1)); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;&#125; 结果为： 结果为毫秒数，这个差距也是不小的。 结合以上我们看到：目前的序列化过程中使用java本身的肯定是不行，使用二进制编码的话又的我们自己去手写，所以为了让我们少搬砖前辈们早已经写好了工具让我们调用，目前社区比较活跃的有google的Protobuf和Apache的Thrift。 2. Protobuf序列化的使用 我们先来使用Protobuf进行序列化，他和XML，json一样都有自己的语法，xml的后缀是.xml，json文件的后缀是.json，自然Protobuf文件的后缀就是.proto（哈哈，当然不是全称）。 下面我们使用Protobuf来封装一段消息，通过一个案例简单介绍一下它的使用。 首先我们用Protobuf的语法格式来写一段需要序列化的对象，命名格式为：Msg.proto 1234567891011121314option java_package = "cn.edu.hust.netty.demo10";option java_outer_classname = "MessageProto";message RequestMsg&#123; required bytes msgType = 1; required string receiveOne = 2; required string msg = 3;&#125;message ResponseMsg&#123; required bytes msgType = 1; required string receiveOne = 2; required string msg = 3;&#125; 关于Message.proto中的语法格式，详情大家google一下相关的说明，网上很多介绍，再次简单就上面的语法说明一下： option java_package：表示生成的.java文件的包名 option java_outer_classname：生成的java文件的文件名 message ： 为他的基本类型，如同java中的class一样 字段修饰符： required：一个格式良好的消息一定要含有1个这种字段。表示该值是必须要设置的； optional：消息格式中该字段可以有0个或1个值（不超过1个）。 repeated：在一个格式良好的消息中，这种字段可以重复任意多次（包括0次）。重复的值的顺序会被保留。表示该值可以重复，相当于java中的List。 字符类型稍微有些不同：double,float,int32,int64,bool(boolean) ,string,bytes。稍微有些不同，String，boolean，int有差别。 另外我们看到上面3个字段分别赋值了，这个值是什么意思呢？消息定义中，每个字段都有唯一的一个数字标识符。这些标识符是用来在消息的二进制格式中识别各个字段的，一旦开始使用就不能够再改变。注：[1,15]之内的标识号在编码的时候会占用一个字节。[16,2047]之内的标识号则占用2个字节。所以应该为那些频繁出现的消息元素保留 [1,15]之内的标识号。 关于Protobuf 的语法我们就简单的介绍这么多，更多细节大家自己去查阅文档吧。下面我们开始使用Protobuf 来进行序列化。 首先我们的在工程中引入protobuf的jar包，目前官方版本最高3.2，我们用3.0的吧： 12345&lt;dependency&gt; &lt;groupId&gt;com.google.protobuf&lt;/groupId&gt; &lt;artifactId&gt;protobuf-java&lt;/artifactId&gt; &lt;version&gt;3.0.2&lt;/version&gt;&lt;/dependency&gt; Protobuf的文件已经定义好了，下就需要把它编译成java代码，这里我们的借助到google为我们提供的脚本工具protoc，链接在这里，点击下载这里提供的是protoc-3.0.2。要注意protoc的版本需要和Protobuf的版本对应上，不然不同的版本之间会有一些差异解析可能会有问题。现在知道我们为啥非得选用protobuf3.0.2版本吧，因为我没有找到别的版本的protoc。。。 下载好了我们解压缩然后把刚才写好的Msg.proto文件复制进去。 接着我们进cmd输入如下命令： 主要是第三句命令。如果你输入没有报错的话你的proto文件夹应该会生成一个子文件夹： 进去该文件夹你会看到已经生成了MessageProto.java文件，恭喜你，这时候你已经完成了protobuf序列化文件的生成。然后你把该文件拷贝至工程目录下。接下来我们用生成的文件去发消息吧。还是老套路服务端和客户端。 服务端： 12345678910111213141516171819202122232425262728293031public class ProtoBufServer &#123; private int port; public ProtoBufServer(int port) &#123; this.port = port; &#125; public void start()&#123; EventLoopGroup bossGroup = new NioEventLoopGroup(); EventLoopGroup workGroup = new NioEventLoopGroup(); ServerBootstrap server = new ServerBootstrap().group(bossGroup,workGroup) .channel(NioServerSocketChannel.class) .childHandler(new ServerChannelInitializer()); try &#123; ChannelFuture future = server.bind(port).sync(); future.channel().closeFuture().sync(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;finally &#123; bossGroup.shutdownGracefully(); workGroup.shutdownGracefully(); &#125; &#125; public static void main(String[] args) &#123; ProtoBufServer server = new ProtoBufServer(7788); server.start(); &#125;&#125; 服务端Initializer： 12345678910public class ServerChannelInitializer extends ChannelInitializer&lt;SocketChannel&gt; &#123; @Override protected void initChannel(SocketChannel socketChannel) throws Exception &#123; ChannelPipeline pipeline = socketChannel.pipeline(); pipeline.addLast(new ProtobufVarint32FrameDecoder()); pipeline.addLast(new ProtobufDecoder(MessageProto.RequestMsg.getDefaultInstance())); pipeline.addLast(new ProtoBufServerHandler()); &#125;&#125; 服务端handler： 123456789101112131415161718192021222324public class ProtoBufServerHandler extends ChannelInboundHandlerAdapter &#123; @Override public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; MessageProto.ResponseMsg.Builder builder = MessageProto.ResponseMsg.newBuilder(); builder.setMsgType(ByteString.copyFromUtf8("CBSP")); builder.setReceiveOne("小红"); builder.setMsg("你好，你有啥事"); ctx.writeAndFlush(builder.build()); &#125; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; MessageProto.RequestMsg m = (MessageProto.RequestMsg)msg; System.out.println("Client say: "+m.getReceiveOne()+","+m.getMsg()); &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; super.exceptionCaught(ctx, cause); ctx.close(); &#125;&#125; 客户端： 12345678910111213141516171819202122232425262728293031323334public class ProtoBufClient &#123; private int port; private String address; public ProtoBufClient(int port, String address) &#123; this.port = port; this.address = address; &#125; public void start()&#123; EventLoopGroup group = new NioEventLoopGroup(); Bootstrap bootstrap = new Bootstrap(); bootstrap.group(group) .channel(NioSocketChannel.class) .handler(new ClientChannelInitializer()); try &#123; ChannelFuture future = bootstrap.connect(address,port).sync(); future.channel().closeFuture().sync(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;finally &#123; group.shutdownGracefully(); &#125; &#125; public static void main(String[] args) &#123; ProtoBufClient client = new ProtoBufClient(7788,"127.0.0.1"); client.start(); &#125;&#125; 客户端Initializer： 12345678910public class ClientChannelInitializer extends ChannelInitializer&lt;SocketChannel&gt; &#123; protected void initChannel(SocketChannel socketChannel) throws Exception &#123; ChannelPipeline pipeline = socketChannel.pipeline(); pipeline.addLast(new ProtobufVarint32LengthFieldPrepender()); pipeline.addLast(new ProtobufEncoder()); pipeline.addLast(new ProtoBufClientHandler()); &#125;&#125; 客户端handler： 12345678910111213141516171819202122public class ProtoBufClientHandler extends ChannelInboundHandlerAdapter &#123; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; MessageProto.ResponseMsg m = (MessageProto.ResponseMsg)msg; System.out.println("Server say: "+m.getReceiveOne()+","+m.getMsg()); &#125; @Override public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; MessageProto.RequestMsg.Builder builder = MessageProto.RequestMsg.newBuilder(); builder.setMsgType(ByteString.copyFromUtf8("CBSP")); builder.setReceiveOne("小明"); builder.setMsg("你好，我找你有事"); ctx.writeAndFlush(builder.build()); &#125; @Override public void channelInactive(ChannelHandlerContext ctx) throws Exception &#123; System.out.println("Client is close"); &#125;&#125; 启动服务端和客户端，输出如下： 最简单的protoBuf应用案例我们就写完了，真实的使用场景大同小异，随机应变即可。 3. thrift序列化的使用 哈哈，我本来是打算讲thrift的安装和使用的，但是现在却讲不了，因为这玩意儿的安装是个问题。由于我没有linux环境，thrift如果在linux环境下安装使用是挺简单的，但是在windows环境下挺麻烦。thrift在windows下，还使用C++，搭环境是最难的。 libthrift依赖boost libthriftnb依赖boost，libevent 等于你得安装boost，libevent 除此之外，还需要openssl 装openssl，又需要perl，nasm 期间，还会涉及版本兼容问题，总而言之，比较折磨，而这还仅是安装编译。 所以暂时我就跳过这一部分，等我安装linux环境之后再来讲解吧。]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netty学习(一)-为什么选择Netty]]></title>
    <url>%2Fposts%2F704e9e4d.html</url>
    <content type="text"><![CDATA[前面我们简单学习了NIO。我们知道java的I/O模型一共有四种，分别是：传统的BIO，伪异步I/O，NIO和AIO。为了澄清概念和分清区别，我们还是先简单的介绍一下他们的概念，然后再去比较优劣。以及探讨我们为什么使用netty。 1.概念澄清 1.1 BIO BIO，即Blocking I/O。网络编程的基本模型是Client/Server 模型，也就是两个进程之间进行相互通信，其中服务端提供位置信息(绑定的Ip 地址和监听端口) ，客户端通过连接操作向服务端监听的地址发起连接请求，通过三次握手建立连接，如果连接建在成功，双方就可以通过网络套接字( Socket ) 进行通信。在基于传统同步阻塞模型开发中， ServerSocket 负责绑定IP 地址，启动监听端口:Socket 负责发起连接操作。连接成功之后，双方通过输入和输出流进行同步阻塞式通信。 BIO通信模型图： 解释一下上图： 采用BIO通信模型的服务端，通常由一个独立的Acceptor线程负责监听客户端的连接，它接收到客户端连接请求之后为每个客户端创建一个新的线程进行链路处理，处理完成之后，通过输出流返回应答给客户端， 统程销毁。这就是典型的一请求一回答通信模型。 对于这种IO模型我们知道：用户线程发出IO请求之后，内核会去查看数据是否就绪，如果没有就绪就会等待数据就绪，而用户线程就会处于阻塞状态，用户线程交出CPU。当数据就绪之后，内核会将数据拷贝到用户线程，并返回结果给用户线程，用户线程才解除block状态。即在读写数据过程中会发生阻塞现象。 1.2 伪异步IO 为了解决同步阻塞 I/O 面临的一个链路需要一个线程处理的问题，后来有人对它的线程模型进行了优化一一后端通过一个线程池来处理多个客户端的请求接入，形成客户端个数M: 线程池最大线程数N 的比例关系，其中M 可以远远大于N。通过线程地可以灵活地调配线程资源，设置线程的最大值，防止由于海量并发接入导致线程耗尽。 伪异步IO通信模型图： 采用线程池和任务队列可以实现伪异步I/O通信框架。当有新的客户端接入时，将客户端的Socket 封装成一个Task (该任务实现java.lang. Runnable 接口）投递到后端的线程池中进行处理， JDK 的线程将维护一个消息队列和N个活跃线程， 对消息队列中的任务进行处理。由于统程池可以设置消息队列的大小和最大线程数，因此，它的资源占用是可控的， 无论多少个客户端并发访问， 都不会导致资源的耗尽和省机。 伪异步I/O 通信框架采用了线程池实现，因此避免了为每个请求都创建一个独立线程造成的线程资源耗尽问题。但是由于它底层的通信依然采用同步阻塞模型，因此无法从根本上解决问题。伪异步I/O 实际上仅仅是对之前I/O 线程模型的一个简单优化，它无法从根本上解决同步I/O 导致的通信线程阻塞问题。下面我们就简单分析下通信对方返回应答时间过长会引起的级联故障。 服务端处理缓慢，返回应答消息耗费60s，平时只需要10ms; 采用伪异步I/O 的线程在读取故障服务节点的响应，由于读/取输入流是阻塞的，它将会被同步阻塞60s; 假如所有的可用线程都被故障服务器阻塞，那后续的所有的I/O消息都将在队列中排队; 由于线程地采用阻塞队列实现，当队列积满之后，后续入队列的操作将被阻塞; 由于前端只有一个Accptor 线程接收客户端接入，它被阻塞在线程池的同步阻塞队列之后，新的客户端请求消息将被拒绝，客户端会发生大量的连接超时; 由于几于所有的连接都超时，调用者会认为系统已经崩溃，无法接收新的请求消息。 如何破解这个难题?下面我们再看一下NIO。 1.3 NIO NIO,很多人叫他New I/O，由于之前老的I/O 类库是阻塞I/O ，New I/O 类库的目标就是要让Java 支持非阻塞I/O，所以，更多的人喜欢称之为非阻塞I/O(Non-block I/O)。 与Socket类和ServerSocket 类相对应， NIO也提供了SocketChannel 和ServerSocketChannel两种不同的套接字通道实现。这两种新增的通道都支持阻塞和非阻塞两种模式。阻塞模式使用非常简单，但是性能和可靠性都不好，非阻塞模式则正好相反。开发人员可以根据自己的需要来选择合适的模式。一般来说，低负载、低并发的应用程序可以选择同步阻塞I/O以降低编程复杂度:对于高负载、高并发的网络应用，需要使用NIO 的非阻塞模式进行开发。 前面我们已经对NIO进行了介绍，我们知道NIO中引入了缓冲区Buffer，通道Channel和多路复用器Selector的概念。一个多路复用器Selector 可以同时轮询多个Channel，而Channel又是全双工的，同时支持读写操作，使用NIO 编程的优点总结如下： 客户端发起的连接操作是异步的，可以通过在多路复用器注册OP_CONNECT 等待后续结果，不需要像之前的客户端那样被同步阻塞。 SocketChannel 的读写操作都是异步的，如果没有可读写的数据它不会同步等待，直接返回，这样I/O 通信线程就可以处理其他的链路，不需要同步等待这个链路可用。 线程模型的优化:由于JDK 的Selector 在Linux 等主流操作系统上通过epoll 实现，它没有连接句柄数的限制(只受限于操作系统的最大句柄数或者对单个进程的句柄限制)，这意味着一个Selector 线程可以同时处理成千上万个客户端连接，而且性能不会随着客户端的增加而线性下降。因此，它非常适合做高性能、高负载的网络服务器。 1.4 AIO NIO 2.0 引入了新的异步通道的概念，并提供了异步文件通道和异步套接字通道的实现。异步通道提供以下两种方式获取获取操作结果： ▷通过java.util.concurrent.Future 类来表示异步操作的结果; ▷在执行异步操作的时候传入一个java.nio.channels; NIO 2.0 的异步套接字通道是真正的异步非阻塞I/O ，对应于UNIX 网络编程中的事件 驱动I/O (AIO) 。它不需要通过多路复用器( Selector) 对注册的通道进行轮询操作即可实 现异步读写，从而简化了NIO 的编程模型。 前面对不同的I/O模型进行了简单介绍，不同的I/O 模型由于线程模型、API 等差别很大，所以用法的差异也非常大。我们用一个表格来做一个统一说明： 2. 为什么用Netty 开发出高质量的NIO 程序并不是一件简单的事情，除去NIO 固有的复杂性和Bug不谈，作为一个NIO 服务端,需要能够处理网络的闪断、客户端的重复接入、客户端的安全认证、消息的编解码、半包读写等情况， 如果你没有足够的NIO 编程经验积累， 一个NIO 框架的稳定往往需要半年甚至更长的时间。更为糟糕的是， 一旦在生产环境中发生问题， 往往会导致跨节点的服务调用中断， 严重的可能 会导致整个集群环境都不可用， 需要重启服务器，这种非正常停机会带来巨大的损失。 从可维护性角度看，由于NIO 采用了异步非阻塞编程模型，而且是一个I/O 线程处理多条链路，它的调试和跟踪非常麻烦， 特别是生产环境中的问题，我们无法进行有效的调试和跟踪， 往往只能靠一些日志来帮助分析，定位难度很大。 对于java原生的IO我们之所以不选择使用是因为： NIO的类库和API繁杂使用麻烦，你需要熟练掌握Selectol,ServerSocketChannel, SocketChannel,ByteBuffer 等。 需妥具备其他的额外技能做制垫，例如熟悉Java 多线程编程。这是因为NIO编程涉及到Reactor 模式，你必须对多钱程和网络编程非常如悉，才能编写出高质量的NIO程序。 可靠性能力补齐， 工作量和难度都非常大。例如客户端面临断连重连、网络间断、半包读写、失败缓存、网络拥塞和异常码流的处理等问题， NI0 编程的特点是功能开发相对容易，但是可靠性能力补齐的工作量和难度都非常大。 JDK NIO的BUG，比如epoll bug，这个BUG会在linux上导致cpu 100%，使得nio server/client不可用，这个BUG直到jdk 6u4才解决，但是直到JDK1.7中仍然有这个问题，该问题并未被完全解决，只是发生的频率降低了而已。 基于上述原因大多数场景下都不建议直接使原生NIO，除非你精通NIO编程或者是有特殊的需要，否则作为服务器编程的NIO可能会带来巨大的生产隐患。 关于Netty： Netty是一个高性能、异步事件驱动的NIO框架，它提供了对TCP、UDP和文件传输的支持，作为一个异步NIO框架，Netty的所有IO操作都是异步非阻塞的，通过Future-Listener机制，用户可以方便的主动获取或者通过通知机制获得IO操作结果。作为当前最流行的NIO框架，Netty在互联网领域、大数据分布式计算领域、游戏行业、通信行业等获得了广泛的应用，一些业界著名的开源组件也基于Netty的NIO框架构建。 与Netty同样功能的NIO框架还有Mina，Netty的主导作者与Mina的主导作者是同一人，在设计理念上与Mina基本上是一致的。Mina出身于开源界的大牛Apache组织，Netty出身于商业开源大亨Jboss。 这几年Netty社区相对比较活跃，所以我们就先选择Netty作为入手网络编程的首选，有时间再学习一下Mina。]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[消息中间件-activemq消息机制和持久化介绍(三)]]></title>
    <url>%2Fposts%2F28fe9931.html</url>
    <content type="text"><![CDATA[前面一节简单学习了activemq的使用，我们知道activemq的使用方式非常简单有如下几个步骤： 创建连接工厂 创建连接 创建会话 创建目的地 创建生产者或消费者 生产或消费消息 关闭生产或消费者、关闭会话、关闭连接 前面我们的实例代码中已经按照这个步骤完成了P2P和Pub/Sub模式的消息发送和接收。那么这一节我们就针对他的消息传播机制和持久化方式做一个简单的学习。在会用的同时我们也需要理解一些基本的概念，这样才不至于在出错后无从下手。 1.activemq服务器工作模型 我们先看一下消息发送的时序图： ConnectionFactory 对象创建一个连接工厂，消息的发送和接受服务均由此进行； ConnectionFactory 创建一个活动Connection作为当前使用的连接； Session 是一个用于生成和使用消息的单线程上下文，它用于创建发送的生产者和接收消息的消费者，并为所发送的消息定义发送顺序。会话通过大量确认选项或通过事务来支持可靠传送。 户端使用 MessageProducer 向指定的物理目标发送消息，生产者可指定一个默认传送模式（持久性消息与非持久性消息）、优先级和有效期值，以控制生产者向物理目标发送的所有消息； 消费者可以支持同步或异步消息接收。异步使用可通过向消费者注册 MessageListener 来实现。当会话线程调用 MessageListener 对象的 onMessage 方法时，客户端将使用消息。 2.ActiveMQ消息传送模型 ActiveMQ 支持两种消息传送模型：PTP（即点对点模型）和Pub/Sub（即发布 /订阅模型）,前面我们已经讲过，在此就不赘述。 3.消息选择器 ActiveMQ提供了一种机制，使用它，消息服务可根据消息选择器中的标准来执行消息过滤。生产者可在消息中放入应用程序特有的属性，而消费者可使用基于这些属性的选择标准来表明对消息是否感兴趣。 消息选择器是根据 header 和 properties 允许客户端选择性的制定需要接收的消息，消息选择器是无法利用 消息主体(Body)进行过滤的。无论你的消息主题是什么类型， 文本、或者对象、或者键值对。下面我们讲一下消息选择器的语法以及使用规范： 可接收的类型包括：byte,int,double,boolean,String; 属性标识符定义： 变量名与java定义一样； 要么在heads中定义 要么在 properties中定义，如果在sender中是在heads中定义而receiver中却从properties中寻找的话，找不到的情况下他是不会自动去heads中寻找的，而是会返回null； 根据不同类型的变量选择不同的方法： message.setIntProperty(&quot;test&quot;,14); 那么在接收端可以对该变量进行拦截： session.createConsumer(destination,&quot;test &gt; 14&quot;)； 属性标志符是区分大小写的； 拦截器中的部分表示方式： 可以是条件表达式 可以是算术表达式 可以是比较运算和逻辑运算组成的表达式 支持 () 左右括号； 支持逻辑运算的优先顺序表达式 例如: NOT , AND , OR； 比较运算符有: = , &gt; , &gt;= , &lt; , &lt;= , &lt;&gt; (not equal)； eg： 标识符是null &quot;prop_name IS NULL&quot; 标识符非空 not null &quot;prop_name IS NOT NULL&quot; &quot;age BETWEEN 15 AND 19&quot; is equivalent to &quot;age &gt;= 15 AND age &lt;= 19&quot; &quot;Country NOT IN (' UK', 'US', 'France') &quot; 代码很简单，只需要在Sender端做如下改写： 123TextMessage message = session.createTextMessage();message.setIntProperty("test",14);message.setText("test"); Receiver端： 1consumer = session.createConsumer(destination,"test &gt; 14"); 对发送端的特定字符做一个判断符合条件即被拦截 4.消息确认机制 jms消息只有在被确认之后才认为成功消费了这条消息。消息的成功消费通常包括三个步骤： （1）client接收消息 （2）client处理消息 （3）消息被确认（也就是client给一个确认消息） 在事务性会话中当一个事务被提交的时候，确认自动发生，和应答模式没关系，这个值可以随便写。（这里多提一句异步消息接收中不能使用事务性会话）。 在非事务性会话中消息何时被确认取决于创建的session中设置的消息应答模式（acknowledge model）该参数有三个值： Session.AUTO_ACKNOWLEDGE：当client端成功的从receive方法或从onMessage(Message message) 方法返回的时候，会话自动确认client收到消息。 Session.CLIENT_ACKNOWLEDGE: 客户单通过调用acknowledge方法来确认客户端收到消息。但需要注意在这种应答模式下，确认是在会话层上进行的，确认一个被消费的消息将自动确认所有已消费的其他消息。比如一个消费者已经消费了10条消息，然后确认了第5条消息被消费，则这10条都被确认消费了。、 acknowledge（）通知方法是在Message对象上，同步接收，调用acknowledge（）方法进行确认如下所示： consumer = session.createConsumer(queue); Message message = consumer.receive(); message.acknowledge(); 异步接受，调用acknowledge（）方法进行确认： 12345678910111213consumer.setMessageListener(new MessageListener() &#123; @Override public void onMessage(Message message) &#123; TextMessage textMessage = (TextMessage) message; try &#123; String value = textMessage.getText(); System.out.println("value: " + value); message.acknowledge(); //消息消费确认通知 &#125; catch (JMSException e) &#123; e.printStackTrace(); &#125; &#125;&#125;); 3.Session.DUPS_ACKNOWLEDGE：不是必须签收，消息可能会重复发送。在第二次重新传送消息的时候，消息头的JmsDelivered会被置为true标示当前消息已经传送过一次，客户端需要进行消息的重复处理控制。 5. 持久化消息 JMS 支持以下两种消息提交模式： 5.1 ERSISTENT 持久消息 是activemq默认的传送方式，此方式下的消息在配合activemq.xml中配置的消息存储方式，会被存储在特定的地方，直到有消费者将消息消费或者消息过期进入DLQ队列，消息生命周期才会结束。此模式下可以保证消息只会被成功传送一次和成功使用一次，消息具有可靠性。在消息传递到目标消费者，在消费者没有成功应答前，消息不会丢失。所以很自然的，需要一个地方来持久性存储。如果消息消费者在进行消费过程发生失败，则消息会被再次投递。 DeliveryMode.PERSISTENT 指示JMS provider持久保存消息，以保证消息不会因为JMS provider的失败而丢失。 消息持久化在硬盘中，ActiveMQ持久化有三种方式：AMQ、KahaDB、JDBC。 AMQ AMQ是一种文件存储形式，它具有写入速度快和容易恢复的特点。消息存储在一个个文件中，文件的默认大小为32M，如果一条消息的大小超过了32M，那么这个值必须设置大一点。当一个存储文件中的消息已经全部被消费，那么这个文件将被标识为可删除，在下一个清除阶段，这个文件被删除。AMQ适用于ActiveMQ5.3之前的版本。 KahaDB KahaDB是基于文件的本地数据库储存形式，虽然没有AMQ的速度快，但是它具有强扩展性，恢复的时间比AMQ短，从5.4版本之后KahaDB做为默认的持久化方式。 JDBC 可以将消息存储到数据库中，例如：Mysql、SQL Server、Oracle、DB2。 具体使用方式大家下去查一下，限于篇幅在此就不做太详细的介绍。 5.2 NON_PERSISTENT 非持久消息 非持久的消息适用于不重要的，可以接受消息丢失的哪一类消息，这种消息只会被投递一次，消息不会在持久性存储中存储，也不会保证消息丢失后的重新投递。 DeliveryMode.NON_PERSISTENT 不要求JMS provider持久保存消息，消息存放在内存中，读写速度快，在JMS服务停止后消息会消失，没有持久化到硬盘。 6. ActiveMQ消息过期设置 允许消息过期 。默认情况下，消息永不会过期。如果消息在特定周期内失去意义，那么可以设置过期时间。 有两种方法设置消息的过期时间，时间单位为毫秒： 使用 setTimeToLive 方法为所有的消息设置过期时间； 使用 send 方法为每一条消息设置过期时间。 消息过期时间，send 方法中的 timeToLive 值加上发送时刻的 GMT 时间值。如果 timeToLive 值等于零，则 JMSExpiration 被设为零，表示该消息永不过期。如果发送后，在消息过期时间之后消息还没有被发送到目的地，则该消息被清除。 这一节对activemq的消息机制和持久化我们就简单介绍到这里，后面我们结合具体的工程来把它应用到生产中，再来讲解如何持久化如何高效的应用于生产环境。]]></content>
      <categories>
        <category>activeMQ</category>
      </categories>
      <tags>
        <tag>学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java学习-NIO(四)Selector]]></title>
    <url>%2Fposts%2Fe87423e8.html</url>
    <content type="text"><![CDATA[这一节我们将探索选择器(selectors)。选择器提供选择执行已经就绪的任务的能力，这使得多元 I/O 成为可能。就像在第一章中描述的那样，就绪选择和多元执行使得单线程能够有效率地同时管理多个 I/O 通道(channels)。C/C++代码的工具箱中，许多年前就已经有 select()和 poll()这两个POSIX（可移植性操作系统接口）系统调用可供使用了。许过操作系统也提供相似的功能，但对Java 程序员来说，就绪选择功能直到 JDK 1.4 才成为可行的方案。 下面我们来使用选择器： 通过 Selector.open()方法, 我们可以创建一个选择器: 1Selector selector = Selector.open(); 将 Channel 注册到选择器中： 123channel.configureBlocking(false); SelectionKey key = channel.register(selector, SelectionKey.OP_READ); ***注意, 如果一个 Channel 要注册到 Selector 中, 那么这个 Channel 必须是非阻塞的, 即channel.configureBlocking(false);因为 Channel 必须要是非阻塞的, 因此 FileChannel 不能够使用选择器, 因为 FileChannel 都是阻塞的.*** 注意到, 在使用 Channel.register()方法时, 第二个参数指定了我们对 Channel 的什么类型的事件感兴趣, 这些事件有: Connect, 即连接事件(TCP 连接), 对应于SelectionKey.OP_CONNECT Accept, 即确认事件, 对应于SelectionKey.OP_ACCEPT Read, 即读事件, 对应于SelectionKey.OP_READ, 表示 buffer 可读. Write, 即写事件, 对应于SelectionKey.OP_WRITE, 表示 buffer 可写. 一个 Channel发出一个事件也可以称为 对于某个事件, Channel 准备好了. 因此一个 Channel 成功连接到了另一个服务器也可以被称为 connect ready. 我们可以使用或运算|来组合多个事件, 例如: 1int interestSet = SelectionKey.OP_READ | SelectionKey.OP_WRITE; 注意, 一个 Channel 仅仅可以被注册到一个 Selector 一次, 如果将 Channel 注册到 Selector 多次, 那么其实就是相当于更新 SelectionKey 的 interest set. 例如: 12channel.register(selector, SelectionKey.OP_READ);channel.register(selector, SelectionKey.OP_READ | SelectionKey.OP_WRITE); 上面的 channel 注册到同一个 Selector 两次了, 那么第二次的注册其实就是相当于更新这个 Channel 的 interest set 为 SelectionKey.OP_READ | SelectionKey.OP_WRITE. 但是Java NIO的selector允许一个单一线程监听多个channel输入。我们可以注册多个channel到selector上，然后然后用一个线程来挑出一个处于可读或者可写状态的channel。selector机制使得单线程管理多个channel变得容易。 下面我们写一个完整的例子，看一下Selector的用法： 12345678910111213141516171819202122232425262728//创建选择器Selector selector = Selector.open();channel.configureBlocking(false);//注册通道SelectionKey key = channel.register(selector, SelectionKey.OP_READ);while(true) &#123; //查看selector中的key是否准备好 int readyChannels = selector.select(); //小于0超时，等于0没准备好，大于0已经准备完毕 if(readyChannels == 0) continue; //获取选择器中的key Set&lt;SelectionKey&gt; selectedKeys = selector.selectedKeys(); Iterator&lt;SelectionKey&gt; keyIterator = selectedKeys.iterator(); while(keyIterator.hasNext()) &#123; SelectionKey key = keyIterator.next(); //遍历已选择键集中的每个键，并检测各个键所对应的通道的就绪事件 if(key.isAcceptable()) &#123; // 连接已经被ServerSocketChannel所接受 &#125; else if (key.isConnectable()) &#123; // 连接已经被远程终止. &#125; else if (key.isReadable()) &#123; // 通道已经准备好读数据 &#125; else if (key.isWritable()) &#123; // 通道已经准备好写数据 &#125; keyIterator.remove(); &#125;&#125; 选择器的使用还有很多的细节，我们应该多查看api文档了解各个方法的用法。下一节我们做一个综合练习，总结一下NIO的使用。]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[消息中间件-activemq实战整合Spring之Topic模式(五)]]></title>
    <url>%2Fposts%2F1d6950fa.html</url>
    <content type="text"><![CDATA[这一节我们看一下Topic模式下的消息发布是如何处理的。 applicationContext-ActiveMQ.xml配置： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:amq="http://activemq.apache.org/schema/core" xmlns:context="http://www.springframework.org/schema/context" xmlns:mvc="http://www.springframework.org/schema/mvc" xsi:schemaLocation="http://www.springframework.org/schema/beanshttp://www.springframework.org/schema/beans/spring-beans-4.1.xsdhttp://www.springframework.org/schema/contexthttp://www.springframework.org/schema/context/spring-context-4.1.xsdhttp://www.springframework.org/schema/mvchttp://www.springframework.org/schema/mvc/spring-mvc-4.1.xsdhttp://activemq.apache.org/schema/corehttp://activemq.apache.org/schema/core/activemq-core-5.12.1.xsd"&gt; &lt;context:component-scan base-package="cn.edu.hust.activemq" /&gt; &lt;mvc:annotation-driven /&gt; &lt;amq:connectionFactory id="amqConnectionFactory" brokerURL="tcp://127.0.0.1:61616" userName="admin" password="admin" /&gt; &lt;!-- 配置JMS连接工厂 --&gt; &lt;bean id="connectionFactory" class="org.springframework.jms.connection.CachingConnectionFactory"&gt; &lt;constructor-arg ref="amqConnectionFactory" /&gt; &lt;property name="sessionCacheSize" value="100" /&gt; &lt;/bean&gt; &lt;!-- 定义消息队列（topic） --&gt; &lt;bean id="demoTopicDestination" class="org.apache.activemq.command.ActiveMQTopic"&gt; &lt;!-- 设置消息队列的名字 --&gt; &lt;constructor-arg&gt; &lt;value&gt;first-queue&lt;/value&gt; &lt;/constructor-arg&gt; &lt;/bean&gt; &lt;!-- 配置JMS模板（topic），Spring提供的JMS工具类，它发送、接收消息。 --&gt; &lt;bean id="jmsTemplate" class="org.springframework.jms.core.JmsTemplate"&gt; &lt;property name="connectionFactory" ref="connectionFactory" /&gt; &lt;property name="defaultDestination" ref="demoTopicDestination" /&gt; &lt;property name="receiveTimeout" value="10000" /&gt; &lt;property name="pubSubDomain" value="true" /&gt; &lt;/bean&gt; &lt;!-- 配置消息队列监听者（topic） --&gt; &lt;bean id="topicMessageListener" class="cn.edu.hust.activemq.filter.QueueMessageListener" /&gt; &lt;bean id="topicMessageListener1" class="cn.edu.hust.activemq.filter.QueueMessageListener1" /&gt; &lt;!-- 显示注入消息监听容器（topic），配置连接工厂，监听的目标是demoQueueDestination，监听器是上面定义的监听器 --&gt; &lt;bean id="queueListenerContainer" class="org.springframework.jms.listener.DefaultMessageListenerContainer"&gt; &lt;property name="connectionFactory" ref="connectionFactory" /&gt; &lt;property name="destination" ref="demoTopicDestination" /&gt; &lt;property name="messageListener" ref="topicMessageListener" /&gt; &lt;!--消息接收超时 --&gt; &lt;property name="receiveTimeout" value="10000" /&gt; &lt;/bean&gt; &lt;bean id="queueListenerContainerB" class="org.springframework.jms.listener.DefaultMessageListenerContainer"&gt; &lt;property name="connectionFactory" ref="connectionFactory" /&gt; &lt;property name="destination" ref="demoTopicDestination" /&gt; &lt;property name="messageListener" ref="topicMessageListener1" /&gt; &lt;!--消息接收超时 --&gt; &lt;property name="receiveTimeout" value="10000" /&gt; &lt;/bean&gt; &lt;/beans&gt; 这里与queue模式不一样的地方在于订阅者有一个或是多个，有几个订阅者就需要配置监听器。 applicationContext.xml 12345678910111213141516171819202122232425262728293031&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;!-- 查找最新的schemaLocation 访问 http://www.springframework.org/schema/ --&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:context="http://www.springframework.org/schema/context" xmlns:mvc="http://www.springframework.org/schema/mvc" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.springframework.org/schema/aophttp://www.springframework.org/schema/aop/spring-aop-4.0.xsdhttp://www.springframework.org/schema/beanshttp://www.springframework.org/schema/beans/spring-beans-4.0.xsdhttp://www.springframework.org/schema/contexthttp://www.springframework.org/schema/context/spring-context-4.0.xsdhttp://www.springframework.org/schema/mvchttp://www.springframework.org/schema/mvc/spring-mvc-4.0.xsd"&gt; &lt;!-- 指定Sping组件扫描的基本包路径 --&gt; &lt;context:component-scan base-package="cn.edu.hust.activemq" &gt; &lt;!-- 这里只扫描Controller，不可重复加载Service --&gt; &lt;context:include-filter type="annotation" expression="org.springframework.stereotype.Controller"/&gt; &lt;/context:component-scan&gt; &lt;!-- 启用MVC注解 --&gt; &lt;mvc:annotation-driven /&gt; &lt;!-- JSP视图解析器--&gt; &lt;bean class="org.springframework.web.servlet.view.InternalResourceViewResolver"&gt; &lt;property name="prefix" value="/" /&gt; &lt;property name="suffix" value=".jsp" /&gt; &lt;!-- 定义其解析视图的order顺序为1 --&gt; &lt;property name="order" value="1" /&gt; &lt;/bean&gt;&lt;/beans&gt; web.xml 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;web-app xmlns="http://java.sun.com/xml/ns/javaee" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://java.sun.com/xml/ns/javaee http://java.sun.com/xml/ns/javaee/web-app_3_0.xsd" version="3.0"&gt;&lt;display-name&gt;Archetype Created Web Application&lt;/display-name&gt; &lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:applicationContext-ActiveMQ.xml&lt;/param-value&gt; &lt;/context-param&gt; &lt;listener&gt; &lt;listener-class&gt;org.springframework.web.context.ContextLoaderListener&lt;/listener-class&gt; &lt;/listener&gt; &lt;servlet&gt; &lt;servlet-name&gt;springMVC&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:applicationContext.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;springMVC&lt;/servlet-name&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; &lt;!-- 处理编码格式 --&gt; &lt;filter&gt; &lt;filter-name&gt;characterEncodingFilter&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;encoding&lt;/param-name&gt; &lt;param-value&gt;UTF-8&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;forceEncoding&lt;/param-name&gt; &lt;param-value&gt;true&lt;/param-value&gt; &lt;/init-param&gt; &lt;/filter&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;characterEncodingFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt; &lt;/filter-mapping&gt;&lt;/web-app&gt; 其余部分与queue模式代码一样，只是需要多配置一个监听器，目录结构如下： ProducerService.java 123456789101112import javax.jms.Destination;/** * Created by Administrator on 2017/5/3. */public interface ProducerService &#123; void sendMessage(Destination destination,final String msg); void sendMessage(final String msg);&#125; ProducerServiceImpl.java 1234567891011121314151617181920212223242526272829303132333435363738394041import cn.edu.hust.activemq.service.ProducerService;import org.springframework.jms.core.JmsTemplate;import org.springframework.jms.core.MessageCreator;import org.springframework.stereotype.Service;import javax.annotation.Resource;import javax.jms.Destination;import javax.jms.JMSException;import javax.jms.Message;import javax.jms.Session;/** * Created by Administrator on 2017/5/3. */@Servicepublic class ProducerServiceImpl implements ProducerService &#123; @Resource(name="jmsTemplate") private JmsTemplate jmsTemplate; @Override public void sendMessage(Destination destination, final String msg) &#123; System.out.println(Thread.currentThread().getName()+" 向队列"+destination.toString()+"发送消息---------&gt;"+msg); jmsTemplate.send(destination, new MessageCreator() &#123; public Message createMessage(Session session) throws JMSException &#123; return session.createTextMessage(msg); &#125; &#125;); &#125; @Override public void sendMessage(final String msg) &#123; String destination = jmsTemplate.getDefaultDestinationName(); System.out.println(Thread.currentThread().getName()+" 向队列"+destination+"发送消息--------&gt;"+msg); jmsTemplate.send(new MessageCreator() &#123; public Message createMessage(Session session) throws JMSException &#123; return session.createTextMessage(msg); &#125; &#125;); &#125;&#125; ConsumerService.java 123456789import javax.jms.Destination;import javax.jms.TextMessage;/** * Created by Administrator on 2017/5/3. */public interface ConsumerService &#123; TextMessage receive(Destination destination);&#125; ConsumerServiceImpl.java 123456789101112131415161718192021222324252627282930import cn.edu.hust.activemq.service.ConsumerService;import javax.jms.Destination;import javax.jms.TextMessage;import org.springframework.jms.core.JmsTemplate;import org.springframework.stereotype.Service;import javax.annotation.Resource;import javax.jms.JMSException;/** * Created by Administrator on 2017/5/3. */@Servicepublic class ConsumerServiceImpl implements ConsumerService &#123; @Resource(name="jmsTemplate") private JmsTemplate jmsTemplate; @Override public TextMessage receive(Destination destination)&#123; TextMessage textMessage = (TextMessage) jmsTemplate.receive(destination); try&#123; System.out.println("从队列" + destination.toString() + "收到了消息：\t" + textMessage.getText()); &#125; catch (JMSException e) &#123; e.printStackTrace(); &#125; return textMessage; &#125;&#125; QueueMessageListener.java 1234567891011121314151617181920import javax.jms.JMSException;import javax.jms.Message;import javax.jms.MessageListener;import javax.jms.TextMessage;/** * Created by Administrator on 2017/5/3. */public class QueueMessageListener implements MessageListener &#123; public void onMessage(Message message) &#123; TextMessage tm = (TextMessage) message; try &#123; System.out.println("topicMessageListener监听到了文本消息：\t" + tm.getText()); //do something ... &#125; catch (JMSException e) &#123; e.printStackTrace(); &#125; &#125;&#125; QueueMessageListener1.java 1234567891011121314151617181920import javax.jms.JMSException;import javax.jms.Message;import javax.jms.MessageListener;import javax.jms.TextMessage;/** * Created by Administrator on 2017/5/3. */public class QueueMessageListener1 implements MessageListener &#123; public void onMessage(Message message) &#123; TextMessage tm = (TextMessage) message; try &#123; System.out.println("topicMessageListener1监听到了文本消息：\t" + tm.getText()); //do something ... &#125; catch (JMSException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 接下来是controller: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import cn.edu.hust.activemq.service.ConsumerService;import cn.edu.hust.activemq.service.ProducerService;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RequestMethod;import org.springframework.web.bind.annotation.ResponseBody;import javax.annotation.Resource;import javax.jms.Destination;import javax.jms.TextMessage;/** * Created by Administrator on 2017/5/3. */@Controllerpublic class MessageController &#123; private Logger logger = LoggerFactory.getLogger(MessageController.class); @Resource(name = "demoQueueDestination") private Destination destination; //队列消息生产者 @Resource private ProducerService producer; //队列消息消费者 @Resource private ConsumerService consumer; @RequestMapping(value = "/SendMessage", method = RequestMethod.GET) @ResponseBody public void send(String msg) &#123; logger.info(Thread.currentThread().getName()+"------------开始发送消息"); producer.sendMessage(msg); logger.info(Thread.currentThread().getName()+"------------发送完毕"); &#125; @RequestMapping(value= "/ReceiveMessage",method = RequestMethod.GET) @ResponseBody public Object receive()&#123; logger.info(Thread.currentThread().getName()+"------------开始接受消息"); TextMessage tm = consumer.receive(destination); logger.info(Thread.currentThread().getName()+"------------接受完毕"); return tm; &#125;&#125; 我们启动工程，在地址栏中输入：http://localhost:8080/SendMessage?msg=nihao， 代码很简单我就没有写前台页面啦，msg部分你可以随便写。回车之后我们去看一下控制台两个订阅者都接收到消息。]]></content>
      <categories>
        <category>activeMQ</category>
      </categories>
      <tags>
        <tag>学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netty学习(八)-Netty的心跳机制]]></title>
    <url>%2Fposts%2F9afa0af8.html</url>
    <content type="text"><![CDATA[我们知道在TCP长连接或者WebSocket长连接中一般我们都会使用心跳机制–即发送特殊的数据包来通告对方自己的业务还没有办完，不要关闭链接。那么心跳机制可以用来做什么呢？我们知道网络的传输是不可靠的，当我们发起一个链接请求的过程之中会发生什么事情谁都无法预料，或者断电，服务器重启，断网线之类。如果有这种情况的发生对方也无法判断你是否还在线。所以这时候我们引入心跳机制，在长链接中双方没有数据交互的时候互相发送数据(可能是空包，也可能是特殊数据)，对方收到该数据之后也回复相应的数据用以确保双方都在线，这样就可以确保当前链接是有效的。 1. 如何实现心跳机制 一般实现心跳机制由两种方式： TCP协议自带的心跳机制来实现； 在应用层来实现。 但是TCP协议自带的心跳机制系统默认是设置的是2小时的心跳频率。它检查不到机器断电、网线拔出、防火墙这些断线。而且逻辑层处理断线可能也不是那么好处理。另外该心跳机制是与TCP协议绑定的，那如果我们要是使用UDP协议岂不是用不了？所以一般我们都不用。 而一般我们自己实现呢大致的策略是这样的： Client启动一个定时器，不断发送心跳； Server收到心跳后，做出回应； Server启动一个定时器，判断Client是否存在，这里做判断有两种方法：时间差和简单标识。 时间差： 收到一个心跳包之后记录当前时间； 判断定时器到达时间，计算多久没收到心跳时间=当前时间-上次收到心跳时间。如果改时间大于设定值则认为超时。 简单标识： 收到心跳后设置连接标识为true; 判断定时器到达时间，如果未收到心跳则设置连接标识为false; 今天我们来看一下Netty的心跳机制的实现，在Netty中提供了IdleStateHandler类来进行心跳的处理，它可以对一个 Channel 的 读/写设置定时器, 当 Channel 在一定事件间隔内没有数据交互时(即处于 idle 状态), 就会触发指定的事件。 该类可以对三种类型的超时做心跳机制检测： 123public IdleStateHandler(int readerIdleTimeSeconds, int writerIdleTimeSeconds, int allIdleTimeSeconds) &#123; this((long)readerIdleTimeSeconds, (long)writerIdleTimeSeconds, (long)allIdleTimeSeconds, TimeUnit.SECONDS);&#125; readerIdleTimeSeconds：设置读超时时间； writerIdleTimeSeconds：设置写超时时间； allIdleTimeSeconds：同时为读或写设置超时时间； 下面我们还是通过一个例子来讲解IdleStateHandler的使用。 服务端： 12345678910111213141516171819202122232425262728293031public class HeartBeatServer &#123; private int port; public HeartBeatServer(int port) &#123; this.port = port; &#125; public void start()&#123; EventLoopGroup bossGroup = new NioEventLoopGroup(); EventLoopGroup workGroup = new NioEventLoopGroup(); ServerBootstrap server = new ServerBootstrap().group(bossGroup,workGroup) .channel(NioServerSocketChannel.class) .childHandler(new HeartBeatServerChannelInitializer()); try &#123; ChannelFuture future = server.bind(port).sync(); future.channel().closeFuture().sync(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;finally &#123; bossGroup.shutdownGracefully(); workGroup.shutdownGracefully(); &#125; &#125; public static void main(String[] args) &#123; HeartBeatServer server = new HeartBeatServer(7788); server.start(); &#125;&#125; 服务端Initializer： 1234567891011public class HeartBeatServerChannelInitializer extends ChannelInitializer&lt;SocketChannel&gt; &#123; @Override protected void initChannel(SocketChannel socketChannel) throws Exception &#123; ChannelPipeline pipeline = socketChannel.pipeline(); pipeline.addLast("handler",new IdleStateHandler(3, 0, 0, TimeUnit.SECONDS)); pipeline.addLast("decoder", new StringDecoder()); pipeline.addLast("encoder", new StringEncoder()); pipeline.addLast(new HeartBeatServerHandler()); &#125;&#125; 在这里IdleStateHandler也是handler的一种，所以加入addLast。我们分别设置4个参数：读超时时间为3s，写超时和读写超时为0，然后加入时间控制单元。 服务端handler： 12345678910111213141516171819202122232425262728293031public class HeartBeatServerHandler extends ChannelInboundHandlerAdapter&#123; private int loss_connect_time = 0; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; System.out.println(ctx.channel().remoteAddress() + "Server :" + msg.toString()); &#125; @Override public void userEventTriggered(ChannelHandlerContext ctx, Object evt) throws Exception &#123; if(evt instanceof IdleStateEvent)&#123; //服务端对应着读事件，当为READER_IDLE时触发 IdleStateEvent event = (IdleStateEvent)evt; if(event.state() == IdleState.READER_IDLE)&#123; loss_connect_time++; System.out.println("接收消息超时"); if(loss_connect_time &gt; 2)&#123; System.out.println("关闭不活动的链接"); ctx.channel().close(); &#125; &#125;else&#123; super.userEventTriggered(ctx,evt); &#125; &#125; &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; ctx.close(); &#125;&#125; 我们看到在handler中调用了userEventTriggered方法，IdleStateEvent的state()方法一个有三个值： READER_IDLE，WRITER_IDLE，ALL_IDLE。正好对应读事件写事件和读写事件。 再来写一下客户端： 123456789101112131415161718192021222324252627282930313233public class HeartBeatsClient &#123; private int port; private String address; public HeartBeatsClient(int port, String address) &#123; this.port = port; this.address = address; &#125; public void start()&#123; EventLoopGroup group = new NioEventLoopGroup(); Bootstrap bootstrap = new Bootstrap(); bootstrap.group(group) .channel(NioSocketChannel.class) .handler(new HeartBeatsClientChannelInitializer()); try &#123; ChannelFuture future = bootstrap.connect(address,port).sync(); future.channel().closeFuture().sync(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;finally &#123; group.shutdownGracefully(); &#125; &#125; public static void main(String[] args) &#123; HeartBeatsClient client = new HeartBeatsClient(7788,"127.0.0.1"); client.start(); &#125;&#125; 客户端Initializer： 1234567891011public class HeartBeatsClientChannelInitializer extends ChannelInitializer&lt;SocketChannel&gt; &#123; protected void initChannel(SocketChannel socketChannel) throws Exception &#123; ChannelPipeline pipeline = socketChannel.pipeline(); pipeline.addLast("handler", new IdleStateHandler(0, 3, 0, TimeUnit.SECONDS)); pipeline.addLast("decoder", new StringDecoder()); pipeline.addLast("encoder", new StringEncoder()); pipeline.addLast(new HeartBeatClientHandler()); &#125;&#125; 这里我们设置了IdleStateHandler的写超时为3秒，客户端执行的动作为写消息到服务端，服务端执行读动作。 客户端handler: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public class HeartBeatClientHandler extends ChannelInboundHandlerAdapter &#123; private static final ByteBuf HEARTBEAT_SEQUENCE = Unpooled.unreleasableBuffer(Unpooled.copiedBuffer("Heartbeat", CharsetUtil.UTF_8)); private static final int TRY_TIMES = 3; private int currentTime = 0; @Override public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; System.out.println("激活时间是："+new Date()); System.out.println("链接已经激活"); ctx.fireChannelActive(); &#125; @Override public void channelInactive(ChannelHandlerContext ctx) throws Exception &#123; System.out.println("停止时间是："+new Date()); System.out.println("关闭链接"); &#125; @Override public void userEventTriggered(ChannelHandlerContext ctx, Object evt) throws Exception &#123; System.out.println("当前轮询时间："+new Date()); if (evt instanceof IdleStateEvent) &#123; IdleStateEvent event = (IdleStateEvent) evt; if (event.state() == IdleState.WRITER_IDLE) &#123; if(currentTime &lt;= TRY_TIMES)&#123; System.out.println("currentTime:"+currentTime); currentTime++; ctx.channel().writeAndFlush(HEARTBEAT_SEQUENCE.duplicate()); &#125; &#125; &#125; &#125; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; String message = (String) msg; System.out.println(message); if (message.equals("Heartbeat")) &#123; ctx.write("has read message from server"); ctx.flush(); &#125; ReferenceCountUtil.release(msg); &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; ctx.close(); &#125;&#125; 启动服务端和客户端我们看到输出为： 我们再来屡一下思路： 首先客户端激活channel，因为客户端中并没有发送消息所以会触发客户端的IdleStateHandler，它设置的写超时时间为3s； 然后触发客户端的事件机制进入userEventTriggered方法，在触发器中计数并向客户端发送消息； 服务端接收消息； 客户端触发器继续轮询发送消息，直到计数器满不再向服务端发送消息； 服务端在IdleStateHandler设置的读消息超时时间5s内未收到消息，触发了服务端中handler的userEventTriggered方法，于是关闭客户端的链接。 大体我们的简单心跳机制就是这样的思路，通过事件触发机制以及计数器的方式来实现，上面我们的案例中最后客户端没有发送消息的时候我们是强制断开了客户端的链接，那么既然可以关闭，我们是不是也可是重新链接客户端呢？因为万一客户端本身并不想关闭而是由于别的原因导致他无法与服务端通信。下面我们来说一下重连机制。 当我们的服务端在未读到客户端消息超时而关闭客户端的时候我们一般在客户端的finally块中方的是关闭客户端的代码，这时我们可以做一下修改的，finally是一定会被执行新的，所以我们可以在finally块中重新调用一下启动客户端的代码，这样就又重新启动了客户端了，上客户端代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748/** * 本Client为测试netty重连机制 * Server端代码都一样，所以不做修改 * 只用在client端中做一下判断即可 */public class HeartBeatsClient2 &#123; private int port; private String address; ChannelFuture future; public HeartBeatsClient2(int port, String address) &#123; this.port = port; this.address = address; &#125; public void start()&#123; EventLoopGroup group = new NioEventLoopGroup(); Bootstrap bootstrap = new Bootstrap(); bootstrap.group(group) .channel(NioSocketChannel.class) .handler(new HeartBeatsClientChannelInitializer()); try &#123; future = bootstrap.connect(address,port).sync(); future.channel().closeFuture().sync(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;finally &#123; //group.shutdownGracefully(); if (null != future) &#123; if (future.channel() != null &amp;&amp; future.channel().isOpen()) &#123; future.channel().close(); &#125; &#125; System.out.println("准备重连"); start(); System.out.println("重连成功"); &#125; &#125; public static void main(String[] args) &#123; HeartBeatsClient2 client = new HeartBeatsClient2(7788,"127.0.0.1"); client.start(); &#125;&#125; 其余部分的代码与上面的实例并无异同，只需改造客户端即可，我们再运行服务端和客户端会看到客户端虽然被关闭了，但是立马又被重启： 当然生产级别的代码应该不是这样实现的吧，哈哈，下一节我们再好好探讨。]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java并发编程（三）----线程的同步]]></title>
    <url>%2Fposts%2F71e4d230.html</url>
    <content type="text"><![CDATA[在现实开发中，我们或多或少的都经历过这样的情景：某一个变量被多个用户并发式的访问并修改，如何保证该变量在并发过程中对每一个用户的正确性呢？今天我们来聊聊线程同步的概念。 一般来说，程序并行化是为了获得更高的执行效率，但前提是，高效率不能以牺牲正确性为代价。如果程序并行化后， 连基本的执行结果的正确性都无法保证， 那么并行程序本身也就没有任何意义了。因此， 线程安全就是并行程序的根本和根基。解决这些问题从临界区的概念开始。临界区是访问一个共享资源在同一时间不能被超过一个线程执行的代码块。 java为我们提供了同步机制，帮助程序员实现临界区。当一个线程想要访问一个临界区,它使用其中的一个同步机制来找出是否有任何其他线程执行临界区。如果没有，这个线程就进入临界区。否则，这个线程通过同步机制暂停直到另一个线程执行完临界区。当多个线程正在等待一个线程完成执行的一个临界区，JVM选择其中一个线程执行，其余的线程会等待直到轮到它们。临界区有如下的规则： 如果有若干进程要求进入空闲的临界区，一次仅允许一个进程进入。 任何时候，处于临界区内的进程不可多于一个。如已有进程进入自己的临界区，则其它所有试图进入临界区的进程必须等待。 进入临界区的进程要在有限时间内退出，以便其它进程能及时进入自己的临界区。 如果进程不能进入自己的临界区，则应让出CPU，避免进程出现“忙等”现象。 java语言为解决同步问题帮我们提供了两种机制来实现： 1. synchronized关键字； 2. Lock锁及其实现； synchronized的作用 关键字synchronized 的作用是实现线程间的同步。它的工作是对同步的代码加锁，使得每一次， 只能有一个线程进入同步块，从而保证线程间的安全性。 关键宇synchronized 可以有多种用法。这里做一个简单的整理。 · 指定加锁对象: 对给定对象加锁，进入同步代码前要获得给定对象的锁。 · 直接作用于实例方法: 相当于对当前实例加锁，进入同步代码前要获得当前实例的锁。 . 直接作用于静态方法: 相当于对当前类加锁， 进入同步代码前要获得当前类的锁。 1.给指定对象加锁： 123456789101112131415161718192021222324252627282930313233343536public class AccountingSync implements Runnable&#123; static AccountingSync instance=new AccountingSync() ; static int i =O; @Override public void run() ( for(int j=O; j&lt;lOOOOOOO; j++) &#123; synchronized (instance) &#123; //对象锁 i++ ; &#125; &#125; &#125; public static void main(String[] args) throws InterruptedException ( Thread t1=new Thread(instance); Thread t2=new Thread(instance); t1.start(); t2.start(); t1.join(); t2.join(); System.out.println(i); &#125; /* public static void main(String[] args) throws InterruptedException ( Thread t1=new Thread(new AccountingSync()); Thread t2=new Thread(new AccountingSync()); t1.start(); t2.start(); t1.join(); t2.join(); System.out.println(i); &#125; */&#125; 知道我为什么要给出两个main方法让大家参考吗？上述锁对象是锁定AccountingSync实例对象。第一个main方法中t1 和 t2 两个线程同时指向了instance实例，所以第7行的锁对象synchronized (instance)在线程t1 和 线程 t2 获得锁的时候是获取同一个对象的，这个时候的锁是同一把锁。但是在第二个main方法中我们可以看到线程t1 和 线程 t2分别对应的是两个不同的AccountingSync对象，这时候锁对象获得的是不同的AccountingSync实例，安全性是没有保证的，大家可以动手尝试一下。 2.直接作用于实例方法： 12345678910111213141516171819202122232425262728293031323334public class TestSynchronized &#123; public static void main(String[] args) &#123; Tester2 a1 = new Tester2(); Th t1 = new Th(a1); t1.start(); Th t2 = new Th(a1); t2.start(); &#125;&#125;class Tester2 &#123; public synchronized void say(String name) throws InterruptedException&#123; for(int i = 0;i&lt;5;i++)&#123; Thread.sleep(1000); System.out.println(); System.out.println(name +","+i+new Date().toLocaleString() ); &#125; &#125;&#125;class Th extends Thread&#123; Tester2 test; public Th(Tester2 test1)&#123; test = test1; &#125; public void run()&#123; try &#123; test.say(Thread.currentThread().getName()); &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125;&#125;&#125; 对Tester2类中的方法使用synchronized很好理解，同一时刻如果t1正在调用say()方法，在他没有执行完毕并退出方法之前其余的线程是无法获得该方法的。只能排队等待知道t1执行完毕。 3.作用于静态方法： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475public class Test1 &#123; public static void main(String[] args) &#123; for(int i=0;i&lt;50;i++)&#123; Thread t1 = new Thread(new Sale(5)); Thread t2 = new Thread(new Producted(5)); t1.start(); t2.start(); &#125; &#125; &#125; class Shop&#123; static int a = 40; synchronized static void shopping(int b)&#123; a -= b; System.out.println("售出 "+b+" 张大饼，"+"还剩 "+a+" 张大饼"); &#125; synchronized static void factory(int c)&#123; a += c; System.out.println("仓库还有 "+a+" 张大饼"); &#125; &#125; class Sale implements Runnable&#123; int b = 0; public Sale(int b)&#123; this.b = b; &#125; @Override public void run() &#123; if(b&lt;0)&#123; Thread.interrupted(); &#125; Shop.shopping(b); try &#123; Thread.sleep(1000); Shop.factory(b-5); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; class Producted implements Runnable&#123; int b = 0; public Producted(int b)&#123; this.b = b; &#125; @Override public void run() &#123; Shop.factory(b); try &#123; Thread.sleep(1000); Shop.shopping(b-5); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; ``` 静态方法前加synchronized这个锁等价于锁住了当前类的class对象，因为静态方法或者是静态关键字在本质上是一个类对象，而不是成员对象，在内存中位于方法区被所有的实例共享。即等同于synchronized(Shop.class)。我们需要注意的是锁住了类并不代表锁住了类所在的对象，类本身也是一种对象。它与类的实例是完全不同的两个对象，在加锁时不是相互依赖的，即对类加锁并不与上面例子中的加锁互斥，锁住了子类或子类的对象与锁住父类或父类的对象是不相关的。synchronized的使用其实主要是前面两种，对象锁和方法锁，静态方法锁我们并不常用到。其余的操作方式都是在这两种的基础上演变而来，比如大家经常说的“块级锁”：```javasynchronized(object)&#123; //代码内容&#125; 锁住的其实并不是代码块，而是object这个对象，所以如果在其他的代码中 也发生synchronized(object)时就会发生互斥。我们为什么要研究这些呢，因为如果我们不知道我们锁住的是什么，就不清楚锁住了多大范围的内容，自然就不知道是否锁住了想要得到互斥的效果，同时也不知道如何去优化锁的使用。 因此java中的synchronized就真正能做到临界区的效果，在临界区内多个线程的操作绝对是串行的，这一点java绝对可以保证。同时synchronized造成的开销也是很大的，我们如果无法掌握好他的粒度控制，就会导致频繁的锁征用，进入悲观锁状态。 ####volatile----轻量级的synchronized 既然我们说到了synchronized那就不得不提到volatile，在java中synchronized是控制并发的，我们知道在我们对一个变量执行赋值操作的时候比如：i++，在执行完毕之后i的结果其实是写到缓存中的它并没有及时的写入到内存，后续在某些情况下（比如cpu缓存不够）再将cpu缓存写入内存，假设A线程正在执行i操作，而此时B线程也来执行。B在执行i之前是不会自己跑到缓存中去取变量的值的，它只会去内存中读取i，很显然i的值是没有被更新的，为了防止这种情况出现，volatile应运而生。 Java语言规范第三版中对volatile的定义如下： java编程语言允许线程访问共享变量，为了确保共享变量能被准确和一致的更新，线程应该确保通过排他锁单独获得这个变量。Java语言提供了volatile，在某些情况下比锁更加方便。如果一个字段被声明成volatile，java线程内存模型确保所有线程看到这个变量的值是一致的。 我们来看一个例子： 1234567891011121314151617181920212223242526public class TestWithoutVolatile &#123; private static boolean bChanged; public static void main(String[] args) throws InterruptedException &#123; new Thread() &#123; @Override public void run() &#123; for (;;) &#123; if (bChanged == !bChanged) &#123; System.out.println("!="); System.exit(0); &#125; &#125; &#125; &#125;.start(); Thread.sleep(1); new Thread() &#123; @Override public void run() &#123; for (;;) &#123; bChanged = !bChanged; &#125; &#125; &#125;.start(); &#125; &#125; 在上例中我们如果多次运行会出现两种结果，一种是正常打印：&quot;!=&quot;,还有一种就是程序会陷入死循环。但是我们如果给bChanged前面加上volatile的话则每次都会打印出&quot;!=&quot;,请读者朋友们下去可以尝试。 在此处没有加volatile之前之所以会出现有时可以出现正确结果有时则卡死的原因就在于两个线程同时在运行的过程中双方都在操作bChanged变量，但是该变量的值对于同时在使用它的另一个线程来说并不总是可见的，运气好的时候线程修改完值之后就写入主存，运气不好的时候线程只在缓存中更新了值并未写入主存。但是在加了volatile修饰之后效果则不同，因为volatile可以保证变量的可见性。 说到可见性，我们来看一幅图： 每一个线程都有相应的工作内存，工作内存中有一份主内存变量的副本，线程对变量的操作都在工作内存中进行（避免再次访问主内存，提高性能），不同线程不能访问彼此的工作内存，而通过将操作后的值刷新到主内存来进行彼此的交互，这就会带来一个变量值对其他线程的可见性问题。当一个任务在工作内存中变量值进行改变，其他任务对此是不可见的，导致每一个线程都有一份不同的变量副本。而volatile恰恰可以解决这个可见性的问题，当变量被volatile修饰，如private volatile int stateFlag = 0; 它将直接通过主内存中被读取或者写入，线程从主内存中加载的值将是最新的。 但是volatile的使用有着严格的限制，当对变量的操作依赖于以前值（如i++）,或者其值被其他字段的值约束，这个时候volatile是无法实现线程安全的。被volatile修饰的变量必须独立于程序的其他状态。因为volatile只是保证了变量的可见性，并不能保证操作的原子性，所谓原子性，即有“不可分”的意思，如对基本数据类型(java中排除long和double)的赋值操作a=6,如返回操作return a，这些操作都不会被线程调度器中断，同一时刻只有一个线程对它进行操作。 看以下代码： 123456789101112131415161718192021222324252627public class Counter &#123; public volatile static int count = 0; public static void inc() &#123; //这里延迟1毫秒，使得结果明显 try &#123; Thread.sleep(1); &#125; catch (InterruptedException e) &#123; &#125; count++; &#125; public static void main(String[] args) &#123; //同时启动1000个线程，去进行i++计算，看看实际结果 for (int i = 0; i &lt; 1000; i++) &#123; new Thread(new Runnable() &#123; @Override public void run() &#123; Counter.inc(); &#125; &#125;).start(); &#125; //这里每次运行的值都有可能不同,可能为1000 System.out.println("运行结果:Counter.count=" + Counter.count); &#125;&#125; 运行上面的例子我们可以发现每次运行的结果都不一样，预期结果应该是1000，尽管counter被volatile修饰，保证了可见性，但是counter++并不是一个原子性操作，它被拆分为读取和写入两部分操作，我们需要用synchronized修饰： 123publicstaticsynchronizedvoid incNum() &#123; counter++;&#125; 此时每次运行结果都是1000，实现了线程安全。synchronized是一种独占锁，它对一段操作或内存进行加锁，当线程要操作被synchronized修饰的内存或操作时，必须首先获得锁才能进行后续操作；但是在同一时刻只能有一个线程获得相同的一把锁，所以它只允许一个线程进行操作。synchronized同样能够将变量最新值刷新到主内存，当一个变量只被synchronized方法操作时,是没有必要用volatile修饰的，所以我们接着把变量声明修改为： 1private static int counter; 多次运行结果依旧是1000。 说明： 上例中如果你按照上面这样改完之后其实结果并是不1000，我多次运行的结果都是先打印出&quot;运行结果:Counter.count=0&quot;,然后线程卡死。究其原因，我猜可能是第一个线程等待一秒再执行count++，然后后面的线程在这个等待过程中等不及的原因。java线程的运行具有不确定性，不能保证线程会按部就班的顺序执行，所以会出现什么样的后果很难预测。 正确结果代码如下： 1234567891011121314151617181920public class Counter &#123; public static int count = 0; public synchronized static void inc() &#123; count++; &#125; public static void main(String[] args) &#123; //同时启动1000个线程，去进行i++计算，看看实际结果 for (int i = 0; i &lt; 1000; i++) &#123; new Thread(new Runnable() &#123; @Override public void run() &#123; Counter.inc(); &#125; &#125;).start(); &#125; //这里每次运行的值都有可能不同,可能为1000 System.out.println("运行结果:Counter.count=" + Counter.count); &#125;&#125; 综上所述，由于volatile只能保证变量对多个线程的可见性，但不能保证原子性，它的同步机制是比较脆弱的，它在使用过程中有着诸多限制，对使用者也有更高的要求，相对而言，synchronized锁机制是比较安全的同步机制，有时候出于提高性能的考虑，可以利用volatile对synchronized进行代替和优化，但前提是你必须充分理解其使用场景和涵义。 下一节我们接着分析Lock锁。]]></content>
      <categories>
        <category>多线程与并发编程</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java学习-NIO(三)Channel]]></title>
    <url>%2Fposts%2F12968bae.html</url>
    <content type="text"><![CDATA[通道（Channel）是java.nio的第二个主要创新。它们既不是一个扩展也不是一项增强，而是全新、极好的Java I/O示例，提供与I/O服务的直接连接。Channel用于在字节缓冲区和位于通道另一侧的实体（通常是一个文件或套接字）之间有效地传输数据。 channel介绍 通道是访问I/O服务的导管。I/O可以分为广义的两大类别：File I/O和Stream I/O。那么相应地有两种类型的通道也就不足为怪了，它们是文件（file）通道和套接字（socket）通道。我们看到在api里有一个FileChannel类和三个socket通道类：SocketChannel、ServerSocketChannel和DatagramChannel。 通道可以以多种方式创建。Socket通道有可以直接创建新socket通道的工厂方法。但是一个FileChannel对象却只能通过在一个打开的RandomAccessFile、FileInputStream或FileOutputStream对象上调用getChannel( )方法来获取。你不能直接创建一个FileChannel对象。 我们先来看一下FileChannel的用法： 1234567891011121314151617181920212223242526272829303132333435// 创建文件输出字节流FileOutputStream fos = new FileOutputStream("data.txt");//得到文件通道FileChannel fc = fos.getChannel();//往通道写入ByteBufferfc.write(ByteBuffer.wrap("Some text ".getBytes()));//关闭流fos.close();//随机访问文件RandomAccessFile raf = new RandomAccessFile("data.txt", "rw");//得到文件通道fc = raf.getChannel();//设置通道的文件位置 为末尾fc.position(fc.size()); //往通道写入ByteBufferfc.write(ByteBuffer.wrap("Some more".getBytes()));//关闭raf.close();//创建文件输入流FileInputStream fs = new FileInputStream("data.txt");//得到文件通道fc = fs.getChannel();//分配ByteBuffer空间大小ByteBuffer buff = ByteBuffer.allocate(BSIZE);//从通道中读取ByteBufferfc.read(buff);//调用此方法为一系列通道写入或相对获取 操作做好准备buff.flip();//从ByteBuffer从依次读取字节并打印while (buff.hasRemaining())&#123; System.out.print((char) buff.get());&#125;fs.close(); 再来看一下SocketChannel： 12345SocketChannel sc = SocketChannel.open( );sc.connect (new InetSocketAddress ("somehost", someport)); ServerSocketChannel ssc = ServerSocketChannel.open( ); ssc.socket( ).bind (new InetSocketAddress (somelocalport)); DatagramChannel dc = DatagramChannel.open( ); 可以设置 SocketChannel 为非阻塞模式（non-blocking mode）.设置之后，就可以在异步模式下调用connect(), read() 和write()了。如果SocketChannel在非阻塞模式下，此时调用connect()，该方法可能在连接建立之前就返回了。为了确定连接是否建立，可以调用finishConnect()的方法。像这样： 123456socketChannel.configureBlocking(false);socketChannel.connect(new InetSocketAddress("http://jenkov.com", 80)); while(! socketChannel.finishConnect() )&#123; //wait, or do something else...&#125; 服务器端的使用经常会考虑到非阻塞socket通道，因为它们使同时管理很多socket通道变得更容易。但是，在客户端使用一个或几个非阻塞模式的socket通道也是有益处的，例如，借助非阻塞socket通道，GUI程序可以专注于用户请求并且同时维护与一个或多个服务器的会话。在很多程序上，非阻塞模式都是有用的。 调用finishConnect( )方法来完成连接过程，该方法任何时候都可以安全地进行调用。假如在一个非阻塞模式的SocketChannel对象上调用finishConnect( )方法，将可能出现下列情形之一： connect( )方法尚未被调用。那么将产生NoConnectionPendingException异常。 连接建立过程正在进行，尚未完成。那么什么都不会发生，finishConnect( )方法会立即返回false值。 在非阻塞模式下调用connect( )方法之后，SocketChannel又被切换回了阻塞模式。那么如果有必要的话，调用线程会阻塞直到连接建立完成，finishConnect( )方法接着就会返回true值。在初次调用connect( )或最后一次调用finishConnect( )之后，连接建立过程已经完成。那么SocketChannel对象的内部状态将被更新到已连接状态，finishConnect( )方法会返回true值，然后SocketChannel对象就可以被用来传输数据了。 连接已经建立。那么什么都不会发生，finishConnect( )方法会返回true值。 Socket通道是线程安全的。并发访问时无需特别措施来保护发起访问的多个线程，不过任何时候都只有一个读操作和一个写操作在进行中。请记住，sockets是面向流的而非包导向的。它们可以保证发送的字节会按照顺序到达但无法承诺维持字节分组。某个发送器可能给一个socket写入了20个字节而接收器调用read( )方法时却只收到了其中的3个字节。剩下的17个字节还是传输中。由于这个原因，让多个不配合的线程共享某个流socket的同一侧绝非一个好的设计选择。 最后再看一下DatagramChannel： 最后一个socket通道是DatagramChannel。正如SocketChannel对应Socket，ServerSocketChannel对应ServerSocket，每一个DatagramChannel对象也有一个关联的DatagramSocket对象。不过原命名模式在此并未适用：“DatagramSocketChannel”显得有点笨拙，因此采用了简洁的“DatagramChannel”名称。 正如SocketChannel模拟连接导向的流协议（如TCP/IP），DatagramChannel则模拟包导向的无连接协议（如UDP/IP）： 创建DatagramChannel的模式和创建其他socket通道是一样的：调用静态的open( )方法来创建一个新实例。新DatagramChannel会有一个可以通过调用socket( )方法获取的对等DatagramSocket对象。DatagramChannel对象既可以充当服务器（监听者）也可以充当客户端（发送者）。如果你希望新创建的通道负责监听，那么通道必须首先被绑定到一个端口或地址/端口组合上。绑定DatagramChannel同绑定一个常规的DatagramSocket没什么区别，都是委托对等socket对象上的API实现的： 123DatagramChannel channel = DatagramChannel.open( );DatagramSocket socket = channel.socket( ); socket.bind (new InetSocketAddress (portNumber)); DatagramChannel是无连接的。每个数据报（datagram）都是一个自包含的实体，拥有它自己的目的地址及不依赖其他数据报的数据净荷。与面向流的的socket不同，DatagramChannel可以发送单独的数据报给不同的目的地址。同样，DatagramChannel对象也可以接收来自任意地址的数据包。每个到达的数据报都含有关于它来自何处的信息（源地址）。 一个未绑定的DatagramChannel仍能接收数据包。当一个底层socket被创建时，一个动态生成的端口号就会分配给它。绑定行为要求通道关联的端口被设置为一个特定的值（此过程可能涉及安全检查或其他验证）。不论通道是否绑定，所有发送的包都含有DatagramChannel的源地址（带端口号）。未绑定的DatagramChannel可以接收发送给它的端口的包，通常是来回应该通道之前发出的一个包。已绑定的通道接收发送给它们所绑定的熟知端口（wellknown port）的包。数据的实际发送或接收是通过send( )和receive( )方法来实现的。 **注意：**假如您提供的ByteBuffer没有足够的剩余空间来存放您正在接收的数据包，没有被填充的字节都会被悄悄地丢弃。 Scatter/Gather 通道提供了一种被称为Scatter/Gather的重要新功能（有时也被称为矢量I/O）。它是指在多个缓冲区上实现一个简单的I/O操作。对于一个write操作而言，数据是从几个缓冲区按顺序抽取（称为gather）并沿着通道发送的。缓冲区本身并不需要具备这种gather的能力（通常它们也没有此能力）。该gather过程的效果就好比全部缓冲区的内容被连结起来，并在发送数据前存放到一个大的缓冲区中。对于read操作而言，从通道读取的数据会按顺序被散布（称为scatter）到多个缓冲区，将每个缓冲区填满直至通道中的数据或者缓冲区的最大空间被消耗完。 scatter / gather经常用于需要将传输的数据分开处理的场合，例如传输一个由消息头和消息体组成的消息，你可能会将消息体和消息头分散到不同的buffer中，这样你可以方便的处理消息头和消息体。 Scattering Reads是指数据从一个channel读取到多个buffer中。如下图描述： 代码示例如下： 1234ByteBuffer header = ByteBuffer.allocateDirect (10); ByteBuffer body = ByteBuffer.allocateDirect (80); ByteBuffer [] buffers = &#123; header, body &#125;; int bytesRead = channel.read (buffers); Gathering Writes是指数据从多个buffer写入到同一个channel。如下图描述： 代码示例如下： 1234ByteBuffer header = ByteBuffer.allocateDirect (10); ByteBuffer body = ByteBuffer.allocateDirect (80); ByteBuffer [] buffers = &#123; header, body &#125;; channel.write(bufferArray); 使用得当的话，Scatter/Gather会是一个极其强大的工具。它允许你委托操作系统来完成辛苦活：将读取到的数据分开存放到多个存储桶（bucket）或者将不同的数据区块合并成一个整体。这是一个巨大的成就，因为操作系统已经被高度优化来完成此类工作了。它节省了您来回移动数据的工作，也就避免了缓冲区拷贝和减少了您需要编写、调试的代码数量。既然您基本上通过提供数据容器引用来组合数据，那么按照不同的组合构建多个缓冲区阵列引用，各种数据区块就可以以不同的方式来组合了。下面的例子好地诠释了这一点： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364public class GatheringTest &#123; private static final String DEMOGRAPHIC = "output.txt"; public static void main (String [] argv) throws Exception &#123; int reps = 10; if (argv.length &gt; 0) &#123; reps = Integer.parseInt(argv[0]); &#125; FileOutputStream fos = new FileOutputStream(DEMOGRAPHIC); GatheringByteChannel gatherChannel = fos.getChannel(); ByteBuffer[] bs = utterBS(reps); while (gatherChannel.write(bs) &gt; 0) &#123; // 不做操作，让通道把数据输出到文件写完 &#125; System.out.println("Mindshare paradigms synergized to " + DEMOGRAPHIC); fos.close(); &#125; private static String [] col1 = &#123; "Aggregate", "Enable", "Leverage", "Facilitate", "Synergize", "Repurpose", "Strategize", "Reinvent", "Harness" &#125;; private static String [] col2 = &#123; "cross-platform", "best-of-breed", "frictionless", "ubiquitous", "extensible", "compelling", "mission-critical", "collaborative", "integrated" &#125;; private static String [] col3 = &#123; "methodologies", "infomediaries", "platforms", "schemas", "mindshare", "paradigms", "functionalities", "web services", "infrastructures" &#125;; private static String newline = System.getProperty ("line.separator"); private static ByteBuffer [] utterBS (int howMany) throws Exception &#123; List list = new LinkedList(); for (int i = 0; i &lt; howMany; i++) &#123; list.add(pickRandom(col1, " ")); list.add(pickRandom(col2, " ")); list.add(pickRandom(col3, newline)); &#125; ByteBuffer[] bufs = new ByteBuffer[list.size()]; list.toArray(bufs); return (bufs); &#125; private static Random rand = new Random( ); /** * 随机生成字符 * @param strings * @param suffix * @return * @throws Exception */ private static ByteBuffer pickRandom (String [] strings, String suffix) throws Exception &#123; String string = strings [rand.nextInt (strings.length)]; int total = string.length() + suffix.length( ); ByteBuffer buf = ByteBuffer.allocate (total); buf.put (string.getBytes ("US-ASCII")); buf.put (suffix.getBytes ("US-ASCII")); buf.flip( ); return (buf); &#125;&#125; 输出为： Reinvent integrated web services Aggregate best-of-breed platforms Harness frictionless platforms Repurpose extensible paradigms Facilitate ubiquitous methodologies Repurpose integrated methodologies Facilitate mission-critical paradigms Synergize compelling methodologies Reinvent compelling functionalities Facilitate extensible platforms 虽然这种输出没有什么意义，但是gather确是很容易的让我们把它输出出来。 Pipe java.nio.channels包中含有一个名为Pipe（管道）的类。广义上讲，管道就是一个用来在两个实体之间单向传输数据的导管。 Java NIO 管道是2个线程之间的单向数据连接。Pipe有一个source通道和一个sink通道。数据会被写到sink通道，从source通道读取。Pipe类创建一对提供环回机制的Channel对象。这两个通道的远端是连接起来的，以便任何写在SinkChannel对象上的数据都能出现在SourceChannel对象上。 下面我们来创建一条Pipe，并向Pipe中写数据： 123456789101112131415//通过Pipe.open()方法打开管道Pipe pipe = Pipe.open();//要向管道写数据，需要访问sink通道Pipe.SinkChannel sinkChannel = pipe.sink();//通过调用SinkChannel的write()方法，将数据写入SinkChannelString newData = "New String to write to file..." + System.currentTimeMillis();ByteBuffer buf = ByteBuffer.allocate(48);buf.clear();buf.put(newData.getBytes());buf.flip();while(buf.hasRemaining()) &#123; sinkChannel.write(buf);&#125; 再看如何从管道中读取数据： 读取管道的数据，需要访问source通道： 1Pipe.SourceChannel sourceChannel = pipe.source(); 调用source通道的read()方法来读取数据： 12ByteBuffer buf = ByteBuffer.allocate(48);int bytesRead = sourceChannel.read(buf); read()方法返回的int值会告诉我们多少字节被读进了缓冲区。 到此我们就把通道的简单用法讲完了，要想会用还是得多去练习，多模拟使用，这样才知道什么时候用以及怎么用，下节我们来讲选择器-Selectors。]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[消息中间件-activemq实战之整合Spring(四)]]></title>
    <url>%2Fposts%2F64e3c39e.html</url>
    <content type="text"><![CDATA[前面的理论准备已经很充分，这一节我们来实战：将activemq整合到Spring框架才行中，因为Spring已经集成了JMS，这也为我们配置activermq带来了方便。 1. Spring对jms的支持 因为Spring已经将JMS集成到框架里面了，对jms做了自己的封装，我们使用起来更加方便，在Spring中使用jms比较麻烦的就是配置，在Spring中配置JMS大体需要8个部分： ConnectionFactory： 和jms服务器的连接, 可以是外部的jms server, 也可以使用embedded ActiveMQ Broker； Destination： 有topic和queue两种方式； JmsTemplate： spring提供的jms模板； MessageConverter： 消息转换器； MessageProducer： 消息生产者； MessageConsumer： 消息消费者； MessageListener： 消息监听器； MessageListenerContainer： 消息监听容器。 下面我把完整的配置文件按照上面的步骤拆开分别讲解： 1.1首先我们配置ConnectionFactory： 1234&lt;amq:connectionFactory id="amqConnectionFactory" brokerURL="tcp://127.0.0.1:61616" userName="admin" password="admin" /&gt; brokerURL是指要连接的activeMQ server的地址，该配置即使用activemq独立的消息存储环境，即使服务器重启消息也不会丢失。 123456&lt;!-- 配置JMS连接工厂 --&gt;&lt;bean id="connectionFactory" class="org.springframework.jms.connection.CachingConnectionFactory"&gt; &lt;constructor-arg ref="amqConnectionFactory" /&gt; &lt;property name="sessionCacheSize" value="100" /&gt;&lt;/bean&gt; 我们从Spring给我们提供的connectionFactory中获取Connection，并且把该connectionFactory注册到上面定义的activemq server中。 1.2 Destination： 由前面我们知道Destination有两种形式：P2P和Pub/Sub。那么在配置中表示就是： 1234567&lt;!-- 定义消息队列（Queue） --&gt;&lt;bean id="demoQueueDestination" class="org.apache.activemq.command.ActiveMQQueue"&gt; &lt;!-- 设置消息队列的名字 --&gt; &lt;constructor-arg&gt; &lt;value&gt;first-queue&lt;/value&gt; &lt;/constructor-arg&gt;&lt;/bean&gt; 或： 1234567&lt;!-- 定义消息队列（topic） --&gt;&lt;bean id="demoQueueDestination" class="org.apache.activemq.command.ActiveMQTopic"&gt; &lt;!-- 设置消息队列的名字 --&gt; &lt;constructor-arg&gt; &lt;value&gt;first-queue&lt;/value&gt; &lt;/constructor-arg&gt;&lt;/bean&gt; 1.3 JmsTemplate： 将connectionFactory和defaultDestination注入JmsTemplate中： 12345678&lt;!-- 配置JMS模板（Queue），Spring提供的JMS工具类，它发送、接收消息。 --&gt;&lt;bean id="jmsTemplate" class="org.springframework.jms.core.JmsTemplate"&gt; &lt;property name="connectionFactory" ref="connectionFactory" /&gt; &lt;property name="defaultDestination" ref="demoQueueDestination" /&gt; &lt;property name="receiveTimeout" value="10000" /&gt; &lt;!-- true是topic，false是queue，默认是false，此处显示写出false --&gt; &lt;property name="pubSubDomain" value="false" /&gt;&lt;/bean&gt; 在Java相关处理文件中添加(这里用的是@Inject注解，当然也可以用@Autowired)： 1234@Resource(name="jmsTemplate")private JmsTemplate jmsTemplate;TextMessage textMessage = (TextMessage) jmsTemplate.receive(destination); 1.4 MessageConverter MessageConverter实现的是org.springframework.jms.support.converter.MessageConverter接口, 提供消息的转换功能。 1&lt;bean id="defaultMessageConverter" class="cn.edu.hust.activemq.filter.DefaultMessageConverter" /&gt; 1.5 MessageProducer和MessageConsumer 此处灵活使用，可以以服务的形式提供也可以以工具类的形式提供，详情见下面的示例代码。 1.6 MessageListener 消息的消费者应有的有对应的Listener。 12&lt;!-- 配置消息队列监听者（Queue） --&gt;&lt;bean id="queueMessageListener" class="cn.edu.hust.activemq.filter.QueueMessageListener" /&gt; = 1.7 MessageListenerContainer MessageListenerContainer即Listener的容器，用来对Listener坐一些配置，每一个listener都对应着一个Container： 1234567&lt;!-- 显示注入消息监听容器（Queue），配置连接工厂，监听的目标是demoQueueDestination，监听器是上面定义的监听器 --&gt;&lt;bean id="queueListenerContainer" class="org.springframework.jms.listener.DefaultMessageListenerContainer"&gt; &lt;property name="connectionFactory" ref="connectionFactory" /&gt; &lt;property name="destination" ref="demoQueueDestination" /&gt; &lt;property name="messageListener" ref="queueMessageListener" /&gt;&lt;/bean&gt; Spring为我们听过了两种类型的MessageListenerContainer：SimpleMessageListenerContainer和DefaultMessageListenerContainer。 SimpleMessageListenerContainer会在一开始的时候就创建一个会话Session和消费者Consumer，并且会适用标准的JMS的MessageConsumer.setMessageListener()方法注册监听器让JMS提供调用监听器的回调函数。它不会动态的适应运行时需要和参与外部的事务管理。兼容性方面，它非常接近于独立的JMS规范，但一般不兼容J2EE的JMS限制。大多数情况下，我们还是使用DefaultMessageListenerContainer。 DefaultMessageListenerContainer，与SimpleMessageListenerContainer相比，它会动态的适应运行时的需求，并且能够参与外部的事务管理。 上面就是mq的配置文件部分，如果从上到下的配置部分都清楚地话使用起来肯定没有问题，我们再做一个简要的总结： 可以有一个或者多个消息生产者向同一个destination发送消息； queue类型的只能有一个消息消费者； topic类型的可以有多个消息消费者； 每个消费者对应一个MessageListener和一个MessageListenerContainer。 下面我们看一下整合的全部代码： 首先上pom.xml看一下依赖的jar包： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;q&lt;/groupId&gt; &lt;artifactId&gt;q&lt;/artifactId&gt; &lt;packaging&gt;war&lt;/packaging&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;name&gt;q Maven Webapp&lt;/name&gt; &lt;url&gt;http://maven.apache.org&lt;/url&gt; &lt;properties&gt; &lt;springframework&gt;4.3.0.RELEASE&lt;/springframework&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.10&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- spring --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-core&lt;/artifactId&gt; &lt;version&gt;$&#123;springframework&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;$&#123;springframework&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-tx&lt;/artifactId&gt; &lt;version&gt;$&#123;springframework&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;$&#123;springframework&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-jms&lt;/artifactId&gt; &lt;version&gt;$&#123;springframework&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- xbean 如&lt;amq:connectionFactory /&gt; --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.xbean&lt;/groupId&gt; &lt;artifactId&gt;xbean-spring&lt;/artifactId&gt; &lt;version&gt;3.16&lt;/version&gt; &lt;/dependency&gt; &lt;!-- activemq --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.activemq&lt;/groupId&gt; &lt;artifactId&gt;activemq-core&lt;/artifactId&gt; &lt;version&gt;5.7.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.activemq&lt;/groupId&gt; &lt;artifactId&gt;activemq-pool&lt;/artifactId&gt; &lt;version&gt;5.12.1&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;finalName&gt;q&lt;/finalName&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;source&gt;1.6&lt;/source&gt; &lt;target&gt;1.6&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 然后是我们的Spring配置文件applicationContext.xml: 12345678910111213141516171819202122232425262728293031&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;!-- 查找最新的schemaLocation 访问 http://www.springframework.org/schema/ --&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:context="http://www.springframework.org/schema/context" xmlns:mvc="http://www.springframework.org/schema/mvc" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.springframework.org/schema/aophttp://www.springframework.org/schema/aop/spring-aop-4.0.xsdhttp://www.springframework.org/schema/beanshttp://www.springframework.org/schema/beans/spring-beans-4.0.xsdhttp://www.springframework.org/schema/contexthttp://www.springframework.org/schema/context/spring-context-4.0.xsdhttp://www.springframework.org/schema/mvchttp://www.springframework.org/schema/mvc/spring-mvc-4.0.xsd"&gt; &lt;!-- 指定Sping组件扫描的基本包路径 --&gt; &lt;context:component-scan base-package="cn.edu.hust.activemq" &gt; &lt;!-- 这里只扫描Controller，不可重复加载Service --&gt; &lt;context:include-filter type="annotation" expression="org.springframework.stereotype.Controller"/&gt; &lt;/context:component-scan&gt; &lt;!-- 启用MVC注解 --&gt; &lt;mvc:annotation-driven /&gt; &lt;!-- JSP视图解析器--&gt; &lt;bean class="org.springframework.web.servlet.view.InternalResourceViewResolver"&gt; &lt;property name="prefix" value="/" /&gt; &lt;property name="suffix" value=".jsp" /&gt; &lt;!-- 定义其解析视图的order顺序为1 --&gt; &lt;property name="order" value="1" /&gt; &lt;/bean&gt;&lt;/beans&gt; activemq的配置文件applicationContext-ActiveMQ.xml： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:amq="http://activemq.apache.org/schema/core" xmlns:context="http://www.springframework.org/schema/context" xmlns:mvc="http://www.springframework.org/schema/mvc" xsi:schemaLocation="http://www.springframework.org/schema/beanshttp://www.springframework.org/schema/beans/spring-beans-4.1.xsdhttp://www.springframework.org/schema/contexthttp://www.springframework.org/schema/context/spring-context-4.1.xsdhttp://www.springframework.org/schema/mvchttp://www.springframework.org/schema/mvc/spring-mvc-4.1.xsdhttp://activemq.apache.org/schema/corehttp://activemq.apache.org/schema/core/activemq-core-5.12.1.xsd"&gt; &lt;context:component-scan base-package="cn.edu.hust.activemq" /&gt; &lt;mvc:annotation-driven /&gt; &lt;amq:connectionFactory id="amqConnectionFactory" brokerURL="tcp://127.0.0.1:61616" userName="admin" password="admin" /&gt; &lt;!-- 配置JMS连接工厂 --&gt; &lt;bean id="connectionFactory" class="org.springframework.jms.connection.CachingConnectionFactory"&gt; &lt;constructor-arg ref="amqConnectionFactory" /&gt; &lt;property name="sessionCacheSize" value="100" /&gt; &lt;/bean&gt; &lt;!-- 定义消息队列（Queue） --&gt; &lt;bean id="demoQueueDestination" class="org.apache.activemq.command.ActiveMQQueue"&gt; &lt;!-- 设置消息队列的名字 --&gt; &lt;constructor-arg&gt; &lt;value&gt;first-queue&lt;/value&gt; &lt;/constructor-arg&gt; &lt;/bean&gt; &lt;!-- 配置JMS模板（Queue），Spring提供的JMS工具类，它发送、接收消息。 --&gt; &lt;bean id="jmsTemplate" class="org.springframework.jms.core.JmsTemplate"&gt; &lt;property name="connectionFactory" ref="connectionFactory" /&gt; &lt;property name="defaultDestination" ref="demoQueueDestination" /&gt; &lt;property name="receiveTimeout" value="10000" /&gt; &lt;!-- true是topic，false是queue，默认是false，此处显示写出false --&gt; &lt;property name="pubSubDomain" value="false" /&gt; &lt;/bean&gt; &lt;!-- 配置消息队列监听者（Queue） --&gt; &lt;bean id="queueMessageListener" class="cn.edu.hust.activemq.filter.QueueMessageListener" /&gt; &lt;!-- 显示注入消息监听容器（Queue），配置连接工厂，监听的目标是demoQueueDestination，监听器是上面定义的监听器 --&gt; &lt;bean id="queueListenerContainer" class="org.springframework.jms.listener.DefaultMessageListenerContainer"&gt; &lt;property name="connectionFactory" ref="connectionFactory" /&gt; &lt;property name="destination" ref="demoQueueDestination" /&gt; &lt;property name="messageListener" ref="queueMessageListener" /&gt; &lt;/bean&gt;&lt;/beans&gt; 配置的介绍在上面我已经讲过了，不明白的地方翻到上面去看看。 web.xml文件的配置如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;web-app xmlns="http://java.sun.com/xml/ns/javaee" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://java.sun.com/xml/ns/javaee http://java.sun.com/xml/ns/javaee/web-app_3_0.xsd" version="3.0"&gt;&lt;display-name&gt;Archetype Created Web Application&lt;/display-name&gt; &lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:applicationContext-ActiveMQ.xml&lt;/param-value&gt; &lt;/context-param&gt; &lt;listener&gt; &lt;listener-class&gt;org.springframework.web.context.ContextLoaderListener&lt;/listener-class&gt; &lt;/listener&gt; &lt;servlet&gt; &lt;servlet-name&gt;springMVC&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:applicationContext.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;springMVC&lt;/servlet-name&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; &lt;!-- 处理编码格式 --&gt; &lt;filter&gt; &lt;filter-name&gt;characterEncodingFilter&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;encoding&lt;/param-name&gt; &lt;param-value&gt;UTF-8&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;forceEncoding&lt;/param-name&gt; &lt;param-value&gt;true&lt;/param-value&gt; &lt;/init-param&gt; &lt;/filter&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;characterEncodingFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt; &lt;/filter-mapping&gt;&lt;/web-app&gt; 我们的工程目录结构如下： 上service的代码： ProducerService.java 123456789101112import javax.jms.Destination;/** * Created by Administrator on 2017/5/3. */public interface ProducerService &#123; void sendMessage(Destination destination,final String msg); void sendMessage(final String msg);&#125; ProducerServiceImpl.java 1234567891011121314151617181920212223242526272829303132333435363738394041import cn.edu.hust.activemq.service.ProducerService;import org.springframework.jms.core.JmsTemplate;import org.springframework.jms.core.MessageCreator;import org.springframework.stereotype.Service;import javax.annotation.Resource;import javax.jms.Destination;import javax.jms.JMSException;import javax.jms.Message;import javax.jms.Session;/** * Created by Administrator on 2017/5/3. */@Servicepublic class ProducerServiceImpl implements ProducerService &#123; @Resource(name="jmsTemplate") private JmsTemplate jmsTemplate; @Override public void sendMessage(Destination destination, final String msg) &#123; System.out.println(Thread.currentThread().getName()+" 向队列"+destination.toString()+"发送消息---------&gt;"+msg); jmsTemplate.send(destination, new MessageCreator() &#123; public Message createMessage(Session session) throws JMSException &#123; return session.createTextMessage(msg); &#125; &#125;); &#125; @Override public void sendMessage(final String msg) &#123; String destination = jmsTemplate.getDefaultDestinationName(); System.out.println(Thread.currentThread().getName()+" 向队列"+destination+"发送消息--------&gt;"+msg); jmsTemplate.send(new MessageCreator() &#123; public Message createMessage(Session session) throws JMSException &#123; return session.createTextMessage(msg); &#125; &#125;); &#125;&#125; ConsumerService.java 123456789import javax.jms.Destination;import javax.jms.TextMessage;/** * Created by Administrator on 2017/5/3. */public interface ConsumerService &#123; TextMessage receive(Destination destination);&#125; ConsumerServiceImpl.java 123456789101112131415161718192021222324252627282930import cn.edu.hust.activemq.service.ConsumerService;import javax.jms.Destination;import javax.jms.TextMessage;import org.springframework.jms.core.JmsTemplate;import org.springframework.stereotype.Service;import javax.annotation.Resource;import javax.jms.JMSException;/** * Created by Administrator on 2017/5/3. */@Servicepublic class ConsumerServiceImpl implements ConsumerService &#123; @Resource(name="jmsTemplate") private JmsTemplate jmsTemplate; @Override public TextMessage receive(Destination destination)&#123; TextMessage textMessage = (TextMessage) jmsTemplate.receive(destination); try&#123; System.out.println("从队列" + destination.toString() + "收到了消息：\t" + textMessage.getText()); &#125; catch (JMSException e) &#123; e.printStackTrace(); &#125; return textMessage; &#125;&#125; QueueMessageListener.java 1234567891011121314151617181920import javax.jms.JMSException;import javax.jms.Message;import javax.jms.MessageListener;import javax.jms.TextMessage;/** * Created by Administrator on 2017/5/3. */public class QueueMessageListener implements MessageListener &#123; public void onMessage(Message message) &#123; TextMessage tm = (TextMessage) message; try &#123; System.out.println("QueueMessageListener监听到了文本消息：\t" + tm.getText()); //do something ... &#125; catch (JMSException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 接下来是controller: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import cn.edu.hust.activemq.service.ConsumerService;import cn.edu.hust.activemq.service.ProducerService;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RequestMethod;import org.springframework.web.bind.annotation.ResponseBody;import javax.annotation.Resource;import javax.jms.Destination;import javax.jms.TextMessage;/** * Created by Administrator on 2017/5/3. */@Controllerpublic class MessageController &#123; private Logger logger = LoggerFactory.getLogger(MessageController.class); @Resource(name = "demoQueueDestination") private Destination destination; //队列消息生产者 @Resource private ProducerService producer; //队列消息消费者 @Resource private ConsumerService consumer; @RequestMapping(value = "/SendMessage", method = RequestMethod.GET) @ResponseBody public void send(String msg) &#123; logger.info(Thread.currentThread().getName()+"------------开始发送消息"); producer.sendMessage(msg); logger.info(Thread.currentThread().getName()+"------------发送完毕"); &#125; @RequestMapping(value= "/ReceiveMessage",method = RequestMethod.GET) @ResponseBody public Object receive()&#123; logger.info(Thread.currentThread().getName()+"------------开始接受消息"); TextMessage tm = consumer.receive(destination); logger.info(Thread.currentThread().getName()+"------------接受完毕"); return tm; &#125;&#125; 代码就是上面这些，我们先启动acticemq server，然后下启动工程，在地址栏中输入：http://localhost:8080/SendMessage?msg=nihao， 代码很简单我就没有写前台页面啦，msg部分你可以随便写。回车之后我们去看一下控制台消息就发送出去了。]]></content>
      <categories>
        <category>activeMQ</category>
      </categories>
      <tags>
        <tag>学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java并发编程（三）----线程的同步]]></title>
    <url>%2Fposts%2F71e4d230.html</url>
    <content type="text"><![CDATA[在现实开发中，我们或多或少的都经历过这样的情景：某一个变量被多个用户并发式的访问并修改，如何保证该变量在并发过程中对每一个用户的正确性呢？今天我们来聊聊线程同步的概念。 一般来说，程序并行化是为了获得更高的执行效率，但前提是，高效率不能以牺牲正确性为代价。如果程序并行化后， 连基本的执行结果的正确性都无法保证， 那么并行程序本身也就没有任何意义了。因此， 线程安全就是并行程序的根本和根基。解决这些问题从临界区的概念开始。临界区是访问一个共享资源在同一时间不能被超过一个线程执行的代码块。 java为我们提供了同步机制，帮助程序员实现临界区。当一个线程想要访问一个临界区,它使用其中的一个同步机制来找出是否有任何其他线程执行临界区。如果没有，这个线程就进入临界区。否则，这个线程通过同步机制暂停直到另一个线程执行完临界区。当多个线程正在等待一个线程完成执行的一个临界 区，JVM选择其中一个线程执行，其余的线程会等待直到轮到它们。临界区有如下的规则： 如果有若干进程要求进入空闲的临界区，一次仅允许一个进程进入。 任何时候，处于临界区内的进程不可多于一个。如已有进程进入自己的临界区，则其它所有试图进入临界区的进程必须等待。 进入临界区的进程要在有限时间内退出，以便其它进程能及时进入自己的临界区。 如果进程不能进入自己的临界区，则应让出CPU，避免进程出现“忙等”现象。 java语言为解决同步问题帮我们提供了两种机制来实现： 1. synchronized关键字； 2. Lock锁及其实现； 1 synchronized的作用 关键字synchronized 的作用是实现线程间的同步。它的工作是对同步的代码加锁，使得每一次， 只能有一个线程进入同步块，从而保证线程间的安全性。 关键宇synchronized 可以有多种用法。这里做一个简单的整理。 · 指定加锁对象: 对给定对象加锁，进入同步代码前要获得给定对象的锁。 · 直接作用于实例方法: 相当于对当前实例加锁，进入同步代码前要获得当前实例的锁。 . 直接作用于静态方法: 相当于对当前类加锁， 进入同步代码前要获得当前类的锁。 1.给指定对象加锁： 1234567891011121314151617181920212223242526272829303132333435public class AccountingSync implements Runnable&#123; static AccountingSync instance=new AccountingSync() ; static int i =O; @Override public void run() ( for(int j=O; j&lt;lOOOOOOO; j++) &#123; synchronized (instance) &#123; //对象锁 i++ ; &#125; &#125;&#125;public static void main(String[] args) throws InterruptedException ( Thread t1=new Thread(instance); Thread t2=new Thread(instance); t1.start(); t2.start(); t1.join(); t2.join(); System.out.println(i);&#125;/*public static void main(String[] args) throws InterruptedException ( Thread t1=new Thread(new AccountingSync()); Thread t2=new Thread(new AccountingSync()); t1.start(); t2.start(); t1.join(); t2.join(); System.out.println(i);&#125;*/ 知道我为什么要给出两个main方法让大家参考吗？上述锁对象是锁定AccountingSync实例对象。第一个main方法中t1 和 t2 两个线程同时指向了instance实例，所以第7行的锁对象synchronized (instance)在线程t1 和 线程 t2 获得锁的时候是获取同一个对象的，这个时候的锁是同一把锁。但是在第二个main方法中我们可以看到线程t1 和 线程 t2分别对应的是两个不同的AccountingSync对象，这时候锁对象获得的是不同的AccountingSync实例，安全性是没有保证的，大家可以动手尝试一下。 2.直接作用于实例方法： 123456789101112131415161718192021222324252627282930313233public class TestSynchronized &#123; public static void main(String[] args) &#123; Tester2 a1 = new Tester2(); Th t1 = new Th(a1); t1.start(); Th t2 = new Th(a1); t2.start(); &#125;&#125;class Tester2 &#123; public synchronized void say(String name) throws InterruptedException&#123; for(int i = 0;i&lt;5;i++)&#123; Thread.sleep(1000); System.out.println(); System.out.println(name +","+i+new Date().toLocaleString() ); &#125; &#125;&#125;class Th extends Thread&#123; Tester2 test; public Th(Tester2 test1)&#123; test = test1; &#125; public void run()&#123; try &#123; test.say(Thread.currentThread().getName()); &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125;&#125; 对Tester2类中的方法使用synchronized很好理解，同一时刻如果t1正在调用say()方法，在他没有执行完毕并退出方法之前其余的线程是无法获得该方法的。只能排队等待知道t1执行完毕。 3.作用于静态方法： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162public class Test1 &#123; public static void main(String[] args) &#123; for(int i=0;i&lt;50;i++)&#123; Thread t1 = new Thread(new Sale(5)); Thread t2 = new Thread(new Producted(5)); t1.start(); t2.start(); &#125; &#125;&#125;class Shop&#123; static int a = 40; synchronized static void shopping(int b)&#123; a -= b; System.out.println("售出 "+b+" 张大饼，"+"还剩 "+a+" 张大饼"); &#125; synchronized static void factory(int c)&#123; a += c; System.out.println("仓库还有 "+a+" 张大饼"); &#125;&#125;class Sale implements Runnable&#123; int b = 0; public Sale(int b)&#123; this.b = b; &#125; @Override public void run() &#123; if(b&lt;0)&#123; Thread.interrupted(); &#125; Shop.shopping(b); try &#123; Thread.sleep(1000); Shop.factory(b-5); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;&#125;class Producted implements Runnable&#123; int b = 0; public Producted(int b)&#123; this.b = b; &#125; @Override public void run() &#123; Shop.factory(b); try &#123; Thread.sleep(1000); Shop.shopping(b-5); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 静态方法前加synchronized这个锁等价于锁住了当前类的class对象，因为静态方法或者是静态关键字在本质上是一个类对象，而不是成员对象，在内存中位于方法区被所有的实例共享。即等同于synchronized(Shop.class)。我们需要注意的是锁住了类并不代表锁住了类所在的对象，类本身也是一种对象。它与类的实例是完全不同的两个对象，在加锁时不是相互依赖的，即对类加锁并不与上面例子中的加锁互斥，锁住了子类或子类的对象与锁住父类或父类的对象是不相关的。 synchronized的使用其实主要是前面两种，对象锁和方法锁，静态方法锁我们并不常用到。其余的操作方式都是在这两种的基础上演变而来，比如大家经常说的“块级锁”： 123synchronized(object)&#123; //代码内容&#125; 锁住的其实并不是代码块，而是object这个对象，所以如果在其他的代码中 也发生synchronized(object)时就会发生互斥。我们为什么要研究这些呢，因为如果我们不知道我们锁住的是什么，就不清楚锁住了多大范围的内容，自然就不知道是否锁住了想要得到互斥的效果，同时也不知道如何去优化锁的使用。 因此java中的synchronized就真正能做到临界区的效果，在临界区内多个线程的操作绝对是串行的，这一点java绝对可以保证。同时synchronized造成的开销也是很大的，我们如果无法掌握好他的粒度控制，就会导致频繁的锁征用，进入悲观锁状态。 2 volatile----轻量级的synchronized 既然我们说到了synchronized那就不得不提到volatile，在java中synchronized是控制并发的，我们知道在我们对一个变量执行赋值操作的时候比如：i++，在执行完毕之后i的结果其实是写到缓存中的它并没有及时的写入到内存，后续在某些情况下（比如cpu缓存不够）再将cpu缓存写入内存，假设A线程正在执行i操作，而此时B线程也来执行。B在执行i之前是不会自己跑到缓存中去取变量的值的，它只会去内存中读取i，很显然i的值是没有被更新的，为了防止这种情况出现，volatile应运而生。 Java语言规范第三版中对volatile的定义如下： java编程语言允许线程访问共享变量，为了确保共享变量能被准确和一致的更新，线程应该确保通过排他锁单独获得这个变量。Java语言提供了volatile，在某些情况下比锁更加方便。如果一个字段被声明成volatile，java线程内存模型确保所有线程看到这个变量的值是一致的。 我们来看一个例子： 1234567891011121314151617181920212223242526public class TestWithoutVolatile &#123; private static boolean bChanged; public static void main(String[] args) throws InterruptedException &#123; new Thread() &#123; @Override public void run() &#123; for (;;) &#123; if (bChanged == !bChanged) &#123; System.out.println("!="); System.exit(0); &#125; &#125; &#125; &#125;.start(); Thread.sleep(1); new Thread() &#123; @Override public void run() &#123; for (;;) &#123; bChanged = !bChanged; &#125; &#125; &#125;.start(); &#125; &#125; 在上例中我们如果多次运行会出现两种结果，一种是正常打印：&quot;!=&quot;,还有一种就是程序会陷入死循环。但是我们如果给bChanged前面加上volatile的话则每次都会打印出&quot;!=&quot;,请读者朋友们下去可以尝试。 在此处没有加volatile之前之所以会出现有时可以出现正确结果有时则卡死的原因就在于两个线程同时在运行的过程中双方都在操作bChanged变量，但是该变量的值对于同时在使用它的另一个线程来说并不总是可见的，运气好的时候线程修改完值之后就写入主存，运气不好的时候线程只在缓存中更新了值并未写入主存。但是在加了volatile修饰之后效果则不同，因为volatile可以保证变量的可见性。 说到可见性，我们来看一幅图： 每一个线程都有相应的工作内存，工作内存中有一份主内存变量的副本，线程对变量的操作都在工作内存中进行（避免再次访问主内存，提高性能），不同线程不能访问彼此的工作内存，而通过将操作后的值刷新到主内存来进行彼此的交互，这就会带来一个变量值对其他线程的可见性问题。当一个任务在工作内存中变量值进行改变，其他任务对此是不可见的，导致每一个线程都有一份不同的变量副本。而volatile恰恰可以解决这个可见性的问题，当变量被volatile修饰，如private volatile int stateFlag = 0; 它将直接通过主内存中被读取或者写入，线程从主内存中加载的值将是最新的。 但是volatile的使用有着严格的限制，当对变量的操作依赖于以前值（如i++）,或者其值被其他字段的值约束，这个时候volatile是无法实现线程安全的。被volatile修饰的变量必须独立于程序的其他状态。因为volatile只是保证了变量的可见性，并不能保证操作的原子性，所谓原子性，即有“不可分”的意思，如对基本数据类型(java中排除long和double)的赋值操作a=6,如返回操作return a，这些操作都不会被线程调度器中断，同一时刻只有一个线程对它进行操作。 看以下代码： 123456789101112131415161718192021222324252627public class Counter &#123; public volatile static int count = 0; public static void inc() &#123; //这里延迟1毫秒，使得结果明显 try &#123; Thread.sleep(1); &#125; catch (InterruptedException e) &#123; &#125; count++; &#125; public static void main(String[] args) &#123; //同时启动1000个线程，去进行i++计算，看看实际结果 for (int i = 0; i &lt; 1000; i++) &#123; new Thread(new Runnable() &#123; @Override public void run() &#123; Counter.inc(); &#125; &#125;).start(); &#125; //这里每次运行的值都有可能不同,可能为1000 System.out.println("运行结果:Counter.count=" + Counter.count); &#125;&#125; 运行上面的例子我们可以发现每次运行的结果都不一样，预期结果应该是1000，尽管counter被volatile修饰，保证了可见性，但是counter++并不是一个原子性操作，它被拆分为读取和写入两部分操作，我们需要用synchronized修饰： 123publicstaticsynchronizedvoid incNum() &#123; counter++;&#125; 此时每次运行结果都是1000，实现了线程安全。synchronized是一种独占锁，它对一段操作或内存进行加锁，当线程要操作被synchronized修饰的内存或操作时，必须首先获得锁才能进行后续操作；但是在同一时刻只能有一个线程获得相同的一把锁，所以它只允许一个线程进行操作。synchronized同样能够将变量最新值刷新到主内存，当一个变量只被synchronized方法操作时,是没有必要用volatile修饰的，所以我们接着把变量声明修改为： 1private static int counter; 多次运行结果依旧是1000。 说明： 上例中如果你按照上面这样改完之后其实结果并是不1000，我多次运行的结果都是先打印出&quot;运行结果:Counter.count=0&quot;,然后线程卡死。究其原因，我猜可能是第一个线程等待一秒再执行count++，然后后面的线程在这个等待过程中等不及的原因。java线程的运行具有不确定性，不能保证线程会按部就班的顺序执行，所以会出现什么样的后果很难预测。 正确结果代码如下： 1234567891011121314151617181920public class Counter &#123; public static int count = 0; public synchronized static void inc() &#123; count++; &#125; public static void main(String[] args) &#123; //同时启动1000个线程，去进行i++计算，看看实际结果 for (int i = 0; i &lt; 1000; i++) &#123; new Thread(new Runnable() &#123; @Override public void run() &#123; Counter.inc(); &#125; &#125;).start(); &#125; //这里每次运行的值都有可能不同,可能为1000 System.out.println("运行结果:Counter.count=" + Counter.count); &#125;&#125; 综上所述，由于volatile只能保证变量对多个线程的可见性，但不能保证原子性，它的同步机制是比较脆弱的，它在使用过程中有着诸多限制，对使用者也有更高的要求，相对而言，synchronized锁机制是比较安全的同步机制，有时候出于提高性能的考虑，可以利用volatile对synchronized进行代替和优化，但前提是你必须充分理解其使用场景和涵义。 下一节我们接着分析Lock锁。]]></content>
      <categories>
        <category>多线程与并发编程</category>
      </categories>
      <tags>
        <tag>学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netty学习(四)-TCP粘包和拆包]]></title>
    <url>%2Fposts%2F278c909d.html</url>
    <content type="text"><![CDATA[我们都知道TCP是基于字节流的传输协议。那么数据在通信层传播其实就像河水一样并没有明显的分界线，而数据具体表示什么意思什么地方有句号什么地方有分号这个对于TCP底层来说并不清楚。应用层向TCP层发送用于网间传输的、用8位字节表示的数据流，然后TCP把数据流分区成适当长度的报文段，之后TCP把结果包传给IP层，由它来通过网络将包传送给接收端实体的TCP层。所以对于这个数据拆分成大包小包的问题就是我们今天要讲的粘包和拆包的问题。 1 TCP粘包拆包问题说明 粘包和拆包这两个概念估计大家还不清楚，通过下面这张图我们来分析一下： 假设客户端分别发送两个数据包D1,D2个服务端，但是发送过程中数据是何种形式进行传播这个并不清楚，分别有下列4种情况： 服务端一次接受到了D1和D2两个数据包，两个包粘在一起，称为粘包； 服务端分两次读取到数据包D1和D2，没有发生粘包和拆包； 服务端分两次读到了数据包，第一次读到了D1和D2的部分内容，第二次读到了D2的剩下部分，这个称为拆包； 服务器分三次读到了数据部分，第一次读到了D1包，第二次读到了D2包的部分内容，第三次读到了D2包的剩下内容。 2. TCP粘包产生原因 我们知道在TCP协议中，应用数据分割成TCP认为最适合发送的数据块，这部分是通过“MSS”（最大数据包长度）选项来控制的，通常这种机制也被称为一种协商机制，MSS规定了TCP传往另一端的最大数据块的长度。这个值TCP协议在实现的时候往往用MTU值代替（需要减去IP数据包包头的大小20Bytes和TCP数据段的包头20Bytes）所以往往MSS为1460。通讯双方会根据双方提供的MSS值得最小值确定为这次连接的最大MSS值。 tcp为提高性能，发送端会将需要发送的数据发送到缓冲区，等待缓冲区满了之后，再将缓冲中的数据发送到接收方。同理，接收方也有缓冲区这样的机制，来接收数据。 发生粘包拆包的原因主要有以下这些： 应用程序写入数据的字节大小大于套接字发送缓冲区的大小将发生拆包； 进行MSS大小的TCP分段。MSS是TCP报文段中的数据字段的最大长度，当TCP报文长度-TCP头部长度&gt;mss的时候将发生拆包； 应用程序写入数据小于套接字缓冲区大小，网卡将应用多次写入的数据发送到网络上,将发生粘包； 数据包大于MTU的时候将会进行切片。MTU即(Maxitum Transmission Unit) 最大传输单元,由于以太网传输电气方面的限制，每个以太网帧都有最小的大小64bytes最大不能超过1518bytes,刨去以太网帧的帧头14Bytes和帧尾CRC校验部分4Bytes,那么剩下承载上层协议的地方也就是Data域最大就只能有1500Bytes这个值我们就把它称之为MTU。这个就是网络层协议非常关心的地方，因为网络层协议比如IP协议会根据这个值来决定是否把上层传下来的数据进行分片。 3. 如何解决TCP粘包拆包 我们知道tcp是无界的数据流，且协议本身无法避免粘包，拆包的发生，那我们只能在应用层数据协议上，加以控制。通常在制定传输数据时，可以使用如下方法： 设置定长消息，服务端每次读取既定长度的内容作为一条完整消息； 使用带消息头的协议、消息头存储消息开始标识及消息长度信息，服务端获取消息头的时候解析出消息长度，然后向后读取该长度的内容； 设置消息边界，服务端从网络流中按消息边界分离出消息内容。比如在消息末尾加上换行符用以区分消息结束。 当然应用层还有更多复杂的方式可以解决这个问题，这个就属于网络层的问题了，我们还是用java提供的方式来解决这个问题。我们先看一个例子看看粘包是如何发生的。 服务端： 12345678910111213141516171819202122232425262728293031public class HelloWordServer &#123; private int port; public HelloWordServer(int port) &#123; this.port = port; &#125; public void start()&#123; EventLoopGroup bossGroup = new NioEventLoopGroup(); EventLoopGroup workGroup = new NioEventLoopGroup(); ServerBootstrap server = new ServerBootstrap().group(bossGroup,workGroup) .channel(NioServerSocketChannel.class) .childHandler(new ServerChannelInitializer()); try &#123; ChannelFuture future = server.bind(port).sync(); future.channel().closeFuture().sync(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;finally &#123; bossGroup.shutdownGracefully(); workGroup.shutdownGracefully(); &#125; &#125; public static void main(String[] args) &#123; HelloWordServer server = new HelloWordServer(7788); server.start(); &#125;&#125; 服务端Initializer： 12345678910111213public class ServerChannelInitializer extends ChannelInitializer&lt;SocketChannel&gt; &#123; @Override protected void initChannel(SocketChannel socketChannel) throws Exception &#123; ChannelPipeline pipeline = socketChannel.pipeline(); // 字符串解码 和 编码 pipeline.addLast("decoder", new StringDecoder()); pipeline.addLast("encoder", new StringEncoder()); // 自己的逻辑Handler pipeline.addLast("handler", new HelloWordServerHandler()); &#125;&#125; 服务端handler： 1234567891011121314public class HelloWordServerHandler extends ChannelInboundHandlerAdapter &#123; private int counter; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; String body = (String)msg; System.out.println("server receive order : " + body + ";the counter is: " + ++counter); &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; super.exceptionCaught(ctx, cause); &#125;&#125; 客户端： 123456789101112131415161718192021222324252627282930313233public class HelloWorldClient &#123; private int port; private String address; public HelloWorldClient(int port,String address) &#123; this.port = port; this.address = address; &#125; public void start()&#123; EventLoopGroup group = new NioEventLoopGroup(); Bootstrap bootstrap = new Bootstrap(); bootstrap.group(group) .channel(NioSocketChannel.class) .handler(new ClientChannelInitializer()); try &#123; ChannelFuture future = bootstrap.connect(address,port).sync(); future.channel().closeFuture().sync(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;finally &#123; group.shutdownGracefully(); &#125; &#125; public static void main(String[] args) &#123; HelloWorldClient client = new HelloWorldClient(7788,"127.0.0.1"); client.start(); &#125;&#125; 客户端Initializer： 123456789101112public class ClientChannelInitializer extends ChannelInitializer&lt;SocketChannel&gt; &#123; protected void initChannel(SocketChannel socketChannel) throws Exception &#123; ChannelPipeline pipeline = socketChannel.pipeline(); pipeline.addLast("decoder", new StringDecoder()); pipeline.addLast("encoder", new StringEncoder()); // 客户端的逻辑 pipeline.addLast("handler", new HelloWorldClientHandler()); &#125;&#125; 客户端handler： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public class HelloWorldClientHandler extends ChannelInboundHandlerAdapter &#123; private byte[] req; private int counter; public BaseClientHandler() &#123; req = ("Unless required by applicable law or agreed to in writing, software\n" + " distributed under the License is distributed on an \"AS IS\" BASIS,\n" + " WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n" + " See the License for the specific language governing permissions and\n" + " limitations under the License.This connector uses the BIO implementation that requires the JSSE\n" + " style configuration. When using the APR/native implementation, the\n" + " penSSL style configuration is required as described in the APR/native\n" + " documentation.An Engine represents the entry point (within Catalina) that processes\n" + " every request. The Engine implementation for Tomcat stand alone\n" + " analyzes the HTTP headers included with the request, and passes them\n" + " on to the appropriate Host (virtual host)# Unless required by applicable law or agreed to in writing, software\n" + "# distributed under the License is distributed on an \"AS IS\" BASIS,\n" + "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n" + "# See the License for the specific language governing permissions and\n" + "# limitations under the License.# For example, set the org.apache.catalina.util.LifecycleBase logger to log\n" + "# each component that extends LifecycleBase changing state:\n" + "#org.apache.catalina.util.LifecycleBase.level = FINE" ).getBytes(); &#125; @Override public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; ByteBuf message; //将上面的所有字符串作为一个消息体发送出去 message = Unpooled.buffer(req.length); message.writeBytes(req); ctx.writeAndFlush(message); &#125; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; String buf = (String)msg; System.out.println("Now is : " + buf + " ; the counter is : "+ (++counter)); &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; ctx.close(); &#125;&#125; 运行客户端和服务端我们能看到： 我们看到这个长长的字符串被截成了2段发送，这就是发生了拆包的现象。同样粘包我们也很容易去模拟，我们把BaseClientHandler中的channelActive方法里面的： 123message = Unpooled.buffer(req.length);message.writeBytes(req);ctx.writeAndFlush(message); 这几行代码是把我们上面的一长串字符转成的byte数组写进流里发送出去，那么我们可以在这里把上面发送消息的这几行循环几遍这样发送的内容增多了就有可能在拆包的时候把上一条消息的一部分分配到下一条消息里面了，修改如下： 12345for (int i = 0; i &lt; 3; i++) &#123; message = Unpooled.buffer(req.length); message.writeBytes(req); ctx.writeAndFlush(message);&#125; 改完之后我们再运行一下，输出太长不好截图，我们在输出结果中能看到循环3次之后的消息服务端收到的就不是之前的完整的一条了，而是被拆分了4次发送。 对于上面出现的粘包和拆包的问题，Netty已有考虑，并且有实施的方案：LineBasedFrameDecoder。 我们重新改写一下ServerChannelInitializer： 123456789101112131415public class ServerChannelInitializer extends ChannelInitializer&lt;SocketChannel&gt; &#123; @Override protected void initChannel(SocketChannel socketChannel) throws Exception &#123; ChannelPipeline pipeline = socketChannel.pipeline(); pipeline.addLast(new LineBasedFrameDecoder(2048)); // 字符串解码 和 编码 pipeline.addLast("decoder", new StringDecoder()); pipeline.addLast("encoder", new StringEncoder()); // 自己的逻辑Handler pipeline.addLast("handler", new BaseServerHandler()); &#125;&#125; 新增：pipeline.addLast(new LineBasedFrameDecoder(2048))。同时，我们还得对上面发送的消息进行改造BaseClientHandler： 1234567891011121314151617181920212223242526272829303132333435363738394041424344public class BaseClientHandler extends ChannelInboundHandlerAdapter &#123; private byte[] req; private int counter; req = ("Unless required by applicable dfslaw or agreed to in writing, software" + " distributed under the License is distributed on an \"AS IS\" BASIS," + " WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied." + " See the License for the specific language governing permissions and" + " limitations under the License.This connector uses the BIO implementation that requires the JSSE" + " style configuration. When using the APR/native implementation, the" + " penSSL style configuration is required as described in the APR/native" + " documentation.An Engine represents the entry point (within Catalina) that processes" + " every request. The Engine implementation for Tomcat stand alone" + " analyzes the HTTP headers included with the request, and passes them" + " on to the appropriate Host (virtual host)# Unless required by applicable law or agreed to in writing, software" + "# distributed under the License is distributed on an \"AS IS\" BASIS," + "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied." + "# See the License for the specific language governing permissions and" + "# limitations under the License.# For example, set the org.apache.catalina.util.LifecycleBase logger to log" + "# each component that extends LifecycleBase changing state:" + "#org.apache.catalina.util.LifecycleBase.level = FINE\n" ).getBytes(); @Override public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; ByteBuf message; message = Unpooled.buffer(req.length); message.writeBytes(req); ctx.writeAndFlush(message); &#125; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; String buf = (String)msg; System.out.println("Now is : " + buf + " ; the counter is : "+ (++counter)); &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; ctx.close(); &#125;&#125; 去掉所有的&quot;\n&quot;,只保留字符串末尾的这一个。原因稍后再说。channelActive方法中我们不必再用循环多次发送消息了，只发送一次就好(第一个例子中发送一次的时候是发生了拆包的)，然后我们再次运行，大家会看到这么长一串字符只发送了一串就发送完毕。程序输出我就不截图了。下面来解释一下LineBasedFrameDecoder。 LineBasedFrameDecoder的工作原理是它依次遍历ByteBuf 中的可读字节，判断看是否有&quot;\n&quot; 或者&quot; \r\n&quot;，如果有，就以此位置为结束位置，从可读索引到结束位置区间的字节就组成了一行。它是以换行符为结束标志的解码器。支持携带结束符或者不携带结束符两种解码方式，同时支持配置单行的最大长度。如果连续读取到最大长度后仍然没有发现换行符，就会抛出异常，同时忽略掉之前读到的异常码流。这个对于我们确定消息最大长度的应用场景还是很有帮助。 对于上面的判断看是否有&quot;\n&quot; 或者&quot; \r\n&quot;以此作为结束的标志我们可能回想，要是没有&quot;\n&quot; 或者&quot; \r\n&quot;那还有什么别的方式可以判断消息是否结束呢。别担心，Netty对于此已经有考虑，还有别的解码器可以帮助我们解决问题，下节我们继续学习。]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java学习-NIO(五)NIO学习总结以及NIO新特性介绍]]></title>
    <url>%2Fposts%2Fda918f82.html</url>
    <content type="text"><![CDATA[我们知道是NIO是在2002年引入到J2SE 1.4里的，很多Java开发者比如我还是不知道怎么充分利用NIO，更少的人知道在Java SE 7里引入了更新的输入/输出 API（NIO.2）。但是对于普通的开发者来说基本的I/O操作就够用了，而NIO则是在处理I/O性能优化方面带来显著性效果。更快的速度则意味着NIO和NIO.2的API暴露了更多低层次的系统操作的入口，这对于开发者而言则意味着更复杂的操作和精巧的程序设计。从前面的几节的讲解来看NIO的操作无不繁琐。要完全掌握还是有点难度的。前面我们讲解了Buffer，Channel，Selector,都是从大的面上去探讨NIO的主要组件。这一节我们则从NIO的特性方面去探讨更细节的一些问题。 1.NIO的新特性 总的来说java 中的IO 和NIO的区别主要有3点： IO是面向流的，NIO是面向缓冲的； IO是阻塞的，NIO是非阻塞的； IO是单线程的，NIO 是通过选择器来模拟多线程的； NIO在基础的IO流上发展处新的特点，分别是：内存映射技术，字符及编码，非阻塞I/O和文件锁定。下面我们分别就这些技术做一些说明。 2. 内存映射 这个功能主要是为了提高大文件的读写速度而设计的。内存映射文件(memory-mappedfile)能让你创建和修改那些大到无法读入内存的文件。有了内存映射文件，你就可以认为文件已经全部读进了内存，然后把它当成一个非常大的数组来访问了。将文件的一段区域映射到内存中，比传统的文件处理速度要快很多。内存映射文件它虽然最终也是要从磁盘读取数据，但是它并不需要将数据读取到OS内核缓冲区，而是直接将进程的用户私有地址空间中的一部分区域与文件对象建立起映射关系，就好像直接从内存中读、写文件一样，速度当然快了。 NIO中内存映射主要用到以下两个类： java.nio.MappedByteBuffer java.nio.channels.FileChannel 下面我们通过一个例子来看一下内存映射读取文件和普通的IO流读取一个150M大文件的速度对比： 1234567891011121314151617181920212223242526272829303132333435363738public class MemMap &#123; public static void main(String[] args) &#123; try &#123; RandomAccessFile file = new RandomAccessFile("c://1.pdf","rw"); FileChannel channel = file.getChannel(); MappedByteBuffer buffer = channel.map(FileChannel.MapMode.READ_ONLY,0,channel.size()); ByteBuffer buffer1 = ByteBuffer.allocate(1024); byte[] b = new byte[1024]; long len = file.length(); long startTime = System.currentTimeMillis(); //读取内存映射文件 for(int i=0;i&lt;file.length();i+=1024*10)&#123; if (len - i &gt; 1024) &#123; buffer.get(b); &#125; else &#123; buffer.get(new byte[(int)(len - i)]); &#125; &#125; long endTime = System.currentTimeMillis(); System.out.println("使用内存映射方式读取文件总耗时： "+(endTime - startTime)); //普通IO流方式 long startTime1 = System.currentTimeMillis(); while(channel.read(buffer1) &gt; 0)&#123; buffer1.flip(); buffer1.clear(); &#125; long endTime1 = System.currentTimeMillis(); System.out.println("使用普通IO流方式读取文件总耗时： "+(endTime1 - startTime1)); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 实验结果为： 效果对比还是挺明显的。我们看到在上面程序中调用FileChannel类的map方法进行内存映射，第一个参数设置映射模式,现在支持3种模式： FileChannel.MapMode.READ_ONLY：只读缓冲区，在缓冲区中如果发生写操作则会产生ReadOnlyBufferException； FileChannel.MapMode.READ_WRITE：读写缓冲区，任何时刻如果通过内存映射的方式修改了文件则立刻会对磁盘上的文件执行相应的修改操作。别的进程如果也共享了同一个映射，则也会同步看到变化。而不是像标准IO那样每个进程有各自的内核缓冲区，比如JAVA代码中，没有执行 IO输出流的 flush() 或者 close() 操作，那么对文件的修改不会更新到磁盘去，除非进程运行结束； FileChannel.MapMode.PRIVATE ：这个比较狠，可写缓冲区，但任何修改是缓冲区私有的，不会回到文件中。所以尽情的修改吧，结局跟突然停电是一样的。 我们注意到FileChannel类中有map方法来建立内存映射，按理说是否应用的有相应的unmap方法来卸载映射内存呢。但是竟然没有找到该方法。一旦建立映射保持有效，直到MappedByteBuffer对象被垃圾收集。 此外，映射缓冲区不会绑定到创建它们的通道。 关闭相关的FileChannel不会破坏映射; 只有缓冲对象本身的处理打破了映射。 内存映射文件的优点： 用户进程将文件数据视为内存，因此不需要发出read()或write()系统调用。 当用户进程触摸映射的内存空间时，将自动生成页面错误，以从磁盘引入文件数据。 如果用户修改映射的内存空间，受影响的页面将自动标记为脏，并随后刷新到磁盘以更新文件。 操作系统的虚拟内存子系统将执行页面的智能缓存，根据系统负载自动管理内存。 数据始终是页面对齐的，不需要缓冲区复制。 可以映射非常大的文件，而不消耗大量内存来复制数据。 下面我们再写一个复制文件的例子来看一下对于一个120M的文件通过这种方式到底能有多快速度的提升： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758public class MemMapReadWrite &#123; private static int len; /** * 读文件 * * @param fileName * @return */ public static ByteBuffer readFile(String fileName) &#123; try &#123; RandomAccessFile file = new RandomAccessFile(fileName, "rw"); len = (int) file.length(); FileChannel channel = file.getChannel(); MappedByteBuffer buffer = channel.map(FileChannel.MapMode.READ_ONLY, 0, len); return buffer.get(new byte[(int) file.length()]); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return null; &#125; /** * 写文件 * * @param readFileName * @param writeFileName */ public static void writeFile(String readFileName, String writeFileName) &#123; try &#123; RandomAccessFile file = new RandomAccessFile(writeFileName, "rw"); FileChannel channel = file.getChannel(); ByteBuffer buffer = readFile(readFileName); MappedByteBuffer bytebuffer = channel.map(FileChannel.MapMode.READ_WRITE, 0, len); long startTime = System.currentTimeMillis(); for (int i = 0; i &lt; len; i++) &#123; bytebuffer.put(i, buffer.get(i)); &#125; bytebuffer.flip(); long endTime = System.currentTimeMillis(); System.out.println("写文件耗时： " + (endTime - startTime)); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; public static void main(String[] args) &#123; String readFileName = "c://1.pdf"; String writeFileName = "c://2.pdf"; writeFile(readFileName, writeFileName); &#125;&#125; 结果为： 这个速度还是相当惊人的！ 2. 字符及编码 说到字符和编码，我们的先说一个概念，字符编码方案： 编码方案定义了如何把字符编码的序列表达为字节序列。字符编码的数值不需要与编码字节相同，也不需要是一对一或一对多个的关系。原则上，把字符集编码和解码近似视为对象的序列化和反序列化。 通常字符数据编码是用于网络传输或文件存储。编码方案不是字符集，它是映射；但是因为它们之间的紧密联系，大部分编码都与一个独立的字符集相关联。例如，UTF-8，仅用来编码Unicode字符集。尽管如此，用一个编码方案处理多个字符集还是可能发生的。例如，EUC可以对几个亚洲语言的字符进行编码。 目前字符编码方案有US-ASCII,UTF-8,GB2312, BIG5,GBK,GB18030,UTF-16BE, UTF-16LE, UTF-16,UNICODE。其中Unicode试图把全世界所有语言的字符集统一到全面的映射之中。虽然战友一定的市场份额，但是目前其余的字符方案仍然广被采用。大部分的操作系统在I/O与文件存储方面仍是以字节为导向的，所以无论使用何种编码，Unicode或其他编码，在字节序列和字符集编码之间仍需要进行转化。 由java.nio.charset包组成的类满足了这个需求。这不是Java平台第一次处理字符集编码，但是它是最系统、最全面、以及最灵活的解决方式。 下面我们通过一个小例子来看一下通过不同的Charset实现如何把字符翻译成字节序列： 12345678910111213141516171819202122232425262728293031323334353637383940414243public class CharsetTest &#123; public static void main(String[] args) &#123; Scanner input = new Scanner(System.in); String str = input.next(); String[] charsetNames = &#123;"US-ASCII", "ISO-8859-1", "UTF-8", "UTF-16BE", "UTF-16LE", "UTF-16" &#125;; for (int i = 0; i &lt; charsetNames.length; i++) &#123; doEncode(Charset.forName(charsetNames[i]), str); &#125; &#125; private static void doEncode(Charset cs, String input) &#123; ByteBuffer bb = cs.encode(input); System.out.println("Charset: " + cs.name()); System.out.println(" Input: " + input); System.out.println("Encoded: "); for (int i = 0; bb.hasRemaining(); i++) &#123; int b = bb.get(); int ival = ((int) b) &amp; 0xff; char c = (char) ival; // Keep tabular alignment pretty if (i &lt; 10) System.out.print(" "); // 打印索引序列 System.out.print(" " + i + ": "); // Better formatted output is coming someday... if (ival &lt; 16) System.out.print("0"); // 输出该字节位值的16进制形式 System.out.print(Integer.toHexString(ival)); // 打印出刚才我们输入的字符，如果是空格或者标准字符集中没有包含 //该字符输出空格，否则输出该字符 if (Character.isWhitespace(c) || Character.isISOControl(c)) &#123; System.out.println(""); &#125; else &#123; System.out.println(" (" + c + ")"); &#125; &#125; System.out.println(""); &#125;&#125; 输出为： abc Charset: US-ASCII Input: abc Encoded: 0: 61 (a) 1: 62 (b) 2: 63 (c) Charset: ISO-8859-1 Input: abc Encoded: 0: 61 (a) 1: 62 (b) 2: 63 (c) Charset: UTF-8 Input: abc Encoded: 0: 61 (a) 1: 62 (b) 2: 63 (c) Charset: UTF-16BE Input: abc Encoded: 0: 00 1: 61 (a) 2: 00 3: 62 (b) 4: 00 5: 63 (c) Charset: UTF-16LE Input: abc Encoded: 0: 61 (a) 1: 00 2: 62 (b) 3: 00 4: 63 (c) 5: 00 Charset: UTF-16 Input: abc Encoded: 0: fe (þ) 1: ff (ÿ) 2: 00 3: 61 (a) 4: 00 5: 62 (b) 6: 00 7: 63 (c) Process finished with exit code 0 2.1 字符集编码器和解码器 字符的编码和解码是使用很频繁的，试想如果使用UTF-8字符集进行编码，但是却是用UTF-16字符集进行解码，那么这条信息对于用户来说其实是无用的。因为没人能看得懂。在NIO中提供了两个类CharsetEncoder和CharsetDecoder来实现编码转换方案。 CharsetEncoder类是一个状态编码引擎。实际上，编码器有状态意味着它们不是线程安全的：CharsetEncoder对象不应该在线程中共享。CharsetEncoder对象是一个状态转换引擎：字符进去，字节出来。一些编码器的调用可能需要完成转换。编码器存储在调用之间转换的状态。 字符集解码器是编码器的逆转。通过特殊的编码方案把字节编码转化成16-位Unicode字符的序列。与CharsetEncoder类似的, CharsetDecoder也是状态转换引擎。 3. 非阻塞IO 一般来说 I/O 模型可以分为：同步阻塞，同步非阻塞，异步阻塞，异步非阻塞 四种IO模型。 同步阻塞 IO ： 在此种方式下，用户进程在发起一个 IO 操作以后，必须等待 IO 操作的完成，只有当真正完成了 IO 操作以后，用户进程才能运行。 JAVA传统的 IO 模型属于此种方式！ 同步非阻塞 IO: 在此种方式下，用户进程发起一个 IO 操作以后可以返回做其它事情，但是用户进程需要时不时的询问 IO 操作是否就绪，这就要求用户进程不停的去询问，从而引入不必要的 CPU 资源浪费。其中目前 JAVA 的 NIO 就属于同步非阻塞 IO 。 异步阻塞 IO ： 此种方式下是指应用发起一个 IO 操作以后，不等待内核 IO 操作的完成，等内核完成 IO 操作以后会通知应用程序，这其实就是同步和异步最关键的区别，同步必须等待或者主动的去询问 IO 是否完成，那么为什么说是阻塞的呢？因为此时是通过 select 系统调用来完成的，而 select 函数本身的实现方式是阻塞的，而采用 select 函数有个好处就是它可以同时监听多个文件句柄，从而提高系统的并发性！ 异步非阻塞 IO: 在此种模式下，用户进程只需要发起一个 IO 操作然后立即返回，等 IO 操作真正的完成以后，应用程序会得到 IO 操作完成的通知，此时用户进程只需要对数据进行处理就好了，不需要进行实际的 IO 读写操作，因为 真正的 IO读取或者写入操作已经由 内核完成了。目前 Java 中还没有支持此种 IO 模型。 上面我们说到nio是使用了同步非阻塞模型。我们知道典型的非阻塞IO模型一般如下： 1234567while(true)&#123; data = socket.read(); if(data!= error)&#123; 处理数据 break; &#125;&#125; 但是对于非阻塞IO就有一个非常严重的问题，在while循环中需要不断地去询问内核数据是否就绪，这样会导致CPU占用率非常高，因此一般情况下很少使用while循环这种方式来读取数据。所以这就不得不说到下面这个概念–多路复用IO模型。 多路复用IO模型 在多路复用IO模型中，会有一个线程不断去轮询多个socket的状态，只有当socket真正有读写事件时，才真正调用实际的IO读写操作。因为在多路复用IO模型中，只需要使用一个线程就可以管理多个socket，系统不需要建立新的进程或者线程，也不必维护这些线程和进程，并且只有在真正有socket读写事件进行时，才会使用IO资源，所以它大大减少了资源占用。 NIO 的非阻塞 I/O 机制是围绕 选择器和 通道构建的。 Channel 类表示服务器和客户机之间的一种通信机制。Selector 类是 Channel 的多路复用器。 Selector 类将传入客户机请求多路分用并将它们分派到各自的请求处理程序。NIO 设计背后的基石是反应器(Reactor)设计模式。 关于Reactor模式在此就不多做介绍，网上很多。Reactor负责IO事件的响应，一旦有事件发生，便广播发送给相应的handler去处理。而NIO的设计则是完全按照Reactor模式来设计的。Selector发现某个channel有数据时，会通过SelectorKey来告知，然后实现事件和handler的绑定。 在Reactor模式中，包含如下角色： Reactor 将I/O事件发派给对应的Handler Acceptor 处理客户端连接请求 Handlers 执行非阻塞读/写 我们简单写一个利用了Reactor模式的NIO服务端: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374public class NIOServer &#123; private static final Logger LOGGER = LoggerFactory.getLogger(NIOServer.class); public static void main(String[] args) throws IOException &#123; Selector selector = Selector.open(); ServerSocketChannel serverSocketChannel = ServerSocketChannel.open(); serverSocketChannel.configureBlocking(false); serverSocketChannel.bind(new InetSocketAddress(1234)); serverSocketChannel.register(selector, SelectionKey.OP_ACCEPT); while (true) &#123; if(selector.selectNow() &lt; 0) &#123; continue; &#125; //获取注册的channel Set&lt;SelectionKey&gt; keys = selector.selectedKeys(); //遍历所有的key Iterator&lt;SelectionKey&gt; iterator = keys.iterator(); while(iterator.hasNext()) &#123; SelectionKey key = iterator.next(); iterator.remove(); //如果通道上有事件发生 if (key.isAcceptable()) &#123; //获取该通道 ServerSocketChannel acceptServerSocketChannel = (ServerSocketChannel) key.channel(); SocketChannel socketChannel = acceptServerSocketChannel.accept(); socketChannel.configureBlocking(false); LOGGER.info("Accept request from &#123;&#125;", socketChannel.getRemoteAddress()); //同时将SelectionKey标记为可读，以便读取。 SelectionKey readKey = socketChannel.register(selector, SelectionKey.OP_READ); //利用SelectionKey的attache功能绑定Acceptor 如果有事情，触发Acceptor //Processor对象为自定义处理请求的类 readKey.attach(new Processor()); &#125; else if (key.isReadable()) &#123; Processor processor = (Processor) key.attachment(); processor.process(key); &#125; &#125; &#125; &#125;&#125;/** * Processor类中设置一个线程池来处理请求， * 这样就可以充分利用多线程的优势 */class Processor &#123; private static final Logger LOGGER = LoggerFactory.getLogger(Processor.class); private static final ExecutorService service = Executors.newFixedThreadPool(16); public void process(final SelectionKey selectionKey) &#123; service.submit(new Runnable() &#123; @Override public void run() &#123; ByteBuffer buffer = null; SocketChannel socketChannel = null; try &#123; buffer = ByteBuffer.allocate(1024); socketChannel = (SocketChannel) selectionKey.channel(); int count = socketChannel.read(buffer); if (count &lt; 0) &#123; socketChannel.close(); selectionKey.cancel(); LOGGER.info("&#123;&#125;\t Read ended", socketChannel); &#125; else if(count == 0) &#123; &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; LOGGER.info("&#123;&#125;\t Read message &#123;&#125;", socketChannel, new String(buffer.array())); &#125; &#125;); &#125;&#125; 这种方式带来的好处也是不言而喻的。利用多路复用机制避免了线程的阻塞，提高了连接的数量。一个线程就可以管理多个socket，只有当socket真正有读写事件发生才会占用资源来进行实际的读写操作。虽然多线程+ 阻塞IO 达到类似的效果，但是由于在多线程 + 阻塞IO 中，每个socket对应一个线程，这样会造成很大的资源占用，并且尤其是对于长连接来说，线程的资源一直不会释放，如果后面陆续有很多连接的话，就会造成性能上的瓶颈。 另外多路复用IO为何比非阻塞IO模型的效率高是因为在非阻塞IO中，不断地询问socket状态时通过用户线程去进行的，而在多路复用IO中，轮询每个socket状态是内核在进行的，这个效率要比用户线程要高的多。 4. 文件锁定 NIO中的文件通道（FileChannel）在读写数据的时候主 要使用了阻塞模式，它不能支持非阻塞模式的读写，而且FileChannel的对象是不能够直接实例化的， 他的实例只能通过getChannel()从一个打开的文件对象上边读取（RandomAccessFile、 FileInputStream、FileOutputStream），并且通过调用getChannel()方法返回一个 Channel对象去连接同一个文件，也就是针对同一个文件进行读写操作。 文件锁的出现解决了很多Java应用程序和非Java程序之间共享文件数据的问题，在以前的JDK版本中，没有文件锁机制使得Java应用程序和其他非Java进程程序之间不能够针对同一个文件共享 数据，有可能造成很多问题，JDK1.4里面有了FileChannel，它的锁机制使得文件能够针对很多非 Java应用程序以及其他Java应用程序可见。但是Java里面 的文件锁机制主要是基于共 享锁模型，在不支持共享锁模型的操作系统上，文件锁本身也起不了作用，JDK1.4使用文件通道读写方式可以向一些文件 发送锁请求， FileChannel的 锁模型主要针对的是每一个文件，并不是每一个线程和每一个读写通道，也就是以文件为中心进行共享以及独占，也就是文件锁本身并不适合于同一个JVM的不同 线程之间。 我们简要看一下相关API： // 如果请求的锁定范围是有效的，阻塞直至获取锁 public final FileLock lock() // 尝试获取锁非阻塞，立刻返回结果 public final FileLock tryLock() // 第一个参数：要锁定区域的起始位置 // 第二个参数：要锁定区域的尺寸, // 第三个参数：true为共享锁，false为独占锁 public abstract FileLock lock (long position, long size, boolean shared) public abstract FileLock tryLock (long position, long size, boolean shared) 锁定区域的范围不一定要限制在文件的size值以内，锁可以扩展从而超出文件尾。因此，我们可以提前把待写入数据的区域锁定，我们也可以锁定一个不包含任何文件内容的区域，比如文件最后一个字节以外的区域。如果之后文件增长到达那块区域，那么你的文件锁就可以保护该区域的文件内容了。相反地，如果你锁定了文件的某一块区域，然后文件增长超出了那块区域，那么新增加 的文件内容将不会受到您的文件锁的保护。 我们写一个简单实例： 1234567891011121314151617181920212223242526272829303132333435public class NIOLock &#123; private static final Logger LOGGER = LoggerFactory.getLogger(NIOServer.class); public static void main(String[] args) throws IOException &#123; FileChannel fileChannel = new RandomAccessFile("c://1.txt", "rw").getChannel(); // 写入4个字节 fileChannel.write(ByteBuffer.wrap("abcd".getBytes())); // 将前2个字节区域锁定（共享锁） FileLock lock1 = fileChannel.lock(0, 2, true); // 当前锁持有锁的类型（共享锁/独占锁） lock1.isShared(); // IOException 不能修改只读的共享区域 // fileChannel.write(ByteBuffer.wrap("a".getBytes())); // 可以修改共享锁之外的区域，从第三个字节开始写入 fileChannel.write(ByteBuffer.wrap("ef".getBytes()), 2); // OverlappingFileLockException 重叠的文件锁异常 // FileLock lock2 = fileChannel.lock(0, 3, true); // FileLock lock3 = fileChannel.lock(0, 3, false); //得到创建锁的通道 lock1.channel(); //锁的起始位置 long position = lock1.position(); //锁的范围 long size = lock1.size(); //判断锁是否与指定文件区域有重叠 lock1.overlaps(position, size); // 记得用try/catch/finally&#123;release()&#125;方法释放锁 lock1.release(); &#125;&#125; 上面我们总结了NIO的4个新特性，对于IO来说都是很重要的功能以及性能的升级。下面我们写一个完整的NIO Socket客户端和服务端，总结一下NIO 的用法，每一行都加了注释： 服务端： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103public class Server &#123; //标识数字/ private int flag = 0; //缓冲区大小/ private int BLOCK = 4096; //接受数据缓冲区/ private ByteBuffer sendbuffer = ByteBuffer.allocate(BLOCK); //发送数据缓冲区/ private ByteBuffer receivebuffer = ByteBuffer.allocate(BLOCK); private Selector selector; public static void main(String[] args) throws IOException &#123; // TODO Auto-generated method stub int port = 7788; Server server = new Server(port); server.listen(); &#125; public Server(int port) throws IOException &#123; // 打开服务器套接字通道 ServerSocketChannel serverSocketChannel = ServerSocketChannel.open(); // 服务器配置为非阻塞 serverSocketChannel.configureBlocking(false); // 检索与此通道关联的服务器套接字 ServerSocket serverSocket = serverSocketChannel.socket(); // 进行服务的绑定 serverSocket.bind(new InetSocketAddress(port)); // 通过open()方法找到Selector selector = Selector.open(); // 注册到selector，等待连接 serverSocketChannel.register(selector, SelectionKey.OP_ACCEPT); System.out.println("Server Start----7788:"); &#125; // 监听 private void listen() throws IOException &#123; while (true) &#123; // 选择一组键，并且相应的通道已经打开 selector.select(); // 返回此选择器的已选择键集。 Set&lt;SelectionKey&gt; selectionKeys = selector.selectedKeys(); Iterator&lt;SelectionKey&gt; iterator = selectionKeys.iterator(); while (iterator.hasNext()) &#123; SelectionKey selectionKey = iterator.next(); iterator.remove(); handleKey(selectionKey); &#125; &#125; &#125; // 处理请求 private void handleKey(SelectionKey selectionKey) throws IOException &#123; // 接受请求 ServerSocketChannel server = null; SocketChannel client = null; String receiveText; String sendText; int count = 0; // 测试此键的通道是否已准备好接受新的套接字连接。 if (selectionKey.isAcceptable()) &#123; // 返回为之创建此键的通道。 server = (ServerSocketChannel) selectionKey.channel(); // 接受到此通道套接字的连接。 // 此方法返回的套接字通道（如果有）将处于阻塞模式。 client = server.accept(); // 配置为非阻塞 client.configureBlocking(false); // 注册到selector，等待连接 client.register(selector, SelectionKey.OP_READ); &#125; else if (selectionKey.isReadable()) &#123; // 返回为之创建此键的通道。 client = (SocketChannel) selectionKey.channel(); //将缓冲区清空以备下次读取 receivebuffer.clear(); //读取服务器发送来的数据到缓冲区中 count = client.read(receivebuffer); if (count &gt; 0) &#123; receiveText = new String(receivebuffer.array(), 0, count); System.out.println("服务器端接受客户端数据--:" + receiveText); client.register(selector, SelectionKey.OP_WRITE); &#125; &#125; else if (selectionKey.isWritable()) &#123; //将缓冲区清空以备下次写入 sendbuffer.clear(); // 返回为之创建此键的通道。 client = (SocketChannel) selectionKey.channel(); sendText = "message from server--" + flag++; //向缓冲区中输入数据 sendbuffer.put(sendText.getBytes()); //将缓冲区各标志复位,因为向里面put了数据标志被改变要想从中读取数据发向服务器,就要复位 sendbuffer.flip(); //输出到通道 client.write(sendbuffer); System.out.println("服务器端向客户端发送数据--：" + sendText); client.register(selector, SelectionKey.OP_READ); &#125; &#125;&#125; 客户端： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687public class Client &#123; //标识数字/ private static int flag = 0; //缓冲区大小/ private static int BLOCK = 4096; //接受数据缓冲区/ private static ByteBuffer sendbuffer = ByteBuffer.allocate(BLOCK); //发送数据缓冲区/ private static ByteBuffer receivebuffer = ByteBuffer.allocate(BLOCK); //服务器端地址/ private final static InetSocketAddress SERVER_ADDRESS = new InetSocketAddress( "localhost", 7788); public static void main(String[] args) throws IOException &#123; // TODO Auto-generated method stub // 打开socket通道 SocketChannel socketChannel = SocketChannel.open(); // 设置为非阻塞方式 socketChannel.configureBlocking(false); // 打开选择器 Selector selector = Selector.open(); // 注册连接服务端socket动作 socketChannel.register(selector, SelectionKey.OP_CONNECT); // 连接 socketChannel.connect(SERVER_ADDRESS); // 分配缓冲区大小内存 Set&lt;SelectionKey&gt; selectionKeys; Iterator&lt;SelectionKey&gt; iterator; SelectionKey selectionKey; SocketChannel client; String receiveText; String sendText; int count = 0; while (true) &#123; //选择一组键，其相应的通道已为 I/O 操作准备就绪。 //此方法执行处于阻塞模式的选择操作。 selector.select(); //返回此选择器的已选择键集。 selectionKeys = selector.selectedKeys(); //System.out.println(selectionKeys.size()); iterator = selectionKeys.iterator(); while (iterator.hasNext()) &#123; selectionKey = iterator.next(); if (selectionKey.isConnectable()) &#123; System.out.println("client connect"); client = (SocketChannel) selectionKey.channel(); // 判断此通道上是否正在进行连接操作。 // 完成套接字通道的连接过程。 if (client.isConnectionPending()) &#123; client.finishConnect(); System.out.println("完成连接!"); sendbuffer.clear(); sendbuffer.put("Hello,Server".getBytes()); sendbuffer.flip(); client.write(sendbuffer); &#125; client.register(selector, SelectionKey.OP_READ); &#125; else if (selectionKey.isReadable()) &#123; client = (SocketChannel) selectionKey.channel(); //将缓冲区清空以备下次读取 receivebuffer.clear(); //读取服务器发送来的数据到缓冲区中 count = client.read(receivebuffer); if (count &gt; 0) &#123; receiveText = new String(receivebuffer.array(), 0, count); System.out.println("客户端接受服务器端数据--:" + receiveText); client.register(selector, SelectionKey.OP_WRITE); &#125; &#125; else if (selectionKey.isWritable()) &#123; sendbuffer.clear(); client = (SocketChannel) selectionKey.channel(); sendText = "message from client--" + (flag++); sendbuffer.put(sendText.getBytes()); //将缓冲区各标志复位,因为向里面put了数据标志被改变要想从中读取数据发向服务器,就要复位 sendbuffer.flip(); client.write(sendbuffer); System.out.println("客户端向服务器端发送数据--：" + sendText); client.register(selector, SelectionKey.OP_READ); &#125; &#125; selectionKeys.clear(); &#125; &#125;&#125;]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mybatis Generator最完整配置详解]]></title>
    <url>%2Fposts%2Fa9e1e2fd.html</url>
    <content type="text"><![CDATA[转自：【http://www.jianshu.com/p/e09d2370b796】 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288&lt;?xml version="1.0" encoding="UTF-8"?&gt; &lt;!DOCTYPE generatorConfiguration PUBLIC "-//mybatis.org//DTD MyBatis Generator Configuration 1.0//EN" "http://mybatis.org/dtd/mybatis-generator-config_1_0.dtd"&gt; &lt;!-- 配置生成器 --&gt; &lt;generatorConfiguration&gt; &lt;!-- 可以用于加载配置项或者配置文件，在整个配置文件中就可以使用$&#123;propertyKey&#125;的方式来引用配置项 resource：配置资源加载地址，使用resource，MBG从classpath开始找，比如com/myproject/generatorConfig.properties url：配置资源加载地质，使用URL的方式，比如file:///C:/myfolder/generatorConfig.properties. 注意，两个属性只能选址一个; 另外，如果使用了mybatis-generator-maven-plugin，那么在pom.xml中定义的properties都可以直接在generatorConfig.xml中使用 &lt;properties resource="" url="" /&gt; --&gt; &lt;!-- 在MBG工作的时候，需要额外加载的依赖包 location属性指明加载jar/zip包的全路径 &lt;classPathEntry location="/Program Files/IBM/SQLLIB/java/db2java.zip" /&gt; --&gt; &lt;!-- context:生成一组对象的环境 id:必选，上下文id，用于在生成错误时提示 defaultModelType:指定生成对象的样式 1，conditional：类似hierarchical； 2，flat：所有内容（主键，blob）等全部生成在一个对象中； 3，hierarchical：主键生成一个XXKey对象(key class)，Blob等单独生成一个对象，其他简单属性在一个对象中(record class) targetRuntime: 1，MyBatis3：默认的值，生成基于MyBatis3.x以上版本的内容，包括XXXBySample； 2，MyBatis3Simple：类似MyBatis3，只是不生成XXXBySample； introspectedColumnImpl：类全限定名，用于扩展MBG --&gt; &lt;context id="mysql" defaultModelType="hierarchical" targetRuntime="MyBatis3Simple" &gt; &lt;!-- 自动识别数据库关键字，默认false，如果设置为true，根据SqlReservedWords中定义的关键字列表； 一般保留默认值，遇到数据库关键字（Java关键字），使用columnOverride覆盖 --&gt; &lt;property name="autoDelimitKeywords" value="false"/&gt; &lt;!-- 生成的Java文件的编码 --&gt; &lt;property name="javaFileEncoding" value="UTF-8"/&gt; &lt;!-- 格式化java代码 --&gt; &lt;property name="javaFormatter" value="org.mybatis.generator.api.dom.DefaultJavaFormatter"/&gt; &lt;!-- 格式化XML代码 --&gt; &lt;property name="xmlFormatter" value="org.mybatis.generator.api.dom.DefaultXmlFormatter"/&gt; &lt;!-- beginningDelimiter和endingDelimiter：指明数据库的用于标记数据库对象名的符号，比如ORACLE就是双引号，MYSQL默认是`反引号； --&gt; &lt;property name="beginningDelimiter" value="`"/&gt; &lt;property name="endingDelimiter" value="`"/&gt; &lt;!-- 必须要有的，使用这个配置链接数据库 @TODO:是否可以扩展 --&gt; &lt;jdbcConnection driverClass="com.mysql.jdbc.Driver" connectionURL="jdbc:mysql:///pss" userId="root" password="admin"&gt; &lt;!-- 这里面可以设置property属性，每一个property属性都设置到配置的Driver上 --&gt; &lt;/jdbcConnection&gt; &lt;!-- java类型处理器 用于处理DB中的类型到Java中的类型，默认使用JavaTypeResolverDefaultImpl； 注意一点，默认会先尝试使用Integer，Long，Short等来对应DECIMAL和 NUMERIC数据类型； --&gt; &lt;javaTypeResolver type="org.mybatis.generator.internal.types.JavaTypeResolverDefaultImpl"&gt; &lt;!-- true：使用BigDecimal对应DECIMAL和 NUMERIC数据类型 false：默认, scale&gt;0;length&gt;18：使用BigDecimal; scale=0;length[10,18]：使用Long； scale=0;length[5,9]：使用Integer； scale=0;length&lt;5：使用Short； --&gt; &lt;property name="forceBigDecimals" value="false"/&gt; &lt;/javaTypeResolver&gt; &lt;!-- java模型创建器，是必须要的元素 负责：1，key类（见context的defaultModelType）；2，java类；3，查询类 targetPackage：生成的类要放的包，真实的包受enableSubPackages属性控制； targetProject：目标项目，指定一个存在的目录下，生成的内容会放到指定目录中，如果目录不存在，MBG不会自动建目录 --&gt; &lt;javaModelGenerator targetPackage="com._520it.mybatis.domain" targetProject="src/main/java"&gt; &lt;!-- for MyBatis3/MyBatis3Simple 自动为每一个生成的类创建一个构造方法，构造方法包含了所有的field；而不是使用setter； --&gt; &lt;property name="constructorBased" value="false"/&gt; &lt;!-- 在targetPackage的基础上，根据数据库的schema再生成一层package，最终生成的类放在这个package下，默认为false --&gt; &lt;property name="enableSubPackages" value="true"/&gt; &lt;!-- for MyBatis3 / MyBatis3Simple 是否创建一个不可变的类，如果为true， 那么MBG会创建一个没有setter方法的类，取而代之的是类似constructorBased的类 --&gt; &lt;property name="immutable" value="false"/&gt; &lt;!-- 设置一个根对象， 如果设置了这个根对象，那么生成的keyClass或者recordClass会继承这个类；在Table的rootClass属性中可以覆盖该选项 注意：如果在key class或者record class中有root class相同的属性，MBG就不会重新生成这些属性了，包括： 1，属性名相同，类型相同，有相同的getter/setter方法； --&gt; &lt;property name="rootClass" value="com._520it.mybatis.domain.BaseDomain"/&gt; &lt;!-- 设置是否在getter方法中，对String类型字段调用trim()方法 --&gt; &lt;property name="trimStrings" value="true"/&gt; &lt;/javaModelGenerator&gt; &lt;!-- 生成SQL map的XML文件生成器， 注意，在Mybatis3之后，我们可以使用mapper.xml文件+Mapper接口（或者不用mapper接口）， 或者只使用Mapper接口+Annotation，所以，如果 javaClientGenerator配置中配置了需要生成XML的话，这个元素就必须配置 targetPackage/targetProject:同javaModelGenerator --&gt; &lt;sqlMapGenerator targetPackage="com._520it.mybatis.mapper" targetProject="src/main/resources"&gt; &lt;!-- 在targetPackage的基础上，根据数据库的schema再生成一层package，最终生成的类放在这个package下，默认为false --&gt; &lt;property name="enableSubPackages" value="true"/&gt; &lt;/sqlMapGenerator&gt; &lt;!-- 对于mybatis来说，即生成Mapper接口，注意，如果没有配置该元素，那么默认不会生成Mapper接口 targetPackage/targetProject:同javaModelGenerator type：选择怎么生成mapper接口（在MyBatis3/MyBatis3Simple下）： 1，ANNOTATEDMAPPER：会生成使用Mapper接口+Annotation的方式创建（SQL生成在annotation中），不会生成对应的XML； 2，MIXEDMAPPER：使用混合配置，会生成Mapper接口，并适当添加合适的Annotation，但是XML会生成在XML中； 3，XMLMAPPER：会生成Mapper接口，接口完全依赖XML； 注意，如果context是MyBatis3Simple：只支持ANNOTATEDMAPPER和XMLMAPPER --&gt; &lt;javaClientGenerator targetPackage="com._520it.mybatis.mapper" type="ANNOTATEDMAPPER" targetProject="src/main/java"&gt; &lt;!-- 在targetPackage的基础上，根据数据库的schema再生成一层package，最终生成的类放在这个package下，默认为false --&gt; &lt;property name="enableSubPackages" value="true"/&gt; &lt;!-- 可以为所有生成的接口添加一个父接口，但是MBG只负责生成，不负责检查 &lt;property name="rootInterface" value=""/&gt; --&gt; &lt;/javaClientGenerator&gt; &lt;!-- 选择一个table来生成相关文件，可以有一个或多个table，必须要有table元素 选择的table会生成一下文件： 1，SQL map文件 2，生成一个主键类； 3，除了BLOB和主键的其他字段的类； 4，包含BLOB的类； 5，一个用户生成动态查询的条件类（selectByExample, deleteByExample），可选； 6，Mapper接口（可选） tableName（必要）：要生成对象的表名； 注意：大小写敏感问题。正常情况下，MBG会自动的去识别数据库标识符的大小写敏感度，在一般情况下，MBG会 根据设置的schema，catalog或tablename去查询数据表，按照下面的流程： 1，如果schema，catalog或tablename中有空格，那么设置的是什么格式，就精确的使用指定的大小写格式去查询； 2，否则，如果数据库的标识符使用大写的，那么MBG自动把表名变成大写再查找； 3，否则，如果数据库的标识符使用小写的，那么MBG自动把表名变成小写再查找； 4，否则，使用指定的大小写格式查询； 另外的，如果在创建表的时候，使用的""把数据库对象规定大小写，就算数据库标识符是使用的大写，在这种情况下也会使用给定的大小写来创建表名； 这个时候，请设置delimitIdentifiers="true"即可保留大小写格式； 可选： 1，schema：数据库的schema； 2，catalog：数据库的catalog； 3，alias：为数据表设置的别名，如果设置了alias，那么生成的所有的SELECT SQL语句中，列名会变成：alias_actualColumnName 4，domainObjectName：生成的domain类的名字，如果不设置，直接使用表名作为domain类的名字；可以设置为somepck.domainName，那么会自动把domainName类再放到somepck包里面； 5，enableInsert（默认true）：指定是否生成insert语句； 6，enableSelectByPrimaryKey（默认true）：指定是否生成按照主键查询对象的语句（就是getById或get）； 7，enableSelectByExample（默认true）：MyBatis3Simple为false，指定是否生成动态查询语句； 8，enableUpdateByPrimaryKey（默认true）：指定是否生成按照主键修改对象的语句（即update)； 9，enableDeleteByPrimaryKey（默认true）：指定是否生成按照主键删除对象的语句（即delete）； 10，enableDeleteByExample（默认true）：MyBatis3Simple为false，指定是否生成动态删除语句； 11，enableCountByExample（默认true）：MyBatis3Simple为false，指定是否生成动态查询总条数语句（用于分页的总条数查询）； 12，enableUpdateByExample（默认true）：MyBatis3Simple为false，指定是否生成动态修改语句（只修改对象中不为空的属性）； 13，modelType：参考context元素的defaultModelType，相当于覆盖； 14，delimitIdentifiers：参考tableName的解释，注意，默认的delimitIdentifiers是双引号，如果类似MYSQL这样的数据库，使用的是`（反引号，那么还需要设置context的beginningDelimiter和endingDelimiter属性） 15，delimitAllColumns：设置是否所有生成的SQL中的列名都使用标识符引起来。默认为false，delimitIdentifiers参考context的属性 注意，table里面很多参数都是对javaModelGenerator，context等元素的默认属性的一个复写； --&gt; &lt;table tableName="userinfo" &gt; &lt;!-- 参考 javaModelGenerator 的 constructorBased属性--&gt; &lt;property name="constructorBased" value="false"/&gt; &lt;!-- 默认为false，如果设置为true，在生成的SQL中，table名字不会加上catalog或schema； --&gt; &lt;property name="ignoreQualifiersAtRuntime" value="false"/&gt; &lt;!-- 参考 javaModelGenerator 的 immutable 属性 --&gt; &lt;property name="immutable" value="false"/&gt; &lt;!-- 指定是否只生成domain类，如果设置为true，只生成domain类，如果还配置了sqlMapGenerator，那么在mapper XML文件中，只生成resultMap元素 --&gt; &lt;property name="modelOnly" value="false"/&gt; &lt;!-- 参考 javaModelGenerator 的 rootClass 属性 &lt;property name="rootClass" value=""/&gt; --&gt; &lt;!-- 参考javaClientGenerator 的 rootInterface 属性 &lt;property name="rootInterface" value=""/&gt; --&gt; &lt;!-- 如果设置了runtimeCatalog，那么在生成的SQL中，使用该指定的catalog，而不是table元素上的catalog &lt;property name="runtimeCatalog" value=""/&gt; --&gt; &lt;!-- 如果设置了runtimeSchema，那么在生成的SQL中，使用该指定的schema，而不是table元素上的schema &lt;property name="runtimeSchema" value=""/&gt; --&gt; &lt;!-- 如果设置了runtimeTableName，那么在生成的SQL中，使用该指定的tablename，而不是table元素上的tablename &lt;property name="runtimeTableName" value=""/&gt; --&gt; &lt;!-- 注意，该属性只针对MyBatis3Simple有用； 如果选择的runtime是MyBatis3Simple，那么会生成一个SelectAll方法，如果指定了selectAllOrderByClause，那么会在该SQL中添加指定的这个order条件； --&gt; &lt;property name="selectAllOrderByClause" value="age desc,username asc"/&gt; &lt;!-- 如果设置为true，生成的model类会直接使用column本身的名字，而不会再使用驼峰命名方法，比如BORN_DATE，生成的属性名字就是BORN_DATE,而不会是bornDate --&gt; &lt;property name="useActualColumnNames" value="false"/&gt; &lt;!-- generatedKey用于生成生成主键的方法， 如果设置了该元素，MBG会在生成的&lt;insert&gt;元素中生成一条正确的&lt;selectKey&gt;元素，该元素可选 column:主键的列名； sqlStatement：要生成的selectKey语句，有以下可选项： Cloudscape:相当于selectKey的SQL为： VALUES IDENTITY_VAL_LOCAL() DB2 :相当于selectKey的SQL为： VALUES IDENTITY_VAL_LOCAL() DB2_MF :相当于selectKey的SQL为：SELECT IDENTITY_VAL_LOCAL() FROM SYSIBM.SYSDUMMY1 Derby :相当于selectKey的SQL为：VALUES IDENTITY_VAL_LOCAL() HSQLDB :相当于selectKey的SQL为：CALL IDENTITY() Informix :相当于selectKey的SQL为：select dbinfo('sqlca.sqlerrd1') from systables where tabid=1 MySql :相当于selectKey的SQL为：SELECT LAST_INSERT_ID() SqlServer :相当于selectKey的SQL为：SELECT SCOPE_IDENTITY() SYBASE :相当于selectKey的SQL为：SELECT @@IDENTITY JDBC :相当于在生成的insert元素上添加useGeneratedKeys="true"和keyProperty属性 &lt;generatedKey column="" sqlStatement=""/&gt; --&gt; &lt;!-- 该元素会在根据表中列名计算对象属性名之前先重命名列名，非常适合用于表中的列都有公用的前缀字符串的时候， 比如列名为：CUST_ID,CUST_NAME,CUST_EMAIL,CUST_ADDRESS等； 那么就可以设置searchString为"^CUST_"，并使用空白替换，那么生成的Customer对象中的属性名称就不是 custId,custName等，而是先被替换为ID,NAME,EMAIL,然后变成属性：id，name，email； 注意，MBG是使用java.util.regex.Matcher.replaceAll来替换searchString和replaceString的， 如果使用了columnOverride元素，该属性无效； &lt;columnRenamingRule searchString="" replaceString=""/&gt; --&gt; &lt;!-- 用来修改表中某个列的属性，MBG会使用修改后的列来生成domain的属性； column:要重新设置的列名； 注意，一个table元素中可以有多个columnOverride元素哈~ --&gt; &lt;!--生成的实体类字段与表中字段名不同，property属性制定实体类字段名称--&gt; &lt;columnOverride column="para_index" property="index"&gt; &lt;/columnOverride&gt; &lt;columnOverride column="username"&gt; &lt;!-- 使用property属性来指定列要生成的属性名称 --&gt; &lt;property name="property" value="userName"/&gt; &lt;!-- javaType用于指定生成的domain的属性类型，使用类型的全限定名 &lt;property name="javaType" value=""/&gt; --&gt; &lt;!-- jdbcType用于指定该列的JDBC类型 &lt;property name="jdbcType" value=""/&gt; --&gt; &lt;!-- typeHandler 用于指定该列使用到的TypeHandler，如果要指定，配置类型处理器的全限定名 注意，mybatis中，不会生成到mybatis-config.xml中的typeHandler 只会生成类似：where id = #&#123;id,jdbcType=BIGINT,typeHandler=com._520it.mybatis.MyTypeHandler&#125;的参数描述 &lt;property name="jdbcType" value=""/&gt; --&gt; &lt;!-- 参考table元素的delimitAllColumns配置，默认为false &lt;property name="delimitedColumnName" value=""/&gt; --&gt; &lt;/columnOverride&gt; &lt;!-- ignoreColumn设置一个MGB忽略的列，如果设置了改列，那么在生成的domain中，生成的SQL中，都不会有该列出现 column:指定要忽略的列的名字； delimitedColumnName：参考table元素的delimitAllColumns配置，默认为false 注意，一个table元素中可以有多个ignoreColumn元素 &lt;ignoreColumn column="deptId" delimitedColumnName=""/&gt; --&gt; &lt;/table&gt; &lt;/context&gt; &lt;/generatorConfiguration&gt;]]></content>
      <categories>
        <category>Mybatis</category>
      </categories>
      <tags>
        <tag>学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java学习-NIO(一)简介]]></title>
    <url>%2Fposts%2Fec039d95.html</url>
    <content type="text"><![CDATA[I/O简介 在 Java 编程中，直到最近一直使用 流 的方式完成 I/O。所有 I/O 都被视为单个的字节的移动，通过一个称为 Stream 的对象一次移动一个字节。流 I/O 用于与外部世界接触。它也在内部使用，用于将对象转换为字节，然后再转换回对象。 Java NIO即Java Non-blocking IO(Java非阻塞I/O)，因为是在Jdk1.4之后增加的一套新的操作I/O工具包，所以一般会被叫做Java New IO。NIO是为提供I/O吞吐量而专门设计，其卓越的性能甚至可以与C媲美。NIO是通过Reactor模式的事件驱动机制来达到Non blocking的，那么什么是Reactor模式呢？Reactor翻译成中文是“反应器”，就是我们将事件注册到Reactor中，当有相应的事件发生时，Reactor便会告知我们有哪些事件发生了，我们再根据具体的事件去做相应的处理。 NIO 与原来的 I/O 有同样的作用和目的，但是它使用不同的方式–块I/O。块 I/O 的效率可以比流 I/O 高许多。NIO 的创建目的是为了让 Java 程序员可以实现高速 I/O 而无需编写自定义的本机代码。NIO 将最耗时的 I/O 操作(即填充和提取缓冲区)转移回操作系统，因而可以极大地提高速度。 面向流 的 I/O 系统一次一个字节地处理数据。一个输入流产生一个字节的数据，一个输出流消费一个字节的数据。为流式数据创建过滤器非常容易。链接几个过滤器，以便每个过滤器只负责单个复杂处理机制的一部分，这样也是相对简单的。不利的一面是，面向流的 I/O 通常相当慢。 一个 面向块 的 I/O 系统以块的形式处理数据。每一个操作都在一步中产生或者消费一个数据块。按块处理数据比按(流式的)字节处理数据要快得多。但是面向块的 I/O 缺少一些面向流的 I/O 所具有的优雅性和简单性。 NIO介绍 NIO有三个核心模块：Selector(选择器)、Channel(通道)、Buffer(缓冲区)，另外java.nio.charsets包下新增的字符集类也是nio一个重要的模块，但个人觉得不算是NIO的核心，只是一个供NIO核心类使用的工具类。 通道和缓冲区 什么是通道 通道是对原 I/O 包中的流的模拟。到任何目的地(或来自任何地方)的所有数据都必须通过一个 Channel 对象。一个 Buffer 实质上是一个容器对象。发送给一个通道的所有对象都必须首先放到缓冲区中；同样地，从通道中读取的任何数据都要读到缓冲区中。 Channel是一个对象，可以通过它读取和写入数据。拿 NIO 与原来的 I/O 做个比较，通道就像是流。 正如前面提到的，所有数据都通过 Buffer 对象来处理。你永远不会将字节直接写入通道中，相反，你是将数据写入包含一个或者多个字节的缓冲区。同样，你不会直接从通道中读取字节，而是将数据从通道读入缓冲区，再从缓冲区获取这个字节。 下面是JAVA NIO中的一些主要Channel的实现： FileChannel DatagramChannel SocketChannel ServerSocketChannel 正如你所看到的，这些通道涵盖了UDP 和 TCP 网络IO，以及文件IO。 什么是缓冲区 Buffer 是一个对象， 它包含一些要写入或者刚读出的数据。 在 NIO 中加入 Buffer 对象，体现了新库与原 I/O 的一个重要区别。在面向流的 I/O 中，将数据直接写入或者将数据直接读到 Stream 对象中。在 NIO 库中，所有数据都是用缓冲区处理的。在读取数据时，它是直接读到缓冲区中的。在写入数据时，它是写入到缓冲区中的。任何时候访问 NIO 中的数据，您都是将它放到缓冲区中。缓冲区实质上是一个数组。通常它是一个字节数组，但是也可以使用其他种类的数组。但是一个缓冲区不 仅仅 是一个数组。缓冲区提供了对数据的结构化访问，而且还可以跟踪系统的读/写进程。 Buffer与chennel的关系如下： 最常用的缓冲区类型是 ByteBuffer。一个 ByteBuffer 可以在其底层字节数组上进行 get/set 操作(即字节的获取和设置)。ByteBuffer 不是 NIO 中唯一的缓冲区类型。事实上，对于每一种基本 Java 类型都有一种缓冲区类型： ByteBuffer CharBuffer ShortBuffer IntBuffer LongBuffer FloatBuffer DoubleBuffer 每一个 Buffer 类都是 Buffer 接口的一个实例。 除了 ByteBuffer，每一个 Buffer 类都有完全一样的操作，只是它们所处理的数据类型不一样。因为大多数标准 I/O 操作都使用 ByteBuffer，所以它具有所有共享的缓冲区操作以及一些特有的操作。 什么是Selector 在并发型服务器程序中使用NIO，实际上是通过网络事件驱动模型实现的。我们应用Select 机制，不用为每一个客户端连接新启线程处理，而是将其注册到特定的Selector 对象上，这就可以在单线程中利用Selector 对象管理大量并发的网络连接，更好的利用了系统资源；采用非阻塞I/O的通信方式，不要求阻塞等待I/O 操作完成即可返回，从而减少了管理I/O 连接导致的系统开销，大幅度提高了系统性能。 当有读或写等任何注册的事件发生时，可以从Selector 中获得相应的SelectionKey ， 从SelectionKey 中可以找到发生的事件和该事件所发生的具体的SelectableChannel，以获得客户端发送过来的数据。由于在非阻塞网络I/O 中采用了事件触发机制，处理程序可以得到系统的主动通知，从而可以实现底层网络I/O无阻塞、流畅地读写，而不像在原来的阻塞模式下处理程序需要不断循环等待。使用NIO，可以编写出性能更好、更易扩展的并发型服务器程序。 这是在一个单线程中使用一个Selector处理4个Channel的图示： 要使用Selector，得先向Selector注册Channel，然后调用它的select()方法。这个方法会一直阻塞到某个注册的通道有事件就绪。一旦这个方法返回，线程就可以处理这些事件，事件的例子比如有新连接进来或是数据接收等。]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[消息中间件-activemq入门(二)]]></title>
    <url>%2Fposts%2Fa042465a.html</url>
    <content type="text"><![CDATA[上一节我们了解了JMS规范并且知道了JMS规范的良好实现者-activemq。今天我们就去了解一下activemq的使用。另外我们应该抱着目的去学习，别忘了我们为什么要使用消息中间件：解耦系统之间的联系，同步或异步的消息传输，尤其是异步的消息传输，分布式环境下，可靠、高效的消息传输，可以保证消息的重发性和顺序性。即解决业务系统比较多或者是分布式环境下的系统之间安全有效通信的问题，带着这样的目的我们来学习消息中间件就有了方向。 1. 为什么用activemq 在设计分布式应用程序时，应用程序间的耦合（或称集成）方式很重要。耦合意味着两个或者多个应用程序或系统的相互依赖关系。一种简单的方式是在所有的应用程序中从架构上设计他们与其他应用程序间的交叉实现。这样必然导致，一个应用程序的改变，直接导致另一个应用程序的改变。 ActiveMQ采用松耦合方式，应用程序将消息发送给ActiveMQ而并不关心什么时间以何种方式消息投递给接收者。同样的，消息接收者也不会关心消息来源于哪里和消息是怎样投递给ActiveMQ的。对于多语言编写的复杂应用环境中，允许客户端使用不同的编程语言甚至不同的消息包装协议。ActiveMQ作为消息的中间件，允许复杂的多语言应用程序以一种一步的方式集成和交互。所以说，ActiveMQ是一种好的，提供松散耦合的，能够为多语言交叉应用提供集成的中间件。 2. 什么时候用activemq ActiveMQ的设计目标是提供标准的，面向消息的，能够跨越多语言和多系统的应用集成消息通信中间件。大多数情况下ActiveMQ被用于做系统之间的数据交换。 只要是两个应用程序间需要通信的情况，都可以考虑使用JMS，不论这种通信是在本地的（就是通信的两个应用程序在同一台主机上），还是分布在不同机器上。尽管是在同一个主机上的两个应用程序需要通信也可以使用ActiveMQ。ActiveMQ可以确保消息投递成功并采用异步方式通信。 3. activemq特性 支持JMS规范：ActiveMQ完全实现了JMS1.1规范。 连接方式的多样化：ActiveMQ提供了广泛的连接模式，包括HTTP/S、JGroups、JXTA、muticast、SSL、TCP、UDP、XMPP等。提供了如此多的连接模式表明了ActiveMQ具有较高的灵活性。 与其他的Java容器紧密集成：ActiveMQ提供了和其它流行的Java容器的结合，包括Apache Geronimo、Apache Tomcat、JBoss、Jetty等。 客户端API：ActiveMQ提供了多种客户端可访问的API，包括Java、C/C++，.NET，Perl、PHP、Python、Ruby等。当然，ActiveMQ中介必须运行在Java虚拟机中，但是使用它的客户端可以使用其他的语言来实现。 中介集群：多个ActiveMQ中介可以一起协同工作，来完成某项复杂的工作，这被称为网络型中介（network of brokers），这种类型的中介将会支持多种拓扑类型。 4. 使用activemq 首先我们去apache上下载activemq，点此下载。 接下来我是使用maven来管理jar的，如果你不用maven的话就去刚下载的activemq包中找到jar包导入即可。maven引入jar： 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.activemq&lt;/groupId&gt; &lt;artifactId&gt;activemq-all&lt;/artifactId&gt; &lt;version&gt;5.14.5&lt;/version&gt;&lt;/dependency&gt; 然后我们进入刚下载的activemq，我进入的路径如下：apache-activemq-5.14.5-bin\apache-activemq-5.14.5\bin\win64\activemq.bat ,我用的是64位的系统，如果你是32位的同理进入相应文件夹下点击activemq.bat启动activemq客户端，启动完成之后，直接访问ActiveMQ管理页面http://localhost:8161/admin/ 默认用户名密码admin/admin。 客户端界面如下： 接下来该我们写代码的时候了，首先我们还是先写一个P2P(点对点)模式的客户端。代码如下： Sender.java 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889import javax.jms.Connection;import javax.jms.ConnectionFactory;import javax.jms.DeliveryMode;import javax.jms.Destination;import javax.jms.MessageProducer;import javax.jms.Session;import javax.jms.TextMessage;import org.apache.activemq.ActiveMQConnection;import org.apache.activemq.ActiveMQConnectionFactory;import java.io.BufferedReader;import java.io.InputStreamReader;public class Sender &#123; public static void main(String[] args) &#123; //ConnectionFactory是连接工厂，JMS用它创建连接 ConnectionFactory connectionFactory; //Connection JMS客户端到JMS provider的连接 Connection connection = null; //Session 一个发送或者接收消息的线程 Session session; //Destination 消息发送目的地，消息发送给谁接收 Destination destination; //MessageProducer 消息发送者 MessageProducer messageProducer; //构造ConnectionFactory 实例对象，此处采用ActiveMQ的实现jar connectionFactory = new ActiveMQConnectionFactory( ActiveMQConnection.DEFAULT_USER, ActiveMQConnection.DEFAULT_PASSWORD, "tcp://localhost:61616"); try &#123; //构造工厂得到连接对象 connection = connectionFactory.createConnection(); //启动 connection.start(); //获取操作连接 session = connection.createSession(Boolean.TRUE, Session.AUTO_ACKNOWLEDGE); //创建一个Queue，名称为FirstQueue destination = session.createQueue("FirstQueue"); //得到消息生产者【发送者】 messageProducer = session.createProducer(destination); //设置不持久化，根据实际情况而定 messageProducer.setDeliveryMode(DeliveryMode.NON_PERSISTENT); //创建一个消息对象 TextMessage message = session.createTextMessage(); //把我们的消息写入msg对象中 BufferedReader b=new BufferedReader(new InputStreamReader(System.in)); while(true) &#123; System.out.println("Enter Msg, end to terminate:"); String s=b.readLine(); if (s.equals("end")) break; message.setText(s); //发送消息 messageProducer.send(message); System.out.println("Message successfully sent."); &#125; session.commit(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;finally &#123; try &#123; if(null != connection)&#123; connection.close(); &#125; &#125; catch (Throwable ignore) &#123; &#125; &#125; &#125;&#125; Receiver.java 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869import javax.jms.Connection;import javax.jms.ConnectionFactory;import javax.jms.Destination;import javax.jms.MessageConsumer;import javax.jms.Session;import javax.jms.TextMessage;import org.apache.activemq.ActiveMQConnection;import org.apache.activemq.ActiveMQConnectionFactory;public class Receiver &#123; public static void main(String[] args) &#123; //connectionFactory 连接工厂，JMS用它创建连接 ConnectionFactory connectionFactory; //connection JMS客户端到JMS provider 的连接 Connection connection = null; //session一个发送或者接收的线程 Session session; //destination 消息目的地，发送给谁接收 Destination destination; //消费者消息接收者 MessageConsumer consumer; connectionFactory = new ActiveMQConnectionFactory( ActiveMQConnection.DEFAULT_USER, ActiveMQConnection.DEFAULT_PASSWORD, "tcp://localhost:61616"); try &#123; //构造工厂得到连接对象 connection = connectionFactory.createConnection(); //启动 connection.start(); //获取操作连接 session = connection.createSession(Boolean.FALSE, Session.AUTO_ACKNOWLEDGE); destination = session.createQueue("FirstQueue"); consumer = session.createConsumer(destination); while(true)&#123; //设置接收者收消息的时间，为了方便测试，这里暂定设置为100s TextMessage message = (TextMessage)consumer.receive(100); if(null != message)&#123; System.out.println("收到消息==="+message.getText()); &#125;else&#123; break; &#125; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;finally&#123; try &#123; if(null != connection)&#123; connection.close(); &#125; &#125; catch (Throwable ignore) &#123; &#125; &#125; &#125;&#125; 代码已经注释过了，就不多做解释，接着我们先运行Sender,需要你在控制台输入你要发送的消息，当你输入&quot;end&quot;的时候才会结束输入，否则你每一次输入按回车都是发送一条消息。然后去看一下activemq的控制台，点击一下菜单栏上的Queues： 因为我刚发送了两条消息在这里会显示，消息会由activemq这个中间人统一管理，当接受者需要接受消息的时候，他会来请求activemq，从这里获取消息而不是发送端一直等着接收端。 下面你可以运行一下Receiver，这时候就把刚才这两条消息消费了。消息队列此刻就是空的。之所以强调这一点是为了和接下来的 发布/订阅 模式做一个比较，限于篇幅我就不截图了，大家可以尝试。 下面我们接着写一个Pub/Sub模式的例子，并没有多大的变化，在创建消息队列的时候改为topic模式： TopicSender.java 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879import javax.jms.*;import org.apache.activemq.ActiveMQConnection;import org.apache.activemq.ActiveMQConnectionFactory;import java.io.BufferedReader;import java.io.InputStreamReader;/** * Created by Administrator on 2017/4/25. */public class TopicSender &#123; public static void main(String[] args) &#123; //ConnectionFactory是连接工厂，JMS用它创建连接 ConnectionFactory connectionFactory; //Connection JMS客户端到JMS provider的连接 Connection connection = null; //Session 一个发送或者接收消息的线程 Session session; //Destination 消息发送目的地，消息发送给谁接收 Topic destination; //MessageProducer 消息发送者 MessageProducer messageProducer; //构造ConnectionFactory 实例对象，此处采用ActiveMQ的实现jar connectionFactory = new ActiveMQConnectionFactory( ActiveMQConnection.DEFAULT_USER, ActiveMQConnection.DEFAULT_PASSWORD, "tcp://localhost:61616"); try &#123; //构造工厂得到连接对象 connection = connectionFactory.createConnection(); //启动 connection.start(); //获取操作连接 session = connection.createSession(Boolean.TRUE, Session.AUTO_ACKNOWLEDGE); //创建一个Queue，SecondQueue 此处使用的是Topic模式 destination = session.createTopic("SecondQueue"); //得到消息生产者【发送者】 messageProducer = session.createProducer(destination); //设置不持久化，根据实际情况而定 messageProducer.setDeliveryMode(DeliveryMode.NON_PERSISTENT); //创建一个消息对象 TextMessage message = session.createTextMessage(); //把我们的消息写入msg对象中 BufferedReader b=new BufferedReader(new InputStreamReader(System.in)); message.setText("你好"); //发送消息 messageProducer.send(message); System.out.println("Message successfully sent."); session.commit(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;finally &#123; try &#123; if(null != connection)&#123; connection.close(); &#125; &#125; catch (Throwable ignore) &#123; &#125; &#125; &#125;&#125; 同理接受方也是如此： TopicReciever.java 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public class TopicReciever &#123; public static void main(String[] args) &#123; //connectionFactory 连接工厂，JMS用它创建连接 ConnectionFactory connectionFactory; //connection JMS客户端到JMS provider 的连接 Connection connection = null; //session一个发送或者接收的线程 final Session session; //destination 消息目的地，发送给谁接收 这里注意改成Topic类型的 Topic destination; //消费者消息接收者 final MessageConsumer consumer; connectionFactory = new ActiveMQConnectionFactory( ActiveMQConnection.DEFAULT_USER, ActiveMQConnection.DEFAULT_PASSWORD, "tcp://localhost:61616"); try &#123; //构造工厂得到连接对象 connection = connectionFactory.createConnection(); //启动 connection.start(); //获取操作连接 session = connection.createSession(Boolean.TRUE, Session.AUTO_ACKNOWLEDGE); //此处使用的是Topic模式 destination = session.createTopic("SecondQueue"); consumer = session.createConsumer(destination); while(true)&#123; //设置接收者收消息的时间 TextMessage message = (TextMessage)consumer.receive(10000); if(null != message)&#123; System.out.println("收到消息==="+message.getText()); &#125;else&#123; break; &#125; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 这里我们可以把Reciever同样的代码再复制一份Reciever1，然后我们先把两个接收端启动，再启动发送端，这时候我们发现消息被接受到了；但是如果我们先启动发送端再启动接收端，这时候虽然消息是被发送出去了，但是接收端并未收到，这是为什么呢？这就是我前面在讲P2P模式的时候留下的一个对比点： P2P模式是1V1的，我发送只对当前声明的这个标识，接受者也只接受该标识所对应的消息。一旦接受者获取该消息，该标识对应的消息即从消息队列中移除； Pub/Sub模式是1 V N 的，1个发送端发出的消息，可以有多个接收端去消费，但是有一个前提：想消费这条消息的接收端必须先注册，即先启动接收端去activemq的客户端注册，发送端就根据注册的情况主动把消息推送到订阅过该消息的消费者。 我们看到消息队列里面有一条消息，然后有两位消费者来订阅这一条消息，上面我们看到两个消费者分别取队列取一次消息，然后activemq会创建两个临时生产者去他们服务把消息给他们。 好拉，这一节的入门知识就讲到这里，既然是入门我们就不必太深刻，不然适得其反啊！哈哈。下面开始我们就详细的探讨activemq的一些特性以及消息中间件在集群环境中的应用。]]></content>
      <categories>
        <category>activeMQ</category>
      </categories>
      <tags>
        <tag>学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[消息中间件和JMS介绍(一)]]></title>
    <url>%2Fposts%2Fc0d0b105.html</url>
    <content type="text"><![CDATA[在一个公司创立初期，他可能只有几个应用，系统之间的关联也不是那么大，A系统调用B系统就直接调用B提供的API接口；后来这个公司做大了，他一步步发展有了几十个系统，这时候A系统要调用B系统的接口，但是B系统前几天刚改了一下接口A并不知情。所以A发现调不通于是给B系统管理员打电话，小王啊，改了接口咋不告诉我呢。我还以为我们系统出错了呢。弄得小王一顿尴尬，我这自己改个东西还的通知这个通知那个的。 ####** 1 中间件介绍** 我们看到上面的故事中的小王他真的是很累啊。自己修改一个接口还的给所有调用接口的系统管理员打电话告知API发生变化。说到这个问题啊，还是的说我们系统之间的耦合。对于一个小公司来说是无所谓，但是对于一个大公司这种情况简直是致命的。于是最近几年这些越来越大的互联网公司在这种挑战下提出了中间件这个概念：中间件在操作系统软件，网络和数据库之上，应用软件之下，总的作用是为处于自己上层的软件提供灵活的开发环境。因而中间件是指一类软件，是基于分布式处理的软件，最突出的特点是其网络通信功能。也可认为中间件是位于平台和应用之间的通用服务，这些服务具有标准的程序接口和协议。针对不同的操作系统和硬件平台，可以有符合接口和协议的多种实现。 #####** 1.1 中间件分类** 中间件可以分为六类： 终端仿真/屏幕转换 数据访问中间件（UDA） 远程过程调用中间件（RPC） 消息中间件（MOM） 交易中间件（TPM） 对象中间件 然而在实际应用中，一般将中间件分为两大类： 一类是底层中间件，用于支撑单个应用系统或解决一类问题，包括交易中间件、应用服务器、消息中间件、数据访问中间件等； 另一类是高层中间件，更多的用于系统整合，包括企业应用集成中间件、工作流中间件、门户中间件等，他们通常会与多个应用系统打交道，在系统中层次较高，并大多基于前一类的底层中间件运行。 终端仿真/屏幕转换 此类中间件用于实现客户机图形用户接口与已有的字符接口方式的服务器应用程序之间的互操作，应用与早期的大型机系统，现在已很少使用。 数据访问中间件 此类中间件是为了建立数据应用资源互操作的模式，对异构环境下的数据库或文件系统实现联接。 远程过程调用中间件 此类中间件可以使开发人员在需要时调用位于远端服务器上的过程，屏蔽了在调用过程中的通信细节。一个应用程序使用RPC来远程执行一个位于不同地址空间里的过程，在效果上看和执行本地调用相同。 交易中间件 此类中间件是专门针对联机交易系统而设计的。联机交易系统需要处理大量并发进程，处理并发涉及到操作系统，文件系统，编程语言，数据通信，数据库系统，系统管理，应用软件等。而交易中间件根据分布式交易处理的标准及参考模型，对资源管理，交易管理和应用进行了实现，从而使得基于交易中间件开发应用程序更为简单。交易中间件基本上只适用于联机交易系统，是一种较为专用的中间件。 消息中间件 此类中间件是指利用高效可靠的消息传递机制进行平台无关的数据交流，并基于数据通信来进行分布式系统的集成。通过提供消息传递和消息排队模型，它可以在分布式环境下扩展进程间的通信。 消息中间件可以即支持同步方式，又支持异步方式。异步中间件比同步中间件具有更强的容错性，在系统故障时可以保证消息的正常传输。异步中间件技术又分为两类：广播方式和发布/订阅方式。由于发布/订阅方式可以指定哪种类型的用户可以接受哪种类型的消息，更加有针对性，事实上已成为异步中间件的非正式标准。目前主流的消息中间件产品有IBM的MQSeries，BEA的MessageQ和Sun的JMS等[1]。 对象中间件 传统的对象技术通过封装、继承及多态提供了良好的代码重用功能。但这些对象只存在与一个程序中，外界并不知道它们的存在，也无法访问它们。对象中间件提供了一个标准的构建框架，能使不同厂家的软件通过不同的地址空间，网络和操作系统实现交互访问。对象中间件的目标是为软件用户及开发者提供一种应用级的即插即用的互操作性。目前主流的对象中间件有OMG的CORBA，Microsoft 的COM以及IBM的SOM，Sun的RMI等。 中间件的特点 一般来讲，中间件具有以下一些特点：满足大量应用的需求，运行于多种硬件和操作系统平台，支持分布式计算，支持标准接口和协议。开发人员通过调用中间件提供的大量API，实现异构环境的通信，从而屏蔽异构系统中复杂的操作系统和网络协议。 由于标准接口对于可移植性和标准协议对于互操作性的重要性，中间件已成为许多标准化工作的主要部分。分布式应用软件借助中间件可以在不同的技术之间共享资源。 总的来说，中间件屏蔽了底层操作系统的复杂性，使程序开发人员面对一个简单而统一的开发环境，减少了程序设计的复杂性，将注意力集中与自己的业务上，不必再为程序在不同软件系统上的移植而重复工作，从而大大减少了技术上的负担。 ** 2 消息中间件** 面向消息的中间件（MOM），提供了以松散耦合的灵活方式集成应用程序的一种机制。它们提供了基于存储和转发的应用程序之间的异步数据发送，即应用程序彼此不直接通信，而是与作为中介的MOM通信。MOM提供了有保证的消息发送（至少是在尽可能地做到这一点），应用程序开发人员无需了解远程过程调用（RPC）和网络/通信协议的细节。 消息队列技术是分布式应用间交换信息的一种技术。消息队列可驻留在内存或磁盘上,队列存储消息直到它们被用程序读走。通过消息队列，应用程序可独立地执行–它们不需要知道彼此的位置、或在继续执行前不需要等待接收程序接收此消息。在分布式计算环境中，为了集成分布式应用，开发者需要对异构网络环境下的分布式应用提供有效的通信手段。为了管理需要共享的信息，对应用提供公共的信息交换机制是重要的。设计分布式应用的方法主要有：远程过程调用(RPC)–分布式计算环境(DCE)的基础标准成分之一；对象事务监控(OTM)–基于CORBA的面向对象工业标准与事务处理(TP)监控技术的组合；消息队列(MessageQueue)–构造分布式应用的松耦合方法。 MOM将消息路由给应用程B，这样消息就可以存在于完全不同的计算机上，MOM负责处理网络通信。如果网络连接不可用，MOM会存储消息，直到连接变得可用时，再将消息转发给应用程序B。 灵活性的另一方面体现在，当应用程序A发送其消息时，应用程序B甚至可以不处于执行状态。MOM将保留这个消息，直到应用程序B开始执行并试着检索消息为止。这还防止了应用程序A因为等待应用程序B检索消息而出现阻塞。这种异步通信要求应用程序的设计与现在大多数应用程序不同，不过，对于时间无关或并行处理，它可能是一个极其有用的方法。 ** 2.1 消息中间件的传递模式** 消息中间件一般有两种传递模式：点对点模式(P2P)和发布-订阅模式(Pub/Sub)。 点对点模式 Point-to-Point(P2P)我们很容易理解，即生产者和消费者之间的消息往来。 每个消息都被发送到特定的消息队列，接收者从队列中获取消息。队列保留着消息，直到他们被消费或超时。 P2P的特点： 每个消息只有一个消费者（Consumer）(即一旦被消费，消息就不再在消息队列中)； 发送者和接收者之间在时间上没有依赖性，也就是说当发送者发送了消息之后，不管接收者有没有正在运行，它不会影响到消息被发送到队列； 接收者在成功接收消息之后需向队列应答成功。 发布-订阅模式(Pub/Sub) 我们可以联想到卖报纸的过程：印刷厂把当天的报纸印好然后送到邮递员手里，邮递员风雨兼程的把报纸送到每一位订阅者手里。由此我们可以看到发布-订阅模式的一些特点： 每个消息可以有多个消费者； 发布者和订阅者之间有时间上的依赖性。针对某个主题（Topic）的订阅者，它必须创建一个订阅者之后，才能消费发布者的消息，而且为了消费消息，订阅者必须保持运行的状态； 由上介绍我们可以看出这两种模式各有千秋，如果你需要点对点的发送消息那么使用P2P更专注，如果你是群发消息，显然pub/sub模式更适合。 ** 3 基于多种协议的消息传递机制** 目前市场上对于网络消息传递的协议版本很多，不同的协议有不同的规范，我们在使用时要比对实现不同协议的产品。下面我们看一下目前主流的消息传递协议： ** 3.1 AMQP协议** AMQP，即Advanced Message Queuing Protocol，高级消息队列协议，是应用层协议的一个开放标准，为面向消息的中间件设计。AMQP协议是一种二进制协议，提供客户端应用与消息中间件之间异步、安全、高效地交互。 AMQP是一个应用层的异步消息传递协议，为面向消息的中间件而设计。其目的是通过协议使应用模块之间或应用程序与中间件等进行充分解耦。而在设计初期，AMQP的原始用途只是为金融界提供一个可以彼此协作的消息协议。现在已经有相当一部分遵循AMQP的服务器和客户端供使用。其中RabbitMQ是AMQP的一款开源标准实现。 支持所有消息中间件的功能：消息交换、文件传输、流传输、远程进程调用等。 AMQP的服务器(Broker)主要由交换器、消息、队列组成。Broker的主要功能是消息的路由和缓存。对于需要保障可靠性的消息，RabbitMQ可以将消息、队列和交换器的数据写入本地硬盘。而对于响应时间敏感的消息，RabbitMQ可以不配置持久化机制。 解决的问题： 1）信息的发送者和接收者如何维持这个连接，如果一方的连接中断，这期间的数据如何防止丢失？ 2）如何降低发送者和接收者的耦合度？ 3）如何让Priority高的接收者先接到数据？ 4）如何做到load balance？有效均衡接收者的负载？ 5）如何有效的将数据发送到相关的接收者？也就是说将接收者subscribe 不同的数据，如何做有效的filter。 6）如何做到可扩展，甚至将这个通信模块发到cluster上？ 7）如何保证接收者接收到了完整，正确的数据？ AMQP协议解决了以上的问题，而RabbitMQ实现了AMQP。 ** 3.2 STOMP协议** STOMP即Simple (or Streaming) Text Orientated Messaging Protocol，简单(流)文本定向消息协议。 它提供了一个可互操作的连接格式，允许STOMP客户端与任意STOMP消息代理（Broker）进行交互。STOMP协议由于设计简单，易于开发客户端，因此在多种语言和多种平台上得到广泛地应用。 STOMP协议的前身是TTMP协议（一个简单的基于文本的协议），专为消息中间件设计。 STOMP是一个非常简单和容易实现的协议，其设计灵感源自于HTTP的简单性。尽管STOMP协议在服务器端的实现可能有一定的难度，但客户端的实现却很容易。例如，可以使用Telnet登录到任何的STOMP代理，并与STOMP代理进行交互。 STOMP是除AMQP开放消息协议之外地另外一个选择, 实现了被用在JMS brokers中特定的有线协议，比如OpenWire。它仅仅是实现通用消息操作中的一部分，并非想要覆盖全面的消息API。 STOMP server就好像是一系列的目的地, 消息会被发送到这里。STOMP协议把目的地当作不透明的字符串，其语法是服务端具体的实现。 此外STOMP没有定义目的地的交付语义是什么，语义的目的地可以从服务器到服务器，甚至从目的地到目的地。这使得服务器有可创造性的语义，去支持STOMP。 STOMP client的用户代理可以充当两个角色(可能同时)： 作为生产者，通过SENDframe发送消息到server 作为消费者，发送SUBSCRIBEframe到目的地并且通过MESSAGEframe从server获取消息。 STOMP协议工作于TCP协议之上，使用了下列命令： SEND 发送 SUBSCRIBE 订阅 UNSUBSCRIBE 退订 BEGIN 开始 COMMIT 提交 ABORT 取消 ACK 确认 DISCONNECT 断开 目前最流行的STOMP消息代理是Apache ActiveMQ。 ** 3.3 JMS协议** JMS是Java Message Service的缩写，即Java消息服务。 在大型互联网中，我们采用消息中间件可以进行应用之间的解耦以及操作的异步，这是消息中间件两个最基础的特点，也正是我们所需要的。在此基础上，我们着重思考的是消息的顺序保证、扩展性、可靠性、业务操作与消息发送一致性，以及多集群订阅者等方面的问题。当然，这些我们要思考的东西，JMS都已经想到了，先看下JMS能帮开发者做什么： 1、定义一组消息公用概念和实用工具 所有Java应用程序都可以使用JMS中定义的API去完成消息的创建、接收与发送，任何实现了JMS标准的MOM都可以作为消息的中介，完成消息的存储转发 2、最大化消息应用程序的可移植性 MOM提供了有保证的消息发送，应用程序开发人员无需了解远程过程调用（RPC）和网络/通信协议的细节，提供了程序的可移植性 3、最大化降低应用程序与应用程序之间的耦合度 由于MOM的存在，各个应用程序只关心和MOM之间如何进行消息的接收与发送，而无须关注MOM的另一边，其他程序是如何接收和发送的 JMS定义了一套通用的接口和相关语义，提供了诸如持久、验证和事务的消息服务，它最主要的目的是允许Java应用程序访问现有的消息中间件。JMS规范没有指定在消息节点间所使用的通讯底层协议，来保证应用开发人员不用与其细节打交道，一个特定的JMS实现可能提供基于TCP/IP、HTTP、UDP或者其它的协议。 由于没有统一的规范和标准，基于消息中间件的应用不可移植，不同的消息中间件也不能互操作，这大大阻碍了消息中间件的发展。 Java Message Service(JMS, Java消息服务)是SUN及其伙伴公司提出的旨在统一各种消息中间件系统接口的规范。 目前许多厂商采用并实现了JMS API，现在，JMS产品能够为企业提供一套完整的消息传递功能，目前我们看到的比较流行的JMS商业软件和开源产品：WebLogic、SonicMQ、ActiveMQ、OpenJMS都是基于JMS规范的实现。 ** 4 JMS介绍** 在 JMS 之前，每一家 MOM 厂商都用专有 API 为应用程序提供对其产品的访问，通常可用于许多种语言，其中包括 Java 语言。JMS 通过 MOM 产品为 Java 程序提供了一个发送和接收消息的标准的、便利的方法。用 JMS 编写的程序可以在任何实现 JMS 标准的 MOM 上运行。 JMS 可移植性的关键在于：JMS API 是由 Sun 作为一组接口而提供的。提供了 JMS 功能的产品是通过提供一个实现这些接口的提供者来做到这一点的。开发人员可以通过定义一组消息和一组交换这些消息的客户机应用程序建立 JMS 应用程序。 JMS 支持两种消息类型P2P 和Pub/Sub，在JMS消息模型中，根据点对点模式和发布/订阅模式，这些要素由扩展出了各自的内容： JMS标准 点对点模式 发布/订阅模式 ConnectionFactory QueueConnectionFactory TopicConnectionFactory Connection QueueConnection TopicConnection Destination Queue Topic Session QueueSession TopicSession MessageProducer QueueSender TopicPublisher MessageConsumer QueueReceiver TopicSubscriber JMS为发开者提供了很多的要素，看一下比较重要的几个： 要 素 作 用 Destination 表示消息所走通道的目标定义，用来定义消息从发送端发出后要走的通道，而不是接收方。Destination属于管理类对象 ConnectionFactory 顾名思义，用于创建连接对象，ConnectionFactory属于管理类的对象 Connection 连接接口，所负责的重要工作时创建Session Session 会话接口，这是一个非常重要的对象，消息发送者、消息接收者以及消息对象本身，都是通过这个会话对象创建的 MessageConsumer 消息的消费者，也就是订阅消息并处理消息的对象 MessageProducer 消息的生产者，也就是用来发送消息的对象 XXXMessage 指各种类型的消息对象，包括ByteMesage、ObjectMessage、StreamMessage和TextMessage这5种 JMS消息模型 JMS 消息由以下几部分组成：消息头，属性，消息体。 消息头（header）：JMS消息头包含了许多字段，它们是消息发送后由JMS提供者或消息发送者产生，用来表示消息、设置优先权和失效时间等等，并且为消息确定路由。 属性（property）：由消息发送者产生，用来添加删除消息头以外的附加信息。 消息体（body）：由消息发送者产生，JMS中定义了5种消息体：ByteMessage、MapMessage、ObjectMessage、StreamMessage和TextMessage。 JMS编程模型 一般来说我们在开发基于JMS协议的客户端由一下几部构成： 用JNDI 得到ConnectionFactory对象； 用JNDI 得到目标队列或主题对象，即Destination对象； 用ConnectionFactory创建Connection 对象； 用Connection对象创建一个或多个JMS Session； 用Session 和Destination 创建MessageProducer和MessageConsumer； 通知Connection 开始传递消息。 因为jms需要使用到J2EE服务器，我们平常用的tomcat属于J2SE类型的服务器，常见的J2EE服务器包括：Geronimo,JBoss 4, GlassFish,WebLogic 。我们在这里使用glassfish 容器。安装和使用有很多教程，在此就不贴了。首先我们进去glassfish的控制台，设置一下我们的发送者和接受者对象： 下面我们用oracle提供的jms接口来写一个服务端，我们先来写一个P2P模式的例子： MySender.java import java.io.BufferedReader; import java.io.InputStreamReader; import javax.naming.*; import javax.jms.*; public class MySender { public static void main(String[] args) { try { //1)创建一个connection InitialContext ctx=new InitialContext(); QueueConnectionFactory f=(QueueConnectionFactory)ctx.lookup(&quot;myQueueConnectionFactory&quot;); QueueConnection con=f.createQueueConnection(); con.start(); //2) 创建一个会话接口 QueueSession ses=con.createQueueSession(false, Session.AUTO_ACKNOWLEDGE); //3) 获取会话接口对象 Queue t=(Queue)ctx.lookup(&quot;myQueue&quot;); //4)创建一个发送者对象 QueueSender sender=ses.createSender(t); //5) 创建一个消息对象 TextMessage msg=ses.createTextMessage(); //6) 把我们的消息写入msg对象中 BufferedReader b=new BufferedReader(new InputStreamReader(System.in)); while(true) { System.out.println(&quot;Enter Msg, end to terminate:&quot;); String s=b.readLine(); if (s.equals(&quot;end&quot;)) break; msg.setText(s); //7) 发送消息 sender.send(msg); System.out.println(&quot;Message successfully sent.&quot;); } //8) 关闭连接 con.close(); }catch(Exception e){System.out.println(e);} } } MyReceiver.java import javax.jms.*; import javax.naming.InitialContext; public class MyReceiver { public static void main(String[] args) { try{ //1) 创建一个connection InitialContext ctx=new InitialContext(); QueueConnectionFactory f=(QueueConnectionFactory)ctx.lookup(&quot;myQueueConnectionFactory&quot;); QueueConnection con=f.createQueueConnection(); con.start(); //2) 创建一个会话接口 QueueSession ses=con.createQueueSession(false, Session.AUTO_ACKNOWLEDGE); //3) 获取会话接口对象 Queue t=(Queue)ctx.lookup(&quot;myQueue&quot;); //4)创建一个发送者对象 QueueReceiver receiver=ses.createReceiver(t); //5) 创建一个消监听对象 MyListener listener=new MyListener(); //6) 将监听器注册到receiver，用来监听receiver receiver.setMessageListener(listener); System.out.println(&quot;Receiver1 is ready, waiting for messages...&quot;); System.out.println(&quot;press Ctrl+c to shutdown...&quot;); while(true){ Thread.sleep(1000); } }catch(Exception e){System.out.println(e);} } } MyListener.java import javax.jms.*; public class MyListener implements MessageListener { public void onMessage(Message m) { try{ TextMessage msg=(TextMessage)m; System.out.println(&quot;following message is received:&quot;+msg.getText()); }catch(JMSException e){System.out.println(e);} } } Pub/Sub模式： MySender.java import javax.jms.*; import javax.naming.InitialContext; import java.io.BufferedReader; import java.io.InputStreamReader; public class MySender { public static void main(String[] args) { try { //1)创建一个connection InitialContext ctx=new InitialContext(); TopicConnectionFactory f=(TopicConnectionFactory)ctx.lookup(&quot;myTopicConnectionFactory&quot;); TopicConnection con=f.createTopicConnection(); con.start(); //2) 创建一个会话接口 TopicSession ses=con.createTopicSession(false, Session.AUTO_ACKNOWLEDGE); //3) 获取会话接口对象 Topic t=(Topic)ctx.lookup(&quot;myTopic&quot;); //4)创建一个发送者对象 TopicPublisher publisher=ses.createPublisher(t); //5) 创建一个消息对象 TextMessage msg=ses.createTextMessage(); //6) 把我们的消息写入msg对象中 BufferedReader b=new BufferedReader(new InputStreamReader(System.in)); while(true) { System.out.println(&quot;Enter Msg, end to terminate:&quot;); String s=b.readLine(); if (s.equals(&quot;end&quot;)) break; msg.setText(s); //7) 发送消息 publisher.publish(msg); System.out.println(&quot;Message successfully sent.&quot;); } //8) 关闭连接 con.close(); }catch(Exception e){System.out.println(e);} } } MyReceiver.java import javax.jms.*; import javax.naming.InitialContext; public class MyReceiver { public static void main(String[] args) { try{ //1) 创建一个connection InitialContext ctx=new InitialContext(); TopicConnectionFactory f=(TopicConnectionFactory)ctx.lookup(&quot;myTopicConnectionFactory&quot;); TopicConnection con=f.createTopicConnection(); //2) 创建一个会话接口 TopicSession ses=con.createTopicSession(false, Session.AUTO_ACKNOWLEDGE); //3) 获取会话接口对象 Topic t=(Topic)ctx.lookup(&quot;myTopic&quot;); //4)创建一个发送者对象 TopicSubscriber receiver=ses.createSubscriber(t); //5) 创建一个消监听对象 MyListener listener=new MyListener(); //6) 将监听器注册到receiver，用来监听receiver receiver.setMessageListener(listener); System.out.println(&quot;Receiver1 is ready, waiting for messages...&quot;); System.out.println(&quot;press Ctrl+c to shutdown...&quot;); while(true){ Thread.sleep(1000); } }catch(Exception e){System.out.println(e);} } } MyListener.java import javax.jms.JMSException; import javax.jms.Message; import javax.jms.MessageListener; import javax.jms.TextMessage; public class MyListener implements MessageListener { public void onMessage(Message m) { try{ TextMessage msg=(TextMessage)m; System.out.println(&quot;following message is received:&quot;+msg.getText()); }catch(JMSException e){System.out.println(e);} } } 上面两个案例我们运行可以看到消息成功的发送出去了。熟悉了JMS的语法，使用起来还是很简单。 上面我们介绍到了JMS，JMS是一个用于提供消息服务的技术规范，它制定了在整个消息服务提供过程中的所有数据结构和交互流程。JMS即Java消息服务（Java Message Service）应用程序接口，是一个Java平台中关于面向消息中间件（MOM）的API。 Java消息服务是一个与具体平台无关的API，绝大多数MOM提供商都对JMS提供支持。 下面我们引入另一个概念：MQ（Message Queue）。 应用程序通过写和检索出入列队的针对应用程序的数据（消息）来通信，而无需专用连接来链接它们。消息传递指的是程序之间通过在消息中发送数据进行通信，而不是通过直接调用彼此来通信，直接调用通常是用于诸如远程过程调用的技术。排队指的是应用程序通过队列来通信。队列的使用除去了接收和发送应用程序同时执行的要求。 MQ和JMS类似，但不同的是JMS是SUN Java消息中间件服务的一个标准和API定义，而MQ则是遵循了AMQP协议的具体实现和产品。JMS是一个用于提供消息服务的技术规范，它制定了在整个消息服务提供过程中的所有数据结构和交互流程。而MQ则是消息队列服务，是面向消息中间件（MOM）的最终实现，是真正的服务提供者；MQ的实现可以基于JMS，也可以基于其他规范或标准。MQ 有很多产品：IBM的，rabbitmq, activemq 等，rabbitmq 只支持点对点的方式。所以没有完全实现JMS的标准，所以说它不是一个JMS产品，而rabitmq 和Jobss JMS 它们实现了JMS的各项标准，是开源的JMS产品。目前完全实现JMS协议的mq是activemq，所以接下来我们先重点看一下activemq。从activemq入手去探索javaEE的世界。]]></content>
      <categories>
        <category>activeMQ</category>
      </categories>
      <tags>
        <tag>学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netty学习(十)-Netty文件上传]]></title>
    <url>%2Fposts%2F208253d2.html</url>
    <content type="text"><![CDATA[今天我们来完成一个使用netty进行文件传输的任务。在实际项目中，文件传输通常采用FTP或者HTTP附件的方式。事实上通过TCP Socket+File的方式进行文件传输也有一定的应用场景，尽管不是主流，但是掌握这种文件传输方式还是比较重要的，特别是针对两个跨主机的JVM进程之间进行持久化数据的相互交换。 而使用netty来进行文件传输也是利用netty天然的优势：零拷贝功能。很多同学都听说过netty的&quot;零拷贝&quot;功能，但是具体体现在哪里又不知道，下面我们就简要介绍下： Netty的“零拷贝”主要体现在如下三个方面： Netty的接收和发送ByteBuffer采用DIRECT BUFFERS，使用堆外直接内存进行Socket读写，不需要进行字节缓冲区的二次拷贝。如果使用传统的堆内存（HEAP BUFFERS）进行Socket读写，JVM会将堆内存Buffer拷贝一份到直接内存中，然后才写入Socket中。相比于堆外直接内存，消息在发送过程中多了一次缓冲区的内存拷贝。 Netty提供了组合Buffer对象，可以聚合多个ByteBuffer对象，用户可以像操作一个Buffer那样方便的对组合Buffer进行操作，避免了传统通过内存拷贝的方式将几个小Buffer合并成一个大的Buffer。 Netty的文件传输采用了transferTo方法，它可以直接将文件缓冲区的数据发送到目标Channel，避免了传统通过循环write方式导致的内存拷贝问题。 具体的分析在此就不多做介绍，有兴趣的可以查阅相关文档。我们还是把重点放在文件传输上。Netty作为高性能的服务器端异步IO框架必然也离不开文件读写功能，我们可以使用netty模拟http的形式通过网页上传文件写入服务器，当然要使用http的形式那你也用不着netty！大材小用。netty4中如果想使用http形式上传文件你还得借助第三方jar包：okhttp。使用该jar完成http请求的发送。但是在netty5 中已经为我们写好了，我们可以直接调用netty5的API就可以实现。所以netty4和5的差别还是挺大的，至于使用哪个，那就看你们公司选择哪一个了！本文目前使用netty4来实现文件上传功能。下面我们上代码： pom文件： 12345&lt;dependency&gt; &lt;groupId&gt;io.netty&lt;/groupId&gt; &lt;artifactId&gt;netty-all&lt;/artifactId&gt; &lt;version&gt;4.1.5.Final&lt;/version&gt;&lt;/dependency&gt; server端： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051import io.netty.bootstrap.ServerBootstrap;import io.netty.channel.Channel;import io.netty.channel.ChannelFuture;import io.netty.channel.ChannelInitializer;import io.netty.channel.ChannelOption;import io.netty.channel.EventLoopGroup;import io.netty.channel.nio.NioEventLoopGroup;import io.netty.channel.socket.nio.NioServerSocketChannel;import io.netty.handler.codec.serialization.ClassResolvers;import io.netty.handler.codec.serialization.ObjectDecoder;import io.netty.handler.codec.serialization.ObjectEncoder;public class FileUploadServer &#123; public void bind(int port) throws Exception &#123; EventLoopGroup bossGroup = new NioEventLoopGroup(); EventLoopGroup workerGroup = new NioEventLoopGroup(); try &#123; ServerBootstrap b = new ServerBootstrap(); b.group(bossGroup, workerGroup).channel(NioServerSocketChannel.class).option(ChannelOption.SO_BACKLOG, 1024).childHandler(new ChannelInitializer&lt;Channel&gt;() &#123; @Override protected void initChannel(Channel ch) throws Exception &#123; ch.pipeline().addLast(new ObjectEncoder()); ch.pipeline().addLast(new ObjectDecoder(Integer.MAX_VALUE, ClassResolvers.weakCachingConcurrentResolver(null))); // 最大长度 ch.pipeline().addLast(new FileUploadServerHandler()); &#125; &#125;); ChannelFuture f = b.bind(port).sync(); f.channel().closeFuture().sync(); &#125; finally &#123; bossGroup.shutdownGracefully(); workerGroup.shutdownGracefully(); &#125; &#125; public static void main(String[] args) &#123; int port = 8080; if (args != null &amp;&amp; args.length &gt; 0) &#123; try &#123; port = Integer.valueOf(args[0]); &#125; catch (NumberFormatException e) &#123; e.printStackTrace(); &#125; &#125; try &#123; new FileUploadServer().bind(port); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; server端handler： 12345678910111213141516171819202122232425262728293031323334353637383940import io.netty.channel.ChannelHandlerContext;import io.netty.channel.ChannelInboundHandlerAdapter;import java.io.File;import java.io.RandomAccessFile;public class FileUploadServerHandler extends ChannelInboundHandlerAdapter &#123; private int byteRead; private volatile int start = 0; private String file_dir = "D:"; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; if (msg instanceof FileUploadFile) &#123; FileUploadFile ef = (FileUploadFile) msg; byte[] bytes = ef.getBytes(); byteRead = ef.getEndPos(); String md5 = ef.getFile_md5();//文件名 String path = file_dir + File.separator + md5; File file = new File(path); RandomAccessFile randomAccessFile = new RandomAccessFile(file, "rw"); randomAccessFile.seek(start); randomAccessFile.write(bytes); start = start + byteRead; if (byteRead &gt; 0) &#123; ctx.writeAndFlush(start); &#125; else &#123; randomAccessFile.close(); ctx.close(); &#125; &#125; &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) &#123; cause.printStackTrace(); ctx.close(); &#125;&#125; client端： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556import io.netty.bootstrap.Bootstrap;import io.netty.channel.Channel;import io.netty.channel.ChannelFuture;import io.netty.channel.ChannelInitializer;import io.netty.channel.ChannelOption;import io.netty.channel.EventLoopGroup;import io.netty.channel.nio.NioEventLoopGroup;import io.netty.channel.socket.nio.NioSocketChannel;import io.netty.handler.codec.serialization.ClassResolvers;import io.netty.handler.codec.serialization.ObjectDecoder;import io.netty.handler.codec.serialization.ObjectEncoder;import java.io.File;public class FileUploadClient &#123; public void connect(int port, String host, final FileUploadFile fileUploadFile) throws Exception &#123; EventLoopGroup group = new NioEventLoopGroup(); try &#123; Bootstrap b = new Bootstrap(); b.group(group).channel(NioSocketChannel.class).option(ChannelOption.TCP_NODELAY, true).handler(new ChannelInitializer&lt;Channel&gt;() &#123; @Override protected void initChannel(Channel ch) throws Exception &#123; ch.pipeline().addLast(new ObjectEncoder()); ch.pipeline().addLast(new ObjectDecoder(ClassResolvers.weakCachingConcurrentResolver(null))); ch.pipeline().addLast(new FileUploadClientHandler(fileUploadFile)); &#125; &#125;); ChannelFuture f = b.connect(host, port).sync(); f.channel().closeFuture().sync(); &#125; finally &#123; group.shutdownGracefully(); &#125; &#125; public static void main(String[] args) &#123; int port = 8080; if (args != null &amp;&amp; args.length &gt; 0) &#123; try &#123; port = Integer.valueOf(args[0]); &#125; catch (NumberFormatException e) &#123; e.printStackTrace(); &#125; &#125; try &#123; FileUploadFile uploadFile = new FileUploadFile(); File file = new File("c:/1.txt"); String fileMd5 = file.getName();// 文件名 uploadFile.setFile(file); uploadFile.setFile_md5(fileMd5); uploadFile.setStarPos(0);// 文件开始位置 new FileUploadClient().connect(port, "127.0.0.1", uploadFile); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; client端handler： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112import io.netty.channel.ChannelHandlerContext;import io.netty.channel.ChannelInboundHandlerAdapter;import java.io.FileNotFoundException;import java.io.IOException;import java.io.RandomAccessFile;public class FileUploadClientHandler extends ChannelInboundHandlerAdapter &#123; private int byteRead; private volatile int start = 0; private volatile int lastLength = 0; public RandomAccessFile randomAccessFile; private FileUploadFile fileUploadFile; public FileUploadClientHandler(FileUploadFile ef) &#123; if (ef.getFile().exists()) &#123; if (!ef.getFile().isFile()) &#123; System.out.println("Not a file :" + ef.getFile()); return; &#125; &#125; this.fileUploadFile = ef; &#125; public void channelActive(ChannelHandlerContext ctx) &#123; try &#123; randomAccessFile = new RandomAccessFile(fileUploadFile.getFile(), "r"); randomAccessFile.seek(fileUploadFile.getStarPos()); lastLength = (int) randomAccessFile.length() / 10; byte[] bytes = new byte[lastLength]; if ((byteRead = randomAccessFile.read(bytes)) != -1) &#123; fileUploadFile.setEndPos(byteRead); fileUploadFile.setBytes(bytes); ctx.writeAndFlush(fileUploadFile); &#125; else &#123; System.out.println("文件已经读完"); &#125; &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException i) &#123; i.printStackTrace(); &#125; &#125; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; if (msg instanceof Integer) &#123; start = (Integer) msg; if (start != -1) &#123; randomAccessFile = new RandomAccessFile(fileUploadFile.getFile(), "r"); randomAccessFile.seek(start); System.out.println("块儿长度：" + (randomAccessFile.length() / 10)); System.out.println("长度：" + (randomAccessFile.length() - start)); int a = (int) (randomAccessFile.length() - start); int b = (int) (randomAccessFile.length() / 10); if (a &lt; b) &#123; lastLength = a; &#125; byte[] bytes = new byte[lastLength]; System.out.println("-----------------------------" + bytes.length); if ((byteRead = randomAccessFile.read(bytes)) != -1 &amp;&amp; (randomAccessFile.length() - start) &gt; 0) &#123; System.out.println("byte 长度：" + bytes.length); fileUploadFile.setEndPos(byteRead); fileUploadFile.setBytes(bytes); try &#123; ctx.writeAndFlush(fileUploadFile); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; else &#123; randomAccessFile.close(); ctx.close(); System.out.println("文件已经读完--------" + byteRead); &#125; &#125; &#125; &#125; // @Override // public void channelRead(ChannelHandlerContext ctx, Object msg) throws // Exception &#123; // System.out.println("Server is speek ："+msg.toString()); // FileRegion filer = (FileRegion) msg; // String path = "E://Apk//APKMD5.txt"; // File fl = new File(path); // fl.createNewFile(); // RandomAccessFile rdafile = new RandomAccessFile(path, "rw"); // FileRegion f = new DefaultFileRegion(rdafile.getChannel(), 0, // rdafile.length()); // // System.out.println("This is" + ++counter + "times receive server:[" // + msg + "]"); // &#125; // @Override // public void channelReadComplete(ChannelHandlerContext ctx) throws // Exception &#123; // ctx.flush(); // &#125; public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) &#123; cause.printStackTrace(); ctx.close(); &#125; // @Override // protected void channelRead0(ChannelHandlerContext ctx, String msg) // throws Exception &#123; // String a = msg; // System.out.println("This is"+ // ++counter+"times receive server:["+msg+"]"); // &#125;&#125; 我们还自定义了一个对象，用于统计文件上传进度的： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253import java.io.File;import java.io.Serializable;public class FileUploadFile implements Serializable &#123; private static final long serialVersionUID = 1L; private File file;// 文件 private String file_md5;// 文件名 private int starPos;// 开始位置 private byte[] bytes;// 文件字节数组 private int endPos;// 结尾位置 public int getStarPos() &#123; return starPos; &#125; public void setStarPos(int starPos) &#123; this.starPos = starPos; &#125; public int getEndPos() &#123; return endPos; &#125; public void setEndPos(int endPos) &#123; this.endPos = endPos; &#125; public byte[] getBytes() &#123; return bytes; &#125; public void setBytes(byte[] bytes) &#123; this.bytes = bytes; &#125; public File getFile() &#123; return file; &#125; public void setFile(File file) &#123; this.file = file; &#125; public String getFile_md5() &#123; return file_md5; &#125; public void setFile_md5(String file_md5) &#123; this.file_md5 = file_md5; &#125;&#125; 输出为： 块儿长度：894 长度：8052 -----------------------------894 byte 长度：894 块儿长度：894 长度：7158 -----------------------------894 byte 长度：894 块儿长度：894 长度：6264 -----------------------------894 byte 长度：894 块儿长度：894 长度：5370 -----------------------------894 byte 长度：894 块儿长度：894 长度：4476 -----------------------------894 byte 长度：894 块儿长度：894 长度：3582 -----------------------------894 byte 长度：894 块儿长度：894 长度：2688 -----------------------------894 byte 长度：894 块儿长度：894 长度：1794 -----------------------------894 byte 长度：894 块儿长度：894 长度：900 -----------------------------894 byte 长度：894 块儿长度：894 长度：6 -----------------------------6 byte 长度：6 块儿长度：894 长度：0 -----------------------------0 文件已经读完--------0 Process finished with exit code 0 这样就实现了服务器端文件的上传，当然我们也可以使用http的形式。 server端： 12345678910111213141516171819202122232425262728293031323334353637383940import io.netty.bootstrap.ServerBootstrap;import io.netty.channel.ChannelFuture;import io.netty.channel.EventLoopGroup;import io.netty.channel.nio.NioEventLoopGroup;import io.netty.channel.socket.nio.NioServerSocketChannel;public class HttpFileServer implements Runnable &#123; private int port; public HttpFileServer(int port) &#123; super(); this.port = port; &#125; @Override public void run() &#123; EventLoopGroup bossGroup = new NioEventLoopGroup(1); EventLoopGroup workerGroup = new NioEventLoopGroup(); ServerBootstrap serverBootstrap = new ServerBootstrap(); serverBootstrap.group(bossGroup, workerGroup); serverBootstrap.channel(NioServerSocketChannel.class); //serverBootstrap.handler(new LoggingHandler(LogLevel.INFO)); serverBootstrap.childHandler(new HttpChannelInitlalizer()); try &#123; ChannelFuture f = serverBootstrap.bind(port).sync(); f.channel().closeFuture().sync(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; bossGroup.shutdownGracefully(); workerGroup.shutdownGracefully(); &#125; &#125; public static void main(String[] args) &#123; HttpFileServer b = new HttpFileServer(9003); new Thread(b).start(); &#125;&#125; Server端initializer： 1234567891011121314151617181920import io.netty.channel.ChannelInitializer;import io.netty.channel.ChannelPipeline;import io.netty.channel.socket.SocketChannel;import io.netty.handler.codec.http.HttpObjectAggregator;import io.netty.handler.codec.http.HttpServerCodec;import io.netty.handler.stream.ChunkedWriteHandler;public class HttpChannelInitlalizer extends ChannelInitializer&lt;SocketChannel&gt; &#123; @Override protected void initChannel(SocketChannel ch) throws Exception &#123; ChannelPipeline pipeline = ch.pipeline(); pipeline.addLast(new HttpServerCodec()); pipeline.addLast(new HttpObjectAggregator(65536)); pipeline.addLast(new ChunkedWriteHandler()); pipeline.addLast(new HttpChannelHandler()); &#125;&#125; server端hadler： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178import static io.netty.handler.codec.http.HttpHeaders.Names.CONTENT_TYPE;import static io.netty.handler.codec.http.HttpResponseStatus.BAD_REQUEST;import static io.netty.handler.codec.http.HttpResponseStatus.FORBIDDEN;import static io.netty.handler.codec.http.HttpResponseStatus.INTERNAL_SERVER_ERROR;import static io.netty.handler.codec.http.HttpResponseStatus.NOT_FOUND;import static io.netty.handler.codec.http.HttpVersion.HTTP_1_1;import io.netty.buffer.Unpooled;import io.netty.channel.ChannelFuture;import io.netty.channel.ChannelFutureListener;import io.netty.channel.ChannelHandlerContext;import io.netty.channel.ChannelProgressiveFuture;import io.netty.channel.ChannelProgressiveFutureListener;import io.netty.channel.SimpleChannelInboundHandler;import io.netty.handler.codec.http.DefaultFullHttpResponse;import io.netty.handler.codec.http.DefaultHttpResponse;import io.netty.handler.codec.http.FullHttpRequest;import io.netty.handler.codec.http.FullHttpResponse;import io.netty.handler.codec.http.HttpChunkedInput;import io.netty.handler.codec.http.HttpHeaders;import io.netty.handler.codec.http.HttpResponse;import io.netty.handler.codec.http.HttpResponseStatus;import io.netty.handler.codec.http.HttpVersion;import io.netty.handler.codec.http.LastHttpContent;import io.netty.handler.stream.ChunkedFile;import io.netty.util.CharsetUtil;import io.netty.util.internal.SystemPropertyUtil;import java.io.File;import java.io.FileNotFoundException;import java.io.RandomAccessFile;import java.io.UnsupportedEncodingException;import java.net.URLDecoder;import java.util.regex.Pattern;import javax.activation.MimetypesFileTypeMap;public class HttpChannelHandler extends SimpleChannelInboundHandler&lt;FullHttpRequest&gt; &#123; public static final String HTTP_DATE_FORMAT = "EEE, dd MMM yyyy HH:mm:ss zzz"; public static final String HTTP_DATE_GMT_TIMEZONE = "GMT"; public static final int HTTP_CACHE_SECONDS = 60; @Override protected void channelRead0(ChannelHandlerContext ctx, FullHttpRequest request) throws Exception &#123; // 监测解码情况 if (!request.getDecoderResult().isSuccess()) &#123; sendError(ctx, BAD_REQUEST); return; &#125; final String uri = request.getUri(); final String path = sanitizeUri(uri); System.out.println("get file："+path); if (path == null) &#123; sendError(ctx, FORBIDDEN); return; &#125; //读取要下载的文件 File file = new File(path); if (file.isHidden() || !file.exists()) &#123; sendError(ctx, NOT_FOUND); return; &#125; if (!file.isFile()) &#123; sendError(ctx, FORBIDDEN); return; &#125; RandomAccessFile raf; try &#123; raf = new RandomAccessFile(file, "r"); &#125; catch (FileNotFoundException ignore) &#123; sendError(ctx, NOT_FOUND); return; &#125; long fileLength = raf.length(); HttpResponse response = new DefaultHttpResponse(HttpVersion.HTTP_1_1, HttpResponseStatus.OK); HttpHeaders.setContentLength(response, fileLength); setContentTypeHeader(response, file); //setDateAndCacheHeaders(response, file); if (HttpHeaders.isKeepAlive(request)) &#123; response.headers().set("CONNECTION", HttpHeaders.Values.KEEP_ALIVE); &#125; // Write the initial line and the header. ctx.write(response); // Write the content. ChannelFuture sendFileFuture = ctx.write(new HttpChunkedInput(new ChunkedFile(raf, 0, fileLength, 8192)), ctx.newProgressivePromise()); //sendFuture用于监视发送数据的状态 sendFileFuture.addListener(new ChannelProgressiveFutureListener() &#123; @Override public void operationProgressed(ChannelProgressiveFuture future, long progress, long total) &#123; if (total &lt; 0) &#123; // total unknown System.err.println(future.channel() + " Transfer progress: " + progress); &#125; else &#123; System.err.println(future.channel() + " Transfer progress: " + progress + " / " + total); &#125; &#125; @Override public void operationComplete(ChannelProgressiveFuture future) &#123; System.err.println(future.channel() + " Transfer complete."); &#125; &#125;); // Write the end marker ChannelFuture lastContentFuture = ctx.writeAndFlush(LastHttpContent.EMPTY_LAST_CONTENT); // Decide whether to close the connection or not. if (!HttpHeaders.isKeepAlive(request)) &#123; // Close the connection when the whole content is written out. lastContentFuture.addListener(ChannelFutureListener.CLOSE); &#125; &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) &#123; cause.printStackTrace(); if (ctx.channel().isActive()) &#123; sendError(ctx, INTERNAL_SERVER_ERROR); &#125; ctx.close(); &#125; private static final Pattern INSECURE_URI = Pattern.compile(".*[&lt;&gt;&amp;\"].*"); private static String sanitizeUri(String uri) &#123; // Decode the path. try &#123; uri = URLDecoder.decode(uri, "UTF-8"); &#125; catch (UnsupportedEncodingException e) &#123; throw new Error(e); &#125; if (!uri.startsWith("/")) &#123; return null; &#125; // Convert file separators. uri = uri.replace('/', File.separatorChar); // Simplistic dumb security check. // You will have to do something serious in the production environment. if (uri.contains(File.separator + '.') || uri.contains('.' + File.separator) || uri.startsWith(".") || uri.endsWith(".") || INSECURE_URI.matcher(uri).matches()) &#123; return null; &#125; // Convert to absolute path. return SystemPropertyUtil.get("user.dir") + File.separator + uri; &#125; private static void sendError(ChannelHandlerContext ctx, HttpResponseStatus status) &#123; FullHttpResponse response = new DefaultFullHttpResponse(HTTP_1_1, status, Unpooled.copiedBuffer("Failure: " + status + "\r\n", CharsetUtil.UTF_8)); response.headers().set(CONTENT_TYPE, "text/plain; charset=UTF-8"); // Close the connection as soon as the error message is sent. ctx.writeAndFlush(response).addListener(ChannelFutureListener.CLOSE); &#125; /** * Sets the content type header for the HTTP Response * * @param response * HTTP response * @param file * file to extract content type */ private static void setContentTypeHeader(HttpResponse response, File file) &#123; MimetypesFileTypeMap m = new MimetypesFileTypeMap(); String contentType = m.getContentType(file.getPath()); if (!contentType.equals("application/octet-stream")) &#123; contentType += "; charset=utf-8"; &#125; response.headers().set(CONTENT_TYPE, contentType); &#125;&#125; client端： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687import io.netty.bootstrap.Bootstrap;import io.netty.channel.ChannelFuture;import io.netty.channel.ChannelInitializer;import io.netty.channel.ChannelOption;import io.netty.channel.EventLoopGroup;import io.netty.channel.nio.NioEventLoopGroup;import io.netty.channel.socket.SocketChannel;import io.netty.channel.socket.nio.NioSocketChannel;import io.netty.handler.codec.http.DefaultFullHttpRequest;import io.netty.handler.codec.http.HttpHeaders;import io.netty.handler.codec.http.HttpMethod;import io.netty.handler.codec.http.HttpRequestEncoder;import io.netty.handler.codec.http.HttpResponseDecoder;import io.netty.handler.codec.http.HttpVersion;import io.netty.handler.stream.ChunkedWriteHandler;import java.net.URI;public class HttpDownloadClient &#123; /** * 下载http资源 向服务器下载直接填写要下载的文件的相对路径 * （↑↑↑建议只使用字母和数字对特殊字符对字符进行部分过滤可能导致异常↑↑↑） * 向互联网下载输入完整路径 * @param host 目的主机ip或域名 * @param port 目标主机端口 * @param url 文件路径 * @param local 本地存储路径 * @throws Exception */ public void connect(String host, int port, String url, final String local) throws Exception &#123; EventLoopGroup workerGroup = new NioEventLoopGroup(); try &#123; Bootstrap b = new Bootstrap(); b.group(workerGroup); b.channel(NioSocketChannel.class); b.option(ChannelOption.SO_KEEPALIVE, true); b.handler(new ChildChannelHandler(local)); // Start the client. ChannelFuture f = b.connect(host, port).sync(); URI uri = new URI(url); DefaultFullHttpRequest request = new DefaultFullHttpRequest( HttpVersion.HTTP_1_1, HttpMethod.GET, uri.toASCIIString()); // 构建http请求 request.headers().set(HttpHeaders.Names.HOST, host); request.headers().set(HttpHeaders.Names.CONNECTION, HttpHeaders.Values.KEEP_ALIVE); request.headers().set(HttpHeaders.Names.CONTENT_LENGTH, request.content().readableBytes()); // 发送http请求 f.channel().write(request); f.channel().flush(); f.channel().closeFuture().sync(); &#125; finally &#123; workerGroup.shutdownGracefully(); &#125; &#125; private class ChildChannelHandler extends ChannelInitializer&lt;SocketChannel&gt; &#123; String local; public ChildChannelHandler(String local) &#123; this.local = local; &#125; @Override protected void initChannel(SocketChannel ch) throws Exception &#123; // 客户端接收到的是httpResponse响应，所以要使用HttpResponseDecoder进行解码 ch.pipeline().addLast(new HttpResponseDecoder()); // 客户端发送的是httprequest，所以要使用HttpRequestEncoder进行编码 ch.pipeline().addLast(new HttpRequestEncoder()); ch.pipeline().addLast(new ChunkedWriteHandler()); ch.pipeline().addLast(new HttpDownloadHandler(local)); &#125; &#125; public static void main(String[] args) throws Exception &#123; HttpDownloadClient client = new HttpDownloadClient(); //client.connect("127.0.0.1", 9003,"/file/pppp/1.doc","1.doc");// client.connect("zlysix.gree.com", 80, "http://zlysix.gree.com/HelloWeb/download/20m.apk", "20m.apk"); client.connect("www.ghost64.com", 80, "http://www.ghost64.com/qqtupian/zixunImg/local/2017/05/27/1495855297602.jpg", "1495855297602.jpg"); &#125;&#125; client端handler： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596import java.io.File;import java.io.FileOutputStream;import io.netty.buffer.ByteBuf;import io.netty.channel.ChannelHandlerContext;import io.netty.channel.ChannelInboundHandlerAdapter;import io.netty.handler.codec.http.HttpContent;//import io.netty.handler.codec.http.HttpHeaders;import io.netty.handler.codec.http.HttpResponse;import io.netty.handler.codec.http.LastHttpContent;import io.netty.util.internal.SystemPropertyUtil;/** * @Author:yangyue * @Description: * @Date: Created in 9:15 on 2017/5/28. */public class HttpDownloadHandler extends ChannelInboundHandlerAdapter &#123; private boolean readingChunks = false; // 分块读取开关 private FileOutputStream fOutputStream = null;// 文件输出流 private File localfile = null;// 下载文件的本地对象 private String local = null;// 待下载文件名 private int succCode;// 状态码 public HttpDownloadHandler(String local) &#123; this.local = local; &#125; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; if (msg instanceof HttpResponse) &#123;// response头信息 HttpResponse response = (HttpResponse) msg; succCode = response.getStatus().code(); if (succCode == 200) &#123; setDownLoadFile();// 设置下载文件 readingChunks = true; &#125; // System.out.println("CONTENT_TYPE:" // + response.headers().get(HttpHeaders.Names.CONTENT_TYPE)); &#125; if (msg instanceof HttpContent) &#123;// response体信息 HttpContent chunk = (HttpContent) msg; if (chunk instanceof LastHttpContent) &#123; readingChunks = false; &#125; ByteBuf buffer = chunk.content(); byte[] dst = new byte[buffer.readableBytes()]; if (succCode == 200) &#123; while (buffer.isReadable()) &#123; buffer.readBytes(dst); fOutputStream.write(dst); buffer.release(); &#125; if (null != fOutputStream) &#123; fOutputStream.flush(); &#125; &#125; &#125; if (!readingChunks) &#123; if (null != fOutputStream) &#123; System.out.println("Download done-&gt;"+ localfile.getAbsolutePath()); fOutputStream.flush(); fOutputStream.close(); localfile = null; fOutputStream = null; &#125; ctx.channel().close(); &#125; &#125; /** * 配置本地参数，准备下载 */ private void setDownLoadFile() throws Exception &#123; if (null == fOutputStream) &#123; local = SystemPropertyUtil.get("user.dir") + File.separator +local; //System.out.println(local); localfile = new File(local); if (!localfile.exists()) &#123; localfile.createNewFile(); &#125; fOutputStream = new FileOutputStream(localfile); &#125; &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; System.out.println("管道异常：" + cause.getMessage()); cause.printStackTrace(); ctx.channel().close(); &#125;&#125; 这里客户端我放的是网络连接，下载的是一副图片，启动服务端和客户端就可以看到这个图片被下载到了工程的根目录下。]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[消息中间件-activemq安全机制]]></title>
    <url>%2Fposts%2F413284da.html</url>
    <content type="text"><![CDATA[activemq作为消息中间件这样一个独立的个体存在，连通用户和服务器。如果没有一套完备的安全机制去设置用户权限设置消息分发机制可想后果是非常严重。ActiveMQ如果不加入安全机制的话，任何人只要知道消息服务的具体地址(包括ip，端口，消息地址[队列或者主题地址，)，都可以肆无忌惮的发送、接收消息。今天我们就探讨一下他的安全机制。 1.安全机制介绍 我们讨论安全机制一般包括两个部分： 验证(Authentication)：就是要验证一个用户的有效性，即用户名、密码是否正确; 授权(Authorization)：就是授予用户某种角色，以使用户只能访问具有相应角色的资源。 activemq考虑到安全方案效率问题，他提供了可插拔的安全机制，你可以使用不同的安全插件灵活为你的系统配置安全访问方式。目前activemq提供两种安全控制插件： 简单认证插件(Simple authentication plugin-in) JAAS认证插件(Java Authentication and Authorization Service) 下面我们分别就这两种插件的使用做一个说明。 2.简单认证插件 简单认证插件的目的就是让用户简单配置。我们打开activemq服务的目录apache-activemq，在 conf目录下找到activemq.xml。进去找到： 123&lt;shutdownHooks&gt; &lt;bean xmlns="http://www.springframework.org/schema/beans" class="org.apache.activemq.hooks.SpringContextHook" /&gt;&lt;/shutdownHooks&gt; 在他下面添加如下即可： 12345678&lt;plugins&gt; &lt;simpleAuthenticationPlugin&gt; &lt;users&gt; &lt;authenticationUser username="admin" password="admin" groups="users,admins"/&gt; &lt;authenticationUser username="user" password="password" groups="users"/&gt; &lt;/users&gt; &lt;/simpleAuthenticationPlugin&gt;&lt;/plugins&gt; ☆注意：此处添加的用户名和密码要和你在项目中配置的activemq用户名密码是一致的，如果在项目中不是此处已经配置过的用户发送消息的话，activemq客户端不会受理该消息。这样就达到了对非命中用户拦截的目的。 比如说你有客户端使用的用户是： 1234&lt;amq:connectionFactory id="amqConnectionFactory" brokerURL="tcp://127.0.0.1:61616" userName="admin" password="admin" /&gt; 那你就把该用户配置到activemq的配置文件中： 1&lt;authenticationUser username="admin" password="admin" groups="users,admins"/&gt; 上面是对用户进行限制，我们也可以对ip进行限制，还是在刚才的配置里面加上下面这一句： 12345678910111213&lt;plugins&gt; &lt;simpleAuthenticationPlugin&gt; &lt;users&gt; &lt;authenticationUser username="admin" password="admin" groups="users,admins"/&gt; &lt;!-- &lt;authenticationUser username="user" password="password" groups="users"/&gt; &lt;authenticationUser username="guest" password="password" groups="guests"/&gt;--&gt; &lt;/users&gt; &lt;transportConnectors&gt; &lt;transportConnector name="connection1" uri="tcp://0.0.0.0:61616" /&gt; &lt;/transportConnectors&gt; &lt;/simpleAuthenticationPlugin&gt;&lt;/plugins&gt; 0.0.0.0代表本网络中的所有主机，意味着该网段的所有主机都是可以通讯的。如果改成localhost或者127.0.0.1这种的那就只有本机了。这样我们就达到了通过IP限制的目的。 3.JAAS认证插件 JAAS(Java Authentication and Authorization Service)也就是java的验证Authentication)、授权(Authorization)服务。简单来说，验证Authentication就是要验证一个用户的有效性，即用户名、密码是否正确。授权Authorization就是授予用户某种角色，可以访问哪些资源。JAASAuthentication Plugin依赖标准的JAAS机制来实现认证。通常情况下，你需要通过设置Java.security.auth.login.config系统属性来配置login modules的配置文件。如果没有指定这个系统属性，那么JAAS Authentication Plugin会缺省使用login.config作为文件名。 首先我们需要编写一个login.config文件： 123456activemq &#123; org.apache.activemq.jaas.PropertiesLoginModule required debug=true org.apache.activemq.jaas.properties.user="users.properties" org.apache.activemq.jaas.properties.group="groups.properties"; &#125;; users.properties文件： 123admin=admin user=ad1guest=ad1 group.properties文件： 123admins=admin users=user guests=guest ☆需要注意的是，PropertiesLoginModule使用本地文件的查找方式，而且查找时采用的base directory即login.config文件所在的目录，所以说这三个文件需要在同一个目录里才会找得到。另外，activemq 5.9 默认提供了以上的配置文件，我们来看一下文件目录： 然后我们还是在activemq.xml配置文件中添加插件。还是上面简单插件添加的位置，添加以下插件即可，只不过你的把之前添加的简单插件注释掉。 123456789101112131415161718192021&lt;plugins&gt; &lt;jaasAuthenticationPlugin configuration="activemq-domain" /&gt; &lt;authorizationPlugin&gt; &lt;map&gt; &lt;authorizationMap&gt; &lt;authorizationEntries&gt; &lt;!-- .表示通配符,例如USERS.&gt;表示以USERS.开头的主题,&gt;表示所有主题,read表示读的权限,write表示写的权限，admin表示角色组--&gt; &lt;authorizationEntry queue="&gt;" read="admins,guests" write="guests" admin="admins,guests" /&gt; &lt;authorizationEntry queue="USERS.&gt;" read="users" write="users" admin="users" /&gt; &lt;authorizationEntry queue="GUEST.&gt;" read="guests" write="guests,users" admin="guests,users" /&gt; &lt;authorizationEntry topic="&gt;" read="admins" write="admins" admin="admins" /&gt; &lt;authorizationEntry topic="USERS.&gt;" read="users" write="users" admin="users" /&gt; &lt;authorizationEntry topic="GUEST.&gt;" read="guests" write="guests,users" admin="guests,users" /&gt; &lt;authorizationEntry topic="ActiveMQ.Advisory.&gt;" read="guests,users" write="guests,users" admin="guests,users"/&gt; &lt;/authorizationEntries&gt; &lt;/authorizationMap&gt; &lt;/map&gt; &lt;/authorizationPlugin&gt;&lt;/plugins&gt; 添加完以上配置部分，重启avtivemq服务端，就会按照上面配置的用户进行读写的权限配置。 从上面看JAAS插件的权限分配要比简单插件的权限更加细致，不同的用户可以分别配置读写的权限，admin用户拥有创建topic或是queue的特权等等这样细致的划分，不同的用户各司其职，减少了误操作，或是刻意破换的可能性。]]></content>
      <categories>
        <category>activeMQ</category>
      </categories>
      <tags>
        <tag>学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netty学习(九)-Netty编解码技术之Marshalling]]></title>
    <url>%2Fposts%2Fccb5d663.html</url>
    <content type="text"><![CDATA[前面我们讲过protobuf的使用，主流的编解码框架其实还有很多种： ①JBoss的Marshalling包 ②google的Protobuf ③基于Protobuf的Kyro ④Apache的Thrift JBoss Marshalling是一个Java对象的序列化API包，修正了JDK自带的序列化包的很多问题，但又保持跟java.io.Serializable接口的兼容；同时增加了一些可调的参数和附加的特性，并且这些参数和特性可通过工厂类进行配置。 相比于传统的Java序列化机制，它的优点如下： 1) 可插拔的类解析器，提供更加便捷的类加载定制策略，通过一个接口即可实现定制； 2) 可插拔的对象替换技术，不需要通过继承的方式； 3) 可插拔的预定义类缓存表，可以减小序列化的字节数组长度，提升常用类型的对象序列化性能； 4) 无须实现java.io.Serializable接口，即可实现Java序列化； 5) 通过缓存技术提升对象的序列化性能。 相比于protobuf和thrift的两种编解码框架，JBoss Marshalling更多是在JBoss内部使用，应用范围有限。 Protobuf全称Google Protocol Buffers，它由谷歌开源而来，在谷歌内部久经考验。它将数据结构以.proto文件进行描述，通过代码生成工具可以生成对应数据结构的POJO对象和Protobuf相关的方法和属性。 它的特点如下： 1) 结构化数据存储格式（XML，JSON等）； 2) 高效的编解码性能； 3) 语言无关、平台无关、扩展性好； 4) 官方支持Java、C++和Python三种语言。 首先我们来看下为什么不使用XML，尽管XML的可读性和可扩展性非常好，也非常适合描述数据结构，但是XML解析的时间开销和XML为了可读性而牺牲的空间开销都非常大，因此不适合做高性能的通信协议。Protobuf使用二进制编码，在空间和性能上具有更大的优势。 Protobuf另一个比较吸引人的地方就是它的数据描述文件和代码生成机制，利用数据描述文件对数据结构进行说明的优点如下： 1) 文本化的数据结构描述语言，可以实现语言和平台无关，特别适合异构系统间的集成； 2) 通过标识字段的顺序，可以实现协议的前向兼容； 3) 自动代码生成，不需要手工编写同样数据结构的C++和Java版本； 4) 方便后续的管理和维护。相比于代码，结构化的文档更容易管理和维护。 Thrift源于Facebook，在2007年Facebook将Thrift作为一个开源项目提交给Apache基金会。对于当时的Facebook来说，创造Thrift是为了解决Facebook各系统间大数据量的传输通信以及系统之间语言环境不同需要跨平台的特性，因此Thrift可以支持多种程序语言，如C++、C#、Cocoa、Erlang、Haskell、Java、Ocami、Perl、PHP、Python、Ruby和Smalltalk。 在多种不同的语言之间通信，Thrift可以作为高性能的通信中间件使用，它支持数据（对象）序列化和多种类型的RPC服务。Thrift适用于静态的数据交换，需要先确定好它的数据结构，当数据结构发生变化时，必须重新编辑IDL文件，生成代码和编译，这一点跟其他IDL工具相比可以视为是Thrift的弱项。Thrift适用于搭建大型数据交换及存储的通用工具，对于大型系统中的内部数据传输，相对于JSON和XML在性能和传输大小上都有明显的优势。 Thrift主要由5部分组成： 1) 语言系统以及IDL编译器：负责由用户给定的IDL文件生成相应语言的接口代码； 2) TProtocol：RPC的协议层，可以选择多种不同的对象序列化方式，如JSON和Binary； 3) TTransport：RPC的传输层，同样可以选择不同的传输层实现，如socket、NIO、MemoryBuffer等； 4) TProcessor：作为协议层和用户提供的服务实现之间的纽带，负责调用服务实现的接口； 5) TServer：聚合TProtocol、TTransport和TProcessor等对象。 我们重点关注的是编解码框架，与之对应的就是TProtocol。由于Thrift的RPC服务调用和编解码框架绑定在一起，所以，通常我们使用Thrift的时候会采取RPC框架的方式。但是，它的TProtocol编解码框架还是可以以类库的方式独立使用的。 与Protobuf比较类似的是，Thrift通过IDL描述接口和数据结构定义，它支持8种Java基本类型、Map、Set和List，支持可选和必选定义，功能非常强大。因为可以定义数据结构中字段的顺序，所以它也可以支持协议的前向兼容。 Thrift支持三种比较典型的编解码方式： 1) 通用的二进制编解码； 2) 压缩二进制编解码； 3) 优化的可选字段压缩编解码。 由于支持二进制压缩编解码，Thrift的编解码性能表现也相当优异，远远超过Java序列化和RMI等。 这一节我们来讲解JBoss的Marshalling的使用。 和protobuf的使用不同，netty默认支持protobuf，所以为他预设了一个编解码器：ProtobufVarint32LengthFieldPrepender，ProtobufVarint32FrameDecoder。那如果采用jboss-marshalling进行编解码，则没有这么好的运气我们需要自己优先创建一个编解码的工厂类，供信息通讯时候对信息的编解码。 pom文件如下，需要新增两个jar包：jboss-marshalling，jboss-marshalling-serial。 1234567891011121314151617181920212223242526272829303132333435363738394041&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;cn.edu.hust.netty&lt;/groupId&gt; &lt;artifactId&gt;netty&lt;/artifactId&gt; &lt;packaging&gt;war&lt;/packaging&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;name&gt;netty Maven Webapp&lt;/name&gt; &lt;url&gt;http://maven.apache.org&lt;/url&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;io.netty&lt;/groupId&gt; &lt;artifactId&gt;netty-all&lt;/artifactId&gt; &lt;version&gt;4.1.5.Final&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.10&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.google.protobuf&lt;/groupId&gt; &lt;artifactId&gt;protobuf-java&lt;/artifactId&gt; &lt;version&gt;2.6.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.jboss.marshalling&lt;/groupId&gt; &lt;artifactId&gt;jboss-marshalling-river&lt;/artifactId&gt; &lt;version&gt;1.4.10.Final&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.jboss.marshalling&lt;/groupId&gt; &lt;artifactId&gt;jboss-marshalling-serial&lt;/artifactId&gt; &lt;version&gt;1.4.11.Final&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;finalName&gt;netty&lt;/finalName&gt; &lt;/build&gt;&lt;/project&gt; 我们先来写一个工厂类，手动创建编解码器： 1234567891011121314151617181920212223242526272829303132333435363738394041import io.netty.handler.codec.marshalling.*;import org.jboss.marshalling.MarshallerFactory;import org.jboss.marshalling.Marshalling;import org.jboss.marshalling.MarshallingConfiguration;/** * Marshalling工厂 */public final class MarshallingCodeCFactory &#123; /** * 创建Jboss Marshalling解码器MarshallingDecoder * @return MarshallingDecoder */ public static MarshallingDecoder buildMarshallingDecoder() &#123; //首先通过Marshalling工具类的精通方法获取Marshalling实例对象 参数serial标识创建的是java序列化工厂对象。 final MarshallerFactory marshallerFactory = Marshalling.getProvidedMarshallerFactory("serial"); //创建了MarshallingConfiguration对象，配置了版本号为5 final MarshallingConfiguration configuration = new MarshallingConfiguration(); configuration.setVersion(5); //根据marshallerFactory和configuration创建provider UnmarshallerProvider provider = new DefaultUnmarshallerProvider(marshallerFactory, configuration); //构建Netty的MarshallingDecoder对象，俩个参数分别为provider和单个消息序列化后的最大长度 MarshallingDecoder decoder = new MarshallingDecoder(provider, 1024 * 1024 * 1); return decoder; &#125; /** * 创建Jboss Marshalling编码器MarshallingEncoder * @return MarshallingEncoder */ public static MarshallingEncoder buildMarshallingEncoder() &#123; final MarshallerFactory marshallerFactory = Marshalling.getProvidedMarshallerFactory("serial"); final MarshallingConfiguration configuration = new MarshallingConfiguration(); configuration.setVersion(5); MarshallerProvider provider = new DefaultMarshallerProvider(marshallerFactory, configuration); //构建Netty的MarshallingEncoder对象，MarshallingEncoder用于实现序列化接口的POJO对象序列化为二进制数组 MarshallingEncoder encoder = new MarshallingEncoder(provider); return encoder; &#125;&#125; 下面是服务端： 12345678910111213141516171819202122232425262728293031323334353637383940import io.netty.bootstrap.ServerBootstrap;import io.netty.channel.ChannelFuture;import io.netty.channel.EventLoopGroup;import io.netty.channel.nio.NioEventLoopGroup;import io.netty.channel.socket.nio.NioServerSocketChannel;/** * Created by Administrator on 2017/3/11. */public class HelloWordServer &#123; private int port; public HelloWordServer(int port) &#123; this.port = port; &#125; public void start()&#123; EventLoopGroup bossGroup = new NioEventLoopGroup(); EventLoopGroup workGroup = new NioEventLoopGroup(); ServerBootstrap server = new ServerBootstrap().group(bossGroup,workGroup) .channel(NioServerSocketChannel.class) .childHandler(new ServerChannelInitializer()); try &#123; ChannelFuture future = server.bind(port).sync(); future.channel().closeFuture().sync(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;finally &#123; bossGroup.shutdownGracefully(); workGroup.shutdownGracefully(); &#125; &#125; public static void main(String[] args) &#123; HelloWordServer server = new HelloWordServer(7788); server.start(); &#125;&#125; 服务端Initializer： 1234567891011121314151617181920212223import io.netty.channel.ChannelInitializer;import io.netty.channel.ChannelPipeline;import io.netty.channel.socket.SocketChannel;import io.netty.handler.codec.DelimiterBasedFrameDecoder;import io.netty.handler.codec.Delimiters;import io.netty.handler.codec.string.StringDecoder;import io.netty.handler.codec.string.StringEncoder;/** * Created by Administrator on 2017/3/11. */public class ServerChannelInitializer extends ChannelInitializer&lt;SocketChannel&gt; &#123; @Override protected void initChannel(SocketChannel socketChannel) throws Exception &#123; ChannelPipeline pipeline = socketChannel.pipeline(); pipeline.addLast(MarshallingCodeCFactory.buildMarshallingDecoder()); pipeline.addLast(MarshallingCodeCFactory.buildMarshallingEncoder()); // 自己的逻辑Handler pipeline.addLast("handler", new HelloWordServerHandler()); &#125;&#125; 注意我们在这里加入了刚才我们写的编解码器哈，顺序没有关系。 服务端handler： 123456789101112131415161718192021222324252627import io.netty.channel.ChannelHandlerContext;import io.netty.channel.ChannelInboundHandlerAdapter;/** * Created by Administrator on 2017/3/11. */public class HelloWordServerHandler extends ChannelInboundHandlerAdapter &#123; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; if(msg instanceof String)&#123; System.out.println(msg.toString()); &#125;else&#123; ctx.writeAndFlush("received your msg"); Msg m = (Msg)msg; System.out.println("client: "+m.getBody()); m.setBody("人生苦短，快用python"); ctx.writeAndFlush(m); &#125; &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; super.exceptionCaught(ctx, cause); ctx.close(); &#125;&#125; 接下来是客户端： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import io.netty.bootstrap.Bootstrap;import io.netty.channel.Channel;import io.netty.channel.ChannelFuture;import io.netty.channel.EventLoopGroup;import io.netty.channel.nio.NioEventLoopGroup;import io.netty.channel.socket.nio.NioSocketChannel;import java.io.BufferedReader;import java.io.InputStreamReader;/** * Created by Administrator on 2017/3/11. */public class HelloWorldClient &#123; private int port; private String address; public HelloWorldClient(int port,String address) &#123; this.port = port; this.address = address; &#125; public void start()&#123; EventLoopGroup group = new NioEventLoopGroup(); Bootstrap bootstrap = new Bootstrap(); bootstrap.group(group) .channel(NioSocketChannel.class) .handler(new ClientChannelInitializer()); try &#123; ChannelFuture future = bootstrap.connect(address,port).sync(); future.channel().writeAndFlush("Hello Netty Server ,I am a common client"); future.channel().closeFuture().sync(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;finally &#123; group.shutdownGracefully(); &#125; &#125; public static void main(String[] args) &#123; HelloWorldClient client = new HelloWorldClient(7788,"127.0.0.1"); client.start(); &#125;&#125; 客户端Initializer： 123456789101112131415161718192021222324import io.netty.channel.ChannelInitializer;import io.netty.channel.ChannelPipeline;import io.netty.channel.socket.SocketChannel;import io.netty.handler.codec.DelimiterBasedFrameDecoder;import io.netty.handler.codec.Delimiters;import io.netty.handler.codec.string.StringDecoder;import io.netty.handler.codec.string.StringEncoder;/** * Created by Administrator on 2017/3/11. */public class ClientChannelInitializer extends ChannelInitializer&lt;SocketChannel&gt; &#123; protected void initChannel(SocketChannel socketChannel) throws Exception &#123; ChannelPipeline pipeline = socketChannel.pipeline(); pipeline.addLast(MarshallingCodeCFactory.buildMarshallingDecoder()); pipeline.addLast(MarshallingCodeCFactory.buildMarshallingEncoder()); // 客户端的逻辑 pipeline.addLast("handler", new HelloWorldClientHandler()); &#125;&#125; 同样这里也加入编解码器。 客户端handler： 1234567891011121314151617181920212223242526272829303132import io.netty.channel.ChannelHandlerContext;import io.netty.channel.ChannelInboundHandlerAdapter;/** * Created by Administrator on 2017/3/11. */public class HelloWorldClientHandler extends ChannelInboundHandlerAdapter &#123; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; if(msg instanceof String)&#123; System.out.println(msg); &#125;else&#123; Msg m = (Msg)msg; System.out.println("client: "+m.getBody()); &#125; &#125; @Override public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; Msg msg = new Msg(); msg.setHeader((byte)0xa); msg.setLength(34); msg.setBody("放纵自己，你好兄弟"); ctx.writeAndFlush(msg); &#125; @Override public void channelInactive(ChannelHandlerContext ctx) throws Exception &#123; System.out.println("Client is close"); &#125;&#125; 我们注意上面有一个Msg对象，这个就是我们自己定义的一个对象，用于网络传输用的： 12345678910111213141516171819202122232425262728293031323334353637383940414243import java.io.Serializable;/** * 自定义一个对象 */public class Msg implements Serializable &#123; private byte header; private String body; private long length; private byte type; public byte getHeader() &#123; return header; &#125; public void setHeader(byte header) &#123; this.header = header; &#125; public String getBody() &#123; return body; &#125; public void setBody(String body) &#123; this.body = body; &#125; public long getLength() &#123; return length; &#125; public void setLength(long length) &#123; this.length = length; &#125; public byte getType() &#123; return type; &#125; public void setType(byte type) &#123; this.type = type; &#125;&#125; 下面我们运行客户端和服务端，可以看到消息已经发出去了：]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[消息中间件-activemq实战之消息持久化(六)]]></title>
    <url>%2Fposts%2Faf8467f6.html</url>
    <content type="text"><![CDATA[对于activemq消息的持久化我们在第二节的时候就简单介绍过，今天我们详细的来分析一下activemq的持久化过程以及持久化插件。在生产环境中为确保消息的可靠性，我们肯定的面临持久化消息的问题，今天就一起来攻克他吧。 1. 持久化方式介绍 前面我们也简单提到了activemq提供的插件式的消息存储，在这里再提一下，主要有以下几种方式： AMQ消息存储-基于文件的存储方式，是activemq开始的版本默认的消息存储方式； KahaDB消息存储-提供了容量的提升和恢复能力，是现在的默认存储方式； JDBC消息存储-消息基于JDBC存储的； Memory消息存储-基于内存的消息存储，由于内存不属于持久化范畴，而且如果使用内存队列，可以考虑使用更合适的产品，如ZeroMQ。所以内存存储不在讨论范围内。 上面几种消息存储方式对于消息存储的逻辑来说并没有什么区别，只是在性能以及存储方式上来说有所不同。但是对于消息发送的方式来说，p2p和Pub/Sub两种类型的消息他们的持久化方式却是不同的： 对于点对点的消息一旦消费者完成消费这条消息将从broker上删除；对于发布订阅类型的消息，即使所有的订阅者都完成了消费，Broker也不一定会马上删除无用消息，而是保留推送历史，之后会异步清除无用消息。而每个订阅者消费到了哪条消息的offset会记录在Broker，以免下次重复消费。因为消息是顺序消费，先进先出，所以只需要记录上次消息消费到哪里就可以了。 因为AMQ现在已经被不再使用被KahaDB所替代，所以我们就讲KahaDB，JDBC消息存储在许多对可靠性要求高而对性能要求低一些的大公司还是经常使用的，下面我们就这两种持久化方式的使用做一节专题。 2. Kahadb 说到Kahadb之前我们还是得提到他的前身AMQ，AMQ是一种文件存储形式，他具有写入速度快和容易恢复的特点，消息存储在一个个的文件里，文件默认大小为32M，超过这个大小的消息将会存入下一个文件。当一个文件中的消息已经全部消费，那么这个文件将被标志我可删除，在下一个清除阶段这个文件将被删除。 如果需要使用持久化，则需要在前文中的配置文件applicationContext-ActiveMQ.xml中增加如下配置： 123&lt;persistenceAdapter&gt;&lt;amqPersistenceAdapterdirectory="activemq-data"maxFileLength="32mb"/&gt;&lt;/persistenceAdapter&gt; directory : 指定持久化消息的存储目录 journalMaxFileLength : 指定保存消息的日志文件大小，具体根据你的实际应用配置 我们的Kahadb也是基于文件的本地数据库存储形式，他虽然没有AMQ快，但是扩展性很强，从activemq5.4版本之后就把Kahadb作为默认的持久化方式。 Kahadb的配置方式如下： 123&lt;persistenceAdapter&gt; &lt;kahaDB directory="activemq-data"journalMaxFileLength="32mb"/&gt; &lt;/persistenceAdapter&gt; KahaDB的属性件下表格： 属性名称 属性值 描述 directory activemq-data 消息文件和日志的存储目录 indexWriteBatchSize 1000 一批索引的大小，当要更新的索引量到达这个值时，更新到消息文件中 indexCacheSize 1000 内存中，索引的页大小 enableIndexWriteAsync false 索引是否异步写到消息文件中 journalMaxFileLength 32mb 一个消息文件的大小 enableJournalDiskSyncs true 是否讲非事务的消息同步写入到磁盘 cleanupInterval 30000 清除操作周期，单位ms checkpointInterval 5000 索引写入到消息文件的周期，单位ms ignoreMissingJournalfiles false 忽略丢失的消息文件，false，当丢失了消息文件，启动异常 checkForCorruptJournalFiles false 检查消息文件是否损坏，true，检查发现损坏会尝试修复 checksumJournalFiles false 产生一个checksum，以便能够检测journal文件是否损坏。 5.4版本之后有效的属性: archiveDataLogs false 当为true时，归档的消息文件被移到directoryArchive,而不是直接删除 directoryArchive null 存储被归档的消息文件目录 databaseLockedWaitDelay 10000 在使用负载时，等待获得文件锁的延迟时间，单位ms maxAsyncJobs 10000 同个生产者产生等待写入的异步消息最大量 concurrentStoreAndDispatchTopics false 当写入消息的时候，是否转发主题消息 concurrentStoreAndDispatchQueues true 当写入消息的时候，是否转发队列消息 5.6版本之后有效的属性: archiveCorruptedIndex false 是否归档错误的索引 由于在ActiveMQ V5.4+的版本中，KahaDB是默认的持久化存储方案。所以即使你不配置任何的KahaDB参数信息，ActiveMQ也会启动KahaDB。这种情况下，KahaDB文件所在位置是你的ActiveMQ安装路径下的/data/broker.Name/KahaDB子目录。其中{broker.Name}/KahaDB子目录。其中broker.Name/KahaDB子目录。其中{broker.Name}代表这个ActiveMQ服务节点的名称。下面我把刚启动服务并发送了消息之后的activemq安装目录打开给大家看看： 正式的生产环境还是建议在主配置文件中明确设置KahaDB的工作参数： 12345678910&lt;broker xmlns="http://activemq.apache.org/schema/core" brokerName="broker" persistent="true" useShutdownHook="false"&gt; ... &lt;persistenceAdapter&gt; &lt;kahaDB directory="activemq-data" journalMaxFileLength="32mb" concurrentStoreAndDispatchQueues="false" concurrentStoreAndDispatchTopics="false" /&gt; &lt;/persistenceAdapter&gt;&lt;/broker&gt; 3. 关系型数据库存储方案 从ActiveMQ 4+版本开始，ActiveMQ就支持使用关系型数据库进行持久化存储——通过JDBC实现的数据库连接。可以使用的关系型数据库囊括了目前市面的主流数据库。 使用JDBC的方式持久化我们就得修改之前的配置文件： 将其中的这段配置： 123&lt;persistenceAdapter&gt; &lt;kahaDB directory="$&#123;activemq.base&#125;/data/kahadb"/&gt;&lt;/persistenceAdapter&gt; 修改为下面这段内容： 123&lt;persistenceAdapter&gt; &lt;jdbcPersistenceAdapter dataSource="# mysql-ds "/&gt;&lt;/persistenceAdapter&gt; 在结点之后，增加数据源的配置,如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110&lt;!-- MySql DataSource Sample Setup --&gt; &lt;bean id="mysql-ds" class="org.apache.commons.dbcp.BasicDataSource" destroy-method="close"&gt; &lt;property name="driverClassName" value="com.mysql.jdbc.Driver"/&gt; &lt;property name="url" value="jdbc:mysql://localhost:3306/activemqdb?relaxAutoCommit=true&amp;amp;useUnicode=true&amp;amp;characterEncoding=utf-8"/&gt; &lt;property name="username" value="root"/&gt; &lt;property name="password" value="root"/&gt; &lt;property name="poolPreparedStatements" value="true"/&gt; &lt;/bean&gt; &lt;!-- Oracle DataSource Sample Setup --&gt; &lt;bean id="oracle-ds" class="org.apache.commons.dbcp.BasicDataSource" destroy-method="close"&gt; &lt;property name="driverClassName" value="oracle.jdbc.driver.OracleDriver"/&gt; &lt;property name="url" value="jdbc:oracle:thin:@localhost:1521:activemqdb"/&gt; &lt;property name="username" value="root"/&gt; &lt;property name="password" value="root"/&gt; &lt;property name="poolPreparedStatements" value="true"/&gt; &lt;/bean&gt; &lt;!-- Oracle DataSource Sample Setup --&gt; &lt;bean id="db2-ds" class="org.apache.commons.dbcp.BasicDataSource" destroy-method="close"&gt; &lt;property name="driverClassName" value="com.ibm.db2.jcc.DB2Driver"/&gt; &lt;property name="url" value="jdbc:db2://hndb02.bf.ctc.com:50002/activemq"/&gt; &lt;property name="username" value="root"/&gt; &lt;property name="password" value="root"/&gt; &lt;property name="maxActive" value="200"/&gt; &lt;property name="poolPreparedStatements" value="true"/&gt; &lt;/bean&gt; ``` 还是在上一篇的实例工程中，我们改变一下applicationContext-ActiveMQ.xml的配置如下：```java&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:amq="http://activemq.apache.org/schema/core" xmlns:context="http://www.springframework.org/schema/context" xmlns:mvc="http://www.springframework.org/schema/mvc" xsi:schemaLocation="http://www.springframework.org/schema/beanshttp://www.springframework.org/schema/beans/spring-beans-4.1.xsdhttp://www.springframework.org/schema/contexthttp://www.springframework.org/schema/context/spring-context-4.1.xsdhttp://www.springframework.org/schema/mvchttp://www.springframework.org/schema/mvc/spring-mvc-4.1.xsdhttp://activemq.apache.org/schema/corehttp://activemq.apache.org/schema/core/activemq-core-5.12.1.xsd"&gt; &lt;context:component-scan base-package="cn.edu.hust.activemq" /&gt; &lt;mvc:annotation-driven /&gt; &lt;amq:connectionFactory id="amqConnectionFactory" brokerURL="tcp://127.0.0.1:61616" userName="admin" password="admin" /&gt; &lt;!-- 配置JMS连接工厂 --&gt; &lt;bean id="connectionFactory" class="org.springframework.jms.connection.CachingConnectionFactory"&gt; &lt;constructor-arg ref="amqConnectionFactory" /&gt; &lt;property name="sessionCacheSize" value="100" /&gt; &lt;/bean&gt; &lt;!-- 定义消息队列（Queue） --&gt; &lt;bean id="demoQueueDestination" class="org.apache.activemq.command.ActiveMQQueue"&gt; &lt;!-- 设置消息队列的名字 --&gt; &lt;constructor-arg&gt; &lt;value&gt;first-queue&lt;/value&gt; &lt;/constructor-arg&gt; &lt;/bean&gt; &lt;!-- 配置JMS模板（Queue），Spring提供的JMS工具类，它发送、接收消息。 --&gt; &lt;bean id="jmsTemplate" class="org.springframework.jms.core.JmsTemplate"&gt; &lt;property name="connectionFactory" ref="connectionFactory" /&gt; &lt;property name="defaultDestination" ref="demoQueueDestination" /&gt; &lt;property name="receiveTimeout" value="10000" /&gt; &lt;!-- true是topic，false是queue，默认是false，此处显示写出false --&gt; &lt;property name="pubSubDomain" value="false" /&gt; &lt;/bean&gt; &lt;!-- 配置消息队列监听者（Queue） --&gt; &lt;bean id="queueMessageListener" class="cn.edu.hust.activemq.filter.QueueMessageListener" /&gt; &lt;!-- 显示注入消息监听容器（Queue），配置连接工厂，监听的目标是demoQueueDestination，监听器是上面定义的监听器 --&gt; &lt;bean id="queueListenerContainer" class="org.springframework.jms.listener.DefaultMessageListenerContainer"&gt; &lt;property name="connectionFactory" ref="connectionFactory" /&gt; &lt;property name="destination" ref="demoQueueDestination" /&gt; &lt;property name="messageListener" ref="queueMessageListener" /&gt; &lt;/bean&gt; &lt;broker xmlns="http://activemq.apache.org/schema/core" brokerName="localhost" dataDirectory="$&#123;activemq.data&#125;" persistent="true"&gt; &lt;!--&lt;persistenceAdapter&gt; &lt;kahaDB directory="$&#123;activemq.data&#125;/kahadb"/&gt; &lt;/persistenceAdapter&gt; --&gt; &lt;persistenceAdapter&gt; &lt;jdbcPersistenceAdapter dataDirectory="$&#123;activemq.data&#125;" dataSource="#mysql-ds"&gt; &lt;/jdbcPersistenceAdapter&gt; &lt;/persistenceAdapter&gt; &lt;/broker&gt; &lt;bean id="mysql-ds" class="org.apache.commons.dbcp.BasicDataSource" destroy-method="close"&gt; &lt;property name="driverClassName" value="com.mysql.jdbc.Driver"/&gt; &lt;property name="url" value="jdbc:mysql://127.0.0.1/activemq?relaxAutoCommit=true"/&gt; &lt;property name="username" value="root"/&gt; &lt;property name="password" value="123456"/&gt; &lt;property name="maxActive" value="200"/&gt; &lt;property name="poolPreparedStatements" value="true"/&gt; &lt;/bean&gt;&lt;/beans&gt; 此时，重新启动MQ，就会发现db数据库中多了三张表：activemq_acks，activemq_lock，activemq_msgs，OK，说明activemq已经持久化成功啦！ activemq_acks：用于存储订阅关系。如果是持久化Topic，订阅者和服务器的订阅关系在这个表保存，主要数据库字段如下： container：消息的destination sub_dest：如果是使用static集群，这个字段会有集群其他系统的信息 client_id：每个订阅者都必须有一个唯一的客户端id用以区分 sub_name：订阅者名称 selector：选择器，可以选择只消费满足条件的消息。条件可以用自定义属性实现，可支持多属性and和or操作 last_acked_id：记录消费过的消息的id activemq_lock：在集群环境中才有用，只有一个Broker可以获得消息，称为Master Broker，其他的只能作为备份等待Master Broker不可用，才可能成为下一个Master Broker。这个表用于记录哪个Broker是当前的Master Broker。 activemq_msgs：用于存储消息，Queue和Topic都存储在这个表中。主要的数据库字段如下： id：自增的数据库主键 container：消息的destination msgid_prod：消息发送者客户端的主键 msg_seq：是发送消息的顺序，msgid_prod+msg_seq可以组成jms的messageid expiration：消息的过期时间，存储的是从1970-01-01到现在的毫秒数 msg：消息本体的java序列化对象的二进制数据 priority：优先级，从0-9，数值越大优先级越高 activemq_acks用于存储订阅关系。如果是持久化topic，订阅者和服务器的订阅关系在这个表保存。]]></content>
      <categories>
        <category>activeMQ</category>
      </categories>
      <tags>
        <tag>学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netty学习(三）-Netty重要接口讲解]]></title>
    <url>%2Fposts%2Fd48afec7.html</url>
    <content type="text"><![CDATA[上一节我们写了一个HelloWorld，对于Netty的运行有了一定的了解，知道Netty是如何启动客户端和服务器端。这一节我们简要的讲解一下几个重要的接口，初步探讨Netty的运行机制，当然刚学Netty就深入原理肯定是很枯燥的，所以我们就点到为止。 1 ChannelPipeLine和ChannelHandler 在上一篇中我们在ChannelInitializer类的initChannel方法中使用了ChannelPipeline，然后在ChannelPipeline中使用了handler来处理业务逻辑。 ChannelPipeline是ChannelHandler的容器，它负责ChannelHandler的管理和事件拦截与调度。Netty的ChannelPipeline和ChannelHandler机制类似于Servlet 和Filter 过滤器，这类拦截器实际上是职责链模式的一种变形，主要是为了方便事件的拦截和用户业务逻辑的定制。 Netty的channel运用机制和Filter过滤器机制一样，它将Channel 的数据管道抽象为ChannelPipeline. 消息在ChannelPipeline中流动和传递。ChannelPipeline 持有I/O事件拦截器ChannelHandler 的链表，由ChannelHandler 对I/0 事件进行拦截和处理，可以方便地通过新增和删除ChannelHandler 来实现小同的业务逻辑定制，不需要对已有的ChannelHandler进行修改，能够实现对修改封闭和对扩展的支持。 通过一张图我们来看一下他们之间的关系： 一个Channel中包含一个ChannelPipeline，用来处理Channel中的事件，一个ChannelPipeline中可以包含很多个handler，第二节的示例代码中我们也看到了，使用各种handler来处理通信信息。 同时我们也注意到在hadler中继承了ChannelInboundHandlerAdapter类并实现了他的一些方法，比如：channelRead，channelActive，channelInactive等等，我们看到这些方法中都有一个参数：ChannelHandlerContext ctx。这个ChannelHandlerContext就是handler的上下文对象，有了这个ChannelHandlerContext你就获得了一切，你可以获得通道，获得事件的控制权。 事实上，用户不需要自己创建pipeline，因为使用ServerBootstrap 或者Bootstrap 启动 服务端或者客户端时， Netty 会为每个Channel 连接创建一个独立的pipeline。 12345678ChannelPipeline pipeline = socketChannel.pipeline();pipeline.addLast("framer", new DelimiterBasedFrameDecoder(8192,Delimiters.lineDelimiter()));pipeline.addLast("decoder", new StringDecoder());pipeline.addLast("encoder", new StringEncoder());// 客户端的逻辑pipeline.addLast("handler", new HelloWorldClientHandler()); ChannelPipeline 是线程安全的， 这意味着N个业务线程可以并发地操作ChannelPipeline 而不存在多线程并发问题。但是，ChannelHandler却不是线程安全的，这意味着尽管 ChannelPipeline 是线程去全的， 但是仍然需要自己保证ChannelHandler的线程安全。 Netty 中的事件分为inbound 事件和outbound 事件。inbound 事件通常由I/O线程触发，例如TCP 链路建立事件、链路关闭事件、读事件、异常通知事件等。Outbound 事件通常是I/O 用户主动发起的网络I/O 操作，例如用户发起的连接操作、绑定操作、消息发送等操作。 我们常用的inbound事件有： ChannelHandlerContext fireChannelRegistered() //channel注册事件 ChannelHandlerContext fireChannelActive() //channel激活事件 ChannelHandlerContext fireExceptionCaught(Throwable var1) //channel异常处理事件 ChannelHandlerContext fireUserEventTriggered(Object var1) //用户自定义事件 ChannelHandlerContext fireChannelRead(Object var1) //读事件 pipeline 中以fireXXX命名的方法都是从I/O 线程流向用户业务Handler的inbound 事件，它们的实现因功能而异，但是处理步骤类似： 调用HeadHandler对应的fireXXX 方法 执行事件相关的逻辑操作 常用的outbound事件有： ChannelFuture bind(SocketAddress var1, ChannelPromise var2) //绑定地址 ChannelFuture connect(SocketAddress var1, ChannelPromise var2) //连接服务器 ChannelFuture write(Object var1) //发送事件 ChannelHandlerContext flush() //刷新事件 上面我们说到事件，netty的事件机制是由前至后的，一般来说，都是一个channel的ChannnelActive方法中调用fireChannelActive来触发调用下一个handler中的ChannelActive方法，即你在ChannelPipeline中添加handler的时候，要在第一个handler的channelActive方法中调用fireChannelActive，以此来触发下一个事件。我们再来写一个案例说明一下： 客户端： 12345678910111213141516171819202122232425262728293031323334public class HWClient &#123; private int port; private String address; public HWClient(int port, String address) &#123; this.port = port; this.address = address; &#125; public void start()&#123; EventLoopGroup group = new NioEventLoopGroup(); Bootstrap bootstrap = new Bootstrap(); bootstrap.group(group) .channel(NioSocketChannel.class) .handler(new ClientChannelInitializer()); try &#123; ChannelFuture future = bootstrap.connect(address,port).sync(); future.channel().writeAndFlush("Hello Netty Server ,I am a common client"); future.channel().closeFuture().sync(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;finally &#123; group.shutdownGracefully(); &#125; &#125; public static void main(String[] args) &#123; HWClient client = new HWClient(7788,"127.0.0.1"); client.start(); &#125;&#125; 客户端ClientChannelInitializer： 1234567891011121314public class ClientChannelInitializer extends ChannelInitializer&lt;SocketChannel&gt; &#123; protected void initChannel(SocketChannel socketChannel) throws Exception &#123; ChannelPipeline pipeline = socketChannel.pipeline(); pipeline.addLast("decoder", new StringDecoder()); pipeline.addLast("encoder", new StringEncoder()); // 客户端的handler //先调用handler在ChannnelActive方法中调用fireChannelActive会激活handler1 pipeline.addLast("handler", new HWClientHandler()); pipeline.addLast("handler1", new BaseClientHandler()); &#125;&#125; 客户端handler： 123456789101112131415161718public class HWClientHandler extends ChannelInboundHandlerAdapter &#123; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; System.out.println("server say : "+msg.toString()); &#125; @Override public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; System.out.println("Handler1"); ctx.fireChannelActive(); &#125; @Override public void channelInactive(ChannelHandlerContext ctx) throws Exception &#123; System.out.println("Client is close"); &#125;&#125; 客户端的第二个handler： 12345678910111213public class BaseClientHandler extends ChannelInboundHandlerAdapter &#123; @Override public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; System.out.println("Handler2"); &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; ctx.close(); &#125;&#125; 服务端： 12345678910111213141516171819202122232425262728293031public class HWServer &#123; private int port; public HWServer(int port) &#123; this.port = port; &#125; public void start()&#123; EventLoopGroup bossGroup = new NioEventLoopGroup(); EventLoopGroup workGroup = new NioEventLoopGroup(); ServerBootstrap server = new ServerBootstrap().group(bossGroup,workGroup) .channel(NioServerSocketChannel.class) .childHandler(new ServerChannelInitializer()); try &#123; ChannelFuture future = server.bind(port).sync(); future.channel().closeFuture().sync(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;finally &#123; bossGroup.shutdownGracefully(); workGroup.shutdownGracefully(); &#125; &#125; public static void main(String[] args) &#123; HWServer server = new HWServer(7788); server.start(); &#125;&#125; 服务端ServerChannelInitializer: 12345678910111213public class ServerChannelInitializer extends ChannelInitializer&lt;SocketChannel&gt; &#123; @Override protected void initChannel(SocketChannel socketChannel) throws Exception &#123; ChannelPipeline pipeline = socketChannel.pipeline(); // 字符串解码 和 编码 pipeline.addLast("decoder", new StringDecoder()); pipeline.addLast("encoder", new StringEncoder()); // 自己的逻辑Handler pipeline.addLast("handler", new HWServerHandler()); &#125;&#125; 服务端handler： 1234567891011121314151617181920public class HWServerHandler extends ChannelInboundHandlerAdapter &#123; @Override public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; System.out.println("channelActive"); &#125; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; System.out.println(ctx.channel().remoteAddress()+"===&gt;server: "+msg.toString()); ctx.write("received your msg"); ctx.flush(); &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; super.exceptionCaught(ctx, cause); ctx.close(); &#125;&#125; 我们启动服务端和客户端，会发现客户端的两个handler都通过了。 先调用HWClientHandler，打印出：HWClientHandler channelActive；继而调用了BaseClientHandler ，打印出：BaseClient1Handler channelActive.]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java学习-NIO(二)Buffer]]></title>
    <url>%2Fposts%2Fe8d11d6a.html</url>
    <content type="text"><![CDATA[当我们需要与 NIO Channel 进行交互时, 我们就需要使用到 NIO Buffer, 即数据从 Buffer读取到 Channel 中, 并且从 Channel 中写入到 Buffer 中。缓冲区本质上是一块可以写入数据，然后可以从中读取数据的内存。这块内存被包装成NIO Buffer对象，并提供了一组方法，用来方便的访问该块内存。 缓冲区基础 Buffer 类型有: 缓冲区是包在一个对象内的基础数据的数组，Buffer类相比一般简单数组而言其优点是将数据的内容和相关信息放在一个对象里面，这个对象提供了处理缓冲区数据的丰富的API。 所有缓冲区都有4个属性：capacity、limit、position、mark，并遵循：capacity&gt;=limit&gt;=position&gt;=mark&gt;=0，下面是对这4个属性的解释： Capacity: 容量，即可以容纳的最大数据量；在缓冲区创建时被设定并且不能改变 Limit: 上界，缓冲区中当前数据量 Position: 位置，下一个要被读或写的元素的索引 Mark: 标记，调用mark()来设置mark=position，再调用reset()可以让position恢复到标记的位置即position=mark 我们通过一个简单的操作流程来说明buffer的使用，下图是新创建的容量为10的缓冲区逻辑视图： 然后进行5次调用put： buffer.put((byte)‘A’).put((byte)‘B’).put((byte)‘C’).put((byte)‘D’).put((byte)‘E’) 5次调用put之后的缓冲区为： 现在缓冲区满了，我们必须将其清空。我们想把这个缓冲区传递给一个通道，以使内容能被全部写出，但现在执行get()无疑会取出未定义的数据。我们必须将 posistion设为0，然后通道就会从正确的位置开始读了，但读到哪算读完了呢？这正是limit引入的原因，它指明缓冲区有效内容的未端。这个操作 在缓冲区中叫做翻转：buffer.flip()。 Buffer的基本用法 使用Buffer读写数据一般遵循以下四个步骤： 写入数据到Buffer 调用flip()方法 从Buffer中读取数据 调用clear()方法或者compact()方法 当向buffer写入数据时，buffer会记录下写了多少数据。 一旦要读取数据，需要通过flip()方法将Buffer从写模式切换到读模式。在读模式下，可以读取之前写入到buffer的所有数据。 一旦读完了所有的数据，就需要清空缓冲区，让它可以再次被写入。有两种方式能清空缓冲区：调用clear()或compact()方法。clear()方法会清空整个缓冲区。compact()方法只会清除已经读过的数据。任何未读的数据都被移到缓冲区的起始处，新写入的数据将放到缓冲区未读数据的后面。 下面我们看一段程序来看一下Buffer的基本用法： 12345678910111213141516171819202122232425262728public static void readFile(String fileName) &#123; RandomAccessFile aFile = null; try &#123; //文件流 aFile = new RandomAccessFile(fileName, "rw"); //将文件输入到管道 FileChannel inChannel = aFile.getChannel(); //为buffer分配1024个字节大小的空间 ByteBuffer buf = ByteBuffer.allocate(1024); //将buffer中的内容读取到管道中 int bytesRead = inChannel.read(buf); while (bytesRead != -1) &#123; //反转buffer，将写模式改为读模式 buf.flip(); while (buf.hasRemaining()) &#123; //获取buffer中的数据 System.out.print((char) buf.get()); &#125; //将上次分配的1024字节的内容清空，为下次接收做准备 buf.clear(); //管道重新读取buffer中的内容 bytesRead = inChannel.read(buf); &#125; aFile.close(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; 字节缓冲区 我们将进一步观察字节缓冲区。所有的基本数据类型都有相应的缓冲区类（布尔型除外），但字节缓冲区有自己的独特之处。字节是操作系统及其I/O设备使用的基本数据类型。当在JVM和操作系统间传递数据时，将其他的数据类型拆分成构成它们的字节是十分必要的。如我们在后面的章节中将要看到的那样，系统层次的I/O面向字节的性质可以在整个缓冲区的设计以及它们互相配合的服务中感受到。 直接缓冲区 我们知道操作系统是在内存中进行I/O操作，这些内存区域，就操作系统方面而言，是相连的字节序列。于是，毫无疑问，只有字节缓冲区有资格参与I/O操作。即操作系统会直接存取进程，那么我们现在在JVM中进行操作，java中的内存空间是由JVM直接进行管理，但是在JVM中，字节数组可能不会在内存中连续存储，或者无用存储单元收集可能随时对其进行移动，这就不能保证I/O操作的目标是连续的。 出于这一原因，引入了直接缓冲区的概念。直接缓冲区被用于与通道和固有I/O例程交互。它们通过使用固有代码来告知操作系统直接释放或填充内存区域，对用于通道直接或原始存取的内存区域中的字节元素的存储尽了最大的努力。 直接字节缓冲区通常是I/O操作最好的选择。在设计方面，它们支持JVM可用的最高效I/O机制。非直接字节缓冲区可以被传递给通道，但是这样可能导致性能损耗。通常非直接缓冲不可能成为一个本地I/O操作的目标。如果您向一个通道中传递一个非直接ByteBuffer对象用于写入，通道可能会在每次调用中隐含地进行下面的操作： 创建一个临时的直接ByteBuffer对象。 将非直接缓冲区的内容复制到临时缓冲中。 使用临时缓冲区执行低层次I/O操作。 临时缓冲区对象离开作用域，并最终成为被回收的无用数据。 视图缓冲区 就像我们已经讨论的那样，I/O基本上可以归结成组字节数据的四处传递。在进行大数据量的I/O操作时，很又可能你会使用各种ByteBuffer类去读取文件内容，接收来自网络连接的数据，等等。一旦数据到达了你的ByteBuffer，您就需要查看它以决定怎么做或者在将它发送出去之前对它进行一些操作。ByteBuffer类提供了丰富的API来创建视图缓冲区。 视图缓冲区通过已存在的缓冲区对象实例的工厂方法来创建。这种视图对象维护它自己的属性，容量，位置，上界和标记，但是和原来的缓冲区共享数据元素。但是ByteBuffer类允许创建视图来将byte型缓冲区字节数据映射为其它的原始数据类型。例如，asLongBuffer()函数创建一个将八个字节型数据当成一个long型数据来存取的视图缓冲区。 但是使用视图缓冲区的话，一旦ByteBuffer对于视图的维护对象产生非常规行的使用，那么对于工厂方法创建的缓冲区而言，asLongBuffer()函数就不在使用这个视窗，那么这个8字节的数据当成一个long类型的数据类型来存取的数据视图。]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java并发编程（二）----创建并运行java线程]]></title>
    <url>%2Fposts%2Fc845f6f3.html</url>
    <content type="text"><![CDATA[1 实现线程的两种方式 上一节我们了解了关于线程的一些基本知识，下面我们正式进入多线程的实现环节。实现线程常用的有两种方式，一种是继承Thread类，一种是实现Runnable接口。当然还有第三种方式，那就是通过线程池来生成线程，后面我们还会学习，一步一个脚印打好基础。 Runnable接口： 12345public interface Runnable &#123; public abstract void run(); &#125; Thread类： 1234567891011121314151617181920212223242526public class Thread implements Runnable &#123; public synchronized void start() &#123; if (threadStatus != 0) throw new IllegalThreadStateException(); group.add(this); boolean started = false; try &#123; start0(); started = true; &#125; finally &#123; try &#123; if (!started) &#123; group.threadStartFailed(this); &#125; &#125; catch (Throwable ignore) &#123; /* do nothing. If start0 threw a Throwable then it will be passed up the call stack */ &#125; &#125; @Override public void run() &#123; if (target != null) &#123; target.run(); &#125; &#125;&#125; 上面为Thread类和Runnable类的源码，我们可以看到Thread类也是实现了Runnable接口，即Thread是Runnable的实现，那么他们到底在实现多线程上有什么区别呢？ Thread和Runnable解析： ①Runnable接口： Runnable接口是java中线程的定义类。所有线程都是通过该接口来实现，该接口中的run（）方法为实现方法，即线程所要实现的内容写入该方法里面，当线程启动时会调用该方法。 在大多数情况下，如果只想重写run（）方法而不重写其他方法，应使用Runnable接口。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081public class ThreadDemo3 &#123; public static void main(String[] args) &#123; //new了两个线程对象——s1和s2 //其中两个对象各对应一个内存区域。线程运行过程中运行都是自己内存块中的数据 Shop1 s1 = new Shop1("小武"); s1.start(); Shop1 s2 = new Shop1("小潘"); s2.start(); /* //实例化了两个线程对象，所以分配了两块内存空间 //执行过程中操作的是自己的内存空间 Shop2 s3 = new Shop2("小武"); s3.run(); Shop2 s4 = new Shop2("小潘"); s4.run(); //实际实例化了两个线程对象 //所以同样分配两个内存空间 Thread t1 = new Thread(new Shop2("小武")); t1.start(); Thread t2 = new Thread(new Shop2("小潘")); t2.start(); */ //创建了两个线程对象，但是使用的是同一个对象——s6 Shop2 s5 = new Shop2("w"); Shop1 s6 = new Shop1("T"); Thread t3 = new Thread(s6); t3.start(); Thread t4 =new Thread(s6); t4.start(); &#125;&#125;/** * 武大郎卖烧饼（因为业务的拓展，现在可以实现多窗口的出售） * 要求：每天只卖10个 * */class Shop1 extends Thread&#123; //private int count = 10; //使用静态变量可以有效的实现资源共享（因为在内存中只有一份count） private static int count = 10; public Shop1(String name) &#123; super(name); &#125; public void run()&#123; //判断是否已经卖完 while(count&gt;0)&#123; count--; System.out.println(this.getName() +"卖出了一个烧饼" + ",现在剩余" + count); &#125; &#125;&#125; /** * 使用接口实现上面的代码 * */class Shop2 implements Runnable&#123; //私有变量，存储剩余烧饼的个数 private int count = 10; //存储当前人的姓名 private String name=""; public Shop2(String name) &#123; this.name = name; &#125; /** * 实现销售的方法 */ public void run()&#123; //判断是否已经卖完 while(count&gt;0)&#123; count--; System.out.println(Thread.currentThread().getId() + "、" + this.name +"卖出了一个烧饼" + ",现在剩余" + count); &#125; &#125;&#125; ②Thread类： Thread类是Runnable接口的实现，jdk给我们提供了一个不用我们去想如何实现线程的方式供我们使用。同样你在继承Thread类的时候也需要重写run()方法来实现你想在线程中实现的内容。 123456789101112131415161718192021222324252627282930313233343536373839404142434445public class Test&#123; public static void main(String[] args) &#123; //传统方式——单任务方式 /* SimpleClass sc1 = new SimpleClass(); sc1.say("Mike"); SimpleClass sc2 = new SimpleClass(); sc2.say("Han Meimei"); */ //创建一个线程 ThreadClass tc1 = new ThreadClass("Mike"); //启动线程 tc1.start(); //创建一个线程 ThreadClass tc2 = new ThreadClass("Han Meimei"); tc2.start(); &#125; &#125;&#125; class SimpleClass&#123; public void say(String name)&#123; while(true)&#123; System.out.println("Hi,Im " + name); &#125; &#125;&#125;class ThreadClass extends Thread&#123; public ThreadClass(String name) &#123; super(name); &#125; /** * 将父类（Thread）的run()方法进行重写 * 在run()方法中包含了需要执行的代码 */ public void run()&#123; while(true)&#123; System.out.println("Hi,Im " + this.getName() + "|" + this.getId() + "|" + this.getStackTrace()); &#125; &#125;&#125; Thread类中常用方法： run()：如果该线程时使用独立的Runnable运行对象构造的，则调用该Runnable对象的run方法。否则，该方法不执行任何操作并返回。 sleep(longmillls):在指定的毫秒数内让当前正在执行的线程休眠（暂停执行），此操作受到系统计时器和调度程序精度和准确性的影响。 yield():暂停当前正在执行的线程对象，并执行其他线程 start()：使该线程开始运行，java虚拟机再调用该线程的run方法 。 join():等待该线程结束。 对比： 12345上面给出了Thread和Runnable的实现，我们能看到在使用Runnable的方式实现线程的过程中：Shop1 s6 = new Shop1("T");Thread t3 = new Thread(s6);t3.start(); 即把Runnable对象（实现了Runnable接口的对象）还是塞进了Thread中让Thread来实现。那么我们可以new 多个Thread来实现同一个Runnbale对象，即实现了资源的共享，比如在售票系统中多名用户对同一种票的抢购。另一方面，java是单继承多实现的，如果我们使用Thread的话意味着该类只能继承Thread，对于程序的扩展不利，而实现Runnbale接口则没有这个顾虑。考虑程序的健壮性，我们应该尽量使用Runnable来实现我们的线程。 run和start 初学多线程我们总是分不清楚run()方法和start()方法的区别，其实我们再看一下上面Thread类的源码就不难发现他们的用法是很容易区分的： run()方法是线程的实现方法，即你需要线程去做什么事情，那么这些实现的内容写在run()里面，当线程启动时就会调用run()方法继而实现run()内部的代码； start()方法是线程的启动方法，即如果你new Thread()这样并不算完。你还得new Thread().start()才算启动这个线程，启动完之后线程内部会主动的调用run()方法执行该线程的业务逻辑代码。]]></content>
      <categories>
        <category>多线程与并发编程</category>
      </categories>
      <tags>
        <tag>学习</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java并发编程（一）----线程基础知识]]></title>
    <url>%2Fposts%2Fac18980b.html</url>
    <content type="text"><![CDATA[在任何的生产环境中我们都不可逃避并发这个问题，多线程作为并发问题的技术支持让我们不得不去了解。这一块知识就像一个大蛋糕一样等着我们去分享，抱着学习的心态，记录下自己对并发的认识。 ###1.线程的状态： 线程状态图： 1、新建状态（New）：新创建了一个线程对象。 2、就绪状态（Runnable）：线程对象创建后，其他线程调用了该对象的start()方法。该状态的线程位于可运行线程池中，变得可运行，等待获取CPU的使用权。 3、运行状态（Running）：就绪状态的线程获取了CPU，执行程序代码。 4、阻塞状态（Blocked）：阻塞状态是线程因为某种原因放弃CPU使用权，暂时停止运行。直到线程进入就绪状态，才有机会转到运行状态。阻塞的情况分三种： （一）、等待阻塞：运行的线程执行wait()方法，JVM会把该线程放入等待池中。 （二）、同步阻塞：运行的线程在获取对象的同步锁时，若该同步锁被别的线程占用，则JVM会把该线程放入锁池中。 （三）、其他阻塞：运行的线程执行sleep()或join()方法，或者发出了I/O请求时，JVM会把该线程置为阻塞状态。当sleep()状态超时、join()等待线程终止或者超时、或者I/O处理完毕时，线程重新转入就绪状态。 5、死亡状态（Dead）：线程执行完了或者因异常退出了run()方法，该线程结束生命周期。 ###2.线程调度 2.1、调整线程优先级：Java线程有优先级，优先级高的线程会获得较多的运行机会。Java线程的优先级用整数表示，取值范围是1~10，Thread类有以下三个静态常量： static int MAX_PRIORITY 线程可以具有的最高优先级，取值为10。 static int MIN_PRIORITY 线程可以具有的最低优先级，取值为1。 static int NORM_PRIORITY 分配给线程的默认优先级，取值为5。 Thread类的setPriority()和getPriority()方法分别用来设置和获取线程的优先级。 每个线程都有默认的优先级。主线程的默认优先级为Thread.NORM_PRIORITY。 线程的优先级有继承关系，比如A线程中创建了B线程，那么B将和A具有相同的优先级。 JVM提供了10个线程优先级，但与常见的操作系统都不能很好的映射。如果希望程序能移植到各个操作系统中，应该仅仅使用Thread类有以下三个静态常量作为优先级，这样能保证同样的优先级采用了同样的调度方式。 ###3.线程基本方法使用说明： -线程睡眠：Thread.sleep(longmillis)，使线程转到阻塞状态。millis参数设定睡眠的时间，以毫秒为单位。当睡眠结束后，就转为就绪（Runnable）状态。sleep()平台移植性好。 -线程等待：Object类中的wait()，导致当前的线程等待，直到其他线程调用此对象的 notify() 方法或 notifyAll() 唤醒方法。这个两个唤醒方法也是Object类中的方法，行为等价于调用 wait(0) 一样。 -线程让步：Thread.yield()，暂停当前正在执行的线程对象，把执行机会让给相同或者更高优先级的线程。 -线程加入：join()，等待其他线程终止。在当前线程中调用另一个线程的join()方法，则当前线程转入阻塞状态，直到另一个进程运行结束，当前线程再由阻塞转为就绪状态。 -线程唤醒：Object类中的notify()方法，唤醒在此对象监视器上等待的单个线程。如果所有线程都在此对象上等待，则会选择唤醒其中一个线程。选择是任意性的，并在对实现做出决定时发生。线程通过调用其中一个 wait 方法，在对象的监视器上等待。 直到当前的线程放弃此对象上的锁定，才能继续执行被唤醒的线程。被唤醒的线程将以常规方式与在该对象上主动同步的其他所有线程进行竞争；例如，唤醒的线程在作为锁定此对象的下一个线程方面没有可靠的特权或劣势。类似的方法还有一个notifyAll()，唤醒在此对象监视器上等待的所有线程。 注意：Thread中suspend()和resume()两个方法在JDK1.5中已经废除，不再介绍。因为有死锁倾向。 Java 中的线程可以分为守护线程(Daemon Thread) 和用户线程( User Thread) 。用户线程会阻止JVM 的正常停止，即JVM 正常停止前应用程序中的所有用户线程必须先停止完毕,否则JVM 无法停止。而守护线程则不会影响JVM 的正常停止，即应用程序中有守护线程在运行也不影响JVM 的正常停止。因此，守护线程通常用于执行一些重要性不是很高的任务，例如用于监视其他线程的运行情况。]]></content>
      <categories>
        <category>多线程与并发编程</category>
      </categories>
      <tags>
        <tag>学习</tag>
      </tags>
  </entry>
</search>
