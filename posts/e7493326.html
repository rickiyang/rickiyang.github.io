<!DOCTYPE html>












  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">


  
  
  <link rel="stylesheet" href="/lib/needsharebutton/needsharebutton.css">



  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link rel="stylesheet" href="/lib/pace/pace-theme-center-circle.min.css?v=1.0.2">




















  
  
  
  

  
    
    
  

  
    
      
    

    
  

  

  
    
      
    

    
  

  
    
      
    

    
  

  
    
    
    <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Monda:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|Lobster Two:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext">
  






<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2">

<link rel="stylesheet" href="/css/main.css?v=6.7.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/adele.ico?v=6.7.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/adele.ico?v=6.7.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/adele.ico?v=6.7.0">


  <link rel="mask-icon" href="/images/adele.ico?v=6.7.0" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '6.7.0',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":true,"scrollpercent":true,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="开发人员在编写Spark应用的时候，需要提供一个包含main函数的驱动程序作为程序的入口，开发人员根据自己的需求，在main函数中调用Spark提供的数据操纵接口，利用集群对数据执行并行操作。 Spark为开发人员提供了两类抽象接口。  第一类抽象接口是弹性分布式数据集（Resilient Distributed Dataset，下文简称RDD），顾名思义，RDD是对数据集的抽象封装，开发人员可以">
<meta name="keywords" content="spark学习">
<meta property="og:type" content="article">
<meta property="og:title" content="spark核心API开发">
<meta property="og:url" content="http://blog.rickiyang.cn/posts/e7493326.html">
<meta property="og:site_name" content="Rickiyang&#39;s blog">
<meta property="og:description" content="开发人员在编写Spark应用的时候，需要提供一个包含main函数的驱动程序作为程序的入口，开发人员根据自己的需求，在main函数中调用Spark提供的数据操纵接口，利用集群对数据执行并行操作。 Spark为开发人员提供了两类抽象接口。  第一类抽象接口是弹性分布式数据集（Resilient Distributed Dataset，下文简称RDD），顾名思义，RDD是对数据集的抽象封装，开发人员可以">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://i.imgur.com/pC28WGO.png">
<meta property="og:updated_time" content="2018-12-13T02:14:20.613Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="spark核心API开发">
<meta name="twitter:description" content="开发人员在编写Spark应用的时候，需要提供一个包含main函数的驱动程序作为程序的入口，开发人员根据自己的需求，在main函数中调用Spark提供的数据操纵接口，利用集群对数据执行并行操作。 Spark为开发人员提供了两类抽象接口。  第一类抽象接口是弹性分布式数据集（Resilient Distributed Dataset，下文简称RDD），顾名思义，RDD是对数据集的抽象封装，开发人员可以">
<meta name="twitter:image" content="https://i.imgur.com/pC28WGO.png">



  <link rel="alternate" href="/atom.xml" title="Rickiyang's blog" type="application/atom+xml">




  <link rel="canonical" href="http://blog.rickiyang.cn/posts/e7493326.html">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>spark核心API开发 | Rickiyang's blog</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Rickiyang's blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <h1 class="site-subtitle" itemprop="description">java,大数据,分布式组件</h1>
      
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-首页">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-标签">

    
    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-分类">

    
    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-归档">

    
    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>

  </li>

      
      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>Search</a>
        </li>
      
    </ul>
  

  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="Searching..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://blog.rickiyang.cn/posts/e7493326.html">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Rickiyang">
      <meta itemprop="description" content="分享对coding生活的见解">
      <meta itemprop="image" content="/images/adele.png">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Rickiyang's blog">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">spark核心API开发

              
            
          </h2>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建于: 2018-07-23 22:23:54" itemprop="dateCreated datePublished" datetime="2018-07-23T22:23:54+08:00">2018-07-23</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">编辑于</span>
                
                <time title="Modified: 2018-12-13 10:14:20" itemprop="dateModified" datetime="2018-12-13T10:14:20+08:00">2018-12-13</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/大数据学习/" itemprop="url" rel="index"><span itemprop="name">大数据学习</span></a></span>

                
                
                  , 
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/大数据学习/spark/" itemprop="url" rel="index"><span itemprop="name">spark</span></a></span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/posts/e7493326.html#comments" itemprop="discussionUrl">
                  <span class="post-meta-item-text">评论: </span> <span class="post-comments-count valine-comment-count" data-xid="/posts/e7493326.html" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/posts/e7493326.html" class="leancloud_visitors" data-flag-title="spark核心API开发">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">热度: </span>
               
                 <span class="leancloud-visitors-count"></span>
                 <span>℃</span>
             </span>
          

          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">文章字数: </span>
                
                <span title="文章字数">30k</span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">27 mins.</span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>开发人员在编写Spark应用的时候，需要提供一个包含main函数的驱动程序作为程序的入口，开发人员根据自己的需求，在main函数中调用Spark提供的数据操纵接口，利用集群对数据执行并行操作。</p>
<p>Spark为开发人员提供了两类抽象接口。</p>
<ol>
<li>第一类抽象接口是弹性分布式数据集（Resilient Distributed Dataset，下文简称RDD），顾名思义，RDD是对数据集的抽象封装，开发人员可以通过RDD提供的开发接口来访问和操纵数据集合，而无需了解数据的存储介质（内存或磁盘）、文件系统（本地文件系统、HDFS或Tachyon）、存储节点（本地或远程节点）等诸多实现细节；</li>
<li>第二类抽象是共享变量（Shared Variables），通常情况下，一个应用程序在运行的时候会被划分成分布在不同执行器之上的多个任务，从而提高运算的速度，每个任务都会有一份独立的程序变量拷贝，彼此之间互不干扰，然而在某些情况下需要任务之间相互共享变量，Apache Spark提供了两类共享变量，它们分别是：广播变量（Broadcast Variable）和累加器（Accumulators）。后面将介绍RDD的基本概念和RDD提供的编程接口，并在后面详细解读接口的源码实现，从而加深对RDD的理解，此外会介绍两类共享变量的使用方法。</li>
</ol>
<p>另外，除了单独编写一个应用程序的方式之外，Spark还提供了一个交互式Shell来使用。在Shell中，用户的每条语句都能在输入完毕后及时得到结果，而无需手动编译和运行程序。启用非常简单，在spark的安装目录找到bin目录下面的spark-shell命令即可。</p>
<p>在Shell中，系统根据命令提供的参数自动配置和生成了一个SparkContext对象sc，直接使用即可，无需再手动实例化SparkContext。除了结果会实时显示之外，其余操作与编写单独应用程序类似。读者可直接参考Spark官方提供的Spark ProgrammingGuide等文档，在此不做具体介绍。</p>
<h4 id="1-spark-context"><a class="markdownIt-Anchor" href="#1-spark-context"></a> 1. Spark Context</h4>
<p>SparkContext是整个项目程序的入口，无论从本地读取文件（textfile方法）还是从HDFS读取文件或者通过集合并行化获得RDD，都先要创建SparkContext对象，然后使用SparkContext对RDD进行创建和后续的转换操作。本节主要介绍SparkContext类的作用和创建过程，然后通过一个简单的例子向读者介绍SparkContext的应用方法，从应用角度来理解其作用。</p>
<h5 id="11-sparkcontext的作用"><a class="markdownIt-Anchor" href="#11-sparkcontext的作用"></a> 1.1 SparkContext的作用</h5>
<p>SparkContext除了是Spark的主要入口，它也可以看作是对用户的接口，它代表与Spark集群的连接对象，由下图2可以看到，SparkContext主要存在于Driver Program中。可以使用SparkContext来创建集群中的RDD、累积量和广播量，在后台SparkContext还能发送任务给集群管理器。每一个JVM只能有运行一个程序，即对应只有一个SparkContext处于激活状态，因此在创建新的SparkContext前需要把旧的SparkContext停止。</p>
<p><img src="https://i.imgur.com/pC28WGO.png" alt=""></p>
<p>下面有一个简单的单词统计例子来看一下SparkContext的使用方式：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WordCount</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    <span class="keyword">val</span> inputFile =  <span class="string">"file:///usr/local/spark/test/word.txt"</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"WordCount"</span>).setMaster(<span class="string">"local"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    <span class="keyword">val</span> textFile = sc.textFile(inputFile)</span><br><span class="line">    <span class="keyword">val</span> wordCount = textFile.flatMap(line =&gt; line.split(<span class="string">" "</span>)).map(word =&gt; (word, <span class="number">1</span>)).reduceByKey((a, b) =&gt; a + b)</span><br><span class="line">    wordCount.foreach(println)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这个例子中，首先创建配置文件conf，使用本地模式，将配置文件放入sparkContext上下文并实例化。从sparkContext上下文中读取文件数据并将数据转化RDD，然后统计词频。</p>
<h4 id="2-rdd算子"><a class="markdownIt-Anchor" href="#2-rdd算子"></a> 2. RDD算子</h4>
<p>下面再来说RDD的两个主要操作算子Transformation和Action的使用方法，由于Spark是基于延迟计算，Transforamation算子并不立即执行，这时只是保存计算状态，当Action算子出现才真正执行计算。</p>
<h5 id="21-单值型tranformation算子"><a class="markdownIt-Anchor" href="#21-单值型tranformation算子"></a> 2.1 单值型Tranformation算子</h5>
<p>单值型的算子就是输入为单个值形式，这里主要介绍map、flatMap、mapPartitions、union、cartesian、groupBy、filter、distinct、subtract、foreach、cache、persist、sample以及takeSample方法：</p>
<table>
<thead>
<tr>
<th>方法名</th>
<th>方法定义</th>
</tr>
</thead>
<tbody>
<tr>
<td>map</td>
<td>def map[U](f: (T) ⇒ U)(implicit arg0: ClassTag[U]): RDD[U]</td>
</tr>
<tr>
<td>flatMap</td>
<td>defmapPartitions[U](f: (Iterator[T])⇒ Iterator[U], preservesPartitioning: Boolean = false)</td>
</tr>
<tr>
<td>mapPartition</td>
<td>def mapPartitions[U](f: (Iterator[T])⇒ Iterator[U], preservesPartitioning: Boolean = false)(implicit arg0: ClassTag[U]): RDD[U]</td>
</tr>
<tr>
<td>mapPartitionsWithIndex</td>
<td>def mapPartitionsWithIndex[U](f: (Int, Iterator[T])⇒ Iterator[U], preservesPartitioning: Boolean = false)(implicit arg0: ClassTag[U]): RDD[U]</td>
</tr>
<tr>
<td>foreach</td>
<td>def foreach(f: (T) ⇒ Unit): Unit</td>
</tr>
<tr>
<td>foreachPartition</td>
<td>def foreachPartition(f: (Iterator[T])⇒ Unit): Unit</td>
</tr>
<tr>
<td>glom</td>
<td>def glom(): RDD[Array[T]]</td>
</tr>
<tr>
<td>union</td>
<td>def union(other: RDD[T]): RDD[T]</td>
</tr>
<tr>
<td>cartesian</td>
<td>def cartesian[U](other: RDD[U])(implicit arg0: ClassTag[U]): RDD[(T, U)]</td>
</tr>
<tr>
<td>groupBy</td>
<td>def groupBy[K](f: (T) ⇒ K, p: Partitioner)(implicit kt: ClassTag[K], ord: Ordering[K] = null): RDD[(K, Iterable[T])]</td>
</tr>
<tr>
<td>filter</td>
<td>def filter(f: (T) ⇒ Boolean): RDD[T]</td>
</tr>
<tr>
<td>distinct</td>
<td>def distinct(): RDD[T]</td>
</tr>
<tr>
<td>subtract</td>
<td>def subtract(other: RDD[T], p: Partitioner)(implicit ord: Ordering[T] = null): RDD[T]</td>
</tr>
<tr>
<td>cache</td>
<td>def cache(): RDD.this.type</td>
</tr>
<tr>
<td>persist</td>
<td>def persist(): RDD.this.type</td>
</tr>
<tr>
<td>sample</td>
<td>def sample(withReplacement: Boolean, fraction: Double, seed: Long = Utils.random.nextLong): RDD[T]</td>
</tr>
<tr>
<td>takeSample</td>
<td>def takeSample(withReplacement: Boolean, num: Int, seed: Long = Utils.random.nextLong): Array[T]</td>
</tr>
</tbody>
</table>
<h6 id="211-map"><a class="markdownIt-Anchor" href="#211-map"></a> 2.1.1 <strong>map</strong></h6>
<p>对原来每一个输入的RDD数据集进行函数转换，返回的结果为新的RDD，该方法对分区操作是一对一的。</p>
<p>示例：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> a = sc.parallelize(<span class="type">List</span>(<span class="string">"bit"</span>, <span class="string">"linc"</span>, <span class="string">"xwc"</span>, <span class="string">"fjg"</span>, <span class="string">"wc"</span>,<span class="string">"spark"</span>), <span class="number">3</span>)	<span class="comment">//创建RDD</span></span><br><span class="line"><span class="keyword">val</span> b = a.map(word =&gt; word.length)		<span class="comment">//计算每个单词的长度</span></span><br><span class="line"><span class="keyword">val</span> c = a.zip(b)		<span class="comment">//拉链方法，把两列数据对应配对成键值对格式</span></span><br><span class="line">c.collect		<span class="comment">//把结果转换为数组	</span></span><br><span class="line">结果：</span><br><span class="line">scala&gt; c.collect</span><br><span class="line">res3: <span class="type">Array</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">Array</span>((bit,<span class="number">3</span>), (linc,<span class="number">4</span>), (xwc,<span class="number">3</span>), (fjg,<span class="number">3</span>), (wc,<span class="number">2</span>), (spark,<span class="number">5</span>))</span><br></pre></td></tr></table></figure>
<h6 id="212-flatmap"><a class="markdownIt-Anchor" href="#212-flatmap"></a> 2.1.2  <strong>flatMap</strong></h6>
<p>flapMap方法与map方法类似，但是允许在一次map方法中输出多个对象，而不是map中的一个对象经过函数转换生成另一个对象。</p>
<p>示例：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> a = sc.parallelize(<span class="number">1</span> to <span class="number">3</span>, <span class="number">5</span>)</span><br><span class="line">a: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; a.flatMap(num =&gt; <span class="number">1</span> to num).collect</span><br><span class="line">res0: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<h6 id="213-mappartitions"><a class="markdownIt-Anchor" href="#213-mappartitions"></a> 2.1.3 <strong>mapPartitions</strong></h6>
<p>mapPartitions是map的另一个实现。map的输入函数是应用于RDD中每个元素，而mapPartitio的输入函数是作用于每个分区，也就是把每个分区中的内容作为整体来处理的。</p>
<p>示例：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">scala&gt;  <span class="keyword">val</span> a = sc.parallelize(<span class="number">1</span> to <span class="number">9</span>, <span class="number">3</span>)</span><br><span class="line">a: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line">scala&gt; <span class="function"><span class="keyword">def</span> <span class="title">myfunc</span></span>[<span class="type">T</span>](iter: <span class="type">Iterator</span>[<span class="type">T</span>]) : <span class="type">Iterator</span>[(<span class="type">T</span>, <span class="type">T</span>)] = &#123;</span><br><span class="line">     | <span class="keyword">var</span> res = <span class="type">List</span>[(<span class="type">T</span>, <span class="type">T</span>)]()</span><br><span class="line">     | <span class="keyword">var</span> pre = iter.next</span><br><span class="line">     | <span class="keyword">while</span> (iter.hasNext) &#123;</span><br><span class="line">     | <span class="keyword">val</span> cur = iter.next</span><br><span class="line">     | res .::= (pre, cur)</span><br><span class="line">     | pre = cur</span><br><span class="line">     | &#125;</span><br><span class="line">     | res.iterator</span><br><span class="line">     | &#125;</span><br><span class="line">myfunc: [<span class="type">T</span>](iter: <span class="type">Iterator</span>[<span class="type">T</span>])<span class="type">Iterator</span>[(<span class="type">T</span>, <span class="type">T</span>)]</span><br><span class="line">scala&gt; a.mapPartitions(myfunc).collect</span><br><span class="line">res0: <span class="type">Array</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">Array</span>((<span class="number">2</span>,<span class="number">3</span>), (<span class="number">1</span>,<span class="number">2</span>), (<span class="number">5</span>,<span class="number">6</span>), (<span class="number">4</span>,<span class="number">5</span>), (<span class="number">8</span>,<span class="number">9</span>), (<span class="number">7</span>,<span class="number">8</span>))</span><br></pre></td></tr></table></figure>
<p>解释一下这段程序：先得到一个分为3个分区的1-9的序列然，即每个分区的数据分别是(1,2,3),(4,5,6),(7,8,9)。下面定义了一个myfunc函数，遍历一个集合，集合里面是tuple对象，如果这个tuple的元素有next元素，name将pre和cur构造成一个tuple存入list。下面<br>
mapPartitions函数将myfunc函数作为参数去作用于每一个分区，所以获得的结果为(1,2),(2,3),因为3没有next元素，所以每一个分区中只有两个tuple，下面的分区以此类推。</p>
<h6 id="214-mappartitionwithindex"><a class="markdownIt-Anchor" href="#214-mappartitionwithindex"></a> 2.1.4 <strong>mapPartitionWithIndex</strong></h6>
<p>mapPartitionWithIndex方法与mapPartitions方法功能类似，不同的是mapPartitionWithIndex还会对原始分区的索引进行追踪，这样能知道分区所对应的元素，方法的参数为一个函数，函数的输入为整型索引和迭代器。</p>
<p>示例：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> a = sc.parallelize(<span class="number">1</span> to <span class="number">9</span>, <span class="number">3</span>)</span><br><span class="line">a: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line">scala&gt; <span class="function"><span class="keyword">def</span> <span class="title">myfunc</span></span>(index: <span class="type">Int</span>, iter: <span class="type">Iterator</span>[<span class="type">Int</span>]) : <span class="type">Iterator</span>[<span class="type">String</span>] = &#123;</span><br><span class="line">     | iter.toList.map(x =&gt; index + <span class="string">","</span> + x).iterator</span><br><span class="line">     | &#125;</span><br><span class="line">myfunc: (index: <span class="type">Int</span>, iter: <span class="type">Iterator</span>[<span class="type">Int</span>])<span class="type">Iterator</span>[<span class="type">String</span>]</span><br><span class="line">scala&gt; a.mapPartitionsWithIndex(myfunc).collect()</span><br><span class="line">res1: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="number">0</span>,<span class="number">1</span>, <span class="number">0</span>,<span class="number">2</span>, <span class="number">0</span>,<span class="number">3</span>, <span class="number">1</span>,<span class="number">4</span>, <span class="number">1</span>,<span class="number">5</span>, <span class="number">1</span>,<span class="number">6</span>, <span class="number">2</span>,<span class="number">7</span>, <span class="number">2</span>,<span class="number">8</span>, <span class="number">2</span>,<span class="number">9</span>)</span><br></pre></td></tr></table></figure>
<p>这段程序就是先得到一个3个分区的序列，然后调用mapPartitionWithIndex函数可以获取每个分区的下标，方便追踪当前处理的分区。myfunc函数将每个分区的下标和值用&quot;,&quot;拼装在一起，从输出结果我们可以看到分区下标是从0开始的。</p>
<h6 id="215-foreach"><a class="markdownIt-Anchor" href="#215-foreach"></a> 2.1.5 <strong>foreach</strong></h6>
<p>foreach方法主要是对输入的数据对象执行循环操作，该方法常用来输出RDD中的内容。</p>
<p>这个方法比较直观：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> c = sc.parallelize(<span class="type">List</span>(<span class="string">"aa"</span>, <span class="string">"bb"</span>, <span class="string">"cc"</span>, <span class="string">"dd"</span>, <span class="string">"ee"</span>, <span class="string">"ff"</span>, <span class="string">"mm"</span>, <span class="string">"kk"</span>, <span class="string">"zz"</span>, <span class="string">"rr"</span>), <span class="number">3</span>)</span><br><span class="line">c: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">2</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; c.foreach(x =&gt; println(<span class="string">"word is : "</span>+x))</span><br></pre></td></tr></table></figure>
<h6 id="216-foreachpartition"><a class="markdownIt-Anchor" href="#216-foreachpartition"></a> 2.1.6 <strong>foreachPartition</strong></h6>
<p>foreachPartition方法的作用是通过迭代器参数对RDD中每一个分区的数据对象应用函数。mapPartitions方法的作用于foreachPartition方法作用非常相似，区别就在于使用的参数是否有返回值。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> b = sc.parallelize(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>), <span class="number">3</span>)</span><br><span class="line">b.foreachPartition(x =&gt; println((a,b) =&gt; x.reduce(a + b)</span><br></pre></td></tr></table></figure>
<h6 id="217-glom"><a class="markdownIt-Anchor" href="#217-glom"></a> 2.1.7 <strong>glom</strong></h6>
<p>作用类似collect，但它不是直接将所有RDD直接转化为数组形式，glom方法的作用是将RDD中分区数据进行组装到数组类型RDD中，每一个返回的数组包含一个分区的元素，按分区转化为数组，最后有几个分区就返回几个数组类型的RDD。</p>
<p>示例：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> a = sc.parallelize(<span class="number">1</span> to <span class="number">10</span>, <span class="number">3</span>)</span><br><span class="line">a: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">1</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line">scala&gt; a.glom.collect</span><br><span class="line">res2: <span class="type">Array</span>[<span class="type">Array</span>[<span class="type">Int</span>]] = <span class="type">Array</span>(<span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>), <span class="type">Array</span>(<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>), <span class="type">Array</span>(<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure>
<h6 id="218-union"><a class="markdownIt-Anchor" href="#218-union"></a> 2.1.8 <strong>union</strong></h6>
<p>union方法（等价于“++”）是将两个RDD取并集，取并集过程中不会把相同元素去掉。union操作是输入分区与输出分区多对一模式。</p>
<p>示例：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> a = sc.parallelize(<span class="number">1</span> to <span class="number">4</span>, <span class="number">2</span>)</span><br><span class="line">a: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">3</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> b = sc.parallelize(<span class="number">2</span> to <span class="number">4</span>, <span class="number">1</span>)</span><br><span class="line">b: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">4</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; (a ++ b).collect</span><br><span class="line">res3: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; a.union(b).collect</span><br><span class="line">res4: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<p>可见结果集没有执行distinct操作。</p>
<h6 id="219-cartesian"><a class="markdownIt-Anchor" href="#219-cartesian"></a> 2.1.9 <strong>cartesian</strong></h6>
<p>计算两个RDD中每个对象的笛卡尔积（例如第一个RDD中的每一个对象与第二个RDD中的对象join连接），但使用该方法时要注意可能出现内存不够的情况。</p>
<p>示例：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> x =sc.parallelize(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>),<span class="number">1</span>)</span><br><span class="line">x: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">7</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> y =sc.parallelize(<span class="type">List</span>(<span class="number">4</span>,<span class="number">5</span>),<span class="number">1</span>)</span><br><span class="line">y: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">8</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; x.cartesian(y).collect</span><br><span class="line">res5: <span class="type">Array</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">Array</span>((<span class="number">1</span>,<span class="number">4</span>), (<span class="number">1</span>,<span class="number">5</span>), (<span class="number">2</span>,<span class="number">4</span>), (<span class="number">2</span>,<span class="number">5</span>), (<span class="number">3</span>,<span class="number">4</span>), (<span class="number">3</span>,<span class="number">5</span>))</span><br></pre></td></tr></table></figure>
<p>谨慎执行笛卡尔积操作。</p>
<h6 id="2110-groupby"><a class="markdownIt-Anchor" href="#2110-groupby"></a> 2.1.10 <strong>groupBy</strong></h6>
<p>groupBy方法有三个重载方法，功能是将元素通过map函数生成Key-Value格式，然后使用reduceByKey方法对Key-Value对进行聚合。</p>
<p>示例：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> a = sc.parallelize(<span class="number">1</span> to <span class="number">9</span>, <span class="number">3</span>)</span><br><span class="line">a: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">10</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; a.groupBy(x =&gt; &#123; <span class="keyword">if</span> (x % <span class="number">2</span> == <span class="number">0</span>) <span class="string">"even"</span> <span class="keyword">else</span> <span class="string">"odd"</span> &#125;).collect</span><br><span class="line">res6: <span class="type">Array</span>[(<span class="type">String</span>, <span class="type">Iterable</span>[<span class="type">Int</span>])] = <span class="type">Array</span>((even,<span class="type">CompactBuffer</span>(<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>)), (odd,<span class="type">CompactBuffer</span>(<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">9</span>)))</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="function"><span class="keyword">def</span> <span class="title">myfunc</span></span>(a: <span class="type">Int</span>) : <span class="type">Int</span> =&#123;a % <span class="number">2</span>&#125;</span><br><span class="line">myfunc: (a: <span class="type">Int</span>)<span class="type">Int</span></span><br><span class="line"></span><br><span class="line">scala&gt; a.groupBy(myfunc).collect</span><br><span class="line">res7: <span class="type">Array</span>[(<span class="type">Int</span>, <span class="type">Iterable</span>[<span class="type">Int</span>])] = <span class="type">Array</span>((<span class="number">0</span>,<span class="type">CompactBuffer</span>(<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>)), (<span class="number">1</span>,<span class="type">CompactBuffer</span>(<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">9</span>)))</span><br><span class="line"></span><br><span class="line">scala&gt; a.groupBy(myfunc(_),<span class="number">1</span>).collect</span><br><span class="line">res9: <span class="type">Array</span>[(<span class="type">Int</span>, <span class="type">Iterable</span>[<span class="type">Int</span>])] = <span class="type">Array</span>((<span class="number">0</span>,<span class="type">CompactBuffer</span>(<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>)), (<span class="number">1</span>,<span class="type">CompactBuffer</span>(<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">9</span>)))</span><br></pre></td></tr></table></figure>
<p>第一个方法使用的是默认分区器，只需要传入自定义函数即可，第二个和第三个方法本质没有区别。</p>
<h6 id="2111-filter"><a class="markdownIt-Anchor" href="#2111-filter"></a> 2.1.11 <strong>filter</strong></h6>
<p>filter方法通过名称就能猜出来功能，其实就是对输入元素进行过滤，参数是一个返回值为boolean的函数，如果函数对输入元素运算结果为true，则通过该元素，否则将该元素过滤，不能进入结果集。</p>
<p>示例：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> a = sc.parallelize(<span class="number">1</span> to <span class="number">10</span>, <span class="number">3</span>)</span><br><span class="line">a: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">25</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> b = a.filter(x =&gt; x % <span class="number">2</span> == <span class="number">0</span>)</span><br><span class="line">b: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">26</span>] at filter at &lt;console&gt;:<span class="number">27</span></span><br><span class="line"></span><br><span class="line">scala&gt; b.collect</span><br><span class="line">res13: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<h6 id="2112-distinct"><a class="markdownIt-Anchor" href="#2112-distinct"></a> 2.1.12 <strong>distinct</strong></h6>
<p>将RDD中重复的元素去掉，只留下唯一的RDD元素。</p>
<p>示例：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> x = sc.parallelize(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">5</span>,<span class="number">7</span>,<span class="number">7</span>,<span class="number">9</span>,<span class="number">10</span>), <span class="number">3</span>)</span><br><span class="line">x: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">27</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; x.distinct.collect</span><br><span class="line">res14: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">3</span>, <span class="number">9</span>, <span class="number">1</span>, <span class="number">7</span>, <span class="number">10</span>, <span class="number">5</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<h6 id="2112-subtract"><a class="markdownIt-Anchor" href="#2112-subtract"></a> 2.1.12  <strong>subtract</strong></h6>
<p>subtract的含义就是求集合A-B的差，即把集合A中包含集合B的元素都删除，结果是剩下的元素。</p>
<p>示例：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> a = sc.parallelize(<span class="number">1</span> to <span class="number">9</span>, <span class="number">3</span>)</span><br><span class="line">a: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">31</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> b = sc.parallelize(<span class="number">1</span> to <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">b: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">32</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> c = a.subtract(b)</span><br><span class="line">c: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">36</span>] at subtract at &lt;console&gt;:<span class="number">27</span></span><br><span class="line"></span><br><span class="line">scala&gt; c.collect</span><br><span class="line">res15: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">6</span>, <span class="number">9</span>, <span class="number">4</span>, <span class="number">7</span>, <span class="number">5</span>, <span class="number">8</span>)</span><br></pre></td></tr></table></figure>
<h6 id="2113-persistcache"><a class="markdownIt-Anchor" href="#2113-persistcache"></a> 2.1.13 <strong>persist,cache</strong></h6>
<p>顾名思义，是缓存数据，其作用是把RDD缓存到内存中，以方便下一次计算被再次调用。</p>
<p>有所不同的是：cache只是将数据保存在内存中，而persist方法的作用是把RDD根据不同的级别进行持久化，通过使用带参数方法能指定持久化级别，如果不带参数则为默认持久化级别，即只保存到内存，与cache等价。</p>
<p>persist方法通过StorageLevel函数来设置持久化级别，如果不设置持久化级别默认为持久化级别，即只保存到内准中，与cache一样。</p>
<p>示例：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> c = sc.parallelize(<span class="type">List</span>(<span class="string">"a"</span>, <span class="string">"b"</span>, <span class="string">"c"</span>, <span class="string">"d"</span>, <span class="string">"e"</span>, <span class="string">"f"</span>),<span class="number">1</span>)</span><br><span class="line">c: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">37</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; c.cache</span><br><span class="line">res16: c.<span class="keyword">type</span> = <span class="type">ParallelCollectionRDD</span>[<span class="number">37</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line">scala&gt; c.persist(<span class="type">StorageLevel</span>.<span class="type">MEMORY_ONLY</span>)</span><br></pre></td></tr></table></figure>
<p><em>注意：c.persist(StorageLevel.MEMORY_ONLY) 在spark-shell中是无法执行的，StorageLevel是spark-core包中的对象。</em></p>
<p>Persist StorageLevel说明：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">StorageLevel</span> <span class="title">private</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    private var _useDisk: <span class="type">Boolean</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    private var _useMemory: <span class="type">Boolean</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    private var _useOffHeap: <span class="type">Boolean</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    private var _deserialized: <span class="type">Boolean</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    private var _replication: <span class="type">Int</span> = 1</span>)</span></span><br></pre></td></tr></table></figure>
<p>初始化StorageLevel可以传入5个参数，分别对应是否存入磁盘、是否存入内存、是否使用堆外内存、是否不进行序列化，副本数（默认为1）。</p>
<p>Storagelevel对应的枚举类型：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StorageLevel</span> </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> <span class="type">NONE</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">DISK_ONLY</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">DISK_ONLY_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_ONLY</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_ONLY_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_ONLY_SER</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_ONLY_SER_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_AND_DISK</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_AND_DISK_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_AND_DISK_SER</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_AND_DISK_SER_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">OFF_HEAP</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>在持久化操作中需要注意的是：如果我们希望只是保存某一时刻的RDD信息，而不是在持久化到内存或者磁盘中仍然会变化的RDD，我们可以执行checkpoint()算子，可以把RDD持久化到HDFS，同时切断RDD之间的依赖。</p>
<p>示例：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> c = sc.parallelize(<span class="type">List</span>(<span class="string">"a"</span>, <span class="string">"b"</span>, <span class="string">"c"</span>, <span class="string">"d"</span>, <span class="string">"e"</span>, <span class="string">"f"</span>),<span class="number">1</span>)</span><br><span class="line">c.checkpoint()</span><br></pre></td></tr></table></figure>
<h5 id="checkpoint-可以把rdd持久化到hdfs同时切断rdd之间的依赖"><a class="markdownIt-Anchor" href="#checkpoint-可以把rdd持久化到hdfs同时切断rdd之间的依赖"></a> checkpoint : 可以把RDD持久化到HDFS，同时切断RDD之间的依赖</h5>
<p>对于切断RDD之间的依赖的说明：<br>
当业务逻辑很复杂时，RDD之间频繁转换，RDD的血统很长，如果中间某个RDD的数据丢失，还需要重新从头计算，如果对中间某个RDD调用了checkpoint()方法，把这个RDD上传到HDFS，同时让后面的RDD不再依赖于这个RDD，而是依赖于HDFS上的数据，那么下次计算会方便很多。</p>
<p>checkpoint()执行原理：</p>
<ol>
<li>当RDD的job执行完毕后，会从finalRDD从后往前回溯</li>
<li>当回溯到调用了checkpoint()方法的RDD后，会给这个RDD做一个标记</li>
<li>Spark框架自动启动一个新的job，计算这个RDD的数据，然后把数据持久化到HDFS上</li>
<li>优化：对某个RDD执行checkpoint()之前，对该RDD执行cache()，这样的话，新启动的job只需要把内存中的数据上传到HDFS中即可，不需要重新计算。</li>
</ol>
<h6 id="2114-sample"><a class="markdownIt-Anchor" href="#2114-sample"></a> 2.1.14 <strong>sample</strong></h6>
<p>sample的作用是随机的对RDD中的元素采样，获得一个新的子集RDD，根据参数能指定是否又放回采样、子集占总数的百分比和随机种子。</p>
<p>示例：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> a = sc.parallelize(<span class="number">1</span> to <span class="number">1000</span>, <span class="number">2</span>)</span><br><span class="line">a: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">38</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; a.sample(<span class="literal">false</span>, <span class="number">0.1</span>, <span class="number">0</span>).collect</span><br><span class="line">res21: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">10</span>, <span class="number">39</span>, <span class="number">41</span>, <span class="number">53</span>, <span class="number">54</span>, <span class="number">58</span>, <span class="number">60</span>, <span class="number">80</span>, <span class="number">89</span>, <span class="number">98</span>, <span class="number">113</span>, <span class="number">114</span>, <span class="number">128</span>, <span class="number">134</span>, <span class="number">144</span>, <span class="number">161</span>, <span class="number">173</span>, <span class="number">176</span>, <span class="number">177</span>, <span class="number">198</span>, <span class="number">201</span>, <span class="number">211</span>, <span class="number">212</span>, <span class="number">213</span>, <span class="number">218</span>, <span class="number">224</span>, <span class="number">232</span>, <span class="number">235</span>, <span class="number">241</span>, <span class="number">242</span>, <span class="number">243</span>, <span class="number">246</span>, <span class="number">264</span>, <span class="number">287</span>, <span class="number">307</span>, <span class="number">319</span>, <span class="number">321</span>, <span class="number">325</span>, <span class="number">334</span>, <span class="number">339</span>, <span class="number">353</span>, <span class="number">354</span>, <span class="number">366</span>, <span class="number">367</span>, <span class="number">373</span>, <span class="number">392</span>, <span class="number">403</span>, <span class="number">407</span>, <span class="number">419</span>, <span class="number">426</span>, <span class="number">429</span>, <span class="number">434</span>, <span class="number">458</span>, <span class="number">465</span>, <span class="number">466</span>, <span class="number">478</span>, <span class="number">492</span>, <span class="number">495</span>, <span class="number">514</span>, <span class="number">522</span>, <span class="number">536</span>, <span class="number">547</span>, <span class="number">550</span>, <span class="number">557</span>, <span class="number">560</span>, <span class="number">567</span>, <span class="number">575</span>, <span class="number">595</span>, <span class="number">613</span>, <span class="number">618</span>, <span class="number">638</span>, <span class="number">639</span>, <span class="number">643</span>, <span class="number">658</span>, <span class="number">659</span>, <span class="number">660</span>, <span class="number">662</span>, <span class="number">672</span>, <span class="number">681</span>, <span class="number">682</span>, <span class="number">686</span>, <span class="number">693</span>, <span class="number">694</span>, <span class="number">696</span>, <span class="number">716</span>, <span class="number">720</span>, <span class="number">737</span>, <span class="number">763</span>, <span class="number">769</span>, <span class="number">774</span>, <span class="number">775</span>, <span class="number">791</span>, <span class="number">795</span>, <span class="number">802</span>, <span class="number">804</span>, <span class="number">812</span>, <span class="number">815</span>, <span class="number">817</span>, <span class="number">822</span>, <span class="number">877</span>, <span class="number">888</span>, <span class="number">889</span>, <span class="number">915</span>, <span class="number">942</span>, <span class="number">947</span>, <span class="number">948</span>, <span class="number">971</span>, <span class="number">972</span>, <span class="number">976</span>, <span class="number">985</span>, <span class="number">992</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; a.sample(<span class="literal">true</span>, <span class="number">0.1</span>, <span class="number">0</span>).collect</span><br><span class="line">res22: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">10</span>, <span class="number">23</span>, <span class="number">25</span>, <span class="number">35</span>, <span class="number">50</span>, <span class="number">68</span>, <span class="number">69</span>, <span class="number">79</span>, <span class="number">79</span>, <span class="number">85</span>, <span class="number">91</span>, <span class="number">91</span>, <span class="number">110</span>, <span class="number">122</span>, <span class="number">132</span>, <span class="number">133</span>, <span class="number">133</span>, <span class="number">153</span>, <span class="number">166</span>, <span class="number">171</span>, <span class="number">172</span>, <span class="number">175</span>, <span class="number">180</span>, <span class="number">198</span>, <span class="number">208</span>, <span class="number">211</span>, <span class="number">229</span>, <span class="number">233</span>, <span class="number">241</span>, <span class="number">271</span>, <span class="number">275</span>, <span class="number">287</span>, <span class="number">289</span>, <span class="number">307</span>, <span class="number">318</span>, <span class="number">322</span>, <span class="number">329</span>, <span class="number">337</span>, <span class="number">338</span>, <span class="number">339</span>, <span class="number">354</span>, <span class="number">358</span>, <span class="number">366</span>, <span class="number">369</span>, <span class="number">374</span>, <span class="number">377</span>, <span class="number">379</span>, <span class="number">388</span>, <span class="number">394</span>, <span class="number">407</span>, <span class="number">415</span>, <span class="number">420</span>, <span class="number">424</span>, <span class="number">425</span>, <span class="number">428</span>, <span class="number">436</span>, <span class="number">447</span>, <span class="number">452</span>, <span class="number">462</span>, <span class="number">492</span>, <span class="number">492</span>, <span class="number">494</span>, <span class="number">515</span>, <span class="number">526</span>, <span class="number">529</span>, <span class="number">550</span>, <span class="number">571</span>, <span class="number">587</span>, <span class="number">587</span>, <span class="number">597</span>, <span class="number">601</span>, <span class="number">603</span>, <span class="number">621</span>, <span class="number">626</span>, <span class="number">630</span>, <span class="number">638</span>, <span class="number">643</span>, <span class="number">644</span>, <span class="number">649</span>, <span class="number">650</span>, <span class="number">657</span>, <span class="number">659</span>, <span class="number">661</span>, <span class="number">670</span>, <span class="number">686</span>, <span class="number">689</span>, <span class="number">720</span>, <span class="number">722</span>, <span class="number">722</span>, <span class="number">728</span>, <span class="number">743</span>, <span class="number">748</span>, <span class="number">761</span>, <span class="number">763</span>, <span class="number">782</span>, <span class="number">787</span>, <span class="number">787</span>, <span class="number">796</span>, <span class="number">817</span>, <span class="number">820</span>, <span class="number">825</span>, <span class="number">833</span>, <span class="number">844</span>, <span class="number">846</span>, <span class="number">855</span>, <span class="number">872</span>, <span class="number">873</span>, <span class="number">881</span>, <span class="number">899</span>, <span class="number">904</span>, <span class="number">916</span>, <span class="number">935</span>, <span class="number">936</span>, <span class="number">951</span>, <span class="number">952</span>, <span class="number">967</span>, <span class="number">982</span>, <span class="number">990</span>, <span class="number">992</span>, <span class="number">993</span>)</span><br></pre></td></tr></table></figure>
<p>上述例子中sample方法第一个参数为true时使用放回抽样（<span class="exturl" data-url="aHR0cHM6Ly96aC53aWtpcGVkaWEub3JnL3dpa2kvJUU2JUIzJThBJUU2JTlEJUJFJUU1JTg4JTg2JUU0JUJEJTg4" title="https://zh.wikipedia.org/wiki/%E6%B3%8A%E6%9D%BE%E5%88%86%E4%BD%88">泊松抽样<i class="fa fa-external-link"></i></span>），为false时使用不放回抽样（<span class="exturl" data-url="aHR0cHM6Ly96aC53aWtpcGVkaWEub3JnL3dpa2kvJUU0JUJDJUFGJUU1JThBJUFBJUU1JTg4JUE5JUU1JTg4JTg2JUU1JUI4JTgz" title="https://zh.wikipedia.org/wiki/%E4%BC%AF%E5%8A%AA%E5%88%A9%E5%88%86%E5%B8%83">伯努利抽样<i class="fa fa-external-link"></i></span>），第二个参数fraction是百分比，第三个参数seed是种子，也就是随机取值的起源数字。从例子2中还看出当选择放回抽样时，取出的元素中会出现重复值。</p>
<h5 id="22-键值对型transformation算子"><a class="markdownIt-Anchor" href="#22-键值对型transformation算子"></a> 2.2 <strong>键值对型Transformation算子</strong></h5>
<p>RDD的操作算子除了单值型还有键值对（Key-Value）型。这里开始介绍键值对型的算子，主要包括groupByKey、combineByKey、reduceByKey、sortByKey、cogroup和join，如下表所示：</p>

<table>
<tbody>
<tr>
<td>
<p>方法名</p>
</td>
<td>
<p>方法定义</p>
</td>
</tr>
<tr>
<td>
<p>groupByKey</p>
</td>
<td>
<p>def groupByKey(partitioner: Partitioner): RDD[(K, Iterable[V])]</p>
</td>
</tr>
<tr>
<td>
<p>combineByKey</p>
</td>
<td>
<p>def combineByKey[C](createCombiner: V =&gt; C, mergeValue: (C,V) =&gt; C, mergeCombiners: (C, C) =&gt; C) : RDD[(K, C)]
</p>
</td>
</tr>
<tr>
<td>
<p>reduceByKey</p>
</td>
<td>
<p>def reduceByKey(partitioner: Partitioner, func: (V, V) =&gt; V): RDD[(K, V)]</p>
</td>
</tr>
<tr>
<td>
<p>sortByKey</p>
</td>
<td>
<p>def sortByKey(ascending: Boolean = true, numPartitions: Int = self.partitions.size): RDD[P]
</p>
</td>
</tr>
<tr>
<td>
<p>cogroup</p>
</td>
<td>
<p>def cogroup[W](other: RDD[(K, W)], partitioner: Partitioner): RDD</p>
</td>
</tr>
<tr>
<td>
<p>join</p>
</td>
<td>
<p>def join[W](other: RDD[(K, W)], partitioner: Partitioner): RDD[(K, (V, W))]</p>
</td>
</tr>
</tbody>
</table>

<h6 id="221-groupbykey"><a class="markdownIt-Anchor" href="#221-groupbykey"></a> 2.2.1 <strong>groupByKey</strong></h6>
<p>类似groupBy方法，作用是把每一个相同Key值的的Value聚集起来形成一个序列，可以使用默认分区器和自定义分区器，但是这个方法开销比较大，如果想对同一Key进行Value的聚合或求平均，则推荐使用aggregateByKey或者reduceByKey。</p>
<p>示例：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> a = sc.parallelize(<span class="type">List</span>(<span class="string">"mk"</span>, <span class="string">"zq"</span>, <span class="string">"xwc"</span>, <span class="string">"fjg"</span>, <span class="string">"dcp"</span>, <span class="string">"snn"</span>), <span class="number">2</span>)</span><br><span class="line">a: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">41</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> b = a.keyBy(x =&gt; x.length)</span><br><span class="line">b: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">MapPartitionsRDD</span>[<span class="number">42</span>] at keyBy at &lt;console&gt;:<span class="number">27</span></span><br><span class="line"></span><br><span class="line">scala&gt; b.groupByKey.collect</span><br><span class="line">res23: <span class="type">Array</span>[(<span class="type">Int</span>, <span class="type">Iterable</span>[<span class="type">String</span>])] = <span class="type">Array</span>((<span class="number">2</span>,<span class="type">CompactBuffer</span>(mk, zq)), (<span class="number">3</span>,<span class="type">CompactBuffer</span>(xwc, fjg, dcp, snn)))</span><br></pre></td></tr></table></figure>
<h6 id="222-combinebykey"><a class="markdownIt-Anchor" href="#222-combinebykey"></a> 2.2.2 <strong>combineByKey</strong></h6>
<p>combineByKey方法能高效的将键值对形式的RDD按相同的Key把Value合并成序列形式，用户能自定义RDD的分区器和是否在map端进行聚合操作。</p>
<p>spark新版本中函数名更新为combineByKeyWithClassTag，为了兼容两个函数同时保留都可以使用。</p>
<p>如下为combineByKey的定义：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">combineByKey</span></span>[<span class="type">C</span>](</span><br><span class="line">      createCombiner: <span class="type">V</span> =&gt; <span class="type">C</span>,</span><br><span class="line">      mergeValue: (<span class="type">C</span>, <span class="type">V</span>) =&gt; <span class="type">C</span>,</span><br><span class="line">      mergeCombiners: (<span class="type">C</span>, <span class="type">C</span>) =&gt; <span class="type">C</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">C</span>)] = self.withScope &#123;</span><br><span class="line">    combineByKeyWithClassTag(createCombiner, mergeValue, mergeCombiners)(<span class="literal">null</span>)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>解释下3个重要的函数参数：</p>
<ul>
<li>createCombiner: V =&gt; C ，这个函数把当前的值作为参数，此时我们可以对其做些附加操作(类型转换)并把它返回 (这一步类似于初始化操作)</li>
<li>mergeValue: (C, V) =&gt; C，该函数把元素V合并到之前的元素C(createCombiner)上 (这个操作在每个分区内进行)</li>
<li>mergeCombiners: (C, C) =&gt; C，该函数把2个元素C合并 (这个操作在不同分区间进行)</li>
</ul>
<p>示例：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> initialScores = <span class="type">Array</span>((<span class="string">"Fred"</span>, <span class="number">88.0</span>), (<span class="string">"Fred"</span>, <span class="number">95.0</span>), (<span class="string">"Fred"</span>, <span class="number">91.0</span>), (<span class="string">"Wilma"</span>, <span class="number">93.0</span>), (<span class="string">"Wilma"</span>, <span class="number">95.0</span>), (<span class="string">"Wilma"</span>, <span class="number">98.0</span>))</span><br><span class="line">initialScores: <span class="type">Array</span>[(<span class="type">String</span>, <span class="type">Double</span>)] = <span class="type">Array</span>((<span class="type">Fred</span>,<span class="number">88.0</span>), (<span class="type">Fred</span>,<span class="number">95.0</span>), (<span class="type">Fred</span>,<span class="number">91.0</span>), (<span class="type">Wilma</span>,<span class="number">93.0</span>), (<span class="type">Wilma</span>,<span class="number">95.0</span>), (<span class="type">Wilma</span>,<span class="number">98.0</span>))</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> d1 = sc.parallelize(initialScores)</span><br><span class="line">d1: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Double</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">52</span>] at parallelize at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="class"><span class="keyword">type</span> <span class="title">MVType</span> </span>= (<span class="type">Int</span>, <span class="type">Double</span>)</span><br><span class="line">defined <span class="class"><span class="keyword">type</span> <span class="title">alias</span> <span class="title">MVType</span></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">scala&gt;</span> <span class="title">d1</span>.<span class="title">combineByKey</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">     | score =&gt; (1, score</span>),</span></span><br><span class="line"><span class="class">     <span class="title">|</span> (<span class="params">c1: <span class="type">MVType</span>, newScore</span>) <span class="title">=&gt;</span> (<span class="params">c1._1 + 1, c1._2 + newScore</span>),</span></span><br><span class="line"><span class="class">     <span class="title">|</span> (<span class="params">c1: <span class="type">MVType</span>, c2: <span class="type">MVType</span></span>) <span class="title">=&gt;</span> (<span class="params">c1._1 + c2._1, c1._2 + c2._2</span>)</span></span><br><span class="line"><span class="class">     <span class="title">|</span> ).<span class="title">map</span> </span>&#123; <span class="keyword">case</span> (name, (num, socre)) =&gt; (name, socre / num) &#125;.collect</span><br><span class="line">res27: <span class="type">Array</span>[(<span class="type">String</span>, <span class="type">Double</span>)] = <span class="type">Array</span>((<span class="type">Fred</span>,<span class="number">91.33333333333333</span>), (<span class="type">Wilma</span>,<span class="number">95.33333333333333</span>))</span><br></pre></td></tr></table></figure>
<p>上面是combineByKey来求解平均数的例子，解释一下参数：</p>
<ul>
<li>score =&gt; (1, score)，我们把分数作为参数,并返回了附加的元组类型。 以&quot;Fred&quot;为列，当前其分数为88.0 =&gt;(1,88.0)  1表示当前科目的计数器，此时只有一个科目；</li>
<li>(c1: MVType, newScore) =&gt; (c1._1 + 1, c1._2 + newScore)，注意这里的c1就是createCombiner初始化得到的(1,88.0)。在一个分区内，我们又碰到了&quot;Fred&quot;的一个新的分数91.0。当然我们要把之前的科目分数和当前的分数加起来即c1._2 + newScore,然后把科目计算器加1即c1._1 + 1；</li>
<li>(c1: MVType, c2: MVType) =&gt; (c1._1 + c2._1, c1._2 + c2._2)，注意&quot;Fred&quot;可能是个学霸,他选修的科目可能过多而分散在不同的分区中。所有的分区都进行mergeValue后,接下来就是对分区间进行合并了,分区间科目数和科目数相加分数和分数相加就得到了总分和总科目数。</li>
</ul>
<p>######2.2.3 <strong>reduceByKey</strong></p>
<p>使用一个reduce函数来实现对相同Key的Value的聚集操作，在发送结果给reduce前会在map端的执行本地merge操作。该方法的底层实现就是调用combineByKey方法的一个重载方法。</p>
<p>示例：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> list = sc.parallelize(<span class="type">List</span>(<span class="string">"dcp"</span>, <span class="string">"fjg"</span>, <span class="string">"snn"</span>, <span class="string">"wc"</span>, <span class="string">"zq"</span>), <span class="number">2</span>)</span><br><span class="line">list: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">55</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> map = list.map(x =&gt; (x.length, x))</span><br><span class="line">map: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">MapPartitionsRDD</span>[<span class="number">56</span>] at map at &lt;console&gt;:<span class="number">27</span></span><br><span class="line"></span><br><span class="line">scala&gt; map.reduceByKey((a,b) =&gt; a + b).collect</span><br><span class="line">res28: <span class="type">Array</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">Array</span>((<span class="number">2</span>,wczq), (<span class="number">3</span>,dcpfjgsnn))</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> a = sc.parallelize(<span class="type">List</span>(<span class="number">3</span>,<span class="number">12</span>,<span class="number">124</span>,<span class="number">32</span>,<span class="number">5</span> ), <span class="number">2</span>)</span><br><span class="line">a: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">58</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> b = a.map(x =&gt; (x.toString.length, x))</span><br><span class="line">b: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">MapPartitionsRDD</span>[<span class="number">59</span>] at map at &lt;console&gt;:<span class="number">27</span></span><br><span class="line"></span><br><span class="line">scala&gt; b.reduceByKey(_ + _).collect</span><br><span class="line">res29: <span class="type">Array</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">Array</span>((<span class="number">2</span>,<span class="number">44</span>), (<span class="number">1</span>,<span class="number">8</span>), (<span class="number">3</span>,<span class="number">124</span>))</span><br></pre></td></tr></table></figure>
<p>上面示例中先用map方法映射出键值对，然后调用reduceByKey方法对相同Key的Value值进行累加,因为第一个示例是字符串，所以结果为每个key中字符串的拼接，示例2中为数字，所以结果每个key中数字之和。</p>
<h6 id="224-sortbykey"><a class="markdownIt-Anchor" href="#224-sortbykey"></a> 2.2.4 <strong>sortByKey</strong></h6>
<p>这个函数会根据Key值对键值对进行排序，如果Key是字母，则按字典顺序排序，如果Key是数字，则从小到大排序（或从大到小），该方法的第一个参数控制是否为升序排序，当为true时是升序，反之为降序。</p>
<p>示例：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> a = sc.parallelize(<span class="type">List</span>(<span class="string">"dog"</span>, <span class="string">"cat"</span>, <span class="string">"owl"</span>, <span class="string">"gnu"</span>, <span class="string">"ant"</span>), <span class="number">2</span>)</span><br><span class="line">a: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">61</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> b = sc.parallelize(<span class="number">1</span> to a.count.toInt, <span class="number">2</span>) #得到单词的字母个数</span><br><span class="line">b: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">62</span>] at parallelize at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> c = a.zip(b)</span><br><span class="line">c: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">ZippedPartitionsRDD2</span>[<span class="number">63</span>] at zip at &lt;console&gt;:<span class="number">27</span></span><br><span class="line"></span><br><span class="line">scala&gt; c.sortByKey(<span class="literal">true</span>).collect</span><br><span class="line">res30: <span class="type">Array</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">Array</span>((ant,<span class="number">5</span>), (cat,<span class="number">2</span>), (dog,<span class="number">1</span>), (gnu,<span class="number">4</span>), (owl,<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">scala&gt; c.sortByKey(<span class="literal">false</span>).collect</span><br><span class="line">res31: <span class="type">Array</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">Array</span>((owl,<span class="number">3</span>), (gnu,<span class="number">4</span>), (dog,<span class="number">1</span>), (cat,<span class="number">2</span>), (ant,<span class="number">5</span>))</span><br></pre></td></tr></table></figure>
<h6 id="225-cogroup"><a class="markdownIt-Anchor" href="#225-cogroup"></a> 2.2.5 <strong>cogroup</strong></h6>
<p>cogroup是一个比较高效的函数，能根据Key值聚集最多3个键值对的RDD，把相同Key值对应的Value聚集起来。</p>
<p>示例：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> a = sc.parallelize(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">3</span>), <span class="number">1</span>)</span><br><span class="line">a: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">78</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> b = a.map(x =&gt; (x, <span class="string">"b"</span>))</span><br><span class="line">b: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">MapPartitionsRDD</span>[<span class="number">79</span>] at map at &lt;console&gt;:<span class="number">27</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> c = a.map(y =&gt; (y, <span class="string">"c"</span>))</span><br><span class="line">c: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">MapPartitionsRDD</span>[<span class="number">80</span>] at map at &lt;console&gt;:<span class="number">27</span></span><br><span class="line"></span><br><span class="line">scala&gt; b.cogroup(c).collect</span><br><span class="line">res35: <span class="type">Array</span>[(<span class="type">Int</span>, (<span class="type">Iterable</span>[<span class="type">String</span>], <span class="type">Iterable</span>[<span class="type">String</span>]))] = <span class="type">Array</span>((<span class="number">1</span>,(<span class="type">CompactBuffer</span>(b, b),<span class="type">CompactBuffer</span>(c, c))), (<span class="number">3</span>,(<span class="type">CompactBuffer</span>(b, b),<span class="type">CompactBuffer</span>(c, c))), (<span class="number">2</span>,(<span class="type">CompactBuffer</span>(b, b),<span class="type">CompactBuffer</span>(c, c))))</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> d = a.map(m =&gt; (m, <span class="string">"x"</span>))</span><br><span class="line">d: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">MapPartitionsRDD</span>[<span class="number">83</span>] at map at &lt;console&gt;:<span class="number">25</span></span><br><span class="line"></span><br><span class="line">scala&gt; b.cogroup(c, d).collect</span><br><span class="line">res36: <span class="type">Array</span>[(<span class="type">Int</span>, (<span class="type">Iterable</span>[<span class="type">String</span>], <span class="type">Iterable</span>[<span class="type">String</span>], <span class="type">Iterable</span>[<span class="type">String</span>]))] = <span class="type">Array</span>((<span class="number">1</span>,(<span class="type">CompactBuffer</span>(b, b),<span class="type">CompactBuffer</span>(c, c),<span class="type">CompactBuffer</span>(x, x))), (<span class="number">3</span>,(<span class="type">CompactBuffer</span>(b, b),<span class="type">CompactBuffer</span>(c, c),<span class="type">CompactBuffer</span>(x, x))), (<span class="number">2</span>,(<span class="type">CompactBuffer</span>(b, b),<span class="type">CompactBuffer</span>(c, c),<span class="type">CompactBuffer</span>(x, x))))</span><br></pre></td></tr></table></figure>
<h6 id="226-join"><a class="markdownIt-Anchor" href="#226-join"></a> 2.2.6 <strong>join</strong></h6>
<p>对键值对的RDD进行cogroup操作，然后对每个新的RDD下Key的值进行笛卡尔积操作，再对返回结果使用flatMapValues方法，最后返回结果。</p>
<p>示例：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> a = sc.parallelize(<span class="type">List</span>(<span class="string">"fjg"</span>, <span class="string">"wc"</span>, <span class="string">"xwc"</span>,<span class="string">"dcp"</span>), <span class="number">2</span>)</span><br><span class="line">a: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">90</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> b = a.keyBy(_.length)<span class="comment">//得到诸如（3，"fjg"），（2，"wc"）的键值对序列</span></span><br><span class="line">b: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">MapPartitionsRDD</span>[<span class="number">91</span>] at keyBy at &lt;console&gt;:<span class="number">25</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> c = sc.parallelize(<span class="type">List</span>(<span class="string">"fjg"</span>, <span class="string">"wc"</span>, <span class="string">"snn"</span>, <span class="string">"zq"</span>, <span class="string">"xwc"</span>,<span class="string">"dcp"</span>), <span class="number">2</span>)</span><br><span class="line">c: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">92</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> d = c.keyBy(_.length)</span><br><span class="line">d: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">MapPartitionsRDD</span>[<span class="number">93</span>] at keyBy at &lt;console&gt;:<span class="number">25</span></span><br><span class="line"></span><br><span class="line">scala&gt; b.cogroup(d).collect</span><br><span class="line">res1: <span class="type">Array</span>[(<span class="type">Int</span>, (<span class="type">Iterable</span>[<span class="type">String</span>], <span class="type">Iterable</span>[<span class="type">String</span>]))] = <span class="type">Array</span>((<span class="number">2</span>,(<span class="type">CompactBuffer</span>(wc),<span class="type">CompactBuffer</span>(wc, zq))), (<span class="number">3</span>,(<span class="type">CompactBuffer</span>(fjg, xwc, dcp),<span class="type">CompactBuffer</span>(fjg, snn, xwc, dcp))))</span><br><span class="line"></span><br><span class="line">scala&gt; b.join(d).collect</span><br><span class="line">res38: <span class="type">Array</span>[(<span class="type">Int</span>, (<span class="type">String</span>, <span class="type">String</span>))] = <span class="type">Array</span>((<span class="number">2</span>,(wc,wc)), (<span class="number">2</span>,(wc,zq)), (<span class="number">3</span>,(fjg,fjg)), (<span class="number">3</span>,(fjg,snn)), (<span class="number">3</span>,(fjg,xwc)), (<span class="number">3</span>,(fjg,dcp)), (<span class="number">3</span>,(xwc,fjg)), (<span class="number">3</span>,(xwc,snn)), (<span class="number">3</span>,(xwc,xwc)), (<span class="number">3</span>,(xwc,dcp)), (<span class="number">3</span>,(dcp,fjg)), (<span class="number">3</span>,(dcp,snn)), (<span class="number">3</span>,(dcp,xwc)), (<span class="number">3</span>,(dcp,dcp)))</span><br></pre></td></tr></table></figure>
<p>上面的例子中，cogroup将两个RDD聚集起来，join的操作其实就是在cogroup的基础上做了笛卡尔积。</p>
<h5 id="23-action算子"><a class="markdownIt-Anchor" href="#23-action算子"></a> 2.3 <strong>Action算子</strong></h5>
<p>当Spark的计算模型中出现Action算子时才会执行提交作业的runJob动作，这时会触发后续的DAGScheduler和TaskScheduler工作。这里主要讲解常用的Action算子，有collect、reduce、take、top、count、takeSample、saveAsTextFile、countByKey、aggregate，具体方法和定义如下表所示：</p>
<p align="center">Action算子</p>

<table>
<tbody>
<tr>
<td>
<p>Action</p>
</td>
<td>
<p>算子作用</p>
</td>
</tr>
<tr>
<td>
<p>reduce(func)</p>
</td>
<td>
<p>令原RDD中的每个&#20540;依次经过函数func，func的类型为(T, T) =&gt; T，返回最终结果</p>
</td>
</tr>
<tr>
<td>
<p>collect()</p>
</td>
<td>
<p>将原RDD中的数据打包成数组并返回</p>
</td>
</tr>
<tr>
<td>
<p>count()</p>
</td>
<td>
<p>返回原RDD中数据的个数</p>
</td>
</tr>
<tr>
<td>
<p>first()</p>
</td>
<td>
<p>返回原RDD中的第一个数据项</p>
</td>
</tr>
<tr>
<td>
<p>take(n)</p>
</td>
<td>
<p>返回原RDD中前n个数据项，返回结果为数组</p>
</td>
</tr>
<tr>
<td>
<p>takeSample(withReplacement, num, [seed])</p>
</td>
<td>
<p>对原RDD中的数据进行采样，返回num个数据项</p>
</td>
</tr>
<tr>
<td>
<p>saveAsTextFile(path)</p>
</td>
<td>
<p>将原RDD中的数据写入到文本文件当中</p>
</td>
</tr>
<tr>
<td>
<p>saveAsSequenceFile(path)(Java and Scala)</p>
</td>
<td>
<p>将原RDD中的数据写入到序列文件当中</p>
</td>
</tr>
<tr>
<td>
<p>savaAsObjectFile(path)(Java and Scala)</p>
</td>
<td>
<p>将原RDD中的数据序列化并写入到文件当中。可以通过SparkContext.objectFile()方法加载</p>
</td>
</tr>
<tr>
<td>
<p>countByKey()</p>
</td>
<td>
<p>原RDD数据的类型为(K, V)，返回hashMap(K, Int)，用于统计K出现的次数</p>
</td>
</tr>
<tr>
<td>
<p>foreach(func)</p>
</td>
<td>
<p>对于原RDD中的每个数据执行函数func，返回数组</p>
</td>
</tr>
</tbody>
</table>

<h6 id="231-collect"><a class="markdownIt-Anchor" href="#231-collect"></a> 2.3.1 <strong>collect</strong></h6>
<p>这个我们已经很熟悉了，上面使用很多次，collect方法的作用是把RDD中的元素以数组的方式返回。</p>
<p>示例：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> c = sc.parallelize(<span class="type">List</span>(<span class="string">"a"</span>, <span class="string">"b"</span>, <span class="string">"c"</span>, <span class="string">"d"</span>, <span class="string">"e"</span>, <span class="string">"f"</span>), <span class="number">2</span>)</span><br><span class="line">c: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; c.collect</span><br><span class="line">res0: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(a, b, c, d, e, f)</span><br></pre></td></tr></table></figure>
<h6 id="232-reduce"><a class="markdownIt-Anchor" href="#232-reduce"></a> 2.3.2 <strong>reduce</strong></h6>
<p>reduce方法使用一个带两个参数的函数把元素进行聚集，返回一个元素结果，注意该函数中的二元操作应该满足交换律和结合律，这样才能在并行系统中正确计算。</p>
<p>示例：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> a = sc.parallelize(<span class="number">1</span> to <span class="number">10</span>)</span><br><span class="line">a: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">1</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; a.reduce((a,b)=&gt; a + b)</span><br><span class="line">res1: <span class="type">Int</span> = <span class="number">55</span></span><br></pre></td></tr></table></figure>
<h6 id="233-take"><a class="markdownIt-Anchor" href="#233-take"></a> 2.3.3 <strong>take</strong></h6>
<p>take方法会从RDD中取出前n[[1]](file:///C:/Users/admin/Desktop/%E5%86%99%E4%B9%A6/Spark%E4%B9%A6%E7%B1%8D%E7%AC%AC%E4%BA%8C%E6%AC%A1%E4%BF%AE%E6%94%B9%EF%BC%88%E6%9C%80%E7%BB%88%E7%A8%BF%EF%BC%892015-11-20.docx#_ftn1)个元素。方法是先扫描一个分区并后从分区中得到结果，然后评估得到的结果是否达到取出元素个数，如果没达到则继续从其他分区中扫描获取。</p>
<p>示例：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> b = sc.parallelize(<span class="type">List</span>(<span class="string">"a"</span>, <span class="string">"b"</span>, <span class="string">"c"</span>, <span class="string">"d"</span>, <span class="string">"e"</span>), <span class="number">2</span>)</span><br><span class="line">b: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">2</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; b.take(<span class="number">2</span>)</span><br><span class="line">res2: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(a, b)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> b = sc.parallelize(<span class="number">1</span> to <span class="number">100</span>, <span class="number">5</span>)</span><br><span class="line">b: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">3</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; b.take(<span class="number">30</span>)</span><br><span class="line">res3: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>, <span class="number">16</span>, <span class="number">17</span>, <span class="number">18</span>, <span class="number">19</span>, <span class="number">20</span>, <span class="number">21</span>, <span class="number">22</span>, <span class="number">23</span>, <span class="number">24</span>, <span class="number">25</span>, <span class="number">26</span>, <span class="number">27</span>, <span class="number">28</span>, <span class="number">29</span>, <span class="number">30</span>)</span><br></pre></td></tr></table></figure>
<h6 id="234-top"><a class="markdownIt-Anchor" href="#234-top"></a> 2.3.4 <strong>top</strong></h6>
<p>top方法会利用隐式排序转换方法（见实现源码中implicit修饰的方法）来获取最大的前n个元素。</p>
<p>示例：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> c = sc.parallelize(<span class="type">Array</span>(<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>,<span class="number">4</span>, <span class="number">9</span>, <span class="number">2</span>,<span class="number">11</span>,<span class="number">5</span>), <span class="number">3</span>)</span><br><span class="line">c: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">4</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; c.top(<span class="number">3</span>)</span><br><span class="line">res4: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">11</span>, <span class="number">9</span>, <span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<h6 id="235-count"><a class="markdownIt-Anchor" href="#235-count"></a> 2.3.5 <strong>count</strong></h6>
<p>count方法计算并返回RDD中元素的个数。</p>
<p>示例：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> c = sc.parallelize(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">3</span>, <span class="number">2</span>,<span class="number">4</span>, <span class="number">9</span>, <span class="number">2</span>,<span class="number">11</span>,<span class="number">5</span>), <span class="number">2</span>)</span><br><span class="line">c: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">6</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; c.count</span><br><span class="line">res5: <span class="type">Long</span> = <span class="number">8</span></span><br></pre></td></tr></table></figure>
<h6 id="236-takesample"><a class="markdownIt-Anchor" href="#236-takesample"></a> 2.3.6 <strong>takeSample</strong></h6>
<p>takeSample方法返回一个固定大小的数组形式的采样子集，此外还把返回的元素顺序随机打乱，方法的三个参数含义依次是否放回数据、返回取样的大小和随机数生成器的种子。</p>
<p>示例：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> x = sc.parallelize(<span class="number">1</span> to <span class="number">100</span>, <span class="number">2</span>)</span><br><span class="line">x: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">7</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; x.takeSample(<span class="literal">true</span>, <span class="number">30</span>, <span class="number">1</span>)</span><br><span class="line">res6: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">4</span>, <span class="number">97</span>, <span class="number">99</span>, <span class="number">95</span>, <span class="number">46</span>, <span class="number">86</span>, <span class="number">16</span>, <span class="number">88</span>, <span class="number">55</span>, <span class="number">93</span>, <span class="number">77</span>, <span class="number">10</span>, <span class="number">69</span>, <span class="number">7</span>, <span class="number">48</span>, <span class="number">70</span>, <span class="number">22</span>, <span class="number">4</span>, <span class="number">17</span>, <span class="number">49</span>, <span class="number">25</span>, <span class="number">25</span>, <span class="number">100</span>, <span class="number">86</span>, <span class="number">2</span>, <span class="number">48</span>, <span class="number">70</span>, <span class="number">54</span>, <span class="number">41</span>, <span class="number">91</span>)</span><br></pre></td></tr></table></figure>
<h6 id="237-saveastextfile"><a class="markdownIt-Anchor" href="#237-saveastextfile"></a> 2.3.7 <strong>saveAsTextFile</strong></h6>
<p>把RDD存储为文本文件，一次存一行。</p>
<p>示例：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> a = sc.parallelize(<span class="number">1</span> to <span class="number">100</span>, <span class="number">3</span>)</span><br><span class="line">a: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">9</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; a.saveAsTextFile(<span class="string">"1"</span>)</span><br></pre></td></tr></table></figure>
<h6 id="238-countbykey"><a class="markdownIt-Anchor" href="#238-countbykey"></a> 2.3.8 <strong>countByKey</strong></h6>
<p>类似count方法，不同的是countByKey方法会根据相同的Key计算其对应的Value个数，返回的是map类型的结果。</p>
<p>示例：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> a = sc.parallelize(<span class="type">List</span>((<span class="number">1</span>, <span class="string">"bit"</span>), (<span class="number">2</span>, <span class="string">"xwc"</span>), (<span class="number">2</span>, <span class="string">"fjg"</span>), (<span class="number">3</span>, <span class="string">"wc"</span>),(<span class="number">3</span>, <span class="string">"wc"</span>),(<span class="number">3</span>, <span class="string">"wc"</span>)), <span class="number">2</span>)</span><br><span class="line">a: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; a.countByKey</span><br><span class="line">res1: scala.collection.<span class="type">Map</span>[<span class="type">Int</span>,<span class="type">Long</span>] = <span class="type">Map</span>(<span class="number">2</span> -&gt; <span class="number">2</span>, <span class="number">1</span> -&gt; <span class="number">1</span>, <span class="number">3</span> -&gt; <span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<h6 id="239-aggregate"><a class="markdownIt-Anchor" href="#239-aggregate"></a> 2.3.9 <strong>aggregate</strong></h6>
<p>aggregate方法先将每个分区里面的元素进行聚合，然后用combine函数将每个分区的结果和初始值(zeroValue)进行combine操作。这个函数最终返回的类型不需要和RDD中元素类型一致。</p>
<p>aggregate有两个函数seqOp和combOp，这两个函数都是输入两个参数，输出一个参数，其中seqOp函数可以看成是reduce操作，combOp函数可以看成是第二个reduce操作（一般用于combine各分区结果到一个总体结果），由定义，combOp操作的输入和输出类型必须一致。</p>
<p>先看一下函数定义：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">aggregate</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](zeroValue: <span class="type">U</span>)(seqOp: (<span class="type">U</span>, <span class="type">T</span>) =&gt; <span class="type">U</span>, combOp: (<span class="type">U</span>, <span class="type">U</span>) =&gt; <span class="type">U</span>): <span class="type">U</span> = withScope &#123;</span><br><span class="line">    <span class="comment">// Clone the zero value since we will also be serializing it as part of tasks</span></span><br><span class="line">    <span class="keyword">var</span> jobResult = <span class="type">Utils</span>.clone(zeroValue, sc.env.serializer.newInstance())</span><br><span class="line">    <span class="keyword">val</span> cleanSeqOp = sc.clean(seqOp)</span><br><span class="line">    <span class="keyword">val</span> cleanCombOp = sc.clean(combOp)</span><br><span class="line">    <span class="keyword">val</span> aggregatePartition = (it: <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; it.aggregate(zeroValue)(cleanSeqOp, cleanCombOp)</span><br><span class="line">    <span class="keyword">val</span> mergeResult = (index: <span class="type">Int</span>, taskResult: <span class="type">U</span>) =&gt; jobResult = combOp(jobResult, taskResult)</span><br><span class="line">    sc.runJob(<span class="keyword">this</span>, aggregatePartition, mergeResult)</span><br><span class="line">    jobResult</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>seqOp函数是处理数据，combOp函数是拿到处理完成的数据做处理。aggregate相当于是传入了两个函数进去了。</p>
<p>示例：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">// 分区0的reduce操作是max(0, 2,3) = 3</span></span><br><span class="line"><span class="comment">// 分区1的reduce操作是max(0, 4,5) = 5</span></span><br><span class="line"><span class="comment">// 分区2的reduce操作是max(0, 6,7) = 7</span></span><br><span class="line"><span class="comment">// 最后的combine操作是0 + 3 + 5 + 7 = 15</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> z = sc.parallelize(<span class="type">List</span>(<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>), <span class="number">3</span>)</span><br><span class="line">z: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">3</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt;  z.aggregate(<span class="number">0</span>)((a,b) =&gt; math.max(a, b), (c,d) =&gt; c + d )</span><br><span class="line">res2: <span class="type">Int</span> = <span class="number">15</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 分区0的reduce操作是max(3, 2,3) = 3</span></span><br><span class="line"><span class="comment">// 分区1的reduce操作是max(3, 4,5) = 5</span></span><br><span class="line"><span class="comment">// 分区2的reduce操作是max(3, 6,7) = 7</span></span><br><span class="line"><span class="comment">// 最后的combine操作是3 + 3 + 5 + 7 = 18</span></span><br><span class="line">scala&gt;  z.aggregate(<span class="number">3</span>)((a,b) =&gt; math.max(a, b), (c,d) =&gt; c + d )</span><br><span class="line">res4: <span class="type">Int</span> = <span class="number">18</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> z = sc.parallelize(<span class="type">List</span>(<span class="string">"a"</span>,<span class="string">"b"</span>,<span class="string">"c"</span>,<span class="string">"d"</span>,<span class="string">"e"</span>,<span class="string">"f"</span>),<span class="number">2</span>)</span><br><span class="line">z: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">4</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt;  z.aggregate(<span class="string">""</span>)(_ + _, _+_)</span><br><span class="line">res5: <span class="type">String</span> = defabc</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> z = sc.parallelize(<span class="type">List</span>(<span class="string">"a"</span>,<span class="string">"b"</span>,<span class="string">"c"</span>,<span class="string">"d"</span>,<span class="string">"e"</span>,<span class="string">"f"</span>),<span class="number">2</span>)</span><br><span class="line">z: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">5</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt;  z.aggregate(<span class="string">"x"</span>)(_ + _, _+_)</span><br><span class="line">res6: <span class="type">String</span> = xxabcxdef</span><br></pre></td></tr></table></figure>
<h6 id="2310-fold"><a class="markdownIt-Anchor" href="#2310-fold"></a> 2.3.10 <strong>fold</strong></h6>
<p>fold方法与aggregate方法原理类似，区别就是少了一个seqOp方法。fold方法是把每个分区的元素进行聚合，然后调用reduce（op）方法处理。</p>
<p>示例：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 分区0的reduce操作是0 + 1 + 2 + 3 = 6</span></span><br><span class="line"><span class="comment">// 分区1的reduce操作是0 + 4 + 5 + 6 = 15</span></span><br><span class="line"><span class="comment">// 分区2的reduce操作是0 + 7 + 8 + 9 = 24</span></span><br><span class="line"><span class="comment">// 最后的combine操作是0 + 6 + 15 + 24 = 45</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> a = sc.parallelize(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>), <span class="number">3</span>)</span><br><span class="line">a: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">6</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line">scala&gt; a.fold(<span class="number">0</span>)(_ + _)</span><br><span class="line">res7: <span class="type">Int</span> = <span class="number">45</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 分区0的reduce操作是1 + 1 + 2 + 3 = 7</span></span><br><span class="line"><span class="comment">// 分区1的reduce操作是1 + 4 + 5 + 6 = 16</span></span><br><span class="line"><span class="comment">// 分区2的reduce操作是1 + 7 + 8 + 9 = 25</span></span><br><span class="line"><span class="comment">// 最后的combine操作是1 + 7 + 16 + 25 = 53</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> a = sc.parallelize(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>), <span class="number">3</span>)</span><br><span class="line">a: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">7</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; a.fold(<span class="number">1</span>)(_ + _)</span><br><span class="line">res8: <span class="type">Int</span> = <span class="number">49</span></span><br></pre></td></tr></table></figure>
<p>这个例子中的使用方式与aggregate方法非常相似，注意zeroValue参与所有分区计算。fold计算是保证每个分区能独立计算，它与aggregate最大的区别是aggregate对不同分区提交的最终结果定义了一个专门的comOp函数来处理，而fold方法是采用一个方法来处理aggregate的两个方法过程。</p>
<h4 id="3-共享变量"><a class="markdownIt-Anchor" href="#3-共享变量"></a> 3. <strong>共享变量</strong></h4>
<p>因为在tasks之间读写共享变量会很低效，spark提供两种类型的共享变量类型，即broadcast variables和accumulators。</p>
<h5 id="31-广播变量"><a class="markdownIt-Anchor" href="#31-广播变量"></a> 3.1 广播变量</h5>
<p>广播变量（Broadcast variables）允许用户将一个只读变量缓存到每一台机器之上，而不像传统变量一样，拷贝到每一个任务当中，同一台机器上的不同任务可以共享该变量值。如下面例子代码所示，对于变量v，只需要调用SparkContext.broadcast(v)即可得到变量v的广播变量broadcastVar，通过调用broadcastVar的value方法即可取得变量值。</p>
<p>示例：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> broadcastVar = sc.broadcast(<span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line">broadcastVar:spark.<span class="type">Broadcast</span>[<span class="type">Array</span>[<span class="type">Int</span>]] = spark.<span class="type">Broadcast</span>(b5c40191-a864<span class="number">-4</span>c7d-b9bf-d87e1a4e787c)</span><br><span class="line">scala&gt; broadcastVar.value</span><br><span class="line">res0: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<h5 id="32-累加器"><a class="markdownIt-Anchor" href="#32-累加器"></a> 3.2 <strong>累加器</strong></h5>
<p>累加器（Accumulators）是另外一种共享变量。累加器变量只能执行加法操作，但其支持并行操作，这意味着不同任务多次对累加器执行加法操作后，加法器最后的值等于所有累加的和。累加器的值只能被驱动程序访问，集群中的任务无法访问该值。</p>
<p>示例：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> accum = sc.accumulator(<span class="number">0</span>, <span class="string">"My Accumulator"</span>)</span><br><span class="line">scala&gt; accum.value（）		<span class="comment">//(通过这种方法进行读取原始变量值)</span></span><br><span class="line">accum: spark.<span class="type">Accumulator</span>[<span class="type">Int</span>] = <span class="number">0</span></span><br><span class="line">scala&gt; sc.parallelize(<span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)).foreach(x =&gt; accum += x)</span><br><span class="line">res2:<span class="type">Int</span> = <span class="number">10</span></span><br></pre></td></tr></table></figure>

      
    </div>

    

    
    
    

    <div>
      
        
<div class="my_post_copyright">
  <script src="//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js"></script>

  <!-- JS库 sweetalert 可修改路径 -->
  <script src="https://cdn.bootcss.com/jquery/2.0.0/jquery.min.js"></script>
  <script src="https://unpkg.com/sweetalert/dist/sweetalert.min.js"></script>
  <p><span>本文标题:</span><a href="/posts/e7493326.html">spark核心API开发</a></p>
  <p><span>文章作者:</span><a href="/" title="访问 Rickiyang 的个人博客">Rickiyang</a></p>
  <p><span>发布时间:</span>2018年07月23日 - 22:23</p>
  <p><span>最后更新:</span>2018年12月13日 - 10:14</p>
  <p><span>原始链接:</span><a href="/posts/e7493326.html" title="spark核心API开发">http://blog.rickiyang.cn/posts/e7493326.html</a>
    <span class="copy-path" title="点击复制文章链接"><i class="fa fa-clipboard" data-clipboard-text="http://blog.rickiyang.cn/posts/e7493326.html" aria-label="复制成功！"></i></span>
  </p>
  <p><span>许可协议:</span><i class="fa fa-creative-commons"></i> <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" title="Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)">署名-非商业性使用-禁止演绎 4.0 国际</a> 转载请保留原文链接及作者。</p>
</div>
<script>
    var clipboard = new Clipboard('.fa-clipboard');
    $(".fa-clipboard").click(function(){
      clipboard.on('success', function(){
        swal({
          title: "",
          text: '复制成功',
          icon: "success",
          showConfirmButton: true
          });
    });
    });
</script>


      
    </div>
    
    <div>
      
        <div>
    
        <div style="text-align:center;color: #636363;font-size:14px;letter-spacing: 10px">本文结束啦<i class="fa fa-bell"></i>感谢您的阅读</div>
    
</div>

      
    </div>


    
      
    
    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/spark学习/" rel="tag"># spark学习</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/posts/d422b66e.html" rel="next" title="RDD的设计与运行原理">
                <i class="fa fa-chevron-left"></i> RDD的设计与运行原理
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/posts/64ecab81.html" rel="prev" title="Paxos算法原理">
                Paxos算法原理 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  
    <div class="comments" id="comments">
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/adele.png" alt="Rickiyang">
            
              <p class="site-author-name" itemprop="name">Rickiyang</p>
              <p class="site-description motion-element" itemprop="description">分享对coding生活的见解</p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives">
                
                    <span class="site-state-item-count">101</span>
                    <span class="site-state-item-name">总文章</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  <a href="/categories/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">19</span>
                    <span class="site-state-item-name">分类</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">11</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3JpY2tpeWFuZw==" title="GitHub &rarr; https://github.com/rickiyang"><i class="fa fa-fw fa-github"></i>GitHub</span>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <span class="exturl" data-url="bWFpbHRvOm1lZGVlMTMyMEBnbWFpbC5jb20=" title="E-Mail &rarr; mailto:medee1320@gmail.com"><i class="fa fa-fw fa-envelope"></i>E-Mail</span>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2E5NTM3MTM0Mjg=" title="CSDN &rarr; https://blog.csdn.net/a953713428"><i class="fa fa-fw fa-google"></i>CSDN</span>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="/953713428" title="QQ &rarr; 953713428"><i class="fa fa-fw fa-twitter"></i>QQ</a>
                </span>
              
            </div>
          

          

          
          

          
            
          
          

        </div>
      </div>

      
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-spark-context"><span class="nav-number">1.</span> <span class="nav-text"> 1. Spark Context</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#11-sparkcontext的作用"><span class="nav-number">1.1.</span> <span class="nav-text"> 1.1 SparkContext的作用</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-rdd算子"><span class="nav-number">2.</span> <span class="nav-text"> 2. RDD算子</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#21-单值型tranformation算子"><span class="nav-number">2.1.</span> <span class="nav-text"> 2.1 单值型Tranformation算子</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#211-map"><span class="nav-number">2.1.1.</span> <span class="nav-text"> 2.1.1 map</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#212-flatmap"><span class="nav-number">2.1.2.</span> <span class="nav-text"> 2.1.2  flatMap</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#213-mappartitions"><span class="nav-number">2.1.3.</span> <span class="nav-text"> 2.1.3 mapPartitions</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#214-mappartitionwithindex"><span class="nav-number">2.1.4.</span> <span class="nav-text"> 2.1.4 mapPartitionWithIndex</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#215-foreach"><span class="nav-number">2.1.5.</span> <span class="nav-text"> 2.1.5 foreach</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#216-foreachpartition"><span class="nav-number">2.1.6.</span> <span class="nav-text"> 2.1.6 foreachPartition</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#217-glom"><span class="nav-number">2.1.7.</span> <span class="nav-text"> 2.1.7 glom</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#218-union"><span class="nav-number">2.1.8.</span> <span class="nav-text"> 2.1.8 union</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#219-cartesian"><span class="nav-number">2.1.9.</span> <span class="nav-text"> 2.1.9 cartesian</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#2110-groupby"><span class="nav-number">2.1.10.</span> <span class="nav-text"> 2.1.10 groupBy</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#2111-filter"><span class="nav-number">2.1.11.</span> <span class="nav-text"> 2.1.11 filter</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#2112-distinct"><span class="nav-number">2.1.12.</span> <span class="nav-text"> 2.1.12 distinct</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#2112-subtract"><span class="nav-number">2.1.13.</span> <span class="nav-text"> 2.1.12  subtract</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#2113-persistcache"><span class="nav-number">2.1.14.</span> <span class="nav-text"> 2.1.13 persist,cache</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#checkpoint-可以把rdd持久化到hdfs同时切断rdd之间的依赖"><span class="nav-number">2.2.</span> <span class="nav-text"> checkpoint : 可以把RDD持久化到HDFS，同时切断RDD之间的依赖</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#2114-sample"><span class="nav-number">2.2.1.</span> <span class="nav-text"> 2.1.14 sample</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#22-键值对型transformation算子"><span class="nav-number">2.3.</span> <span class="nav-text"> 2.2 键值对型Transformation算子</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#221-groupbykey"><span class="nav-number">2.3.1.</span> <span class="nav-text"> 2.2.1 groupByKey</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#222-combinebykey"><span class="nav-number">2.3.2.</span> <span class="nav-text"> 2.2.2 combineByKey</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#224-sortbykey"><span class="nav-number">2.3.3.</span> <span class="nav-text"> 2.2.4 sortByKey</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#225-cogroup"><span class="nav-number">2.3.4.</span> <span class="nav-text"> 2.2.5 cogroup</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#226-join"><span class="nav-number">2.3.5.</span> <span class="nav-text"> 2.2.6 join</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#23-action算子"><span class="nav-number">2.4.</span> <span class="nav-text"> 2.3 Action算子</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#231-collect"><span class="nav-number">2.4.1.</span> <span class="nav-text"> 2.3.1 collect</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#232-reduce"><span class="nav-number">2.4.2.</span> <span class="nav-text"> 2.3.2 reduce</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#233-take"><span class="nav-number">2.4.3.</span> <span class="nav-text"> 2.3.3 take</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#234-top"><span class="nav-number">2.4.4.</span> <span class="nav-text"> 2.3.4 top</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#235-count"><span class="nav-number">2.4.5.</span> <span class="nav-text"> 2.3.5 count</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#236-takesample"><span class="nav-number">2.4.6.</span> <span class="nav-text"> 2.3.6 takeSample</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#237-saveastextfile"><span class="nav-number">2.4.7.</span> <span class="nav-text"> 2.3.7 saveAsTextFile</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#238-countbykey"><span class="nav-number">2.4.8.</span> <span class="nav-text"> 2.3.8 countByKey</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#239-aggregate"><span class="nav-number">2.4.9.</span> <span class="nav-text"> 2.3.9 aggregate</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#2310-fold"><span class="nav-number">2.4.10.</span> <span class="nav-text"> 2.3.10 fold</span></a></li></ol></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-共享变量"><span class="nav-number">3.</span> <span class="nav-text"> 3. 共享变量</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#31-广播变量"><span class="nav-number">3.1.</span> <span class="nav-text"> 3.1 广播变量</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#32-累加器"><span class="nav-number">3.2.</span> <span class="nav-text"> 3.2 累加器</span></a></li></ol></li></ol></div>
            

          </div>
        </div>
      <!--/noindex-->
      

      
        <div class="back-to-top">
          <i class="fa fa-arrow-up"></i>
          
            <span id="scrollpercent"><span>0</span>%</span>
          
        </div>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2017 – <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Rickiyang</span>

  

  
</div>

<!--<div class="powered-by">
<i class="fa fa-user-md"></i><span id="busuanzi_container_site_uv">
  本站访客数:<span id="busuanzi_value_site_uv"></span>
</span>
</div>-->





<!--
  <div class="theme-info">Theme – <span class="exturl theme-link" data-url="aHR0cHM6Ly90aGVtZS1uZXh0Lm9yZw==">NexT.Gemini</span> v6.7.0</div>
-->



<div class="theme-info">
  <div class="powered-by"></div>
  <span class="post-count">博客全站共237k字</span>
</div>

<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https'){
   bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
  }
  else{
  bp.src = 'http://push.zhanzhang.baidu.com/push.js';
  }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv" title="Total Visitors">
      <i class="fa fa-user"></i>
      网站总访客数 <span class="busuanzi-value" id="busuanzi_value_site_uv"></span> 人次. |
    </span>
  

  
    <span class="site-pv" title="Total Views">
      &nbsp;<i class="fa fa-eye"></i>
      总访问量 <span class="busuanzi-value" id="busuanzi_value_site_pv"></span> 次.
    </span>









        
      </div>
    </div></footer>

    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>




















  
  



  
  
  <script id="ribbon" size="300" alpha="0.6" zindex="-1" src="/lib/canvas-ribbon/canvas-ribbon.js"></script>





  
  <script src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script src="/lib/three/three.min.js"></script>

  
  <script src="/lib/three/canvas_sphere.min.js"></script>


  


  <script src="/js/src/utils.js?v=6.7.0"></script>

  <script src="/js/src/motion.js?v=6.7.0"></script>



  
  


  <script src="/js/src/affix.js?v=6.7.0"></script>

  <script src="/js/src/schemes/pisces.js?v=6.7.0"></script>



  
  <script src="/js/src/scrollspy.js?v=6.7.0"></script>
<script src="/js/src/post-details.js?v=6.7.0"></script>



  


  <script src="/js/src/bootstrap.js?v=6.7.0"></script>



  



  








  
  
  
  
  <script src="//cdn1.lncld.net/static/js/3.11.1/av-min.js"></script>
  <script src="//unpkg.com/valine/dist/Valine.min.js"></script>
  <script>
    var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(function (item) {
      return GUEST.indexOf(item) > -1;
    });
    new Valine({
      el: '#comments' ,
      verify: false,
      notify: false,
      appId: 'X5fy0b9Gknu0SAse5areilGR-gzGzoHsz',
      appKey: 'URk4cyPQ8Ttqtwhop5t26kok',
      placeholder: 'Just go go',
      avatar: 'mm',
      meta:guest,
      pageSize: '10' || 10,
      visitor: false
    });
  </script>




  

  <script>
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url).replace(/\/{2,}/g, '/');
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x"></i></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x"></i></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  
  <script>
    
    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();

      Counter('get', '/classes/Counter', { where: JSON.stringify({ url }) })
        .done(function ({ results }) {
          if (results.length > 0) {
            var counter = results[0];
            
            Counter('put', `/classes/Counter/${counter.objectId}`, JSON.stringify({ time: { "__op":"Increment", "amount":1 } }))
            
            .done(function () {
                  var $element = $(document.getElementById(url));
                  $element.find('.leancloud-visitors-count').text(counter.time + 1);
            })
            
            .fail(function ({ responseJSON }) {
                console.log('Failed to save Visitor num, with error message: ' + responseJSON.error);
            })
          } else {
            
              Counter('post', '/classes/Counter', JSON.stringify({ title: title, url: url, time: 1}))
                .done(function () {
                  var $element = $(document.getElementById(url));
                  $element.find('.leancloud-visitors-count').text(1);
                })
                .fail(function () {
                  console.log('Failed to create');
                });
            
          }
        })
      .fail(function ({ responseJSON }) {
        console.log('LeanCloud Counter Error:' + responseJSON.code + " " + responseJSON.error);
      });
    }
    

    $(function() {
      $.get('https://app-router.leancloud.cn/2/route?appId=' + "X5fy0b9Gknu0SAse5areilGR-gzGzoHsz")
        .done(function ({ api_server }) {
          var Counter = function (method, url, data) {
            return $.ajax({
              method: method,
              url: `https://${api_server}/1.1${url}`,
              headers: {
                'X-LC-Id': "X5fy0b9Gknu0SAse5areilGR-gzGzoHsz",
                'X-LC-Key': "URk4cyPQ8Ttqtwhop5t26kok",
                'Content-Type': 'application/json',
              },
              data: data,
            });
          };
          
          addCount(Counter);
          
        })
    });
  </script>



  

  
  <script>
    (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
      }
      else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
      }
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script>


  

  

  
  
  
  <script src="/lib/needsharebutton/needsharebutton.js"></script>
  <script>
    
    
  </script>


  

  

  

  
  <script src="/js/src/exturl.js?v=6.7.0"></script>


  

  

  


  <!-- 页面点击小红心 -->
    <script type="text/javascript" src="/js/src/love.js"></script>
    <!---->
</body>
</html>
